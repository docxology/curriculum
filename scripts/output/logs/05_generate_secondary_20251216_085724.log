2025-12-16 08:57:24,750 - root - INFO - Logging to file: /Users/4d/Documents/GitHub/curriculum/scripts/output/logs/05_generate_secondary_20251216_085724.log
2025-12-16 08:57:24,750 - generate_secondary - INFO - 
2025-12-16 08:57:24,750 - generate_secondary - INFO - ğŸ”¬ STAGE 05: SECONDARY MATERIALS (Session-Level Synthesis)
2025-12-16 08:57:24,750 - generate_secondary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 08:57:24,751 - generate_secondary - INFO - Generating materials PER SESSION (with full session context)
2025-12-16 08:57:24,751 - generate_secondary - INFO - Reading all content from: [course-specific]/modules/module_XX/session_YY/
2025-12-16 08:57:24,751 - generate_secondary - INFO - Output structure: [course-specific]/modules/module_XX/session_YY/[type].md
2025-12-16 08:57:24,751 - generate_secondary - INFO - 
2025-12-16 08:57:24,751 - generate_secondary - INFO - SECONDARY TYPES GENERATED PER SESSION:
2025-12-16 08:57:24,751 - generate_secondary - INFO -   1. application.md - Real-world applications and case studies
2025-12-16 08:57:24,751 - generate_secondary - INFO -   2. extension.md - Advanced topics beyond core curriculum
2025-12-16 08:57:24,751 - generate_secondary - INFO -   3. visualization.mmd - Additional diagrams and concept maps (Mermaid format)
2025-12-16 08:57:24,751 - generate_secondary - INFO -   4. integration.md - Cross-module connections and synthesis
2025-12-16 08:57:24,751 - generate_secondary - INFO -   5. investigation.md - Research questions and experiments
2025-12-16 08:57:24,751 - generate_secondary - INFO -   6. open_questions.md - Current scientific debates and frontiers
2025-12-16 08:57:24,751 - generate_secondary - INFO - 
2025-12-16 08:57:24,751 - generate_secondary - INFO - 
2025-12-16 08:57:24,751 - generate_secondary - INFO - âš™ï¸ CONFIGURATION
2025-12-16 08:57:24,751 - generate_secondary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 08:57:24,751 - generate_secondary - INFO -   â€¢ Content Validation: DISABLED
2025-12-16 08:57:24,751 - generate_secondary - INFO -   â€¢ Dry Run: DISABLED
2025-12-16 08:57:24,751 - generate_secondary - INFO -   â€¢ Log File: output/logs/05_generate_secondary_20251216_085724.log
2025-12-16 08:57:24,751 - generate_secondary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 08:57:24,751 - src.config.loader - INFO - Initialized ConfigLoader with directory: /Users/4d/Documents/GitHub/curriculum/config
2025-12-16 08:57:24,752 - src.config.loader - INFO - Course configuration validated successfully
2025-12-16 08:57:24,767 - src.config.loader - INFO - All configurations validated successfully
2025-12-16 08:57:24,767 - generate_secondary - INFO - Using most recent outline from output/outlines/ or scripts/output/outlines/
2025-12-16 08:57:24,768 - src.config.loader - INFO - Found most recent outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/active_inference_college/outlines/course_outline_20251216_083317.json
2025-12-16 08:57:24,768 - src.config.loader - INFO - Found most recent outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/active_inference_college/outlines/course_outline_20251216_083317.json
2025-12-16 08:57:24,769 - src.config.loader - INFO - Loaded 15 modules from outline: course_outline_20251216_083317.json
2025-12-16 08:57:24,769 - generate_secondary - INFO - Using course-specific output directory: output/active_inference_college/
2025-12-16 08:57:24,769 - generate_secondary - INFO - Processing ALL modules
2025-12-16 08:57:24,769 - generate_secondary - INFO - Processing 15 modules (15 total sessions)
2025-12-16 08:57:24,769 - generate_secondary - INFO - Secondary types: application, extension, visualization, integration, investigation, open_questions
2025-12-16 08:57:24,769 - src.llm.client - INFO - Initialized OllamaClient: model=gemma3:4b, url=http://localhost:11434/api/generate
2025-12-16 08:57:24,770 - generate_secondary - INFO - 
============================================================
2025-12-16 08:57:24,770 - generate_secondary - INFO - [1/15] Module 1: Introduction to Bayesian Statistics (1 sessions)
2025-12-16 08:57:24,770 - generate_secondary - INFO - ============================================================
2025-12-16 08:57:24,770 - generate_secondary - INFO - 
  Session 1/15: Probability Basics
2025-12-16 08:57:24,772 - generate_secondary - INFO - Generating application for session 1: Probability Basics...
2025-12-16 08:57:24,772 - src.llm.client - INFO - [app:4d463e] ğŸš€ app | m=gemma3:4b | p=31902c | t=150s
2025-12-16 08:57:24,772 - src.llm.client - INFO - [app:4d463e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:57:24,772 - src.llm.client - INFO - [app:4d463e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:57:24,779 - src.llm.client - INFO - [app:4d463e] Sending request to Ollama: model=gemma3:4b, operation=application, payload=33864 bytes, prompt=31902 chars
2025-12-16 08:57:24,779 - src.llm.client - INFO - [app:4d463e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:57:29,511 - src.llm.request_handler - INFO - [app:4d463e] âœ“ Done 4.73s
2025-12-16 08:57:29,511 - src.llm.client - INFO - [app:4d463e] âœ… HTTP 200 in 4.73s
2025-12-16 08:57:29,511 - src.llm.client - INFO - [app:4d463e] ğŸ“¡ Stream active (200)
2025-12-16 08:57:29,511 - src.llm.client - INFO - [app:4d463e] Starting stream parsing, waiting for first chunk...
2025-12-16 08:57:31,514 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 2.0s: 785c @392c/s (130ch, ~196t @98t/s)
2025-12-16 08:57:33,516 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 4.0s: 1700c @424c/s (264ch, ~425t @106t/s)
2025-12-16 08:57:35,526 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 6.0s: 2556c @425c/s (402ch, ~639t @106t/s)
2025-12-16 08:57:37,534 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 8.0s: 3370c @420c/s (531ch, ~842t @105t/s)
2025-12-16 08:57:39,546 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 10.0s: 4103c @409c/s (654ch, ~1026t @102t/s)
2025-12-16 08:57:41,551 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 12.0s: 4838c @402c/s (782ch, ~1210t @100t/s)
2025-12-16 08:57:43,555 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 14.0s: 5617c @400c/s (909ch, ~1404t @100t/s)
2025-12-16 08:57:45,799 - src.llm.client - INFO - [app:4d463e] ğŸ“Š 16.3s: 6315c @388c/s (1031ch, ~1579t @97t/s)
2025-12-16 08:57:45,800 - src.llm.client - INFO - [app:4d463e] âœ“ Done 21.03s: 6315c (~858w @300c/s)
2025-12-16 08:57:45,805 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 08:57:45,806 - generate_secondary - INFO - [COMPLIANT] Application generated âœ“
2025-12-16 08:57:45,806 - generate_secondary - INFO -     - Length: 6304 chars, 856 words
2025-12-16 08:57:45,806 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 08:57:45,806 - generate_secondary - INFO -     - Applications: 5
2025-12-16 08:57:45,806 - generate_secondary - INFO -     - Avg words per application: 166
2025-12-16 08:57:45,806 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_01_introduction_to_bayesian_statistics/session_01/application.md
2025-12-16 08:57:45,806 - generate_secondary - INFO - Generating extension for session 1: Probability Basics...
2025-12-16 08:57:45,806 - src.llm.client - INFO - [ext:2dca7c] ğŸš€ ext | m=gemma3:4b | p=25125c | t=120s
2025-12-16 08:57:45,806 - src.llm.client - INFO - [ext:2dca7c] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:57:45,807 - src.llm.client - INFO - [ext:2dca7c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:57:45,808 - src.llm.client - INFO - [ext:2dca7c] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=29942 bytes, prompt=25125 chars
2025-12-16 08:57:45,808 - src.llm.client - INFO - [ext:2dca7c] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:57:50,660 - src.llm.request_handler - INFO - [ext:2dca7c] âœ“ Done 4.85s
2025-12-16 08:57:50,660 - src.llm.client - INFO - [ext:2dca7c] âœ… HTTP 200 in 4.85s
2025-12-16 08:57:50,660 - src.llm.client - INFO - [ext:2dca7c] ğŸ“¡ Stream active (200)
2025-12-16 08:57:50,660 - src.llm.client - INFO - [ext:2dca7c] Starting stream parsing, waiting for first chunk...
2025-12-16 08:57:52,671 - src.llm.client - INFO - [ext:2dca7c] ğŸ“Š 2.0s: 821c @408c/s (133ch, ~205t @102t/s)
2025-12-16 08:57:54,679 - src.llm.client - INFO - [ext:2dca7c] ğŸ“Š 4.0s: 1650c @411c/s (264ch, ~412t @103t/s)
2025-12-16 08:57:56,687 - src.llm.client - INFO - [ext:2dca7c] ğŸ“Š 6.0s: 2428c @403c/s (399ch, ~607t @101t/s)
2025-12-16 08:57:58,689 - src.llm.client - INFO - [ext:2dca7c] ğŸ“Š 8.0s: 3302c @411c/s (534ch, ~826t @103t/s)
2025-12-16 08:57:59,181 - src.llm.client - INFO - [ext:2dca7c] âœ“ Done 13.37s: 3417c (~449w @255c/s)
2025-12-16 08:57:59,184 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:57:59,185 - generate_secondary - INFO - [COMPLIANT] Extension generated âœ“
2025-12-16 08:57:59,185 - generate_secondary - INFO -     - Length: 3416 chars, 449 words
2025-12-16 08:57:59,185 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 08:57:59,185 - generate_secondary - INFO -     - Topics: 3
2025-12-16 08:57:59,185 - generate_secondary - INFO -     - Avg words per topic: 140
2025-12-16 08:57:59,186 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_01_introduction_to_bayesian_statistics/session_01/extension.md
2025-12-16 08:57:59,186 - generate_secondary - INFO - Generating visualization for session 1: Probability Basics...
2025-12-16 08:57:59,186 - src.llm.client - INFO - [viz:f714e4] ğŸš€ viz | m=gemma3:4b | p=24085c | t=120s
2025-12-16 08:57:59,187 - src.llm.client - INFO - [viz:f714e4] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:57:59,187 - src.llm.client - INFO - [viz:f714e4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:57:59,195 - src.llm.client - INFO - [viz:f714e4] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=28224 bytes, prompt=24085 chars
2025-12-16 08:57:59,195 - src.llm.client - INFO - [viz:f714e4] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:58:04,135 - src.llm.request_handler - INFO - [viz:f714e4] âœ“ Done 4.94s
2025-12-16 08:58:04,135 - src.llm.client - INFO - [viz:f714e4] âœ… HTTP 200 in 4.94s
2025-12-16 08:58:04,136 - src.llm.client - INFO - [viz:f714e4] ğŸ“¡ Stream active (200)
2025-12-16 08:58:04,136 - src.llm.client - INFO - [viz:f714e4] Starting stream parsing, waiting for first chunk...
2025-12-16 08:58:06,142 - src.llm.client - INFO - [viz:f714e4] ğŸ“Š 2.0s: 507c @253c/s (133ch, ~127t @63t/s)
2025-12-16 08:58:07,961 - src.llm.client - INFO - [viz:f714e4] âœ“ Done 8.78s: 944c (~136w @108c/s)
2025-12-16 08:58:07,962 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:58:07,962 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 08:58:07,962 - generate_secondary - INFO -     - Length: 194 chars (cleaned: 194 chars)
2025-12-16 08:58:07,962 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:58:07,962 - generate_secondary - INFO - [CRITICAL] Elements: 12 total (nodes: 6, connections: 6) ğŸ”´
2025-12-16 08:58:07,962 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 08:58:07,962 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-16 08:58:07,962 - generate_secondary - WARNING - [WARNING] Only 6 nodes found (require at least 10, need 4 more - add more nodes to the diagram) âš ï¸
2025-12-16 08:58:07,962 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 08:58:07,962 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 08:58:07,963 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_01_introduction_to_bayesian_statistics/session_01/visualization.mmd
2025-12-16 08:58:07,963 - generate_secondary - INFO - Generating integration for session 1: Probability Basics...
2025-12-16 08:58:07,963 - src.llm.client - INFO - [int:43b564] ğŸš€ int | m=gemma3:4b | p=25434c | t=150s
2025-12-16 08:58:07,963 - src.llm.client - INFO - [int:43b564] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:58:07,963 - src.llm.client - INFO - [int:43b564] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:58:07,964 - src.llm.client - INFO - [int:43b564] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=30590 bytes, prompt=25434 chars
2025-12-16 08:58:07,964 - src.llm.client - INFO - [int:43b564] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:58:12,688 - src.llm.request_handler - INFO - [int:43b564] âœ“ Done 4.72s
2025-12-16 08:58:12,689 - src.llm.client - INFO - [int:43b564] âœ… HTTP 200 in 4.72s
2025-12-16 08:58:12,689 - src.llm.client - INFO - [int:43b564] ğŸ“¡ Stream active (200)
2025-12-16 08:58:12,689 - src.llm.client - INFO - [int:43b564] Starting stream parsing, waiting for first chunk...
2025-12-16 08:58:14,700 - src.llm.client - INFO - [int:43b564] ğŸ“Š 2.0s: 761c @378c/s (135ch, ~190t @95t/s)
2025-12-16 08:58:16,706 - src.llm.client - INFO - [int:43b564] ğŸ“Š 4.0s: 1544c @384c/s (270ch, ~386t @96t/s)
2025-12-16 08:58:18,718 - src.llm.client - INFO - [int:43b564] ğŸ“Š 6.0s: 2351c @390c/s (406ch, ~588t @97t/s)
2025-12-16 08:58:20,730 - src.llm.client - INFO - [int:43b564] ğŸ“Š 8.0s: 3119c @388c/s (542ch, ~780t @97t/s)
2025-12-16 08:58:22,743 - src.llm.client - INFO - [int:43b564] ğŸ“Š 10.1s: 3866c @385c/s (678ch, ~966t @96t/s)
2025-12-16 08:58:24,941 - src.llm.client - INFO - [int:43b564] ğŸ“Š 12.3s: 4496c @367c/s (811ch, ~1124t @92t/s)
2025-12-16 08:58:24,942 - src.llm.client - INFO - [int:43b564] âœ“ Done 16.98s: 4496c (~631w @265c/s)
2025-12-16 08:58:24,944 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:58:24,945 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 08:58:24,945 - generate_secondary - INFO -     - Length: 4495 chars, 631 words
2025-12-16 08:58:24,945 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 08:58:24,945 - generate_secondary - INFO -     - Connections: 14
2025-12-16 08:58:24,945 - generate_secondary - INFO -     - Structure: 1 sections
2025-12-16 08:58:24,945 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_01_introduction_to_bayesian_statistics/session_01/integration.md
2025-12-16 08:58:24,945 - generate_secondary - INFO - Generating investigation for session 1: Probability Basics...
2025-12-16 08:58:24,945 - src.llm.client - INFO - [inv:428219] ğŸš€ inv | m=gemma3:4b | p=24347c | t=150s
2025-12-16 08:58:24,945 - src.llm.client - INFO - [inv:428219] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:58:24,945 - src.llm.client - INFO - [inv:428219] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:58:24,947 - src.llm.client - INFO - [inv:428219] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=28446 bytes, prompt=24347 chars
2025-12-16 08:58:24,947 - src.llm.client - INFO - [inv:428219] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:58:29,558 - src.llm.request_handler - INFO - [inv:428219] âœ“ Done 4.61s
2025-12-16 08:58:29,559 - src.llm.client - INFO - [inv:428219] âœ… HTTP 200 in 4.61s
2025-12-16 08:58:29,559 - src.llm.client - INFO - [inv:428219] ğŸ“¡ Stream active (200)
2025-12-16 08:58:29,559 - src.llm.client - INFO - [inv:428219] Starting stream parsing, waiting for first chunk...
2025-12-16 08:58:31,567 - src.llm.client - INFO - [inv:428219] ğŸ“Š 2.0s: 716c @357c/s (138ch, ~179t @89t/s)
2025-12-16 08:58:33,570 - src.llm.client - INFO - [inv:428219] ğŸ“Š 4.0s: 1492c @372c/s (271ch, ~373t @93t/s)
2025-12-16 08:58:35,579 - src.llm.client - INFO - [inv:428219] ğŸ“Š 6.0s: 2208c @367c/s (404ch, ~552t @92t/s)
2025-12-16 08:58:37,584 - src.llm.client - INFO - [inv:428219] ğŸ“Š 8.0s: 3045c @379c/s (538ch, ~761t @95t/s)
2025-12-16 08:58:39,594 - src.llm.client - INFO - [inv:428219] ğŸ“Š 10.0s: 3823c @381c/s (672ch, ~956t @95t/s)
2025-12-16 08:58:41,602 - src.llm.client - INFO - [inv:428219] ğŸ“Š 12.0s: 4567c @379c/s (808ch, ~1142t @95t/s)
2025-12-16 08:58:42,805 - src.llm.client - INFO - [inv:428219] âœ“ Done 17.86s: 5030c (~694w @282c/s)
2025-12-16 08:58:42,807 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:58:42,808 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 08:58:42,808 - generate_secondary - INFO -     - Length: 5025 chars, 694 words
2025-12-16 08:58:42,808 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 08:58:42,808 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 08:58:42,808 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 08:58:42,808 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_01_introduction_to_bayesian_statistics/session_01/investigation.md
2025-12-16 08:58:42,808 - generate_secondary - INFO - Generating open_questions for session 1: Probability Basics...
2025-12-16 08:58:42,808 - src.llm.client - INFO - [opq:4a2442] ğŸš€ opq | m=gemma3:4b | p=24433c | t=150s
2025-12-16 08:58:42,808 - src.llm.client - INFO - [opq:4a2442] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:58:42,808 - src.llm.client - INFO - [opq:4a2442] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:58:42,810 - src.llm.client - INFO - [opq:4a2442] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=28543 bytes, prompt=24433 chars
2025-12-16 08:58:42,810 - src.llm.client - INFO - [opq:4a2442] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:58:47,526 - src.llm.request_handler - INFO - [opq:4a2442] âœ“ Done 4.72s
2025-12-16 08:58:47,527 - src.llm.client - INFO - [opq:4a2442] âœ… HTTP 200 in 4.72s
2025-12-16 08:58:47,527 - src.llm.client - INFO - [opq:4a2442] ğŸ“¡ Stream active (200)
2025-12-16 08:58:47,527 - src.llm.client - INFO - [opq:4a2442] Starting stream parsing, waiting for first chunk...
2025-12-16 08:58:49,534 - src.llm.client - INFO - [opq:4a2442] ğŸ“Š 2.0s: 771c @384c/s (136ch, ~193t @96t/s)
2025-12-16 08:58:51,539 - src.llm.client - INFO - [opq:4a2442] ğŸ“Š 4.0s: 1684c @420c/s (272ch, ~421t @105t/s)
2025-12-16 08:58:53,541 - src.llm.client - INFO - [opq:4a2442] ğŸ“Š 6.0s: 2408c @400c/s (408ch, ~602t @100t/s)
2025-12-16 08:58:53,830 - src.llm.client - INFO - [opq:4a2442] âœ“ Done 11.02s: 2452c (~322w @222c/s)
2025-12-16 08:58:53,831 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:58:53,832 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 08:58:53,832 - generate_secondary - INFO -     - Length: 2452 chars, 322 words
2025-12-16 08:58:53,832 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 08:58:53,832 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 08:58:53,832 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 08:58:53,832 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_01_introduction_to_bayesian_statistics/session_01/open_questions.md
2025-12-16 08:58:53,832 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 08:58:53,833 - generate_secondary - INFO - 
============================================================
2025-12-16 08:58:53,833 - generate_secondary - INFO - [2/15] Module 2: Conditional Probability & Bayesâ€™ Theorem (1 sessions)
2025-12-16 08:58:53,833 - generate_secondary - INFO - ============================================================
2025-12-16 08:58:53,833 - generate_secondary - INFO - 
  Session 2/15: Conditional Probability
2025-12-16 08:58:53,834 - generate_secondary - INFO - Generating application for session 2: Conditional Probability...
2025-12-16 08:58:53,834 - src.llm.client - INFO - [app:e4b756] ğŸš€ app | m=gemma3:4b | p=30477c | t=150s
2025-12-16 08:58:53,834 - src.llm.client - INFO - [app:e4b756] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:58:53,834 - src.llm.client - INFO - [app:e4b756] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:58:53,836 - src.llm.client - INFO - [app:e4b756] Sending request to Ollama: model=gemma3:4b, operation=application, payload=32490 bytes, prompt=30477 chars
2025-12-16 08:58:53,836 - src.llm.client - INFO - [app:e4b756] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:58:58,669 - src.llm.request_handler - INFO - [app:e4b756] âœ“ Done 4.83s
2025-12-16 08:58:58,670 - src.llm.client - INFO - [app:e4b756] âœ… HTTP 200 in 4.83s
2025-12-16 08:58:58,670 - src.llm.client - INFO - [app:e4b756] ğŸ“¡ Stream active (200)
2025-12-16 08:58:58,670 - src.llm.client - INFO - [app:e4b756] Starting stream parsing, waiting for first chunk...
2025-12-16 08:59:00,676 - src.llm.client - INFO - [app:e4b756] ğŸ“Š 2.0s: 832c @415c/s (132ch, ~208t @104t/s)
2025-12-16 08:59:02,677 - src.llm.client - INFO - [app:e4b756] ğŸ“Š 4.0s: 1722c @430c/s (267ch, ~430t @107t/s)
2025-12-16 08:59:04,680 - src.llm.client - INFO - [app:e4b756] ğŸ“Š 6.0s: 2609c @434c/s (401ch, ~652t @109t/s)
2025-12-16 08:59:06,689 - src.llm.client - INFO - [app:e4b756] ğŸ“Š 8.0s: 3463c @432c/s (533ch, ~866t @108t/s)
2025-12-16 08:59:08,700 - src.llm.client - INFO - [app:e4b756] ğŸ“Š 10.0s: 4288c @428c/s (667ch, ~1072t @107t/s)
2025-12-16 08:59:10,701 - src.llm.client - INFO - [app:e4b756] ğŸ“Š 12.0s: 5064c @421c/s (792ch, ~1266t @105t/s)
2025-12-16 08:59:11,706 - src.llm.client - INFO - [app:e4b756] âœ“ Done 17.87s: 5373c (~692w @301c/s)
2025-12-16 08:59:11,708 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 08:59:11,708 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 08:59:11,708 - generate_secondary - INFO -     - Length: 5361 chars, 690 words
2025-12-16 08:59:11,708 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 08:59:11,708 - generate_secondary - INFO -     - Applications: 5
2025-12-16 08:59:11,708 - generate_secondary - INFO -     - Avg words per application: 133
2025-12-16 08:59:11,708 - generate_secondary - WARNING - [WARNING] Application 2 has 143 words (require 150-200, need 7 more words) âš ï¸
2025-12-16 08:59:11,708 - generate_secondary - WARNING - [WARNING] Application 3 has 126 words (require 150-200, need 24 more words) âš ï¸
2025-12-16 08:59:11,708 - generate_secondary - WARNING - [WARNING] Application 4 has 123 words (require 150-200, need 27 more words) âš ï¸
2025-12-16 08:59:11,708 - generate_secondary - WARNING - [WARNING] Application 5 has 112 words (require 150-200, need 38 more words) âš ï¸
2025-12-16 08:59:11,709 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_02_conditional_probability_bayes_theorem/session_02/application.md
2025-12-16 08:59:11,709 - generate_secondary - INFO - Generating extension for session 2: Conditional Probability...
2025-12-16 08:59:11,709 - src.llm.client - INFO - [ext:81d5fc] ğŸš€ ext | m=gemma3:4b | p=23700c | t=120s
2025-12-16 08:59:11,709 - src.llm.client - INFO - [ext:81d5fc] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:59:11,709 - src.llm.client - INFO - [ext:81d5fc] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:59:11,711 - src.llm.client - INFO - [ext:81d5fc] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=28568 bytes, prompt=23700 chars
2025-12-16 08:59:11,711 - src.llm.client - INFO - [ext:81d5fc] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:59:16,537 - src.llm.request_handler - INFO - [ext:81d5fc] âœ“ Done 4.83s
2025-12-16 08:59:16,538 - src.llm.client - INFO - [ext:81d5fc] âœ… HTTP 200 in 4.83s
2025-12-16 08:59:16,538 - src.llm.client - INFO - [ext:81d5fc] ğŸ“¡ Stream active (200)
2025-12-16 08:59:16,538 - src.llm.client - INFO - [ext:81d5fc] Starting stream parsing, waiting for first chunk...
2025-12-16 08:59:18,552 - src.llm.client - INFO - [ext:81d5fc] ğŸ“Š 2.0s: 841c @418c/s (135ch, ~210t @104t/s)
2025-12-16 08:59:20,557 - src.llm.client - INFO - [ext:81d5fc] ğŸ“Š 4.0s: 1598c @398c/s (267ch, ~400t @99t/s)
2025-12-16 08:59:22,561 - src.llm.client - INFO - [ext:81d5fc] ğŸ“Š 6.0s: 2405c @399c/s (399ch, ~601t @100t/s)
2025-12-16 08:59:24,572 - src.llm.client - INFO - [ext:81d5fc] ğŸ“Š 8.0s: 3188c @397c/s (530ch, ~797t @99t/s)
2025-12-16 08:59:26,586 - src.llm.client - INFO - [ext:81d5fc] ğŸ“Š 10.0s: 4017c @400c/s (662ch, ~1004t @100t/s)
2025-12-16 08:59:27,295 - src.llm.client - INFO - [ext:81d5fc] âœ“ Done 15.59s: 4179c (~548w @268c/s)
2025-12-16 08:59:27,296 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:59:27,297 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 08:59:27,297 - generate_secondary - INFO -     - Length: 4178 chars, 548 words
2025-12-16 08:59:27,297 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 08:59:27,297 - generate_secondary - INFO -     - Topics: 3
2025-12-16 08:59:27,297 - generate_secondary - INFO -     - Avg words per topic: 176
2025-12-16 08:59:27,297 - generate_secondary - WARNING - [WARNING] Topic 1 has 165 words (exceeds 150 by 15 words - consider condensing) âš ï¸
2025-12-16 08:59:27,297 - generate_secondary - WARNING - [WARNING] Topic 2 has 155 words (exceeds 150 by 5 words - consider condensing) âš ï¸
2025-12-16 08:59:27,297 - generate_secondary - WARNING - [WARNING] Topic 3 has 207 words (exceeds 150 by 57 words - consider condensing) âš ï¸
2025-12-16 08:59:27,297 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_02_conditional_probability_bayes_theorem/session_02/extension.md
2025-12-16 08:59:27,297 - generate_secondary - INFO - Generating visualization for session 2: Conditional Probability...
2025-12-16 08:59:27,297 - src.llm.client - INFO - [viz:0afc65] ğŸš€ viz | m=gemma3:4b | p=22660c | t=120s
2025-12-16 08:59:27,297 - src.llm.client - INFO - [viz:0afc65] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:59:27,297 - src.llm.client - INFO - [viz:0afc65] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:59:27,299 - src.llm.client - INFO - [viz:0afc65] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=26850 bytes, prompt=22660 chars
2025-12-16 08:59:27,299 - src.llm.client - INFO - [viz:0afc65] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:59:32,087 - src.llm.request_handler - INFO - [viz:0afc65] âœ“ Done 4.79s
2025-12-16 08:59:32,088 - src.llm.client - INFO - [viz:0afc65] âœ… HTTP 200 in 4.79s
2025-12-16 08:59:32,089 - src.llm.client - INFO - [viz:0afc65] ğŸ“¡ Stream active (200)
2025-12-16 08:59:32,089 - src.llm.client - INFO - [viz:0afc65] Starting stream parsing, waiting for first chunk...
2025-12-16 08:59:34,097 - src.llm.client - INFO - [viz:0afc65] ğŸ“Š 2.0s: 494c @246c/s (134ch, ~124t @61t/s)
2025-12-16 08:59:36,098 - src.llm.client - INFO - [viz:0afc65] ğŸ“Š 4.0s: 916c @228c/s (268ch, ~229t @57t/s)
2025-12-16 08:59:37,535 - src.llm.client - INFO - [viz:0afc65] âœ“ Done 10.24s: 1297c (~202w @127c/s)
2025-12-16 08:59:37,536 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-16 08:59:37,536 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:59:37,536 - generate_secondary - INFO -     - Length: 714 chars (cleaned: 714 chars)
2025-12-16 08:59:37,536 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:59:37,536 - generate_secondary - INFO - [OK] Elements: 48 total (nodes: 16, connections: 32) âœ“
2025-12-16 08:59:37,536 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_02_conditional_probability_bayes_theorem/session_02/visualization.mmd
2025-12-16 08:59:37,536 - generate_secondary - INFO - Generating integration for session 2: Conditional Probability...
2025-12-16 08:59:37,536 - src.llm.client - INFO - [int:7764d7] ğŸš€ int | m=gemma3:4b | p=24009c | t=150s
2025-12-16 08:59:37,537 - src.llm.client - INFO - [int:7764d7] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:59:37,537 - src.llm.client - INFO - [int:7764d7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:59:37,538 - src.llm.client - INFO - [int:7764d7] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=29216 bytes, prompt=24009 chars
2025-12-16 08:59:37,538 - src.llm.client - INFO - [int:7764d7] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:59:42,288 - src.llm.request_handler - INFO - [int:7764d7] âœ“ Done 4.75s
2025-12-16 08:59:42,288 - src.llm.client - INFO - [int:7764d7] âœ… HTTP 200 in 4.75s
2025-12-16 08:59:42,288 - src.llm.client - INFO - [int:7764d7] ğŸ“¡ Stream active (200)
2025-12-16 08:59:42,288 - src.llm.client - INFO - [int:7764d7] Starting stream parsing, waiting for first chunk...
2025-12-16 08:59:44,300 - src.llm.client - INFO - [int:7764d7] ğŸ“Š 2.0s: 815c @405c/s (135ch, ~204t @101t/s)
2025-12-16 08:59:46,304 - src.llm.client - INFO - [int:7764d7] ğŸ“Š 4.0s: 1639c @408c/s (269ch, ~410t @102t/s)
2025-12-16 08:59:48,306 - src.llm.client - INFO - [int:7764d7] ğŸ“Š 6.0s: 2352c @391c/s (403ch, ~588t @98t/s)
2025-12-16 08:59:50,078 - src.llm.client - INFO - [int:7764d7] âœ“ Done 12.54s: 2779c (~385w @222c/s)
2025-12-16 08:59:50,080 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 08:59:50,080 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 08:59:50,080 - generate_secondary - INFO -     - Length: 2766 chars, 383 words
2025-12-16 08:59:50,080 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 08:59:50,080 - generate_secondary - INFO -     - Connections: 13
2025-12-16 08:59:50,080 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 08:59:50,080 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_02_conditional_probability_bayes_theorem/session_02/integration.md
2025-12-16 08:59:50,080 - generate_secondary - INFO - Generating investigation for session 2: Conditional Probability...
2025-12-16 08:59:50,080 - src.llm.client - INFO - [inv:17346f] ğŸš€ inv | m=gemma3:4b | p=22922c | t=150s
2025-12-16 08:59:50,080 - src.llm.client - INFO - [inv:17346f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:59:50,080 - src.llm.client - INFO - [inv:17346f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:59:50,082 - src.llm.client - INFO - [inv:17346f] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=27072 bytes, prompt=22922 chars
2025-12-16 08:59:50,082 - src.llm.client - INFO - [inv:17346f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:59:54,834 - src.llm.request_handler - INFO - [inv:17346f] âœ“ Done 4.75s
2025-12-16 08:59:54,834 - src.llm.client - INFO - [inv:17346f] âœ… HTTP 200 in 4.75s
2025-12-16 08:59:54,834 - src.llm.client - INFO - [inv:17346f] ğŸ“¡ Stream active (200)
2025-12-16 08:59:54,835 - src.llm.client - INFO - [inv:17346f] Starting stream parsing, waiting for first chunk...
2025-12-16 08:59:56,848 - src.llm.client - INFO - [inv:17346f] ğŸ“Š 2.0s: 694c @345c/s (135ch, ~174t @86t/s)
2025-12-16 08:59:58,852 - src.llm.client - INFO - [inv:17346f] ğŸ“Š 4.0s: 1437c @358c/s (269ch, ~359t @89t/s)
2025-12-16 09:00:00,860 - src.llm.client - INFO - [inv:17346f] ğŸ“Š 6.0s: 2188c @363c/s (404ch, ~547t @91t/s)
2025-12-16 09:00:02,861 - src.llm.client - INFO - [inv:17346f] ğŸ“Š 8.0s: 2973c @370c/s (539ch, ~743t @93t/s)
2025-12-16 09:00:04,873 - src.llm.client - INFO - [inv:17346f] ğŸ“Š 10.0s: 3755c @374c/s (674ch, ~939t @94t/s)
2025-12-16 09:00:06,447 - src.llm.client - INFO - [inv:17346f] âœ“ Done 16.37s: 4124c (~546w @252c/s)
2025-12-16 09:00:06,449 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:00:06,449 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:00:06,449 - generate_secondary - INFO -     - Length: 4119 chars, 546 words
2025-12-16 09:00:06,449 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:00:06,449 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:00:06,449 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:00:06,449 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_02_conditional_probability_bayes_theorem/session_02/investigation.md
2025-12-16 09:00:06,449 - generate_secondary - INFO - Generating open_questions for session 2: Conditional Probability...
2025-12-16 09:00:06,449 - src.llm.client - INFO - [opq:66380c] ğŸš€ opq | m=gemma3:4b | p=23008c | t=150s
2025-12-16 09:00:06,449 - src.llm.client - INFO - [opq:66380c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:00:06,449 - src.llm.client - INFO - [opq:66380c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:00:06,451 - src.llm.client - INFO - [opq:66380c] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=27169 bytes, prompt=23008 chars
2025-12-16 09:00:06,451 - src.llm.client - INFO - [opq:66380c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:00:11,197 - src.llm.request_handler - INFO - [opq:66380c] âœ“ Done 4.75s
2025-12-16 09:00:11,199 - src.llm.client - INFO - [opq:66380c] âœ… HTTP 200 in 4.75s
2025-12-16 09:00:11,199 - src.llm.client - INFO - [opq:66380c] ğŸ“¡ Stream active (200)
2025-12-16 09:00:11,199 - src.llm.client - INFO - [opq:66380c] Starting stream parsing, waiting for first chunk...
2025-12-16 09:00:13,200 - src.llm.client - INFO - [opq:66380c] ğŸ“Š 2.0s: 831c @415c/s (135ch, ~208t @104t/s)
2025-12-16 09:00:15,214 - src.llm.client - INFO - [opq:66380c] ğŸ“Š 4.0s: 1619c @403c/s (271ch, ~405t @101t/s)
2025-12-16 09:00:16,626 - src.llm.client - INFO - [opq:66380c] âœ“ Done 10.18s: 2139c (~277w @210c/s)
2025-12-16 09:00:16,628 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:00:16,628 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:00:16,628 - generate_secondary - INFO -     - Length: 2138 chars, 277 words
2025-12-16 09:00:16,628 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:00:16,628 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:00:16,628 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:00:16,628 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_02_conditional_probability_bayes_theorem/session_02/open_questions.md
2025-12-16 09:00:16,628 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:00:16,628 - generate_secondary - INFO - 
============================================================
2025-12-16 09:00:16,628 - generate_secondary - INFO - [3/15] Module 3: Bayesâ€™ Theorem â€“ Derivation & Intuition (1 sessions)
2025-12-16 09:00:16,628 - generate_secondary - INFO - ============================================================
2025-12-16 09:00:16,628 - generate_secondary - INFO - 
  Session 3/15: Bayes' Theorem Derivation
2025-12-16 09:00:16,630 - generate_secondary - INFO - Generating application for session 3: Bayes' Theorem Derivation...
2025-12-16 09:00:16,630 - src.llm.client - INFO - [app:35fde0] ğŸš€ app | m=gemma3:4b | p=29826c | t=150s
2025-12-16 09:00:16,630 - src.llm.client - INFO - [app:35fde0] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:00:16,630 - src.llm.client - INFO - [app:35fde0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:00:16,631 - src.llm.client - INFO - [app:35fde0] Sending request to Ollama: model=gemma3:4b, operation=application, payload=32154 bytes, prompt=29826 chars
2025-12-16 09:00:16,631 - src.llm.client - INFO - [app:35fde0] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:00:21,445 - src.llm.request_handler - INFO - [app:35fde0] âœ“ Done 4.81s
2025-12-16 09:00:21,445 - src.llm.client - INFO - [app:35fde0] âœ… HTTP 200 in 4.81s
2025-12-16 09:00:21,445 - src.llm.client - INFO - [app:35fde0] ğŸ“¡ Stream active (200)
2025-12-16 09:00:21,446 - src.llm.client - INFO - [app:35fde0] Starting stream parsing, waiting for first chunk...
2025-12-16 09:00:23,457 - src.llm.client - INFO - [app:35fde0] ğŸ“Š 2.0s: 800c @398c/s (132ch, ~200t @99t/s)
2025-12-16 09:00:25,466 - src.llm.client - INFO - [app:35fde0] ğŸ“Š 4.0s: 1687c @420c/s (263ch, ~422t @105t/s)
2025-12-16 09:00:27,475 - src.llm.client - INFO - [app:35fde0] ğŸ“Š 6.0s: 2532c @420c/s (396ch, ~633t @105t/s)
2025-12-16 09:00:29,488 - src.llm.client - INFO - [app:35fde0] ğŸ“Š 8.0s: 3284c @408c/s (529ch, ~821t @102t/s)
2025-12-16 09:00:31,494 - src.llm.client - INFO - [app:35fde0] ğŸ“Š 10.0s: 4114c @409c/s (660ch, ~1028t @102t/s)
2025-12-16 09:00:33,509 - src.llm.client - INFO - [app:35fde0] ğŸ“Š 12.1s: 4953c @411c/s (793ch, ~1238t @103t/s)
2025-12-16 09:00:35,736 - src.llm.client - INFO - [app:35fde0] ğŸ“Š 14.3s: 5711c @400c/s (915ch, ~1428t @100t/s)
2025-12-16 09:00:35,737 - src.llm.client - INFO - [app:35fde0] âœ“ Done 19.11s: 5711c (~757w @299c/s)
2025-12-16 09:00:35,739 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:00:35,739 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:00:35,739 - generate_secondary - INFO -     - Length: 5711 chars, 757 words
2025-12-16 09:00:35,739 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:00:35,739 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:00:35,739 - generate_secondary - INFO -     - Avg words per application: 145
2025-12-16 09:00:35,739 - generate_secondary - WARNING - [WARNING] Application 1 has 146 words (require 150-200, need 4 more words) âš ï¸
2025-12-16 09:00:35,739 - generate_secondary - WARNING - [WARNING] Application 4 has 135 words (require 150-200, need 15 more words) âš ï¸
2025-12-16 09:00:35,739 - generate_secondary - WARNING - [WARNING] Application 5 has 132 words (require 150-200, need 18 more words) âš ï¸
2025-12-16 09:00:35,739 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_03_bayes_theorem_derivation_intuition/session_03/application.md
2025-12-16 09:00:35,739 - generate_secondary - INFO - Generating extension for session 3: Bayes' Theorem Derivation...
2025-12-16 09:00:35,740 - src.llm.client - INFO - [ext:e42ced] ğŸš€ ext | m=gemma3:4b | p=23049c | t=120s
2025-12-16 09:00:35,740 - src.llm.client - INFO - [ext:e42ced] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:00:35,740 - src.llm.client - INFO - [ext:e42ced] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:00:35,741 - src.llm.client - INFO - [ext:e42ced] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=28232 bytes, prompt=23049 chars
2025-12-16 09:00:35,741 - src.llm.client - INFO - [ext:e42ced] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:00:40,595 - src.llm.request_handler - INFO - [ext:e42ced] âœ“ Done 4.85s
2025-12-16 09:00:40,596 - src.llm.client - INFO - [ext:e42ced] âœ… HTTP 200 in 4.85s
2025-12-16 09:00:40,596 - src.llm.client - INFO - [ext:e42ced] ğŸ“¡ Stream active (200)
2025-12-16 09:00:40,596 - src.llm.client - INFO - [ext:e42ced] Starting stream parsing, waiting for first chunk...
2025-12-16 09:00:42,598 - src.llm.client - INFO - [ext:e42ced] ğŸ“Š 2.0s: 846c @423c/s (132ch, ~212t @106t/s)
2025-12-16 09:00:44,600 - src.llm.client - INFO - [ext:e42ced] ğŸ“Š 4.0s: 1671c @417c/s (267ch, ~418t @104t/s)
2025-12-16 09:00:46,613 - src.llm.client - INFO - [ext:e42ced] ğŸ“Š 6.0s: 2443c @406c/s (402ch, ~611t @101t/s)
2025-12-16 09:00:48,626 - src.llm.client - INFO - [ext:e42ced] ğŸ“Š 8.0s: 3281c @409c/s (537ch, ~820t @102t/s)
2025-12-16 09:00:49,940 - src.llm.client - INFO - [ext:e42ced] âœ“ Done 14.20s: 3734c (~495w @263c/s)
2025-12-16 09:00:49,941 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:00:49,941 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:00:49,941 - generate_secondary - INFO -     - Length: 3733 chars, 495 words
2025-12-16 09:00:49,941 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:00:49,941 - generate_secondary - INFO -     - Topics: 3
2025-12-16 09:00:49,941 - generate_secondary - INFO -     - Avg words per topic: 158
2025-12-16 09:00:49,941 - generate_secondary - WARNING - [WARNING] Topic 2 has 161 words (exceeds 150 by 11 words - consider condensing) âš ï¸
2025-12-16 09:00:49,941 - generate_secondary - WARNING - [WARNING] Topic 3 has 166 words (exceeds 150 by 16 words - consider condensing) âš ï¸
2025-12-16 09:00:49,942 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_03_bayes_theorem_derivation_intuition/session_03/extension.md
2025-12-16 09:00:49,942 - generate_secondary - INFO - Generating visualization for session 3: Bayes' Theorem Derivation...
2025-12-16 09:00:49,942 - src.llm.client - INFO - [viz:b5e62d] ğŸš€ viz | m=gemma3:4b | p=22009c | t=120s
2025-12-16 09:00:49,942 - src.llm.client - INFO - [viz:b5e62d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:00:49,942 - src.llm.client - INFO - [viz:b5e62d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:00:49,943 - src.llm.client - INFO - [viz:b5e62d] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=26514 bytes, prompt=22009 chars
2025-12-16 09:00:49,943 - src.llm.client - INFO - [viz:b5e62d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:00:54,691 - src.llm.request_handler - INFO - [viz:b5e62d] âœ“ Done 4.75s
2025-12-16 09:00:54,691 - src.llm.client - INFO - [viz:b5e62d] âœ… HTTP 200 in 4.75s
2025-12-16 09:00:54,691 - src.llm.client - INFO - [viz:b5e62d] ğŸ“¡ Stream active (200)
2025-12-16 09:00:54,691 - src.llm.client - INFO - [viz:b5e62d] Starting stream parsing, waiting for first chunk...
2025-12-16 09:00:56,694 - src.llm.client - INFO - [viz:b5e62d] ğŸ“Š 2.0s: 535c @267c/s (131ch, ~134t @67t/s)
2025-12-16 09:00:58,699 - src.llm.client - INFO - [viz:b5e62d] ğŸ“Š 4.0s: 1083c @270c/s (264ch, ~271t @68t/s)
2025-12-16 09:01:00,699 - src.llm.client - INFO - [viz:b5e62d] ğŸ“Š 6.0s: 1529c @255c/s (397ch, ~382t @64t/s)
2025-12-16 09:01:02,711 - src.llm.client - INFO - [viz:b5e62d] ğŸ“Š 8.0s: 2130c @266c/s (531ch, ~532t @66t/s)
2025-12-16 09:01:04,719 - src.llm.client - INFO - [viz:b5e62d] ğŸ“Š 10.0s: 2798c @279c/s (665ch, ~700t @70t/s)
2025-12-16 09:01:05,155 - src.llm.client - INFO - [viz:b5e62d] âœ“ Done 15.21s: 2878c (~426w @189c/s)
2025-12-16 09:01:05,156 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-16 09:01:05,156 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 09:01:05,156 - generate_secondary - INFO -     - Length: 258 chars (cleaned: 258 chars)
2025-12-16 09:01:05,156 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:01:05,156 - generate_secondary - INFO - [CRITICAL] Elements: 4 total (nodes: 4, connections: 0) ğŸ”´
2025-12-16 09:01:05,156 - generate_secondary - WARNING -     - Mermaid syntax warnings: 2 issues fixed (code fences, style commands)
2025-12-16 09:01:05,156 - generate_secondary - WARNING -     - Critical issues: 5 structural problems requiring attention
2025-12-16 09:01:05,156 - generate_secondary - WARNING - [WARNING] Missing diagram type declaration (graph/flowchart/etc.) - add 'graph TD' or 'graph LR' at the start âš ï¸
2025-12-16 09:01:05,156 - generate_secondary - WARNING - [WARNING] Only 6 nodes found (require at least 10, need 4 more - add more nodes to the diagram) âš ï¸
2025-12-16 09:01:05,156 - generate_secondary - WARNING - [WARNING] Only 4 diagram elements found (require at least 6, need 2 more - add nodes and connections) âš ï¸
2025-12-16 09:01:05,156 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:01:05,156 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:01:05,156 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_03_bayes_theorem_derivation_intuition/session_03/visualization.mmd
2025-12-16 09:01:05,156 - generate_secondary - INFO - Generating integration for session 3: Bayes' Theorem Derivation...
2025-12-16 09:01:05,156 - src.llm.client - INFO - [int:63955d] ğŸš€ int | m=gemma3:4b | p=23358c | t=150s
2025-12-16 09:01:05,156 - src.llm.client - INFO - [int:63955d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:01:05,156 - src.llm.client - INFO - [int:63955d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:01:05,158 - src.llm.client - INFO - [int:63955d] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=28880 bytes, prompt=23358 chars
2025-12-16 09:01:05,158 - src.llm.client - INFO - [int:63955d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:01:09,924 - src.llm.request_handler - INFO - [int:63955d] âœ“ Done 4.77s
2025-12-16 09:01:09,924 - src.llm.client - INFO - [int:63955d] âœ… HTTP 200 in 4.77s
2025-12-16 09:01:09,924 - src.llm.client - INFO - [int:63955d] ğŸ“¡ Stream active (200)
2025-12-16 09:01:09,927 - src.llm.client - INFO - [int:63955d] Starting stream parsing, waiting for first chunk...
2025-12-16 09:01:11,938 - src.llm.client - INFO - [int:63955d] ğŸ“Š 2.0s: 752c @374c/s (134ch, ~188t @94t/s)
2025-12-16 09:01:13,938 - src.llm.client - INFO - [int:63955d] ğŸ“Š 4.0s: 1546c @385c/s (267ch, ~386t @96t/s)
2025-12-16 09:01:15,948 - src.llm.client - INFO - [int:63955d] ğŸ“Š 6.0s: 2379c @395c/s (401ch, ~595t @99t/s)
2025-12-16 09:01:17,962 - src.llm.client - INFO - [int:63955d] ğŸ“Š 8.0s: 3147c @392c/s (535ch, ~787t @98t/s)
2025-12-16 09:01:19,962 - src.llm.client - INFO - [int:63955d] ğŸ“Š 10.0s: 3960c @395c/s (669ch, ~990t @99t/s)
2025-12-16 09:01:21,963 - src.llm.client - INFO - [int:63955d] ğŸ“Š 12.0s: 4698c @390c/s (803ch, ~1174t @98t/s)
2025-12-16 09:01:23,975 - src.llm.client - INFO - [int:63955d] ğŸ“Š 14.0s: 5146c @366c/s (937ch, ~1286t @92t/s)
2025-12-16 09:01:24,736 - src.llm.client - INFO - [int:63955d] âœ“ Done 19.58s: 5261c (~722w @269c/s)
2025-12-16 09:01:24,738 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:01:24,739 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:01:24,739 - generate_secondary - INFO -     - Length: 5237 chars, 720 words
2025-12-16 09:01:24,739 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:01:24,739 - generate_secondary - INFO -     - Connections: 41
2025-12-16 09:01:24,739 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:01:24,739 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_03_bayes_theorem_derivation_intuition/session_03/integration.md
2025-12-16 09:01:24,739 - generate_secondary - INFO - Generating investigation for session 3: Bayes' Theorem Derivation...
2025-12-16 09:01:24,739 - src.llm.client - INFO - [inv:b7e5b9] ğŸš€ inv | m=gemma3:4b | p=22271c | t=150s
2025-12-16 09:01:24,739 - src.llm.client - INFO - [inv:b7e5b9] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:01:24,739 - src.llm.client - INFO - [inv:b7e5b9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:01:24,741 - src.llm.client - INFO - [inv:b7e5b9] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=26736 bytes, prompt=22271 chars
2025-12-16 09:01:24,741 - src.llm.client - INFO - [inv:b7e5b9] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:01:29,531 - src.llm.request_handler - INFO - [inv:b7e5b9] âœ“ Done 4.79s
2025-12-16 09:01:29,533 - src.llm.client - INFO - [inv:b7e5b9] âœ… HTTP 200 in 4.79s
2025-12-16 09:01:29,533 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“¡ Stream active (200)
2025-12-16 09:01:29,533 - src.llm.client - INFO - [inv:b7e5b9] Starting stream parsing, waiting for first chunk...
2025-12-16 09:01:31,538 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 2.0s: 690c @344c/s (135ch, ~172t @86t/s)
2025-12-16 09:01:33,542 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 4.0s: 1360c @339c/s (270ch, ~340t @85t/s)
2025-12-16 09:01:35,556 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 6.0s: 2048c @340c/s (404ch, ~512t @85t/s)
2025-12-16 09:01:37,557 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 8.0s: 2662c @332c/s (537ch, ~666t @83t/s)
2025-12-16 09:01:39,564 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 10.0s: 3398c @339c/s (671ch, ~850t @85t/s)
2025-12-16 09:01:41,577 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 12.0s: 4190c @348c/s (805ch, ~1048t @87t/s)
2025-12-16 09:01:43,586 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 14.1s: 4883c @347c/s (940ch, ~1221t @87t/s)
2025-12-16 09:01:45,600 - src.llm.client - INFO - [inv:b7e5b9] ğŸ“Š 16.1s: 5699c @355c/s (1074ch, ~1425t @89t/s)
2025-12-16 09:01:46,614 - src.llm.client - INFO - [inv:b7e5b9] âœ“ Done 21.87s: 5962c (~868w @273c/s)
2025-12-16 09:01:46,616 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:01:46,616 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:01:46,616 - generate_secondary - INFO -     - Length: 5948 chars, 866 words
2025-12-16 09:01:46,616 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:01:46,616 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:01:46,616 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:01:46,616 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_03_bayes_theorem_derivation_intuition/session_03/investigation.md
2025-12-16 09:01:46,616 - generate_secondary - INFO - Generating open_questions for session 3: Bayes' Theorem Derivation...
2025-12-16 09:01:46,616 - src.llm.client - INFO - [opq:3c4cf1] ğŸš€ opq | m=gemma3:4b | p=22357c | t=150s
2025-12-16 09:01:46,617 - src.llm.client - INFO - [opq:3c4cf1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:01:46,617 - src.llm.client - INFO - [opq:3c4cf1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:01:46,618 - src.llm.client - INFO - [opq:3c4cf1] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=26833 bytes, prompt=22357 chars
2025-12-16 09:01:46,618 - src.llm.client - INFO - [opq:3c4cf1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:01:51,402 - src.llm.request_handler - INFO - [opq:3c4cf1] âœ“ Done 4.78s
2025-12-16 09:01:51,403 - src.llm.client - INFO - [opq:3c4cf1] âœ… HTTP 200 in 4.78s
2025-12-16 09:01:51,403 - src.llm.client - INFO - [opq:3c4cf1] ğŸ“¡ Stream active (200)
2025-12-16 09:01:51,403 - src.llm.client - INFO - [opq:3c4cf1] Starting stream parsing, waiting for first chunk...
2025-12-16 09:01:53,408 - src.llm.client - INFO - [opq:3c4cf1] ğŸ“Š 2.0s: 690c @345c/s (117ch, ~172t @86t/s)
2025-12-16 09:01:55,420 - src.llm.client - INFO - [opq:3c4cf1] ğŸ“Š 4.0s: 1427c @355c/s (244ch, ~357t @89t/s)
2025-12-16 09:01:57,399 - src.llm.client - INFO - [opq:3c4cf1] âœ“ Done 10.78s: 2054c (~272w @190c/s)
2025-12-16 09:01:57,400 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:01:57,400 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:01:57,401 - generate_secondary - INFO -     - Length: 2053 chars, 272 words
2025-12-16 09:01:57,401 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:01:57,401 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:01:57,401 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:01:57,401 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_03_bayes_theorem_derivation_intuition/session_03/open_questions.md
2025-12-16 09:01:57,401 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:01:57,401 - generate_secondary - INFO - 
============================================================
2025-12-16 09:01:57,401 - generate_secondary - INFO - [4/15] Module 4: Bayesian Inference â€“ Model Specification (1 sessions)
2025-12-16 09:01:57,401 - generate_secondary - INFO - ============================================================
2025-12-16 09:01:57,401 - generate_secondary - INFO - 
  Session 4/15: Model Selection Criteria
2025-12-16 09:01:57,403 - generate_secondary - INFO - Generating application for session 4: Model Selection Criteria...
2025-12-16 09:01:57,403 - src.llm.client - INFO - [app:2c36e3] ğŸš€ app | m=gemma3:4b | p=31430c | t=150s
2025-12-16 09:01:57,403 - src.llm.client - INFO - [app:2c36e3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:01:57,403 - src.llm.client - INFO - [app:2c36e3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:01:57,404 - src.llm.client - INFO - [app:2c36e3] Sending request to Ollama: model=gemma3:4b, operation=application, payload=33314 bytes, prompt=31430 chars
2025-12-16 09:01:57,404 - src.llm.client - INFO - [app:2c36e3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:02:02,230 - src.llm.request_handler - INFO - [app:2c36e3] âœ“ Done 4.83s
2025-12-16 09:02:02,231 - src.llm.client - INFO - [app:2c36e3] âœ… HTTP 200 in 4.83s
2025-12-16 09:02:02,231 - src.llm.client - INFO - [app:2c36e3] ğŸ“¡ Stream active (200)
2025-12-16 09:02:02,231 - src.llm.client - INFO - [app:2c36e3] Starting stream parsing, waiting for first chunk...
2025-12-16 09:02:04,238 - src.llm.client - INFO - [app:2c36e3] ğŸ“Š 2.0s: 783c @390c/s (131ch, ~196t @98t/s)
2025-12-16 09:02:06,251 - src.llm.client - INFO - [app:2c36e3] ğŸ“Š 4.0s: 1644c @409c/s (264ch, ~411t @102t/s)
2025-12-16 09:02:08,264 - src.llm.client - INFO - [app:2c36e3] ğŸ“Š 6.0s: 2481c @411c/s (389ch, ~620t @103t/s)
2025-12-16 09:02:10,278 - src.llm.client - INFO - [app:2c36e3] ğŸ“Š 8.0s: 3279c @407c/s (521ch, ~820t @102t/s)
2025-12-16 09:02:12,287 - src.llm.client - INFO - [app:2c36e3] ğŸ“Š 10.1s: 4144c @412c/s (654ch, ~1036t @103t/s)
2025-12-16 09:02:14,289 - src.llm.client - INFO - [app:2c36e3] ğŸ“Š 12.1s: 4991c @414c/s (785ch, ~1248t @103t/s)
2025-12-16 09:02:16,298 - src.llm.client - INFO - [app:2c36e3] ğŸ“Š 14.1s: 5744c @408c/s (915ch, ~1436t @102t/s)
2025-12-16 09:02:16,914 - src.llm.client - INFO - [app:2c36e3] âœ“ Done 19.51s: 5852c (~769w @300c/s)
2025-12-16 09:02:16,917 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:02:16,917 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:02:16,917 - generate_secondary - INFO -     - Length: 5841 chars, 767 words
2025-12-16 09:02:16,917 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:02:16,917 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:02:16,917 - generate_secondary - INFO -     - Avg words per application: 148
2025-12-16 09:02:16,917 - generate_secondary - WARNING - [WARNING] Application 3 has 145 words (require 150-200, need 5 more words) âš ï¸
2025-12-16 09:02:16,917 - generate_secondary - WARNING - [WARNING] Application 4 has 134 words (require 150-200, need 16 more words) âš ï¸
2025-12-16 09:02:16,917 - generate_secondary - WARNING - [WARNING] Application 5 has 138 words (require 150-200, need 12 more words) âš ï¸
2025-12-16 09:02:16,918 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_04_bayesian_inference_model_specification/session_04/application.md
2025-12-16 09:02:16,918 - generate_secondary - INFO - Generating extension for session 4: Model Selection Criteria...
2025-12-16 09:02:16,918 - src.llm.client - INFO - [ext:56b83e] ğŸš€ ext | m=gemma3:4b | p=24653c | t=120s
2025-12-16 09:02:16,918 - src.llm.client - INFO - [ext:56b83e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:02:16,918 - src.llm.client - INFO - [ext:56b83e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:02:16,920 - src.llm.client - INFO - [ext:56b83e] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=29392 bytes, prompt=24653 chars
2025-12-16 09:02:16,921 - src.llm.client - INFO - [ext:56b83e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:02:21,816 - src.llm.request_handler - INFO - [ext:56b83e] âœ“ Done 4.90s
2025-12-16 09:02:21,817 - src.llm.client - INFO - [ext:56b83e] âœ… HTTP 200 in 4.90s
2025-12-16 09:02:21,817 - src.llm.client - INFO - [ext:56b83e] ğŸ“¡ Stream active (200)
2025-12-16 09:02:21,817 - src.llm.client - INFO - [ext:56b83e] Starting stream parsing, waiting for first chunk...
2025-12-16 09:02:23,820 - src.llm.client - INFO - [ext:56b83e] ğŸ“Š 2.0s: 779c @389c/s (132ch, ~195t @97t/s)
2025-12-16 09:02:25,822 - src.llm.client - INFO - [ext:56b83e] ğŸ“Š 4.0s: 1610c @402c/s (265ch, ~402t @100t/s)
2025-12-16 09:02:27,828 - src.llm.client - INFO - [ext:56b83e] ğŸ“Š 6.0s: 2438c @406c/s (398ch, ~610t @101t/s)
2025-12-16 09:02:29,835 - src.llm.client - INFO - [ext:56b83e] ğŸ“Š 8.0s: 3220c @402c/s (530ch, ~805t @100t/s)
2025-12-16 09:02:30,374 - src.llm.client - INFO - [ext:56b83e] âœ“ Done 13.46s: 3337c (~436w @248c/s)
2025-12-16 09:02:30,375 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:02:30,376 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:02:30,376 - generate_secondary - INFO -     - Length: 3324 chars, 434 words
2025-12-16 09:02:30,376 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:02:30,376 - generate_secondary - INFO -     - Topics: 3
2025-12-16 09:02:30,376 - generate_secondary - INFO -     - Avg words per topic: 139
2025-12-16 09:02:30,376 - generate_secondary - WARNING - [WARNING] Topic 1 has 159 words (exceeds 150 by 9 words - consider condensing) âš ï¸
2025-12-16 09:02:30,376 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_04_bayesian_inference_model_specification/session_04/extension.md
2025-12-16 09:02:30,376 - generate_secondary - INFO - Generating visualization for session 4: Model Selection Criteria...
2025-12-16 09:02:30,376 - src.llm.client - INFO - [viz:3e164d] ğŸš€ viz | m=gemma3:4b | p=23613c | t=120s
2025-12-16 09:02:30,376 - src.llm.client - INFO - [viz:3e164d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:02:30,376 - src.llm.client - INFO - [viz:3e164d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:02:30,378 - src.llm.client - INFO - [viz:3e164d] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=27674 bytes, prompt=23613 chars
2025-12-16 09:02:30,378 - src.llm.client - INFO - [viz:3e164d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:02:35,180 - src.llm.request_handler - INFO - [viz:3e164d] âœ“ Done 4.80s
2025-12-16 09:02:35,180 - src.llm.client - INFO - [viz:3e164d] âœ… HTTP 200 in 4.80s
2025-12-16 09:02:35,180 - src.llm.client - INFO - [viz:3e164d] ğŸ“¡ Stream active (200)
2025-12-16 09:02:35,181 - src.llm.client - INFO - [viz:3e164d] Starting stream parsing, waiting for first chunk...
2025-12-16 09:02:37,181 - src.llm.client - INFO - [viz:3e164d] ğŸ“Š 2.0s: 498c @249c/s (134ch, ~124t @62t/s)
2025-12-16 09:02:39,194 - src.llm.client - INFO - [viz:3e164d] ğŸ“Š 4.0s: 1019c @254c/s (268ch, ~255t @63t/s)
2025-12-16 09:02:39,387 - src.llm.client - INFO - [viz:3e164d] âœ“ Done 9.01s: 1020c (~152w @113c/s)
2025-12-16 09:02:39,388 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-16 09:02:39,388 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:02:39,388 - generate_secondary - INFO -     - Length: 480 chars (cleaned: 480 chars)
2025-12-16 09:02:39,388 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:02:39,388 - generate_secondary - INFO - [OK] Elements: 30 total (nodes: 15, connections: 15) âœ“
2025-12-16 09:02:39,388 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_04_bayesian_inference_model_specification/session_04/visualization.mmd
2025-12-16 09:02:39,388 - generate_secondary - INFO - Generating integration for session 4: Model Selection Criteria...
2025-12-16 09:02:39,388 - src.llm.client - INFO - [int:2f6ca3] ğŸš€ int | m=gemma3:4b | p=24962c | t=150s
2025-12-16 09:02:39,389 - src.llm.client - INFO - [int:2f6ca3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:02:39,389 - src.llm.client - INFO - [int:2f6ca3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:02:39,390 - src.llm.client - INFO - [int:2f6ca3] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=30040 bytes, prompt=24962 chars
2025-12-16 09:02:39,390 - src.llm.client - INFO - [int:2f6ca3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:02:44,195 - src.llm.request_handler - INFO - [int:2f6ca3] âœ“ Done 4.80s
2025-12-16 09:02:44,195 - src.llm.client - INFO - [int:2f6ca3] âœ… HTTP 200 in 4.81s
2025-12-16 09:02:44,195 - src.llm.client - INFO - [int:2f6ca3] ğŸ“¡ Stream active (200)
2025-12-16 09:02:44,196 - src.llm.client - INFO - [int:2f6ca3] Starting stream parsing, waiting for first chunk...
2025-12-16 09:02:46,205 - src.llm.client - INFO - [int:2f6ca3] ğŸ“Š 2.0s: 777c @387c/s (135ch, ~194t @97t/s)
2025-12-16 09:02:48,206 - src.llm.client - INFO - [int:2f6ca3] ğŸ“Š 4.0s: 1576c @393c/s (269ch, ~394t @98t/s)
2025-12-16 09:02:50,223 - src.llm.client - INFO - [int:2f6ca3] ğŸ“Š 6.0s: 2276c @378c/s (401ch, ~569t @94t/s)
2025-12-16 09:02:52,226 - src.llm.client - INFO - [int:2f6ca3] ğŸ“Š 8.0s: 2732c @340c/s (530ch, ~683t @85t/s)
2025-12-16 09:02:53,825 - src.llm.client - INFO - [int:2f6ca3] âœ“ Done 14.44s: 2987c (~403w @207c/s)
2025-12-16 09:02:53,827 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:02:53,827 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:02:53,827 - generate_secondary - INFO -     - Length: 2986 chars, 403 words
2025-12-16 09:02:53,827 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:02:53,827 - generate_secondary - INFO -     - Connections: 25
2025-12-16 09:02:53,827 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:02:53,827 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_04_bayesian_inference_model_specification/session_04/integration.md
2025-12-16 09:02:53,827 - generate_secondary - INFO - Generating investigation for session 4: Model Selection Criteria...
2025-12-16 09:02:53,828 - src.llm.client - INFO - [inv:cf1e8f] ğŸš€ inv | m=gemma3:4b | p=23875c | t=150s
2025-12-16 09:02:53,828 - src.llm.client - INFO - [inv:cf1e8f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:02:53,828 - src.llm.client - INFO - [inv:cf1e8f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:02:53,830 - src.llm.client - INFO - [inv:cf1e8f] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=27896 bytes, prompt=23875 chars
2025-12-16 09:02:53,830 - src.llm.client - INFO - [inv:cf1e8f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:02:58,607 - src.llm.request_handler - INFO - [inv:cf1e8f] âœ“ Done 4.78s
2025-12-16 09:02:58,607 - src.llm.client - INFO - [inv:cf1e8f] âœ… HTTP 200 in 4.78s
2025-12-16 09:02:58,607 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“¡ Stream active (200)
2025-12-16 09:02:58,608 - src.llm.client - INFO - [inv:cf1e8f] Starting stream parsing, waiting for first chunk...
2025-12-16 09:03:00,616 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“Š 2.0s: 704c @351c/s (138ch, ~176t @88t/s)
2025-12-16 09:03:02,622 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“Š 4.0s: 1467c @365c/s (274ch, ~367t @91t/s)
2025-12-16 09:03:04,626 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“Š 6.0s: 2346c @390c/s (409ch, ~586t @97t/s)
2025-12-16 09:03:06,638 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“Š 8.0s: 3136c @391c/s (541ch, ~784t @98t/s)
2025-12-16 09:03:08,650 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“Š 10.0s: 3952c @394c/s (672ch, ~988t @98t/s)
2025-12-16 09:03:10,658 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“Š 12.0s: 4613c @383c/s (804ch, ~1153t @96t/s)
2025-12-16 09:03:12,900 - src.llm.client - INFO - [inv:cf1e8f] ğŸ“Š 14.3s: 5400c @378c/s (939ch, ~1350t @94t/s)
2025-12-16 09:03:12,900 - src.llm.client - INFO - [inv:cf1e8f] âœ“ Done 19.07s: 5400c (~723w @283c/s)
2025-12-16 09:03:12,902 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:03:12,903 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:03:12,903 - generate_secondary - INFO -     - Length: 5395 chars, 723 words
2025-12-16 09:03:12,903 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:03:12,903 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:03:12,903 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:03:12,903 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_04_bayesian_inference_model_specification/session_04/investigation.md
2025-12-16 09:03:12,903 - generate_secondary - INFO - Generating open_questions for session 4: Model Selection Criteria...
2025-12-16 09:03:12,903 - src.llm.client - INFO - [opq:bcbca9] ğŸš€ opq | m=gemma3:4b | p=23961c | t=150s
2025-12-16 09:03:12,903 - src.llm.client - INFO - [opq:bcbca9] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:03:12,903 - src.llm.client - INFO - [opq:bcbca9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:03:12,904 - src.llm.client - INFO - [opq:bcbca9] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=27993 bytes, prompt=23961 chars
2025-12-16 09:03:12,904 - src.llm.client - INFO - [opq:bcbca9] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:03:17,712 - src.llm.request_handler - INFO - [opq:bcbca9] âœ“ Done 4.81s
2025-12-16 09:03:17,713 - src.llm.client - INFO - [opq:bcbca9] âœ… HTTP 200 in 4.81s
2025-12-16 09:03:17,713 - src.llm.client - INFO - [opq:bcbca9] ğŸ“¡ Stream active (200)
2025-12-16 09:03:17,713 - src.llm.client - INFO - [opq:bcbca9] Starting stream parsing, waiting for first chunk...
2025-12-16 09:03:19,714 - src.llm.client - INFO - [opq:bcbca9] ğŸ“Š 2.0s: 722c @361c/s (136ch, ~180t @90t/s)
2025-12-16 09:03:21,715 - src.llm.client - INFO - [opq:bcbca9] ğŸ“Š 4.0s: 1566c @391c/s (275ch, ~392t @98t/s)
2025-12-16 09:03:23,711 - src.llm.client - INFO - [opq:bcbca9] âœ“ Done 10.81s: 2354c (~302w @218c/s)
2025-12-16 09:03:23,712 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:03:23,713 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:03:23,713 - generate_secondary - INFO -     - Length: 2353 chars, 302 words
2025-12-16 09:03:23,713 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:03:23,713 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:03:23,713 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:03:23,713 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_04_bayesian_inference_model_specification/session_04/open_questions.md
2025-12-16 09:03:23,713 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:03:23,713 - generate_secondary - INFO - 
============================================================
2025-12-16 09:03:23,713 - generate_secondary - INFO - [5/15] Module 5: Variational Inference â€“ Introduction (1 sessions)
2025-12-16 09:03:23,713 - generate_secondary - INFO - ============================================================
2025-12-16 09:03:23,713 - generate_secondary - INFO - 
  Session 5/15: The Challenge of Exact Inference
2025-12-16 09:03:23,715 - generate_secondary - INFO - Generating application for session 5: The Challenge of Exact Inference...
2025-12-16 09:03:23,715 - src.llm.client - INFO - [app:5c8334] ğŸš€ app | m=gemma3:4b | p=32915c | t=150s
2025-12-16 09:03:23,715 - src.llm.client - INFO - [app:5c8334] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:03:23,715 - src.llm.client - INFO - [app:5c8334] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:03:23,716 - src.llm.client - INFO - [app:5c8334] Sending request to Ollama: model=gemma3:4b, operation=application, payload=34926 bytes, prompt=32915 chars
2025-12-16 09:03:23,716 - src.llm.client - INFO - [app:5c8334] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:03:28,377 - src.llm.request_handler - INFO - [app:5c8334] âœ“ Done 4.66s
2025-12-16 09:03:28,378 - src.llm.client - INFO - [app:5c8334] âœ… HTTP 200 in 4.66s
2025-12-16 09:03:28,378 - src.llm.client - INFO - [app:5c8334] ğŸ“¡ Stream active (200)
2025-12-16 09:03:28,378 - src.llm.client - INFO - [app:5c8334] Starting stream parsing, waiting for first chunk...
2025-12-16 09:03:30,390 - src.llm.client - INFO - [app:5c8334] ğŸ“Š 2.0s: 837c @416c/s (139ch, ~209t @104t/s)
2025-12-16 09:03:32,397 - src.llm.client - INFO - [app:5c8334] ğŸ“Š 4.0s: 1712c @426c/s (277ch, ~428t @107t/s)
2025-12-16 09:03:34,400 - src.llm.client - INFO - [app:5c8334] ğŸ“Š 6.0s: 2573c @427c/s (415ch, ~643t @107t/s)
2025-12-16 09:03:36,401 - src.llm.client - INFO - [app:5c8334] ğŸ“Š 8.0s: 3449c @430c/s (553ch, ~862t @107t/s)
2025-12-16 09:03:38,412 - src.llm.client - INFO - [app:5c8334] ğŸ“Š 10.0s: 4298c @428c/s (691ch, ~1074t @107t/s)
2025-12-16 09:03:40,422 - src.llm.client - INFO - [app:5c8334] ğŸ“Š 12.0s: 5156c @428c/s (829ch, ~1289t @107t/s)
2025-12-16 09:03:41,888 - src.llm.client - INFO - [app:5c8334] âœ“ Done 18.17s: 5634c (~753w @310c/s)
2025-12-16 09:03:41,890 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:03:41,890 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:03:41,890 - generate_secondary - INFO -     - Length: 5622 chars, 751 words
2025-12-16 09:03:41,890 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:03:41,890 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:03:41,890 - generate_secondary - INFO -     - Avg words per application: 145
2025-12-16 09:03:41,890 - generate_secondary - WARNING - [WARNING] Application 3 has 146 words (require 150-200, need 4 more words) âš ï¸
2025-12-16 09:03:41,890 - generate_secondary - WARNING - [WARNING] Application 4 has 126 words (require 150-200, need 24 more words) âš ï¸
2025-12-16 09:03:41,890 - generate_secondary - WARNING - [WARNING] Application 5 has 138 words (require 150-200, need 12 more words) âš ï¸
2025-12-16 09:03:41,890 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_05_variational_inference_introduction/session_05/application.md
2025-12-16 09:03:41,890 - generate_secondary - INFO - Generating extension for session 5: The Challenge of Exact Inference...
2025-12-16 09:03:41,891 - src.llm.client - INFO - [ext:6c3e2a] ğŸš€ ext | m=gemma3:4b | p=26138c | t=120s
2025-12-16 09:03:41,891 - src.llm.client - INFO - [ext:6c3e2a] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:03:41,891 - src.llm.client - INFO - [ext:6c3e2a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:03:41,892 - src.llm.client - INFO - [ext:6c3e2a] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=31004 bytes, prompt=26138 chars
2025-12-16 09:03:41,892 - src.llm.client - INFO - [ext:6c3e2a] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:03:46,539 - src.llm.request_handler - INFO - [ext:6c3e2a] âœ“ Done 4.65s
2025-12-16 09:03:46,540 - src.llm.client - INFO - [ext:6c3e2a] âœ… HTTP 200 in 4.65s
2025-12-16 09:03:46,540 - src.llm.client - INFO - [ext:6c3e2a] ğŸ“¡ Stream active (200)
2025-12-16 09:03:46,540 - src.llm.client - INFO - [ext:6c3e2a] Starting stream parsing, waiting for first chunk...
2025-12-16 09:03:48,545 - src.llm.client - INFO - [ext:6c3e2a] ğŸ“Š 2.0s: 861c @429c/s (139ch, ~215t @107t/s)
2025-12-16 09:03:50,553 - src.llm.client - INFO - [ext:6c3e2a] ğŸ“Š 4.0s: 1659c @413c/s (278ch, ~415t @103t/s)
2025-12-16 09:03:52,563 - src.llm.client - INFO - [ext:6c3e2a] ğŸ“Š 6.0s: 2560c @425c/s (417ch, ~640t @106t/s)
2025-12-16 09:03:54,577 - src.llm.client - INFO - [ext:6c3e2a] ğŸ“Š 8.0s: 3337c @415c/s (547ch, ~834t @104t/s)
2025-12-16 09:03:56,591 - src.llm.client - INFO - [ext:6c3e2a] ğŸ“Š 10.1s: 4033c @401c/s (685ch, ~1008t @100t/s)
2025-12-16 09:03:57,165 - src.llm.client - INFO - [ext:6c3e2a] âœ“ Done 15.27s: 4128c (~576w @270c/s)
2025-12-16 09:03:57,166 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:03:57,167 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - INFO -     - Length: 4127 chars, 576 words
2025-12-16 09:03:57,167 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:03:57,167 - generate_secondary - INFO -     - Topics: 6
2025-12-16 09:03:57,167 - generate_secondary - INFO -     - Avg words per topic: 91
2025-12-16 09:03:57,167 - generate_secondary - WARNING - [WARNING] Too many topics (6, maximum 4, 2 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - WARNING - [WARNING] Topic 1 has 166 words (exceeds 150 by 16 words - consider condensing) âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - WARNING - [WARNING] Topic 2 has 164 words (exceeds 150 by 14 words - consider condensing) âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - WARNING - [WARNING] Topic 3 has 199 words (exceeds 150 by 49 words - consider condensing) âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - WARNING - [WARNING] Topic 6 has 17 words (require 100-150, need 83 more words) âš ï¸
2025-12-16 09:03:57,167 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:03:57,167 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:03:57,167 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_05_variational_inference_introduction/session_05/extension.md
2025-12-16 09:03:57,167 - generate_secondary - INFO - Generating visualization for session 5: The Challenge of Exact Inference...
2025-12-16 09:03:57,167 - src.llm.client - INFO - [viz:eee739] ğŸš€ viz | m=gemma3:4b | p=25098c | t=120s
2025-12-16 09:03:57,167 - src.llm.client - INFO - [viz:eee739] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:03:57,167 - src.llm.client - INFO - [viz:eee739] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:03:57,169 - src.llm.client - INFO - [viz:eee739] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=29286 bytes, prompt=25098 chars
2025-12-16 09:03:57,169 - src.llm.client - INFO - [viz:eee739] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:04:01,839 - src.llm.request_handler - INFO - [viz:eee739] âœ“ Done 4.67s
2025-12-16 09:04:01,839 - src.llm.client - INFO - [viz:eee739] âœ… HTTP 200 in 4.67s
2025-12-16 09:04:01,840 - src.llm.client - INFO - [viz:eee739] ğŸ“¡ Stream active (200)
2025-12-16 09:04:01,840 - src.llm.client - INFO - [viz:eee739] Starting stream parsing, waiting for first chunk...
2025-12-16 09:04:03,842 - src.llm.client - INFO - [viz:eee739] ğŸ“Š 2.0s: 494c @247c/s (138ch, ~124t @62t/s)
2025-12-16 09:04:05,848 - src.llm.client - INFO - [viz:eee739] ğŸ“Š 4.0s: 1061c @265c/s (277ch, ~265t @66t/s)
2025-12-16 09:04:07,378 - src.llm.client - INFO - [viz:eee739] âœ“ Done 10.21s: 1532c (~225w @150c/s)
2025-12-16 09:04:07,379 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:04:07,379 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:04:07,379 - generate_secondary - INFO -     - Length: 491 chars (cleaned: 491 chars)
2025-12-16 09:04:07,379 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:04:07,379 - generate_secondary - INFO - [OK] Elements: 29 total (nodes: 16, connections: 13) âœ“
2025-12-16 09:04:07,379 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_05_variational_inference_introduction/session_05/visualization.mmd
2025-12-16 09:04:07,379 - generate_secondary - INFO - Generating integration for session 5: The Challenge of Exact Inference...
2025-12-16 09:04:07,380 - src.llm.client - INFO - [int:eca8a1] ğŸš€ int | m=gemma3:4b | p=26447c | t=150s
2025-12-16 09:04:07,380 - src.llm.client - INFO - [int:eca8a1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:04:07,380 - src.llm.client - INFO - [int:eca8a1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:04:07,381 - src.llm.client - INFO - [int:eca8a1] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=31652 bytes, prompt=26447 chars
2025-12-16 09:04:07,381 - src.llm.client - INFO - [int:eca8a1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:04:12,109 - src.llm.request_handler - INFO - [int:eca8a1] âœ“ Done 4.73s
2025-12-16 09:04:12,109 - src.llm.client - INFO - [int:eca8a1] âœ… HTTP 200 in 4.73s
2025-12-16 09:04:12,109 - src.llm.client - INFO - [int:eca8a1] ğŸ“¡ Stream active (200)
2025-12-16 09:04:12,109 - src.llm.client - INFO - [int:eca8a1] Starting stream parsing, waiting for first chunk...
2025-12-16 09:04:14,113 - src.llm.client - INFO - [int:eca8a1] ğŸ“Š 2.0s: 771c @385c/s (138ch, ~193t @96t/s)
2025-12-16 09:04:16,127 - src.llm.client - INFO - [int:eca8a1] ğŸ“Š 4.0s: 1564c @389c/s (278ch, ~391t @97t/s)
2025-12-16 09:04:18,127 - src.llm.client - INFO - [int:eca8a1] ğŸ“Š 6.0s: 2378c @395c/s (417ch, ~594t @99t/s)
2025-12-16 09:04:20,135 - src.llm.client - INFO - [int:eca8a1] ğŸ“Š 8.0s: 3205c @399c/s (557ch, ~801t @100t/s)
2025-12-16 09:04:22,257 - src.llm.client - INFO - [int:eca8a1] ğŸ“Š 10.1s: 3979c @392c/s (690ch, ~995t @98t/s)
2025-12-16 09:04:22,257 - src.llm.client - INFO - [int:eca8a1] âœ“ Done 14.88s: 3979c (~569w @267c/s)
2025-12-16 09:04:22,259 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:04:22,259 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:04:22,259 - generate_secondary - INFO -     - Length: 3978 chars, 569 words
2025-12-16 09:04:22,259 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:04:22,259 - generate_secondary - INFO -     - Connections: 46
2025-12-16 09:04:22,259 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:04:22,259 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_05_variational_inference_introduction/session_05/integration.md
2025-12-16 09:04:22,259 - generate_secondary - INFO - Generating investigation for session 5: The Challenge of Exact Inference...
2025-12-16 09:04:22,259 - src.llm.client - INFO - [inv:2289d0] ğŸš€ inv | m=gemma3:4b | p=25360c | t=150s
2025-12-16 09:04:22,260 - src.llm.client - INFO - [inv:2289d0] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:04:22,260 - src.llm.client - INFO - [inv:2289d0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:04:22,261 - src.llm.client - INFO - [inv:2289d0] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=29508 bytes, prompt=25360 chars
2025-12-16 09:04:22,261 - src.llm.client - INFO - [inv:2289d0] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:04:26,905 - src.llm.request_handler - INFO - [inv:2289d0] âœ“ Done 4.64s
2025-12-16 09:04:26,906 - src.llm.client - INFO - [inv:2289d0] âœ… HTTP 200 in 4.65s
2025-12-16 09:04:26,907 - src.llm.client - INFO - [inv:2289d0] ğŸ“¡ Stream active (200)
2025-12-16 09:04:26,907 - src.llm.client - INFO - [inv:2289d0] Starting stream parsing, waiting for first chunk...
2025-12-16 09:04:28,911 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 2.0s: 760c @379c/s (139ch, ~190t @95t/s)
2025-12-16 09:04:30,914 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 4.0s: 1488c @371c/s (278ch, ~372t @93t/s)
2025-12-16 09:04:32,918 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 6.0s: 2267c @377c/s (417ch, ~567t @94t/s)
2025-12-16 09:04:34,923 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 8.0s: 3083c @385c/s (556ch, ~771t @96t/s)
2025-12-16 09:04:36,929 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 10.0s: 3826c @382c/s (695ch, ~956t @95t/s)
2025-12-16 09:04:38,939 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 12.0s: 4650c @386c/s (834ch, ~1162t @97t/s)
2025-12-16 09:04:40,944 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 14.0s: 5291c @377c/s (967ch, ~1323t @94t/s)
2025-12-16 09:04:42,948 - src.llm.client - INFO - [inv:2289d0] ğŸ“Š 16.0s: 6063c @378c/s (1104ch, ~1516t @94t/s)
2025-12-16 09:04:44,646 - src.llm.client - INFO - [inv:2289d0] âœ“ Done 22.39s: 6701c (~933w @299c/s)
2025-12-16 09:04:44,649 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:04:44,649 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:04:44,649 - generate_secondary - INFO -     - Length: 6692 chars, 933 words
2025-12-16 09:04:44,649 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:04:44,649 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:04:44,649 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:04:44,649 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_05_variational_inference_introduction/session_05/investigation.md
2025-12-16 09:04:44,649 - generate_secondary - INFO - Generating open_questions for session 5: The Challenge of Exact Inference...
2025-12-16 09:04:44,649 - src.llm.client - INFO - [opq:16360e] ğŸš€ opq | m=gemma3:4b | p=25446c | t=150s
2025-12-16 09:04:44,649 - src.llm.client - INFO - [opq:16360e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:04:44,649 - src.llm.client - INFO - [opq:16360e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:04:44,651 - src.llm.client - INFO - [opq:16360e] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=29605 bytes, prompt=25446 chars
2025-12-16 09:04:44,651 - src.llm.client - INFO - [opq:16360e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:04:49,306 - src.llm.request_handler - INFO - [opq:16360e] âœ“ Done 4.65s
2025-12-16 09:04:49,306 - src.llm.client - INFO - [opq:16360e] âœ… HTTP 200 in 4.66s
2025-12-16 09:04:49,306 - src.llm.client - INFO - [opq:16360e] ğŸ“¡ Stream active (200)
2025-12-16 09:04:49,307 - src.llm.client - INFO - [opq:16360e] Starting stream parsing, waiting for first chunk...
2025-12-16 09:04:51,321 - src.llm.client - INFO - [opq:16360e] ğŸ“Š 2.0s: 841c @418c/s (140ch, ~210t @104t/s)
2025-12-16 09:04:53,327 - src.llm.client - INFO - [opq:16360e] ğŸ“Š 4.0s: 1600c @398c/s (275ch, ~400t @99t/s)
2025-12-16 09:04:54,323 - src.llm.client - INFO - [opq:16360e] âœ“ Done 9.67s: 1924c (~253w @199c/s)
2025-12-16 09:04:54,324 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:04:54,324 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:04:54,324 - generate_secondary - INFO -     - Length: 1923 chars, 253 words
2025-12-16 09:04:54,324 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:04:54,324 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:04:54,324 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:04:54,324 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_05_variational_inference_introduction/session_05/open_questions.md
2025-12-16 09:04:54,324 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:04:54,324 - generate_secondary - INFO - 
============================================================
2025-12-16 09:04:54,324 - generate_secondary - INFO - [6/15] Module 6: Variational Free Energy â€“ Definition & Interpretation (1 sessions)
2025-12-16 09:04:54,324 - generate_secondary - INFO - ============================================================
2025-12-16 09:04:54,324 - generate_secondary - INFO - 
  Session 6/15: Defining VFE
2025-12-16 09:04:54,326 - generate_secondary - INFO - Generating application for session 6: Defining VFE...
2025-12-16 09:04:54,326 - src.llm.client - INFO - [app:3e3091] ğŸš€ app | m=gemma3:4b | p=32256c | t=150s
2025-12-16 09:04:54,326 - src.llm.client - INFO - [app:3e3091] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:04:54,327 - src.llm.client - INFO - [app:3e3091] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:04:54,328 - src.llm.client - INFO - [app:3e3091] Sending request to Ollama: model=gemma3:4b, operation=application, payload=34601 bytes, prompt=32256 chars
2025-12-16 09:04:54,328 - src.llm.client - INFO - [app:3e3091] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:04:58,991 - src.llm.request_handler - INFO - [app:3e3091] âœ“ Done 4.66s
2025-12-16 09:04:58,992 - src.llm.client - INFO - [app:3e3091] âœ… HTTP 200 in 4.66s
2025-12-16 09:04:58,992 - src.llm.client - INFO - [app:3e3091] ğŸ“¡ Stream active (200)
2025-12-16 09:04:58,992 - src.llm.client - INFO - [app:3e3091] Starting stream parsing, waiting for first chunk...
2025-12-16 09:05:01,001 - src.llm.client - INFO - [app:3e3091] ğŸ“Š 2.0s: 858c @427c/s (140ch, ~214t @107t/s)
2025-12-16 09:05:03,012 - src.llm.client - INFO - [app:3e3091] ğŸ“Š 4.0s: 1751c @436c/s (281ch, ~438t @109t/s)
2025-12-16 09:05:05,018 - src.llm.client - INFO - [app:3e3091] ğŸ“Š 6.0s: 2667c @443c/s (422ch, ~667t @111t/s)
2025-12-16 09:05:07,021 - src.llm.client - INFO - [app:3e3091] ğŸ“Š 8.0s: 3648c @454c/s (562ch, ~912t @114t/s)
2025-12-16 09:05:09,035 - src.llm.client - INFO - [app:3e3091] ğŸ“Š 10.0s: 4490c @447c/s (702ch, ~1122t @112t/s)
2025-12-16 09:05:11,042 - src.llm.client - INFO - [app:3e3091] ğŸ“Š 12.0s: 5432c @451c/s (840ch, ~1358t @113t/s)
2025-12-16 09:05:11,560 - src.llm.client - INFO - [app:3e3091] âœ“ Done 17.23s: 5509c (~717w @320c/s)
2025-12-16 09:05:11,562 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:05:11,562 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:05:11,562 - generate_secondary - INFO -     - Length: 5497 chars, 715 words
2025-12-16 09:05:11,562 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:05:11,562 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:05:11,562 - generate_secondary - INFO -     - Avg words per application: 138
2025-12-16 09:05:11,562 - generate_secondary - WARNING - [WARNING] Application 3 has 129 words (require 150-200, need 21 more words) âš ï¸
2025-12-16 09:05:11,562 - generate_secondary - WARNING - [WARNING] Application 4 has 132 words (require 150-200, need 18 more words) âš ï¸
2025-12-16 09:05:11,562 - generate_secondary - WARNING - [WARNING] Application 5 has 114 words (require 150-200, need 36 more words) âš ï¸
2025-12-16 09:05:11,562 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_06_variational_free_energy_definition_interpretation/session_06/application.md
2025-12-16 09:05:11,563 - generate_secondary - INFO - Generating extension for session 6: Defining VFE...
2025-12-16 09:05:11,563 - src.llm.client - INFO - [ext:16ce93] ğŸš€ ext | m=gemma3:4b | p=25479c | t=120s
2025-12-16 09:05:11,563 - src.llm.client - INFO - [ext:16ce93] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:05:11,563 - src.llm.client - INFO - [ext:16ce93] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:05:11,564 - src.llm.client - INFO - [ext:16ce93] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=30679 bytes, prompt=25479 chars
2025-12-16 09:05:11,564 - src.llm.client - INFO - [ext:16ce93] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:05:16,231 - src.llm.request_handler - INFO - [ext:16ce93] âœ“ Done 4.67s
2025-12-16 09:05:16,231 - src.llm.client - INFO - [ext:16ce93] âœ… HTTP 200 in 4.67s
2025-12-16 09:05:16,231 - src.llm.client - INFO - [ext:16ce93] ğŸ“¡ Stream active (200)
2025-12-16 09:05:16,231 - src.llm.client - INFO - [ext:16ce93] Starting stream parsing, waiting for first chunk...
2025-12-16 09:05:18,243 - src.llm.client - INFO - [ext:16ce93] ğŸ“Š 2.0s: 884c @439c/s (139ch, ~221t @110t/s)
2025-12-16 09:05:20,255 - src.llm.client - INFO - [ext:16ce93] ğŸ“Š 4.0s: 1755c @436c/s (279ch, ~439t @109t/s)
2025-12-16 09:05:22,265 - src.llm.client - INFO - [ext:16ce93] ğŸ“Š 6.0s: 2575c @427c/s (419ch, ~644t @107t/s)
2025-12-16 09:05:24,266 - src.llm.client - INFO - [ext:16ce93] ğŸ“Š 8.0s: 3506c @436c/s (558ch, ~876t @109t/s)
2025-12-16 09:05:24,942 - src.llm.client - INFO - [ext:16ce93] âœ“ Done 13.38s: 3705c (~464w @277c/s)
2025-12-16 09:05:24,943 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:05:24,943 - generate_secondary - INFO - [COMPLIANT] Extension generated âœ“
2025-12-16 09:05:24,943 - generate_secondary - INFO -     - Length: 3704 chars, 464 words
2025-12-16 09:05:24,943 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:05:24,943 - generate_secondary - INFO -     - Topics: 3
2025-12-16 09:05:24,943 - generate_secondary - INFO -     - Avg words per topic: 144
2025-12-16 09:05:24,943 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_06_variational_free_energy_definition_interpretation/session_06/extension.md
2025-12-16 09:05:24,943 - generate_secondary - INFO - Generating visualization for session 6: Defining VFE...
2025-12-16 09:05:24,943 - src.llm.client - INFO - [viz:6ddc22] ğŸš€ viz | m=gemma3:4b | p=24439c | t=120s
2025-12-16 09:05:24,944 - src.llm.client - INFO - [viz:6ddc22] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:05:24,944 - src.llm.client - INFO - [viz:6ddc22] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:05:24,945 - src.llm.client - INFO - [viz:6ddc22] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=28961 bytes, prompt=24439 chars
2025-12-16 09:05:24,945 - src.llm.client - INFO - [viz:6ddc22] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:05:29,580 - src.llm.request_handler - INFO - [viz:6ddc22] âœ“ Done 4.63s
2025-12-16 09:05:29,580 - src.llm.client - INFO - [viz:6ddc22] âœ… HTTP 200 in 4.64s
2025-12-16 09:05:29,580 - src.llm.client - INFO - [viz:6ddc22] ğŸ“¡ Stream active (200)
2025-12-16 09:05:29,580 - src.llm.client - INFO - [viz:6ddc22] Starting stream parsing, waiting for first chunk...
2025-12-16 09:05:31,581 - src.llm.client - INFO - [viz:6ddc22] ğŸ“Š 2.0s: 529c @264c/s (139ch, ~132t @66t/s)
2025-12-16 09:05:33,584 - src.llm.client - INFO - [viz:6ddc22] ğŸ“Š 4.0s: 1015c @254c/s (272ch, ~254t @63t/s)
2025-12-16 09:05:34,763 - src.llm.client - INFO - [viz:6ddc22] âœ“ Done 9.82s: 1341c (~199w @137c/s)
2025-12-16 09:05:34,763 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-16 09:05:34,763 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:05:34,763 - generate_secondary - INFO -     - Length: 478 chars (cleaned: 478 chars)
2025-12-16 09:05:34,763 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:05:34,763 - generate_secondary - INFO - [OK] Elements: 33 total (nodes: 12, connections: 21) âœ“
2025-12-16 09:05:34,764 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_06_variational_free_energy_definition_interpretation/session_06/visualization.mmd
2025-12-16 09:05:34,764 - generate_secondary - INFO - Generating integration for session 6: Defining VFE...
2025-12-16 09:05:34,764 - src.llm.client - INFO - [int:65c713] ğŸš€ int | m=gemma3:4b | p=25788c | t=150s
2025-12-16 09:05:34,764 - src.llm.client - INFO - [int:65c713] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:05:34,764 - src.llm.client - INFO - [int:65c713] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:05:34,765 - src.llm.client - INFO - [int:65c713] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=31327 bytes, prompt=25788 chars
2025-12-16 09:05:34,766 - src.llm.client - INFO - [int:65c713] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:05:39,527 - src.llm.request_handler - INFO - [int:65c713] âœ“ Done 4.76s
2025-12-16 09:05:39,527 - src.llm.client - INFO - [int:65c713] âœ… HTTP 200 in 4.76s
2025-12-16 09:05:39,527 - src.llm.client - INFO - [int:65c713] ğŸ“¡ Stream active (200)
2025-12-16 09:05:39,527 - src.llm.client - INFO - [int:65c713] Starting stream parsing, waiting for first chunk...
2025-12-16 09:05:41,535 - src.llm.client - INFO - [int:65c713] ğŸ“Š 2.0s: 828c @413c/s (137ch, ~207t @103t/s)
2025-12-16 09:05:43,545 - src.llm.client - INFO - [int:65c713] ğŸ“Š 4.0s: 1662c @414c/s (276ch, ~416t @103t/s)
2025-12-16 09:05:45,505 - src.llm.client - INFO - [int:65c713] âœ“ Done 10.74s: 2143c (~297w @200c/s)
2025-12-16 09:05:45,506 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:05:45,506 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:05:45,506 - generate_secondary - INFO -     - Length: 2129 chars, 295 words
2025-12-16 09:05:45,506 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:05:45,506 - generate_secondary - INFO -     - Connections: 15
2025-12-16 09:05:45,506 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:05:45,506 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_06_variational_free_energy_definition_interpretation/session_06/integration.md
2025-12-16 09:05:45,506 - generate_secondary - INFO - Generating investigation for session 6: Defining VFE...
2025-12-16 09:05:45,506 - src.llm.client - INFO - [inv:74f270] ğŸš€ inv | m=gemma3:4b | p=24701c | t=150s
2025-12-16 09:05:45,506 - src.llm.client - INFO - [inv:74f270] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:05:45,506 - src.llm.client - INFO - [inv:74f270] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:05:45,509 - src.llm.client - INFO - [inv:74f270] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=29183 bytes, prompt=24701 chars
2025-12-16 09:05:45,510 - src.llm.client - INFO - [inv:74f270] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:05:50,182 - src.llm.request_handler - INFO - [inv:74f270] âœ“ Done 4.67s
2025-12-16 09:05:50,182 - src.llm.client - INFO - [inv:74f270] âœ… HTTP 200 in 4.67s
2025-12-16 09:05:50,182 - src.llm.client - INFO - [inv:74f270] ğŸ“¡ Stream active (200)
2025-12-16 09:05:50,182 - src.llm.client - INFO - [inv:74f270] Starting stream parsing, waiting for first chunk...
2025-12-16 09:05:52,193 - src.llm.client - INFO - [inv:74f270] ğŸ“Š 2.0s: 698c @347c/s (139ch, ~174t @87t/s)
2025-12-16 09:05:54,198 - src.llm.client - INFO - [inv:74f270] ğŸ“Š 4.0s: 1367c @340c/s (266ch, ~342t @85t/s)
2025-12-16 09:05:56,213 - src.llm.client - INFO - [inv:74f270] ğŸ“Š 6.0s: 2145c @356c/s (406ch, ~536t @89t/s)
2025-12-16 09:05:58,216 - src.llm.client - INFO - [inv:74f270] ğŸ“Š 8.0s: 2797c @348c/s (544ch, ~699t @87t/s)
2025-12-16 09:06:00,222 - src.llm.client - INFO - [inv:74f270] ğŸ“Š 10.0s: 3552c @354c/s (678ch, ~888t @88t/s)
2025-12-16 09:06:02,223 - src.llm.client - INFO - [inv:74f270] ğŸ“Š 12.0s: 4178c @347c/s (809ch, ~1044t @87t/s)
2025-12-16 09:06:03,917 - src.llm.client - INFO - [inv:74f270] âœ“ Done 18.41s: 4752c (~692w @258c/s)
2025-12-16 09:06:03,919 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:06:03,919 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:06:03,919 - generate_secondary - INFO -     - Length: 4737 chars, 690 words
2025-12-16 09:06:03,919 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:06:03,919 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:06:03,919 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:06:03,920 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_06_variational_free_energy_definition_interpretation/session_06/investigation.md
2025-12-16 09:06:03,920 - generate_secondary - INFO - Generating open_questions for session 6: Defining VFE...
2025-12-16 09:06:03,920 - src.llm.client - INFO - [opq:6ee961] ğŸš€ opq | m=gemma3:4b | p=24787c | t=150s
2025-12-16 09:06:03,920 - src.llm.client - INFO - [opq:6ee961] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:06:03,920 - src.llm.client - INFO - [opq:6ee961] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:06:03,921 - src.llm.client - INFO - [opq:6ee961] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=29280 bytes, prompt=24787 chars
2025-12-16 09:06:03,921 - src.llm.client - INFO - [opq:6ee961] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:06:08,628 - src.llm.request_handler - INFO - [opq:6ee961] âœ“ Done 4.71s
2025-12-16 09:06:08,632 - src.llm.client - INFO - [opq:6ee961] âœ… HTTP 200 in 4.71s
2025-12-16 09:06:08,632 - src.llm.client - INFO - [opq:6ee961] ğŸ“¡ Stream active (200)
2025-12-16 09:06:08,632 - src.llm.client - INFO - [opq:6ee961] Starting stream parsing, waiting for first chunk...
2025-12-16 09:06:10,646 - src.llm.client - INFO - [opq:6ee961] ğŸ“Š 2.0s: 735c @365c/s (137ch, ~184t @91t/s)
2025-12-16 09:06:12,654 - src.llm.client - INFO - [opq:6ee961] ğŸ“Š 4.0s: 1569c @390c/s (273ch, ~392t @98t/s)
2025-12-16 09:06:13,517 - src.llm.client - INFO - [opq:6ee961] âœ“ Done 9.60s: 1858c (~249w @194c/s)
2025-12-16 09:06:13,518 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:06:13,518 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:06:13,519 - generate_secondary - INFO -     - Length: 1844 chars, 247 words
2025-12-16 09:06:13,519 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:06:13,519 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:06:13,519 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:06:13,519 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_06_variational_free_energy_definition_interpretation/session_06/open_questions.md
2025-12-16 09:06:13,519 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:06:13,519 - generate_secondary - INFO - 
============================================================
2025-12-16 09:06:13,519 - generate_secondary - INFO - [7/15] Module 7: Markov Models â€“ Introduction & State Spaces (1 sessions)
2025-12-16 09:06:13,519 - generate_secondary - INFO - ============================================================
2025-12-16 09:06:13,519 - generate_secondary - INFO - 
  Session 7/15: Markov Property
2025-12-16 09:06:13,522 - generate_secondary - INFO - Generating application for session 7: Markov Property...
2025-12-16 09:06:13,522 - src.llm.client - INFO - [app:7cd976] ğŸš€ app | m=gemma3:4b | p=32666c | t=150s
2025-12-16 09:06:13,522 - src.llm.client - INFO - [app:7cd976] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:06:13,522 - src.llm.client - INFO - [app:7cd976] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:06:13,525 - src.llm.client - INFO - [app:7cd976] Sending request to Ollama: model=gemma3:4b, operation=application, payload=34906 bytes, prompt=32666 chars
2025-12-16 09:06:13,525 - src.llm.client - INFO - [app:7cd976] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:06:18,200 - src.llm.request_handler - INFO - [app:7cd976] âœ“ Done 4.67s
2025-12-16 09:06:18,200 - src.llm.client - INFO - [app:7cd976] âœ… HTTP 200 in 4.68s
2025-12-16 09:06:18,201 - src.llm.client - INFO - [app:7cd976] ğŸ“¡ Stream active (200)
2025-12-16 09:06:18,201 - src.llm.client - INFO - [app:7cd976] Starting stream parsing, waiting for first chunk...
2025-12-16 09:06:20,211 - src.llm.client - INFO - [app:7cd976] ğŸ“Š 2.0s: 808c @402c/s (133ch, ~202t @100t/s)
2025-12-16 09:06:22,220 - src.llm.client - INFO - [app:7cd976] ğŸ“Š 4.0s: 1687c @420c/s (270ch, ~422t @105t/s)
2025-12-16 09:06:24,233 - src.llm.client - INFO - [app:7cd976] ğŸ“Š 6.0s: 2534c @420c/s (408ch, ~634t @105t/s)
2025-12-16 09:06:26,238 - src.llm.client - INFO - [app:7cd976] ğŸ“Š 8.0s: 3447c @429c/s (546ch, ~862t @107t/s)
2025-12-16 09:06:28,248 - src.llm.client - INFO - [app:7cd976] ğŸ“Š 10.0s: 4261c @424c/s (684ch, ~1065t @106t/s)
2025-12-16 09:06:30,249 - src.llm.client - INFO - [app:7cd976] ğŸ“Š 12.0s: 5175c @430c/s (821ch, ~1294t @107t/s)
2025-12-16 09:06:32,206 - src.llm.client - INFO - [app:7cd976] âœ“ Done 18.68s: 5845c (~778w @313c/s)
2025-12-16 09:06:32,209 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:06:32,209 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:06:32,209 - generate_secondary - INFO -     - Length: 5833 chars, 776 words
2025-12-16 09:06:32,209 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:06:32,209 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:06:32,209 - generate_secondary - INFO -     - Avg words per application: 150
2025-12-16 09:06:32,209 - generate_secondary - WARNING - [WARNING] Application 4 has 146 words (require 150-200, need 4 more words) âš ï¸
2025-12-16 09:06:32,209 - generate_secondary - WARNING - [WARNING] Application 5 has 139 words (require 150-200, need 11 more words) âš ï¸
2025-12-16 09:06:32,209 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_07_markov_models_introduction_state_spaces/session_07/application.md
2025-12-16 09:06:32,209 - generate_secondary - INFO - Generating extension for session 7: Markov Property...
2025-12-16 09:06:32,209 - src.llm.client - INFO - [ext:2f8195] ğŸš€ ext | m=gemma3:4b | p=25889c | t=120s
2025-12-16 09:06:32,209 - src.llm.client - INFO - [ext:2f8195] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:06:32,209 - src.llm.client - INFO - [ext:2f8195] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:06:32,211 - src.llm.client - INFO - [ext:2f8195] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=30984 bytes, prompt=25889 chars
2025-12-16 09:06:32,211 - src.llm.client - INFO - [ext:2f8195] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:06:36,862 - src.llm.request_handler - INFO - [ext:2f8195] âœ“ Done 4.65s
2025-12-16 09:06:36,862 - src.llm.client - INFO - [ext:2f8195] âœ… HTTP 200 in 4.65s
2025-12-16 09:06:36,862 - src.llm.client - INFO - [ext:2f8195] ğŸ“¡ Stream active (200)
2025-12-16 09:06:36,863 - src.llm.client - INFO - [ext:2f8195] Starting stream parsing, waiting for first chunk...
2025-12-16 09:06:38,872 - src.llm.client - INFO - [ext:2f8195] ğŸ“Š 2.0s: 943c @469c/s (139ch, ~236t @117t/s)
2025-12-16 09:06:40,886 - src.llm.client - INFO - [ext:2f8195] ğŸ“Š 4.0s: 1793c @446c/s (278ch, ~448t @111t/s)
2025-12-16 09:06:42,887 - src.llm.client - INFO - [ext:2f8195] ğŸ“Š 6.0s: 2672c @444c/s (416ch, ~668t @111t/s)
2025-12-16 09:06:44,902 - src.llm.client - INFO - [ext:2f8195] ğŸ“Š 8.0s: 3535c @440c/s (556ch, ~884t @110t/s)
2025-12-16 09:06:45,536 - src.llm.client - INFO - [ext:2f8195] âœ“ Done 13.33s: 3721c (~482w @279c/s)
2025-12-16 09:06:45,537 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:06:45,537 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:06:45,537 - generate_secondary - INFO -     - Length: 3720 chars, 482 words
2025-12-16 09:06:45,537 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:06:45,537 - generate_secondary - INFO -     - Topics: 3
2025-12-16 09:06:45,537 - generate_secondary - INFO -     - Avg words per topic: 155
2025-12-16 09:06:45,537 - generate_secondary - WARNING - [WARNING] Topic 2 has 153 words (exceeds 150 by 3 words - consider condensing) âš ï¸
2025-12-16 09:06:45,537 - generate_secondary - WARNING - [WARNING] Topic 3 has 163 words (exceeds 150 by 13 words - consider condensing) âš ï¸
2025-12-16 09:06:45,538 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_07_markov_models_introduction_state_spaces/session_07/extension.md
2025-12-16 09:06:45,538 - generate_secondary - INFO - Generating visualization for session 7: Markov Property...
2025-12-16 09:06:45,538 - src.llm.client - INFO - [viz:86f83c] ğŸš€ viz | m=gemma3:4b | p=24849c | t=120s
2025-12-16 09:06:45,538 - src.llm.client - INFO - [viz:86f83c] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:06:45,538 - src.llm.client - INFO - [viz:86f83c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:06:45,539 - src.llm.client - INFO - [viz:86f83c] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=29266 bytes, prompt=24849 chars
2025-12-16 09:06:45,539 - src.llm.client - INFO - [viz:86f83c] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:06:50,173 - src.llm.request_handler - INFO - [viz:86f83c] âœ“ Done 4.63s
2025-12-16 09:06:50,173 - src.llm.client - INFO - [viz:86f83c] âœ… HTTP 200 in 4.63s
2025-12-16 09:06:50,173 - src.llm.client - INFO - [viz:86f83c] ğŸ“¡ Stream active (200)
2025-12-16 09:06:50,173 - src.llm.client - INFO - [viz:86f83c] Starting stream parsing, waiting for first chunk...
2025-12-16 09:06:52,176 - src.llm.client - INFO - [viz:86f83c] ğŸ“Š 2.0s: 509c @254c/s (141ch, ~127t @64t/s)
2025-12-16 09:06:52,596 - src.llm.client - INFO - [viz:86f83c] âœ“ Done 7.06s: 581c (~87w @82c/s)
2025-12-16 09:06:52,596 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-16 09:06:52,596 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 09:06:52,596 - generate_secondary - INFO -     - Length: 186 chars (cleaned: 186 chars)
2025-12-16 09:06:52,596 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:06:52,596 - generate_secondary - INFO - [CRITICAL] Elements: 14 total (nodes: 7, connections: 7) ğŸ”´
2025-12-16 09:06:52,596 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 09:06:52,596 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-16 09:06:52,596 - generate_secondary - WARNING - [WARNING] Only 7 nodes found (require at least 10, need 3 more - add more nodes to the diagram) âš ï¸
2025-12-16 09:06:52,596 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:06:52,596 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:06:52,597 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_07_markov_models_introduction_state_spaces/session_07/visualization.mmd
2025-12-16 09:06:52,597 - generate_secondary - INFO - Generating integration for session 7: Markov Property...
2025-12-16 09:06:52,597 - src.llm.client - INFO - [int:262159] ğŸš€ int | m=gemma3:4b | p=26198c | t=150s
2025-12-16 09:06:52,597 - src.llm.client - INFO - [int:262159] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:06:52,597 - src.llm.client - INFO - [int:262159] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:06:52,598 - src.llm.client - INFO - [int:262159] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=31632 bytes, prompt=26198 chars
2025-12-16 09:06:52,598 - src.llm.client - INFO - [int:262159] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:06:57,233 - src.llm.request_handler - INFO - [int:262159] âœ“ Done 4.63s
2025-12-16 09:06:57,235 - src.llm.client - INFO - [int:262159] âœ… HTTP 200 in 4.64s
2025-12-16 09:06:57,235 - src.llm.client - INFO - [int:262159] ğŸ“¡ Stream active (200)
2025-12-16 09:06:57,235 - src.llm.client - INFO - [int:262159] Starting stream parsing, waiting for first chunk...
2025-12-16 09:06:59,242 - src.llm.client - INFO - [int:262159] ğŸ“Š 2.0s: 798c @398c/s (141ch, ~200t @99t/s)
2025-12-16 09:07:01,242 - src.llm.client - INFO - [int:262159] ğŸ“Š 4.0s: 1613c @403c/s (282ch, ~403t @101t/s)
2025-12-16 09:07:03,248 - src.llm.client - INFO - [int:262159] ğŸ“Š 6.0s: 2427c @404c/s (423ch, ~607t @101t/s)
2025-12-16 09:07:05,250 - src.llm.client - INFO - [int:262159] ğŸ“Š 8.0s: 3249c @405c/s (564ch, ~812t @101t/s)
2025-12-16 09:07:07,252 - src.llm.client - INFO - [int:262159] ğŸ“Š 10.0s: 4057c @405c/s (705ch, ~1014t @101t/s)
2025-12-16 09:07:09,259 - src.llm.client - INFO - [int:262159] ğŸ“Š 12.0s: 4883c @406c/s (847ch, ~1221t @102t/s)
2025-12-16 09:07:11,273 - src.llm.client - INFO - [int:262159] ğŸ“Š 14.0s: 5700c @406c/s (988ch, ~1425t @102t/s)
2025-12-16 09:07:13,278 - src.llm.client - INFO - [int:262159] ğŸ“Š 16.0s: 6493c @405c/s (1128ch, ~1623t @101t/s)
2025-12-16 09:07:15,282 - src.llm.client - INFO - [int:262159] ğŸ“Š 18.0s: 7310c @405c/s (1268ch, ~1828t @101t/s)
2025-12-16 09:07:17,283 - src.llm.client - INFO - [int:262159] ğŸ“Š 20.0s: 8104c @404c/s (1408ch, ~2026t @101t/s)
2025-12-16 09:07:19,297 - src.llm.client - INFO - [int:262159] ğŸ“Š 22.1s: 8926c @405c/s (1548ch, ~2232t @101t/s)
2025-12-16 09:07:21,309 - src.llm.client - INFO - [int:262159] ğŸ“Š 24.1s: 9733c @404c/s (1689ch, ~2433t @101t/s)
2025-12-16 09:07:23,312 - src.llm.client - INFO - [int:262159] ğŸ“Š 26.1s: 10536c @404c/s (1828ch, ~2634t @101t/s)
2025-12-16 09:07:25,324 - src.llm.client - INFO - [int:262159] ğŸ“Š 28.1s: 11332c @403c/s (1968ch, ~2833t @101t/s)
2025-12-16 09:07:27,338 - src.llm.client - INFO - [int:262159] ğŸ“Š 30.1s: 12157c @404c/s (2108ch, ~3039t @101t/s)
2025-12-16 09:07:29,347 - src.llm.client - INFO - [int:262159] ğŸ“Š 32.1s: 12913c @402c/s (2241ch, ~3228t @101t/s)
2025-12-16 09:07:31,354 - src.llm.client - INFO - [int:262159] ğŸ“Š 34.1s: 13696c @401c/s (2376ch, ~3424t @100t/s)
2025-12-16 09:07:33,366 - src.llm.client - INFO - [int:262159] ğŸ“Š 36.1s: 14453c @400c/s (2510ch, ~3613t @100t/s)
2025-12-16 09:07:35,378 - src.llm.client - INFO - [int:262159] ğŸ“Š 38.1s: 15249c @400c/s (2647ch, ~3812t @100t/s)
2025-12-16 09:07:37,381 - src.llm.client - INFO - [int:262159] ğŸ“Š 40.1s: 16033c @399c/s (2784ch, ~4008t @100t/s)
2025-12-16 09:07:39,390 - src.llm.client - INFO - [int:262159] ğŸ“Š 42.2s: 16838c @399c/s (2922ch, ~4210t @100t/s)
2025-12-16 09:07:41,395 - src.llm.client - INFO - [int:262159] ğŸ“Š 44.2s: 17616c @399c/s (3058ch, ~4404t @100t/s)
2025-12-16 09:07:43,404 - src.llm.client - INFO - [int:262159] ğŸ“Š 46.2s: 18416c @399c/s (3195ch, ~4604t @100t/s)
2025-12-16 09:07:45,413 - src.llm.client - INFO - [int:262159] ğŸ“Š 48.2s: 19206c @399c/s (3334ch, ~4802t @100t/s)
2025-12-16 09:07:47,414 - src.llm.client - INFO - [int:262159] ğŸ“Š 50.2s: 20010c @399c/s (3472ch, ~5002t @100t/s)
2025-12-16 09:07:49,414 - src.llm.client - INFO - [int:262159] ğŸ“Š 52.2s: 20796c @399c/s (3610ch, ~5199t @100t/s)
2025-12-16 09:07:51,427 - src.llm.client - INFO - [int:262159] ğŸ“Š 54.2s: 21596c @399c/s (3747ch, ~5399t @100t/s)
2025-12-16 09:07:53,434 - src.llm.client - INFO - [int:262159] ğŸ“Š 56.2s: 22347c @398c/s (3879ch, ~5587t @99t/s)
2025-12-16 09:07:55,435 - src.llm.client - INFO - [int:262159] ğŸ“Š 58.2s: 23104c @397c/s (4013ch, ~5776t @99t/s)
2025-12-16 09:07:57,444 - src.llm.client - INFO - [int:262159] ğŸ“Š 60.2s: 23908c @397c/s (4150ch, ~5977t @99t/s)
2025-12-16 09:07:59,452 - src.llm.client - INFO - [int:262159] ğŸ“Š 62.2s: 24694c @397c/s (4289ch, ~6174t @99t/s)
2025-12-16 09:08:01,455 - src.llm.client - INFO - [int:262159] ğŸ“Š 64.2s: 25479c @397c/s (4424ch, ~6370t @99t/s)
2025-12-16 09:08:03,458 - src.llm.client - INFO - [int:262159] ğŸ“Š 66.2s: 26238c @396c/s (4557ch, ~6560t @99t/s)
2025-12-16 09:08:05,466 - src.llm.client - INFO - [int:262159] ğŸ“Š 68.2s: 27036c @396c/s (4695ch, ~6759t @99t/s)
2025-12-16 09:08:07,475 - src.llm.client - INFO - [int:262159] ğŸ“Š 70.2s: 27836c @396c/s (4835ch, ~6959t @99t/s)
2025-12-16 09:08:09,477 - src.llm.client - INFO - [int:262159] ğŸ“Š 72.2s: 28643c @396c/s (4974ch, ~7161t @99t/s)
2025-12-16 09:08:11,482 - src.llm.client - INFO - [int:262159] ğŸ“Š 74.2s: 29444c @397c/s (5113ch, ~7361t @99t/s)
2025-12-16 09:08:13,495 - src.llm.client - INFO - [int:262159] ğŸ“Š 76.3s: 30259c @397c/s (5253ch, ~7565t @99t/s)
2025-12-16 09:08:15,509 - src.llm.client - INFO - [int:262159] ğŸ“Š 78.3s: 31054c @397c/s (5393ch, ~7764t @99t/s)
2025-12-16 09:08:17,517 - src.llm.client - INFO - [int:262159] ğŸ“Š 80.3s: 31869c @397c/s (5532ch, ~7967t @99t/s)
2025-12-16 09:08:19,518 - src.llm.client - INFO - [int:262159] ğŸ“Š 82.3s: 32653c @397c/s (5670ch, ~8163t @99t/s)
2025-12-16 09:08:21,525 - src.llm.client - INFO - [int:262159] ğŸ“Š 84.3s: 33463c @397c/s (5809ch, ~8366t @99t/s)
2025-12-16 09:08:23,527 - src.llm.client - INFO - [int:262159] ğŸ“Š 86.3s: 34258c @397c/s (5948ch, ~8564t @99t/s)
2025-12-16 09:08:25,533 - src.llm.client - INFO - [int:262159] ğŸ“Š 88.3s: 35067c @397c/s (6087ch, ~8767t @99t/s)
2025-12-16 09:08:27,539 - src.llm.client - INFO - [int:262159] ğŸ“Š 90.3s: 35864c @397c/s (6226ch, ~8966t @99t/s)
2025-12-16 09:08:29,548 - src.llm.client - INFO - [int:262159] ğŸ“Š 92.3s: 36679c @397c/s (6366ch, ~9170t @99t/s)
2025-12-16 09:08:31,555 - src.llm.client - INFO - [int:262159] ğŸ“Š 94.3s: 37487c @397c/s (6506ch, ~9372t @99t/s)
2025-12-16 09:08:33,566 - src.llm.client - INFO - [int:262159] ğŸ“Š 96.3s: 38286c @397c/s (6646ch, ~9572t @99t/s)
2025-12-16 09:08:35,576 - src.llm.client - INFO - [int:262159] ğŸ“Š 98.3s: 39098c @398c/s (6786ch, ~9774t @99t/s)
2025-12-16 09:08:37,584 - src.llm.client - INFO - [int:262159] ğŸ“Š 100.3s: 39893c @398c/s (6926ch, ~9973t @99t/s)
2025-12-16 09:08:39,586 - src.llm.client - INFO - [int:262159] ğŸ“Š 102.4s: 40717c @398c/s (7066ch, ~10179t @99t/s)
2025-12-16 09:08:41,590 - src.llm.client - INFO - [int:262159] ğŸ“Š 104.4s: 41512c @398c/s (7206ch, ~10378t @99t/s)
2025-12-16 09:08:43,595 - src.llm.client - INFO - [int:262159] ğŸ“Š 106.4s: 42321c @398c/s (7346ch, ~10580t @99t/s)
2025-12-16 09:08:45,602 - src.llm.client - INFO - [int:262159] ğŸ“Š 108.4s: 43123c @398c/s (7485ch, ~10781t @99t/s)
2025-12-16 09:08:47,614 - src.llm.client - INFO - [int:262159] ğŸ“Š 110.4s: 43911c @398c/s (7622ch, ~10978t @99t/s)
2025-12-16 09:08:49,616 - src.llm.client - INFO - [int:262159] ğŸ“Š 112.4s: 44701c @398c/s (7759ch, ~11175t @99t/s)
2025-12-16 09:08:51,629 - src.llm.client - INFO - [int:262159] ğŸ“Š 114.4s: 45497c @398c/s (7897ch, ~11374t @99t/s)
2025-12-16 09:08:53,643 - src.llm.client - INFO - [int:262159] ğŸ“Š 116.4s: 46273c @398c/s (8033ch, ~11568t @99t/s)
2025-12-16 09:08:55,651 - src.llm.client - INFO - [int:262159] ğŸ“Š 118.4s: 47064c @397c/s (8169ch, ~11766t @99t/s)
2025-12-16 09:08:57,662 - src.llm.client - INFO - [int:262159] ğŸ“Š 120.4s: 47850c @397c/s (8307ch, ~11962t @99t/s)
2025-12-16 09:08:59,663 - src.llm.client - INFO - [int:262159] ğŸ“Š 122.4s: 48667c @398c/s (8446ch, ~12167t @99t/s)
2025-12-16 09:09:01,672 - src.llm.client - INFO - [int:262159] ğŸ“Š 124.4s: 49462c @397c/s (8586ch, ~12366t @99t/s)
2025-12-16 09:09:03,681 - src.llm.client - INFO - [int:262159] ğŸ“Š 126.4s: 50271c @398c/s (8726ch, ~12568t @99t/s)
2025-12-16 09:09:05,684 - src.llm.client - INFO - [int:262159] ğŸ“Š 128.4s: 51080c @398c/s (8866ch, ~12770t @99t/s)
2025-12-16 09:09:07,696 - src.llm.client - INFO - [int:262159] ğŸ“Š 130.5s: 51883c @398c/s (9005ch, ~12971t @99t/s)
2025-12-16 09:09:09,702 - src.llm.client - INFO - [int:262159] ğŸ“Š 132.5s: 52672c @398c/s (9144ch, ~13168t @99t/s)
2025-12-16 09:09:11,705 - src.llm.client - INFO - [int:262159] ğŸ“Š 134.5s: 53480c @398c/s (9282ch, ~13370t @99t/s)
2025-12-16 09:09:13,713 - src.llm.client - INFO - [int:262159] ğŸ“Š 136.5s: 54263c @398c/s (9421ch, ~13566t @99t/s)
2025-12-16 09:09:15,718 - src.llm.client - INFO - [int:262159] ğŸ“Š 138.5s: 55087c @398c/s (9560ch, ~13772t @99t/s)
2025-12-16 09:09:17,720 - src.llm.client - INFO - [int:262159] ğŸ“Š 140.5s: 55866c @398c/s (9699ch, ~13966t @99t/s)
2025-12-16 09:09:19,733 - src.llm.client - INFO - [int:262159] ğŸ“Š 142.5s: 56706c @398c/s (9839ch, ~14176t @99t/s)
2025-12-16 09:09:21,747 - src.llm.client - INFO - [int:262159] ğŸ“Š 144.5s: 57475c @398c/s (9978ch, ~14369t @99t/s)
2025-12-16 09:09:23,758 - src.llm.client - INFO - [int:262159] ğŸ“Š 146.5s: 58302c @398c/s (10116ch, ~14576t @99t/s)
2025-12-16 09:09:25,769 - src.llm.client - INFO - [int:262159] ğŸ“Š 148.5s: 59046c @398c/s (10251ch, ~14762t @99t/s)
2025-12-16 09:09:27,775 - src.llm.client - INFO - [int:262159] ğŸ“Š 150.5s: 59840c @398c/s (10386ch, ~14960t @99t/s)
2025-12-16 09:09:29,777 - src.llm.client - INFO - [int:262159] ğŸ“Š 152.5s: 60601c @397c/s (10519ch, ~15150t @99t/s)
2025-12-16 09:09:31,791 - src.llm.client - INFO - [int:262159] ğŸ“Š 154.6s: 61370c @397c/s (10652ch, ~15342t @99t/s)
2025-12-16 09:09:33,794 - src.llm.client - INFO - [int:262159] ğŸ“Š 156.6s: 62119c @397c/s (10782ch, ~15530t @99t/s)
2025-12-16 09:09:35,797 - src.llm.client - INFO - [int:262159] ğŸ“Š 158.6s: 62848c @396c/s (10912ch, ~15712t @99t/s)
2025-12-16 09:09:37,801 - src.llm.client - INFO - [int:262159] ğŸ“Š 160.6s: 63639c @396c/s (11048ch, ~15910t @99t/s)
2025-12-16 09:09:39,808 - src.llm.client - INFO - [int:262159] ğŸ“Š 162.6s: 64437c @396c/s (11187ch, ~16109t @99t/s)
2025-12-16 09:09:41,822 - src.llm.client - INFO - [int:262159] ğŸ“Š 164.6s: 65213c @396c/s (11322ch, ~16303t @99t/s)
2025-12-16 09:09:43,826 - src.llm.client - INFO - [int:262159] ğŸ“Š 166.6s: 65975c @396c/s (11454ch, ~16494t @99t/s)
2025-12-16 09:09:45,829 - src.llm.client - INFO - [int:262159] ğŸ“Š 168.6s: 66737c @396c/s (11585ch, ~16684t @99t/s)
2025-12-16 09:09:47,834 - src.llm.client - INFO - [int:262159] ğŸ“Š 170.6s: 67466c @395c/s (11711ch, ~16866t @99t/s)
2025-12-16 09:09:49,834 - src.llm.client - INFO - [int:262159] ğŸ“Š 172.6s: 68208c @395c/s (11839ch, ~17052t @99t/s)
2025-12-16 09:09:51,847 - src.llm.client - INFO - [int:262159] ğŸ“Š 174.6s: 68916c @395c/s (11965ch, ~17229t @99t/s)
2025-12-16 09:09:53,859 - src.llm.client - INFO - [int:262159] ğŸ“Š 176.6s: 69641c @394c/s (12091ch, ~17410t @99t/s)
2025-12-16 09:09:55,870 - src.llm.client - INFO - [int:262159] ğŸ“Š 178.6s: 70352c @394c/s (12213ch, ~17588t @98t/s)
2025-12-16 09:09:57,246 - src.llm.client - INFO - [int:262159] Stream making progress - extending timeout by 75.0s (new limit: 300.0s, max: 300.0s)
2025-12-16 09:09:57,875 - src.llm.client - INFO - [int:262159] ğŸ“Š 180.6s: 71102c @394c/s (12340ch, ~17776t @98t/s)
2025-12-16 09:09:59,888 - src.llm.client - INFO - [int:262159] ğŸ“Š 182.7s: 71827c @393c/s (12469ch, ~17957t @98t/s)
2025-12-16 09:10:01,892 - src.llm.client - INFO - [int:262159] ğŸ“Š 184.7s: 72612c @393c/s (12600ch, ~18153t @98t/s)
2025-12-16 09:10:03,899 - src.llm.client - INFO - [int:262159] ğŸ“Š 186.7s: 73341c @393c/s (12731ch, ~18335t @98t/s)
2025-12-16 09:10:05,909 - src.llm.client - INFO - [int:262159] ğŸ“Š 188.7s: 74116c @393c/s (12864ch, ~18529t @98t/s)
2025-12-16 09:10:07,916 - src.llm.client - INFO - [int:262159] ğŸ“Š 190.7s: 74866c @393c/s (12997ch, ~18716t @98t/s)
2025-12-16 09:10:09,928 - src.llm.client - INFO - [int:262159] ğŸ“Š 192.7s: 75647c @393c/s (13130ch, ~18912t @98t/s)
2025-12-16 09:10:11,931 - src.llm.client - INFO - [int:262159] ğŸ“Š 194.7s: 76407c @392c/s (13263ch, ~19102t @98t/s)
2025-12-16 09:10:13,943 - src.llm.client - INFO - [int:262159] ğŸ“Š 196.7s: 77107c @392c/s (13388ch, ~19277t @98t/s)
2025-12-16 09:10:15,955 - src.llm.client - INFO - [int:262159] ğŸ“Š 198.7s: 77854c @392c/s (13513ch, ~19464t @98t/s)
2025-12-16 09:10:17,972 - src.llm.client - INFO - [int:262159] ğŸ“Š 200.7s: 78551c @391c/s (13636ch, ~19638t @98t/s)
2025-12-16 09:10:19,973 - src.llm.client - INFO - [int:262159] ğŸ“Š 202.7s: 79231c @391c/s (13756ch, ~19808t @98t/s)
2025-12-16 09:10:21,979 - src.llm.client - INFO - [int:262159] ğŸ“Š 204.7s: 79990c @391c/s (13885ch, ~19998t @98t/s)
2025-12-16 09:10:23,985 - src.llm.client - INFO - [int:262159] ğŸ“Š 206.7s: 80727c @390c/s (14017ch, ~20182t @98t/s)
2025-12-16 09:10:25,994 - src.llm.client - INFO - [int:262159] ğŸ“Š 208.8s: 81528c @391c/s (14152ch, ~20382t @98t/s)
2025-12-16 09:10:28,005 - src.llm.client - INFO - [int:262159] ğŸ“Š 210.8s: 82289c @390c/s (14287ch, ~20572t @98t/s)
2025-12-16 09:10:30,018 - src.llm.client - INFO - [int:262159] ğŸ“Š 212.8s: 83095c @391c/s (14425ch, ~20774t @98t/s)
2025-12-16 09:10:32,033 - src.llm.client - INFO - [int:262159] ğŸ“Š 214.8s: 83879c @391c/s (14563ch, ~20970t @98t/s)
2025-12-16 09:10:34,040 - src.llm.client - INFO - [int:262159] ğŸ“Š 216.8s: 84670c @391c/s (14698ch, ~21168t @98t/s)
2025-12-16 09:10:36,043 - src.llm.client - INFO - [int:262159] ğŸ“Š 218.8s: 85445c @391c/s (14833ch, ~21361t @98t/s)
2025-12-16 09:10:38,051 - src.llm.client - INFO - [int:262159] ğŸ“Š 220.8s: 86225c @390c/s (14969ch, ~21556t @98t/s)
2025-12-16 09:10:40,051 - src.llm.client - INFO - [int:262159] ğŸ“Š 222.8s: 87034c @391c/s (15108ch, ~21758t @98t/s)
2025-12-16 09:10:42,060 - src.llm.client - INFO - [int:262159] ğŸ“Š 224.8s: 87807c @391c/s (15244ch, ~21952t @98t/s)
2025-12-16 09:10:44,074 - src.llm.client - INFO - [int:262159] ğŸ“Š 226.8s: 88615c @391c/s (15381ch, ~22154t @98t/s)
2025-12-16 09:10:46,075 - src.llm.client - INFO - [int:262159] ğŸ“Š 228.8s: 89377c @391c/s (15517ch, ~22344t @98t/s)
2025-12-16 09:10:48,080 - src.llm.client - INFO - [int:262159] ğŸ“Š 230.8s: 90193c @391c/s (15655ch, ~22548t @98t/s)
2025-12-16 09:10:50,082 - src.llm.client - INFO - [int:262159] ğŸ“Š 232.8s: 90948c @391c/s (15791ch, ~22737t @98t/s)
2025-12-16 09:10:52,092 - src.llm.client - INFO - [int:262159] ğŸ“Š 234.9s: 91763c @391c/s (15926ch, ~22941t @98t/s)
2025-12-16 09:10:54,104 - src.llm.client - INFO - [int:262159] ğŸ“Š 236.9s: 92489c @390c/s (16056ch, ~23122t @98t/s)
2025-12-16 09:10:56,118 - src.llm.client - INFO - [int:262159] ğŸ“Š 238.9s: 93282c @390c/s (16188ch, ~23320t @98t/s)
2025-12-16 09:10:58,118 - src.llm.client - INFO - [int:262159] ğŸ“Š 240.9s: 94011c @390c/s (16319ch, ~23503t @98t/s)
2025-12-16 09:11:00,132 - src.llm.client - INFO - [int:262159] ğŸ“Š 242.9s: 94787c @390c/s (16453ch, ~23697t @98t/s)
2025-12-16 09:11:02,145 - src.llm.client - INFO - [int:262159] ğŸ“Š 244.9s: 95572c @390c/s (16590ch, ~23893t @98t/s)
2025-12-16 09:11:04,155 - src.llm.client - INFO - [int:262159] ğŸ“Š 246.9s: 96368c @390c/s (16727ch, ~24092t @98t/s)
2025-12-16 09:11:06,169 - src.llm.client - INFO - [int:262159] ğŸ“Š 248.9s: 97144c @390c/s (16864ch, ~24286t @98t/s)
2025-12-16 09:11:08,170 - src.llm.client - INFO - [int:262159] ğŸ“Š 250.9s: 97929c @390c/s (16999ch, ~24482t @98t/s)
2025-12-16 09:11:10,173 - src.llm.client - INFO - [int:262159] ğŸ“Š 252.9s: 98713c @390c/s (17136ch, ~24678t @98t/s)
2025-12-16 09:11:12,177 - src.llm.client - INFO - [int:262159] ğŸ“Š 254.9s: 99517c @390c/s (17273ch, ~24879t @98t/s)
2025-12-16 09:11:14,190 - src.llm.client - INFO - [int:262159] ğŸ“Š 257.0s: 100296c @390c/s (17410ch, ~25074t @98t/s)
2025-12-16 09:11:16,197 - src.llm.client - INFO - [int:262159] ğŸ“Š 259.0s: 101074c @390c/s (17544ch, ~25268t @98t/s)
2025-12-16 09:11:18,201 - src.llm.client - INFO - [int:262159] ğŸ“Š 261.0s: 101847c @390c/s (17679ch, ~25462t @98t/s)
2025-12-16 09:11:20,210 - src.llm.client - INFO - [int:262159] ğŸ“Š 263.0s: 102584c @390c/s (17809ch, ~25646t @98t/s)
2025-12-16 09:11:22,213 - src.llm.client - INFO - [int:262159] ğŸ“Š 265.0s: 103327c @390c/s (17939ch, ~25832t @97t/s)
2025-12-16 09:11:24,227 - src.llm.client - INFO - [int:262159] ğŸ“Š 267.0s: 104114c @390c/s (18073ch, ~26028t @97t/s)
2025-12-16 09:11:26,230 - src.llm.client - INFO - [int:262159] ğŸ“Š 269.0s: 104895c @390c/s (18207ch, ~26224t @97t/s)
2025-12-16 09:11:28,233 - src.llm.client - INFO - [int:262159] ğŸ“Š 271.0s: 105649c @390c/s (18338ch, ~26412t @97t/s)
2025-12-16 09:11:30,246 - src.llm.client - INFO - [int:262159] ğŸ“Š 273.0s: 106416c @390c/s (18473ch, ~26604t @97t/s)
2025-12-16 09:11:32,248 - src.llm.client - INFO - [int:262159] ğŸ“Š 275.0s: 107216c @390c/s (18611ch, ~26804t @97t/s)
2025-12-16 09:11:34,258 - src.llm.client - INFO - [int:262159] ğŸ“Š 277.0s: 108003c @390c/s (18748ch, ~27001t @97t/s)
2025-12-16 09:11:36,264 - src.llm.client - INFO - [int:262159] ğŸ“Š 279.0s: 108803c @390c/s (18886ch, ~27201t @97t/s)
2025-12-16 09:11:38,273 - src.llm.client - INFO - [int:262159] ğŸ“Š 281.0s: 109593c @390c/s (19024ch, ~27398t @97t/s)
2025-12-16 09:11:40,279 - src.llm.client - INFO - [int:262159] ğŸ“Š 283.0s: 110384c @390c/s (19161ch, ~27596t @97t/s)
2025-12-16 09:11:42,292 - src.llm.client - INFO - [int:262159] ğŸ“Š 285.1s: 111183c @390c/s (19300ch, ~27796t @98t/s)
2025-12-16 09:11:44,298 - src.llm.client - INFO - [int:262159] ğŸ“Š 287.1s: 111983c @390c/s (19438ch, ~27996t @98t/s)
2025-12-16 09:11:46,302 - src.llm.client - INFO - [int:262159] ğŸ“Š 289.1s: 112757c @390c/s (19573ch, ~28189t @98t/s)
2025-12-16 09:11:48,311 - src.llm.client - INFO - [int:262159] ğŸ“Š 291.1s: 113526c @390c/s (19707ch, ~28382t @98t/s)
2025-12-16 09:11:50,318 - src.llm.client - INFO - [int:262159] ğŸ“Š 293.1s: 114308c @390c/s (19841ch, ~28577t @98t/s)
2025-12-16 09:11:52,328 - src.llm.client - INFO - [int:262159] ğŸ“Š 295.1s: 115066c @390c/s (19975ch, ~28766t @97t/s)
2025-12-16 09:11:54,331 - src.llm.client - INFO - [int:262159] ğŸ“Š 297.1s: 115793c @390c/s (20102ch, ~28948t @97t/s)
2025-12-16 09:11:56,341 - src.llm.client - INFO - [int:262159] ğŸ“Š 299.1s: 116563c @390c/s (20235ch, ~29141t @97t/s)
2025-12-16 09:11:57,248 - src.llm.client - ERROR - [int:262159] Stream timeout: 300.01s elapsed (limit: 300.00s, base timeout: 150.00s). Operation: integration. Received 20295 chunks, 1964546 bytes, 116907 chars (~29227 tokens) before timeout. Performance: 389.7 chars/s, ~97.4 tok/s. Generation was slow (389.7 chars/s, ~97.4 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.
2025-12-16 09:11:57,250 - generate_secondary - ERROR -   âœ— Module 7 Session 7 - integration generation failed (Request ID: int:262159)
2025-12-16 09:11:57,250 - generate_secondary - ERROR -      Error: [int:262159] Stream timeout: 300.01s elapsed (limit: 300.00s, base timeout: 150.00s). Operation: integration. Received 20295 chunks, 1964546 bytes, 116907 chars (~29227 tokens) before timeout. Performance: 389.7 chars/s, ~97.4 tok/s. Generation was slow (389.7 chars/s, ~97.4 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.
2025-12-16 09:11:57,250 - generate_secondary - ERROR -      Type: Timeout error (operation timeout: 150s)
2025-12-16 09:11:57,250 - generate_secondary - ERROR -      Suggestion: Check logs for request ID int:262159 if available, or increase timeout in config
2025-12-16 09:11:57,250 - generate_secondary - ERROR -   âœ— Timeout for session 7: Markov Property
2025-12-16 09:11:57,250 - generate_secondary - ERROR -      Module: Markov Models â€“ Introduction & State Spaces (ID: 7)
2025-12-16 09:11:57,251 - generate_secondary - ERROR -      Request ID: int:262159 (filter logs: grep '[int:262159]' output/logs/*.log)
2025-12-16 09:11:57,251 - generate_secondary - ERROR -      Error: [int:262159] Stream timeout: 300.01s elapsed (limit: 300.00s, base timeout: 150.00s). Operation: integration. Received 20295 chunks, 1964546 bytes, 116907 chars (~29227 tokens) before timeout. Performance: 389.7 chars/s, ~97.4 tok/s. Generation was slow (389.7 chars/s, ~97.4 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.
2025-12-16 09:11:57,251 - generate_secondary - ERROR -      Troubleshooting: See docs/TROUBLESHOOTING.md for timeout resolution steps
2025-12-16 09:11:57,252 - generate_secondary - INFO - 
============================================================
2025-12-16 09:11:57,252 - generate_secondary - INFO - [8/15] Module 8: Probabilistic State-Space Models â€“ Formulation (1 sessions)
2025-12-16 09:11:57,252 - generate_secondary - INFO - ============================================================
2025-12-16 09:11:57,252 - generate_secondary - INFO - 
  Session 8/15: Model Equations
2025-12-16 09:11:57,259 - generate_secondary - INFO - Generating application for session 8: Model Equations...
2025-12-16 09:11:57,260 - src.llm.client - INFO - [app:1844f1] ğŸš€ app | m=gemma3:4b | p=30878c | t=150s
2025-12-16 09:11:57,260 - src.llm.client - INFO - [app:1844f1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:11:57,260 - src.llm.client - INFO - [app:1844f1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:11:57,263 - src.llm.client - INFO - [app:1844f1] Sending request to Ollama: model=gemma3:4b, operation=application, payload=33014 bytes, prompt=30878 chars
2025-12-16 09:11:57,265 - src.llm.client - INFO - [app:1844f1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:12:02,066 - src.llm.request_handler - INFO - [app:1844f1] âœ“ Done 4.80s
2025-12-16 09:12:02,067 - src.llm.client - INFO - [app:1844f1] âœ… HTTP 200 in 4.80s
2025-12-16 09:12:02,067 - src.llm.client - INFO - [app:1844f1] ğŸ“¡ Stream active (200)
2025-12-16 09:12:02,067 - src.llm.client - INFO - [app:1844f1] Starting stream parsing, waiting for first chunk...
2025-12-16 09:12:04,074 - src.llm.client - INFO - [app:1844f1] ğŸ“Š 2.0s: 791c @394c/s (136ch, ~198t @99t/s)
2025-12-16 09:12:06,084 - src.llm.client - INFO - [app:1844f1] ğŸ“Š 4.0s: 1627c @405c/s (271ch, ~407t @101t/s)
2025-12-16 09:12:08,088 - src.llm.client - INFO - [app:1844f1] ğŸ“Š 6.0s: 2444c @406c/s (405ch, ~611t @101t/s)
2025-12-16 09:12:10,092 - src.llm.client - INFO - [app:1844f1] ğŸ“Š 8.0s: 3291c @410c/s (539ch, ~823t @103t/s)
2025-12-16 09:12:12,098 - src.llm.client - INFO - [app:1844f1] ğŸ“Š 10.0s: 4090c @408c/s (674ch, ~1022t @102t/s)
2025-12-16 09:12:14,110 - src.llm.client - INFO - [app:1844f1] ğŸ“Š 12.0s: 4902c @407c/s (811ch, ~1226t @102t/s)
2025-12-16 09:12:16,255 - src.llm.client - INFO - [app:1844f1] ğŸ“Š 14.2s: 5602c @395c/s (929ch, ~1400t @99t/s)
2025-12-16 09:12:16,256 - src.llm.client - INFO - [app:1844f1] âœ“ Done 19.00s: 5602c (~710w @295c/s)
2025-12-16 09:12:16,258 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:12:16,258 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:12:16,258 - generate_secondary - INFO -     - Length: 5590 chars, 708 words
2025-12-16 09:12:16,258 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:12:16,258 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:12:16,258 - generate_secondary - INFO -     - Avg words per application: 136
2025-12-16 09:12:16,258 - generate_secondary - WARNING - [WARNING] Application 1 has 142 words (require 150-200, need 8 more words) âš ï¸
2025-12-16 09:12:16,258 - generate_secondary - WARNING - [WARNING] Application 2 has 139 words (require 150-200, need 11 more words) âš ï¸
2025-12-16 09:12:16,258 - generate_secondary - WARNING - [WARNING] Application 3 has 128 words (require 150-200, need 22 more words) âš ï¸
2025-12-16 09:12:16,258 - generate_secondary - WARNING - [WARNING] Application 4 has 139 words (require 150-200, need 11 more words) âš ï¸
2025-12-16 09:12:16,258 - generate_secondary - WARNING - [WARNING] Application 5 has 134 words (require 150-200, need 16 more words) âš ï¸
2025-12-16 09:12:16,259 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_08_probabilistic_state_space_models_formulation/session_08/application.md
2025-12-16 09:12:16,259 - generate_secondary - INFO - Generating extension for session 8: Model Equations...
2025-12-16 09:12:16,259 - src.llm.client - INFO - [ext:a7521a] ğŸš€ ext | m=gemma3:4b | p=24101c | t=120s
2025-12-16 09:12:16,259 - src.llm.client - INFO - [ext:a7521a] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:12:16,259 - src.llm.client - INFO - [ext:a7521a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:12:16,261 - src.llm.client - INFO - [ext:a7521a] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=29092 bytes, prompt=24101 chars
2025-12-16 09:12:16,261 - src.llm.client - INFO - [ext:a7521a] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:12:21,001 - src.llm.request_handler - INFO - [ext:a7521a] âœ“ Done 4.74s
2025-12-16 09:12:21,002 - src.llm.client - INFO - [ext:a7521a] âœ… HTTP 200 in 4.74s
2025-12-16 09:12:21,002 - src.llm.client - INFO - [ext:a7521a] ğŸ“¡ Stream active (200)
2025-12-16 09:12:21,002 - src.llm.client - INFO - [ext:a7521a] Starting stream parsing, waiting for first chunk...
2025-12-16 09:12:23,011 - src.llm.client - INFO - [ext:a7521a] ğŸ“Š 2.0s: 815c @406c/s (135ch, ~204t @101t/s)
2025-12-16 09:12:25,012 - src.llm.client - INFO - [ext:a7521a] ğŸ“Š 4.0s: 1572c @392c/s (266ch, ~393t @98t/s)
2025-12-16 09:12:27,027 - src.llm.client - INFO - [ext:a7521a] ğŸ“Š 6.0s: 2402c @399c/s (397ch, ~600t @100t/s)
2025-12-16 09:12:29,033 - src.llm.client - INFO - [ext:a7521a] ğŸ“Š 8.0s: 3231c @402c/s (527ch, ~808t @101t/s)
2025-12-16 09:12:31,041 - src.llm.client - INFO - [ext:a7521a] ğŸ“Š 10.0s: 3964c @395c/s (657ch, ~991t @99t/s)
2025-12-16 09:12:31,418 - src.llm.client - INFO - [ext:a7521a] âœ“ Done 15.16s: 4024c (~530w @265c/s)
2025-12-16 09:12:31,420 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:12:31,420 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:12:31,420 - generate_secondary - INFO -     - Length: 4010 chars, 528 words
2025-12-16 09:12:31,420 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:12:31,420 - generate_secondary - INFO -     - Topics: 3
2025-12-16 09:12:31,420 - generate_secondary - INFO -     - Avg words per topic: 169
2025-12-16 09:12:31,420 - generate_secondary - WARNING - [WARNING] Topic 1 has 179 words (exceeds 150 by 29 words - consider condensing) âš ï¸
2025-12-16 09:12:31,420 - generate_secondary - WARNING - [WARNING] Topic 2 has 156 words (exceeds 150 by 6 words - consider condensing) âš ï¸
2025-12-16 09:12:31,420 - generate_secondary - WARNING - [WARNING] Topic 3 has 171 words (exceeds 150 by 21 words - consider condensing) âš ï¸
2025-12-16 09:12:31,420 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_08_probabilistic_state_space_models_formulation/session_08/extension.md
2025-12-16 09:12:31,420 - generate_secondary - INFO - Generating visualization for session 8: Model Equations...
2025-12-16 09:12:31,420 - src.llm.client - INFO - [viz:271c29] ğŸš€ viz | m=gemma3:4b | p=23061c | t=120s
2025-12-16 09:12:31,420 - src.llm.client - INFO - [viz:271c29] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:12:31,420 - src.llm.client - INFO - [viz:271c29] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:12:31,422 - src.llm.client - INFO - [viz:271c29] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=27374 bytes, prompt=23061 chars
2025-12-16 09:12:31,422 - src.llm.client - INFO - [viz:271c29] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:12:36,332 - src.llm.request_handler - INFO - [viz:271c29] âœ“ Done 4.91s
2025-12-16 09:12:36,332 - src.llm.client - INFO - [viz:271c29] âœ… HTTP 200 in 4.91s
2025-12-16 09:12:36,332 - src.llm.client - INFO - [viz:271c29] ğŸ“¡ Stream active (200)
2025-12-16 09:12:36,332 - src.llm.client - INFO - [viz:271c29] Starting stream parsing, waiting for first chunk...
2025-12-16 09:12:38,337 - src.llm.client - INFO - [viz:271c29] ğŸ“Š 2.0s: 487c @243c/s (125ch, ~122t @61t/s)
2025-12-16 09:12:40,345 - src.llm.client - INFO - [viz:271c29] ğŸ“Š 4.0s: 1034c @258c/s (244ch, ~258t @64t/s)
2025-12-16 09:12:40,831 - src.llm.client - INFO - [viz:271c29] âœ“ Done 9.41s: 1102c (~165w @117c/s)
2025-12-16 09:12:40,831 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-16 09:12:40,831 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 09:12:40,831 - generate_secondary - INFO -     - Length: 106 chars (cleaned: 106 chars)
2025-12-16 09:12:40,831 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:12:40,831 - generate_secondary - INFO - [CRITICAL] Elements: 9 total (nodes: 4, connections: 5) ğŸ”´
2025-12-16 09:12:40,831 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 09:12:40,831 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-16 09:12:40,831 - generate_secondary - WARNING - [WARNING] Only 4 nodes found (require at least 10, need 6 more - add more nodes to the diagram) âš ï¸
2025-12-16 09:12:40,831 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:12:40,832 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:12:40,832 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_08_probabilistic_state_space_models_formulation/session_08/visualization.mmd
2025-12-16 09:12:40,832 - generate_secondary - INFO - Generating integration for session 8: Model Equations...
2025-12-16 09:12:40,832 - src.llm.client - INFO - [int:4351d8] ğŸš€ int | m=gemma3:4b | p=24410c | t=150s
2025-12-16 09:12:40,832 - src.llm.client - INFO - [int:4351d8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:12:40,833 - src.llm.client - INFO - [int:4351d8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:12:40,834 - src.llm.client - INFO - [int:4351d8] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=29740 bytes, prompt=24410 chars
2025-12-16 09:12:40,834 - src.llm.client - INFO - [int:4351d8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:12:45,730 - src.llm.request_handler - INFO - [int:4351d8] âœ“ Done 4.90s
2025-12-16 09:12:45,730 - src.llm.client - INFO - [int:4351d8] âœ… HTTP 200 in 4.90s
2025-12-16 09:12:45,730 - src.llm.client - INFO - [int:4351d8] ğŸ“¡ Stream active (200)
2025-12-16 09:12:45,730 - src.llm.client - INFO - [int:4351d8] Starting stream parsing, waiting for first chunk...
2025-12-16 09:12:47,743 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 2.0s: 736c @366c/s (128ch, ~184t @91t/s)
2025-12-16 09:12:49,754 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 4.0s: 1422c @353c/s (258ch, ~356t @88t/s)
2025-12-16 09:12:51,763 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 6.0s: 2133c @354c/s (387ch, ~533t @88t/s)
2025-12-16 09:12:53,773 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 8.0s: 2840c @353c/s (514ch, ~710t @88t/s)
2025-12-16 09:12:55,785 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 10.1s: 3519c @350c/s (636ch, ~880t @87t/s)
2025-12-16 09:12:57,794 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 12.1s: 4232c @351c/s (765ch, ~1058t @88t/s)
2025-12-16 09:12:59,808 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 14.1s: 4863c @345c/s (894ch, ~1216t @86t/s)
2025-12-16 09:13:01,928 - src.llm.client - INFO - [int:4351d8] ğŸ“Š 16.2s: 5392c @333c/s (1010ch, ~1348t @83t/s)
2025-12-16 09:13:01,929 - src.llm.client - INFO - [int:4351d8] âœ“ Done 21.10s: 5392c (~748w @256c/s)
2025-12-16 09:13:01,931 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:13:01,932 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:13:01,932 - generate_secondary - INFO -     - Length: 5368 chars, 745 words
2025-12-16 09:13:01,932 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:13:01,932 - generate_secondary - INFO -     - Connections: 33
2025-12-16 09:13:01,932 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:13:01,933 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_08_probabilistic_state_space_models_formulation/session_08/integration.md
2025-12-16 09:13:01,933 - generate_secondary - INFO - Generating investigation for session 8: Model Equations...
2025-12-16 09:13:01,933 - src.llm.client - INFO - [inv:edf3eb] ğŸš€ inv | m=gemma3:4b | p=23323c | t=150s
2025-12-16 09:13:01,933 - src.llm.client - INFO - [inv:edf3eb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:13:01,933 - src.llm.client - INFO - [inv:edf3eb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:13:01,935 - src.llm.client - INFO - [inv:edf3eb] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=27596 bytes, prompt=23323 chars
2025-12-16 09:13:01,935 - src.llm.client - INFO - [inv:edf3eb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:13:06,906 - src.llm.request_handler - INFO - [inv:edf3eb] âœ“ Done 4.97s
2025-12-16 09:13:06,906 - src.llm.client - INFO - [inv:edf3eb] âœ… HTTP 200 in 4.97s
2025-12-16 09:13:06,907 - src.llm.client - INFO - [inv:edf3eb] ğŸ“¡ Stream active (200)
2025-12-16 09:13:06,907 - src.llm.client - INFO - [inv:edf3eb] Starting stream parsing, waiting for first chunk...
2025-12-16 09:13:08,913 - src.llm.client - INFO - [inv:edf3eb] ğŸ“Š 2.0s: 707c @352c/s (130ch, ~177t @88t/s)
2025-12-16 09:13:10,919 - src.llm.client - INFO - [inv:edf3eb] ğŸ“Š 4.0s: 1411c @352c/s (259ch, ~353t @88t/s)
2025-12-16 09:13:12,931 - src.llm.client - INFO - [inv:edf3eb] ğŸ“Š 6.0s: 2161c @359c/s (389ch, ~540t @90t/s)
2025-12-16 09:13:14,942 - src.llm.client - INFO - [inv:edf3eb] ğŸ“Š 8.0s: 2738c @341c/s (518ch, ~684t @85t/s)
2025-12-16 09:13:16,944 - src.llm.client - INFO - [inv:edf3eb] ğŸ“Š 10.0s: 3508c @350c/s (646ch, ~877t @87t/s)
2025-12-16 09:13:18,949 - src.llm.client - INFO - [inv:edf3eb] ğŸ“Š 12.0s: 4251c @353c/s (774ch, ~1063t @88t/s)
2025-12-16 09:13:20,949 - src.llm.client - INFO - [inv:edf3eb] ğŸ“Š 14.0s: 4991c @355c/s (902ch, ~1248t @89t/s)
2025-12-16 09:13:22,309 - src.llm.client - INFO - [inv:edf3eb] âœ“ Done 20.38s: 5450c (~765w @267c/s)
2025-12-16 09:13:22,311 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:13:22,311 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:13:22,311 - generate_secondary - INFO -     - Length: 5436 chars, 763 words
2025-12-16 09:13:22,311 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:13:22,311 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:13:22,311 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:13:22,312 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_08_probabilistic_state_space_models_formulation/session_08/investigation.md
2025-12-16 09:13:22,312 - generate_secondary - INFO - Generating open_questions for session 8: Model Equations...
2025-12-16 09:13:22,312 - src.llm.client - INFO - [opq:16adf5] ğŸš€ opq | m=gemma3:4b | p=23409c | t=150s
2025-12-16 09:13:22,312 - src.llm.client - INFO - [opq:16adf5] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:13:22,312 - src.llm.client - INFO - [opq:16adf5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:13:22,313 - src.llm.client - INFO - [opq:16adf5] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=27693 bytes, prompt=23409 chars
2025-12-16 09:13:22,313 - src.llm.client - INFO - [opq:16adf5] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:13:27,228 - src.llm.request_handler - INFO - [opq:16adf5] âœ“ Done 4.91s
2025-12-16 09:13:27,228 - src.llm.client - INFO - [opq:16adf5] âœ… HTTP 200 in 4.91s
2025-12-16 09:13:27,228 - src.llm.client - INFO - [opq:16adf5] ğŸ“¡ Stream active (200)
2025-12-16 09:13:27,228 - src.llm.client - INFO - [opq:16adf5] Starting stream parsing, waiting for first chunk...
2025-12-16 09:13:29,236 - src.llm.client - INFO - [opq:16adf5] ğŸ“Š 2.0s: 724c @361c/s (129ch, ~181t @90t/s)
2025-12-16 09:13:31,244 - src.llm.client - INFO - [opq:16adf5] ğŸ“Š 4.0s: 1396c @348c/s (257ch, ~349t @87t/s)
2025-12-16 09:13:33,247 - src.llm.client - INFO - [opq:16adf5] ğŸ“Š 6.0s: 2167c @360c/s (385ch, ~542t @90t/s)
2025-12-16 09:13:33,848 - src.llm.client - INFO - [opq:16adf5] âœ“ Done 11.54s: 2324c (~301w @201c/s)
2025-12-16 09:13:33,849 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:13:33,849 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:13:33,849 - generate_secondary - INFO -     - Length: 2323 chars, 301 words
2025-12-16 09:13:33,849 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:13:33,849 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:13:33,849 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:13:33,849 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_08_probabilistic_state_space_models_formulation/session_08/open_questions.md
2025-12-16 09:13:33,850 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:13:33,850 - generate_secondary - INFO - 
============================================================
2025-12-16 09:13:33,850 - generate_secondary - INFO - [9/15] Module 9: Precision Weighting & Attention (1 sessions)
2025-12-16 09:13:33,850 - generate_secondary - INFO - ============================================================
2025-12-16 09:13:33,850 - generate_secondary - INFO - 
  Session 9/15: Weighting States
2025-12-16 09:13:33,851 - generate_secondary - INFO - Generating application for session 9: Weighting States...
2025-12-16 09:13:33,851 - src.llm.client - INFO - [app:963d2e] ğŸš€ app | m=gemma3:4b | p=33876c | t=150s
2025-12-16 09:13:33,851 - src.llm.client - INFO - [app:963d2e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:13:33,851 - src.llm.client - INFO - [app:963d2e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:13:33,852 - src.llm.client - INFO - [app:963d2e] Sending request to Ollama: model=gemma3:4b, operation=application, payload=36005 bytes, prompt=33876 chars
2025-12-16 09:13:33,852 - src.llm.client - INFO - [app:963d2e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:13:38,769 - src.llm.request_handler - INFO - [app:963d2e] âœ“ Done 4.92s
2025-12-16 09:13:38,770 - src.llm.client - INFO - [app:963d2e] âœ… HTTP 200 in 4.92s
2025-12-16 09:13:38,770 - src.llm.client - INFO - [app:963d2e] ğŸ“¡ Stream active (200)
2025-12-16 09:13:38,770 - src.llm.client - INFO - [app:963d2e] Starting stream parsing, waiting for first chunk...
2025-12-16 09:13:40,776 - src.llm.client - INFO - [app:963d2e] ğŸ“Š 2.0s: 780c @389c/s (128ch, ~195t @97t/s)
2025-12-16 09:13:42,777 - src.llm.client - INFO - [app:963d2e] ğŸ“Š 4.0s: 1588c @396c/s (257ch, ~397t @99t/s)
2025-12-16 09:13:44,782 - src.llm.client - INFO - [app:963d2e] ğŸ“Š 6.0s: 2359c @392c/s (386ch, ~590t @98t/s)
2025-12-16 09:13:46,782 - src.llm.client - INFO - [app:963d2e] ğŸ“Š 8.0s: 3172c @396c/s (516ch, ~793t @99t/s)
2025-12-16 09:13:48,796 - src.llm.client - INFO - [app:963d2e] ğŸ“Š 10.0s: 3955c @394c/s (646ch, ~989t @99t/s)
2025-12-16 09:13:50,801 - src.llm.client - INFO - [app:963d2e] ğŸ“Š 12.0s: 4751c @395c/s (775ch, ~1188t @99t/s)
2025-12-16 09:13:52,058 - src.llm.client - INFO - [app:963d2e] âœ“ Done 18.21s: 5154c (~671w @283c/s)
2025-12-16 09:13:52,060 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:13:52,060 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:13:52,060 - generate_secondary - INFO -     - Length: 5142 chars, 669 words
2025-12-16 09:13:52,060 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:13:52,060 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:13:52,060 - generate_secondary - INFO -     - Avg words per application: 129
2025-12-16 09:13:52,060 - generate_secondary - WARNING - [WARNING] Application 1 has 135 words (require 150-200, need 15 more words) âš ï¸
2025-12-16 09:13:52,060 - generate_secondary - WARNING - [WARNING] Application 2 has 134 words (require 150-200, need 16 more words) âš ï¸
2025-12-16 09:13:52,060 - generate_secondary - WARNING - [WARNING] Application 3 has 119 words (require 150-200, need 31 more words) âš ï¸
2025-12-16 09:13:52,060 - generate_secondary - WARNING - [WARNING] Application 4 has 117 words (require 150-200, need 33 more words) âš ï¸
2025-12-16 09:13:52,060 - generate_secondary - WARNING - [WARNING] Application 5 has 141 words (require 150-200, need 9 more words) âš ï¸
2025-12-16 09:13:52,061 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_09_precision_weighting_attention/session_09/application.md
2025-12-16 09:13:52,061 - generate_secondary - INFO - Generating extension for session 9: Weighting States...
2025-12-16 09:13:52,061 - src.llm.client - INFO - [ext:79204e] ğŸš€ ext | m=gemma3:4b | p=27099c | t=120s
2025-12-16 09:13:52,061 - src.llm.client - INFO - [ext:79204e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:13:52,061 - src.llm.client - INFO - [ext:79204e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:13:52,062 - src.llm.client - INFO - [ext:79204e] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32083 bytes, prompt=27099 chars
2025-12-16 09:13:52,062 - src.llm.client - INFO - [ext:79204e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:13:56,980 - src.llm.request_handler - INFO - [ext:79204e] âœ“ Done 4.92s
2025-12-16 09:13:56,980 - src.llm.client - INFO - [ext:79204e] âœ… HTTP 200 in 4.92s
2025-12-16 09:13:56,980 - src.llm.client - INFO - [ext:79204e] ğŸ“¡ Stream active (200)
2025-12-16 09:13:56,980 - src.llm.client - INFO - [ext:79204e] Starting stream parsing, waiting for first chunk...
2025-12-16 09:13:58,987 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 2.0s: 798c @398c/s (128ch, ~200t @99t/s)
2025-12-16 09:14:00,998 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 4.0s: 1635c @407c/s (258ch, ~409t @102t/s)
2025-12-16 09:14:03,012 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 6.0s: 2476c @410c/s (389ch, ~619t @103t/s)
2025-12-16 09:14:05,018 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 8.0s: 3268c @407c/s (519ch, ~817t @102t/s)
2025-12-16 09:14:07,027 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 10.0s: 4046c @403c/s (649ch, ~1012t @101t/s)
2025-12-16 09:14:09,034 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 12.1s: 4576c @380c/s (778ch, ~1144t @95t/s)
2025-12-16 09:14:11,044 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 14.1s: 4958c @353c/s (908ch, ~1240t @88t/s)
2025-12-16 09:14:13,050 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 16.1s: 5441c @339c/s (1038ch, ~1360t @85t/s)
2025-12-16 09:14:15,050 - src.llm.client - INFO - [ext:79204e] ğŸ“Š 18.1s: 5948c @329c/s (1166ch, ~1487t @82t/s)
2025-12-16 09:14:15,968 - src.llm.client - INFO - [ext:79204e] âœ“ Done 23.91s: 6077c (~790w @254c/s)
2025-12-16 09:14:15,970 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:14:15,971 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - INFO -     - Length: 6061 chars, 788 words
2025-12-16 09:14:15,971 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:14:15,971 - generate_secondary - INFO -     - Topics: 14
2025-12-16 09:14:15,971 - generate_secondary - INFO -     - Avg words per topic: 54
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Too many topics (14, maximum 4, 10 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 1 has 176 words (exceeds 150 by 26 words - consider condensing) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 2 has 175 words (exceeds 150 by 25 words - consider condensing) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 3 has 216 words (exceeds 150 by 66 words - consider condensing) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 4 has 8 words (require 100-150, need 92 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 5 has 10 words (require 100-150, need 90 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 6 has 23 words (require 100-150, need 77 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 7 has 47 words (require 100-150, need 53 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 8 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 9 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 10 has 75 words (require 100-150, need 25 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 11 has 13 words (require 100-150, need 87 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 12 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 13 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Topic 14 has 3 words (require 100-150, need 97 more words) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - WARNING - [WARNING] Total word count (788) exceeds maximum 600 (exceeds by 188 words - condense content) âš ï¸
2025-12-16 09:14:15,971 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:14:15,971 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:14:15,971 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_09_precision_weighting_attention/session_09/extension.md
2025-12-16 09:14:15,971 - generate_secondary - INFO - Generating visualization for session 9: Weighting States...
2025-12-16 09:14:15,972 - src.llm.client - INFO - [viz:0231a1] ğŸš€ viz | m=gemma3:4b | p=26059c | t=120s
2025-12-16 09:14:15,972 - src.llm.client - INFO - [viz:0231a1] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:14:15,972 - src.llm.client - INFO - [viz:0231a1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:14:15,974 - src.llm.client - INFO - [viz:0231a1] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30365 bytes, prompt=26059 chars
2025-12-16 09:14:15,974 - src.llm.client - INFO - [viz:0231a1] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:14:20,891 - src.llm.request_handler - INFO - [viz:0231a1] âœ“ Done 4.92s
2025-12-16 09:14:20,892 - src.llm.client - INFO - [viz:0231a1] âœ… HTTP 200 in 4.92s
2025-12-16 09:14:20,892 - src.llm.client - INFO - [viz:0231a1] ğŸ“¡ Stream active (200)
2025-12-16 09:14:20,893 - src.llm.client - INFO - [viz:0231a1] Starting stream parsing, waiting for first chunk...
2025-12-16 09:14:22,897 - src.llm.client - INFO - [viz:0231a1] ğŸ“Š 2.0s: 450c @225c/s (130ch, ~112t @56t/s)
2025-12-16 09:14:23,693 - src.llm.client - INFO - [viz:0231a1] âœ“ Done 7.72s: 571c (~90w @74c/s)
2025-12-16 09:14:23,694 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:14:23,694 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:14:23,694 - generate_secondary - INFO -     - Length: 555 chars (cleaned: 555 chars)
2025-12-16 09:14:23,694 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:14:23,694 - generate_secondary - INFO - [OK] Elements: 35 total (nodes: 12, connections: 23) âœ“
2025-12-16 09:14:23,694 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_09_precision_weighting_attention/session_09/visualization.mmd
2025-12-16 09:14:23,694 - generate_secondary - INFO - Generating integration for session 9: Weighting States...
2025-12-16 09:14:23,694 - src.llm.client - INFO - [int:7bd246] ğŸš€ int | m=gemma3:4b | p=27408c | t=150s
2025-12-16 09:14:23,695 - src.llm.client - INFO - [int:7bd246] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:14:23,695 - src.llm.client - INFO - [int:7bd246] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:14:23,696 - src.llm.client - INFO - [int:7bd246] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=32731 bytes, prompt=27408 chars
2025-12-16 09:14:23,696 - src.llm.client - INFO - [int:7bd246] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:14:28,588 - src.llm.request_handler - INFO - [int:7bd246] âœ“ Done 4.89s
2025-12-16 09:14:28,589 - src.llm.client - INFO - [int:7bd246] âœ… HTTP 200 in 4.89s
2025-12-16 09:14:28,589 - src.llm.client - INFO - [int:7bd246] ğŸ“¡ Stream active (200)
2025-12-16 09:14:28,589 - src.llm.client - INFO - [int:7bd246] Starting stream parsing, waiting for first chunk...
2025-12-16 09:14:30,591 - src.llm.client - INFO - [int:7bd246] ğŸ“Š 2.0s: 776c @388c/s (130ch, ~194t @97t/s)
2025-12-16 09:14:32,597 - src.llm.client - INFO - [int:7bd246] ğŸ“Š 4.0s: 1465c @366c/s (259ch, ~366t @91t/s)
2025-12-16 09:14:34,597 - src.llm.client - INFO - [int:7bd246] ğŸ“Š 6.0s: 1912c @318c/s (388ch, ~478t @80t/s)
2025-12-16 09:14:36,602 - src.llm.client - INFO - [int:7bd246] ğŸ“Š 8.0s: 2425c @303c/s (517ch, ~606t @76t/s)
2025-12-16 09:14:37,232 - src.llm.client - INFO - [int:7bd246] âœ“ Done 13.54s: 2572c (~363w @190c/s)
2025-12-16 09:14:37,234 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:14:37,234 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:14:37,234 - generate_secondary - INFO -     - Length: 2569 chars, 363 words
2025-12-16 09:14:37,234 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:14:37,234 - generate_secondary - INFO -     - Connections: 12
2025-12-16 09:14:37,234 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:14:37,234 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_09_precision_weighting_attention/session_09/integration.md
2025-12-16 09:14:37,234 - generate_secondary - INFO - Generating investigation for session 9: Weighting States...
2025-12-16 09:14:37,234 - src.llm.client - INFO - [inv:1b5774] ğŸš€ inv | m=gemma3:4b | p=26321c | t=150s
2025-12-16 09:14:37,234 - src.llm.client - INFO - [inv:1b5774] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:14:37,234 - src.llm.client - INFO - [inv:1b5774] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:14:37,236 - src.llm.client - INFO - [inv:1b5774] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30587 bytes, prompt=26321 chars
2025-12-16 09:14:37,236 - src.llm.client - INFO - [inv:1b5774] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:14:42,145 - src.llm.request_handler - INFO - [inv:1b5774] âœ“ Done 4.91s
2025-12-16 09:14:42,145 - src.llm.client - INFO - [inv:1b5774] âœ… HTTP 200 in 4.91s
2025-12-16 09:14:42,146 - src.llm.client - INFO - [inv:1b5774] ğŸ“¡ Stream active (200)
2025-12-16 09:14:42,146 - src.llm.client - INFO - [inv:1b5774] Starting stream parsing, waiting for first chunk...
2025-12-16 09:14:44,149 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 2.0s: 677c @338c/s (128ch, ~169t @85t/s)
2025-12-16 09:14:46,161 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 4.0s: 1438c @358c/s (258ch, ~360t @90t/s)
2025-12-16 09:14:48,164 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 6.0s: 2193c @364c/s (387ch, ~548t @91t/s)
2025-12-16 09:14:50,172 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 8.0s: 2937c @366c/s (516ch, ~734t @91t/s)
2025-12-16 09:14:52,178 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 10.0s: 3690c @368c/s (645ch, ~922t @92t/s)
2025-12-16 09:14:54,191 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 12.0s: 4396c @365c/s (769ch, ~1099t @91t/s)
2025-12-16 09:14:56,204 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 14.1s: 5163c @367c/s (891ch, ~1291t @92t/s)
2025-12-16 09:14:58,207 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 16.1s: 5665c @353c/s (1018ch, ~1416t @88t/s)
2025-12-16 09:15:00,314 - src.llm.client - INFO - [inv:1b5774] ğŸ“Š 18.2s: 6133c @338c/s (1137ch, ~1533t @84t/s)
2025-12-16 09:15:00,315 - src.llm.client - INFO - [inv:1b5774] âœ“ Done 23.08s: 6133c (~837w @266c/s)
2025-12-16 09:15:00,317 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:15:00,317 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:15:00,317 - generate_secondary - INFO -     - Length: 6132 chars, 837 words
2025-12-16 09:15:00,317 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:15:00,317 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:15:00,317 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:15:00,318 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_09_precision_weighting_attention/session_09/investigation.md
2025-12-16 09:15:00,318 - generate_secondary - INFO - Generating open_questions for session 9: Weighting States...
2025-12-16 09:15:00,318 - src.llm.client - INFO - [opq:8a53c8] ğŸš€ opq | m=gemma3:4b | p=26407c | t=150s
2025-12-16 09:15:00,318 - src.llm.client - INFO - [opq:8a53c8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:15:00,318 - src.llm.client - INFO - [opq:8a53c8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:15:00,320 - src.llm.client - INFO - [opq:8a53c8] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=30684 bytes, prompt=26407 chars
2025-12-16 09:15:00,320 - src.llm.client - INFO - [opq:8a53c8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:15:05,251 - src.llm.request_handler - INFO - [opq:8a53c8] âœ“ Done 4.93s
2025-12-16 09:15:05,251 - src.llm.client - INFO - [opq:8a53c8] âœ… HTTP 200 in 4.93s
2025-12-16 09:15:05,251 - src.llm.client - INFO - [opq:8a53c8] ğŸ“¡ Stream active (200)
2025-12-16 09:15:05,251 - src.llm.client - INFO - [opq:8a53c8] Starting stream parsing, waiting for first chunk...
2025-12-16 09:15:07,256 - src.llm.client - INFO - [opq:8a53c8] ğŸ“Š 2.0s: 759c @379c/s (129ch, ~190t @95t/s)
2025-12-16 09:15:09,264 - src.llm.client - INFO - [opq:8a53c8] ğŸ“Š 4.0s: 1557c @388c/s (258ch, ~389t @97t/s)
2025-12-16 09:15:11,268 - src.llm.client - INFO - [opq:8a53c8] ğŸ“Š 6.0s: 2328c @387c/s (382ch, ~582t @97t/s)
2025-12-16 09:15:12,066 - src.llm.client - INFO - [opq:8a53c8] âœ“ Done 11.75s: 2556c (~334w @218c/s)
2025-12-16 09:15:12,067 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:15:12,067 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:15:12,067 - generate_secondary - INFO -     - Length: 2555 chars, 334 words
2025-12-16 09:15:12,067 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:15:12,067 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:15:12,067 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:15:12,068 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_09_precision_weighting_attention/session_09/open_questions.md
2025-12-16 09:15:12,068 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:15:12,068 - generate_secondary - INFO - 
============================================================
2025-12-16 09:15:12,068 - generate_secondary - INFO - [10/15] Module 10: Predictive Coding â€“ Neural Basis (1 sessions)
2025-12-16 09:15:12,068 - generate_secondary - INFO - ============================================================
2025-12-16 09:15:12,068 - generate_secondary - INFO - 
  Session 10/15: Encoder-Decoder Model
2025-12-16 09:15:12,070 - generate_secondary - INFO - Generating application for session 10: Encoder-Decoder Model...
2025-12-16 09:15:12,070 - src.llm.client - INFO - [app:db332a] ğŸš€ app | m=gemma3:4b | p=34694c | t=150s
2025-12-16 09:15:12,070 - src.llm.client - INFO - [app:db332a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:15:12,070 - src.llm.client - INFO - [app:db332a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:15:12,072 - src.llm.client - INFO - [app:db332a] Sending request to Ollama: model=gemma3:4b, operation=application, payload=36793 bytes, prompt=34694 chars
2025-12-16 09:15:12,072 - src.llm.client - INFO - [app:db332a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:15:17,006 - src.llm.request_handler - INFO - [app:db332a] âœ“ Done 4.93s
2025-12-16 09:15:17,007 - src.llm.client - INFO - [app:db332a] âœ… HTTP 200 in 4.94s
2025-12-16 09:15:17,007 - src.llm.client - INFO - [app:db332a] ğŸ“¡ Stream active (200)
2025-12-16 09:15:17,007 - src.llm.client - INFO - [app:db332a] Starting stream parsing, waiting for first chunk...
2025-12-16 09:15:19,015 - src.llm.client - INFO - [app:db332a] ğŸ“Š 2.0s: 795c @396c/s (129ch, ~199t @99t/s)
2025-12-16 09:15:21,020 - src.llm.client - INFO - [app:db332a] ğŸ“Š 4.0s: 1609c @401c/s (258ch, ~402t @100t/s)
2025-12-16 09:15:23,027 - src.llm.client - INFO - [app:db332a] ğŸ“Š 6.0s: 2382c @396c/s (387ch, ~596t @99t/s)
2025-12-16 09:15:25,043 - src.llm.client - INFO - [app:db332a] ğŸ“Š 8.0s: 3279c @408c/s (517ch, ~820t @102t/s)
2025-12-16 09:15:27,045 - src.llm.client - INFO - [app:db332a] ğŸ“Š 10.0s: 4049c @403c/s (646ch, ~1012t @101t/s)
2025-12-16 09:15:29,051 - src.llm.client - INFO - [app:db332a] ğŸ“Š 12.0s: 4833c @401c/s (775ch, ~1208t @100t/s)
2025-12-16 09:15:29,442 - src.llm.client - INFO - [app:db332a] âœ“ Done 17.37s: 4863c (~633w @280c/s)
2025-12-16 09:15:29,444 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:15:29,444 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:15:29,444 - generate_secondary - INFO -     - Length: 4851 chars, 631 words
2025-12-16 09:15:29,444 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:15:29,444 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:15:29,444 - generate_secondary - INFO -     - Avg words per application: 121
2025-12-16 09:15:29,444 - generate_secondary - WARNING - [WARNING] Application 1 has 143 words (require 150-200, need 7 more words) âš ï¸
2025-12-16 09:15:29,444 - generate_secondary - WARNING - [WARNING] Application 2 has 129 words (require 150-200, need 21 more words) âš ï¸
2025-12-16 09:15:29,444 - generate_secondary - WARNING - [WARNING] Application 3 has 116 words (require 150-200, need 34 more words) âš ï¸
2025-12-16 09:15:29,444 - generate_secondary - WARNING - [WARNING] Application 4 has 115 words (require 150-200, need 35 more words) âš ï¸
2025-12-16 09:15:29,444 - generate_secondary - WARNING - [WARNING] Application 5 has 104 words (require 150-200, need 46 more words) âš ï¸
2025-12-16 09:15:29,444 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_10_predictive_coding_neural_basis/session_10/application.md
2025-12-16 09:15:29,444 - generate_secondary - INFO - Generating extension for session 10: Encoder-Decoder Model...
2025-12-16 09:15:29,444 - src.llm.client - INFO - [ext:d472cf] ğŸš€ ext | m=gemma3:4b | p=27917c | t=120s
2025-12-16 09:15:29,444 - src.llm.client - INFO - [ext:d472cf] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:15:29,445 - src.llm.client - INFO - [ext:d472cf] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:15:29,446 - src.llm.client - INFO - [ext:d472cf] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32871 bytes, prompt=27917 chars
2025-12-16 09:15:29,446 - src.llm.client - INFO - [ext:d472cf] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:15:34,351 - src.llm.request_handler - INFO - [ext:d472cf] âœ“ Done 4.91s
2025-12-16 09:15:34,352 - src.llm.client - INFO - [ext:d472cf] âœ… HTTP 200 in 4.91s
2025-12-16 09:15:34,352 - src.llm.client - INFO - [ext:d472cf] ğŸ“¡ Stream active (200)
2025-12-16 09:15:34,352 - src.llm.client - INFO - [ext:d472cf] Starting stream parsing, waiting for first chunk...
2025-12-16 09:15:36,366 - src.llm.client - INFO - [ext:d472cf] ğŸ“Š 2.0s: 807c @401c/s (129ch, ~202t @100t/s)
2025-12-16 09:15:38,379 - src.llm.client - INFO - [ext:d472cf] ğŸ“Š 4.0s: 1668c @414c/s (258ch, ~417t @104t/s)
2025-12-16 09:15:40,390 - src.llm.client - INFO - [ext:d472cf] ğŸ“Š 6.0s: 2557c @423c/s (388ch, ~639t @106t/s)
2025-12-16 09:15:42,400 - src.llm.client - INFO - [ext:d472cf] ğŸ“Š 8.0s: 3345c @416c/s (517ch, ~836t @104t/s)
2025-12-16 09:15:44,411 - src.llm.client - INFO - [ext:d472cf] ğŸ“Š 10.1s: 4179c @415c/s (646ch, ~1045t @104t/s)
2025-12-16 09:15:44,812 - src.llm.client - INFO - [ext:d472cf] âœ“ Done 15.37s: 4220c (~552w @275c/s)
2025-12-16 09:15:44,814 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:15:44,814 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:15:44,814 - generate_secondary - INFO -     - Length: 4219 chars, 552 words
2025-12-16 09:15:44,814 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:15:44,814 - generate_secondary - INFO -     - Topics: 3
2025-12-16 09:15:44,814 - generate_secondary - INFO -     - Avg words per topic: 178
2025-12-16 09:15:44,814 - generate_secondary - WARNING - [WARNING] Topic 1 has 179 words (exceeds 150 by 29 words - consider condensing) âš ï¸
2025-12-16 09:15:44,814 - generate_secondary - WARNING - [WARNING] Topic 2 has 168 words (exceeds 150 by 18 words - consider condensing) âš ï¸
2025-12-16 09:15:44,814 - generate_secondary - WARNING - [WARNING] Topic 3 has 186 words (exceeds 150 by 36 words - consider condensing) âš ï¸
2025-12-16 09:15:44,815 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_10_predictive_coding_neural_basis/session_10/extension.md
2025-12-16 09:15:44,815 - generate_secondary - INFO - Generating visualization for session 10: Encoder-Decoder Model...
2025-12-16 09:15:44,815 - src.llm.client - INFO - [viz:9c65dd] ğŸš€ viz | m=gemma3:4b | p=26877c | t=120s
2025-12-16 09:15:44,815 - src.llm.client - INFO - [viz:9c65dd] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:15:44,815 - src.llm.client - INFO - [viz:9c65dd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:15:44,816 - src.llm.client - INFO - [viz:9c65dd] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=31153 bytes, prompt=26877 chars
2025-12-16 09:15:44,816 - src.llm.client - INFO - [viz:9c65dd] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:15:49,733 - src.llm.request_handler - INFO - [viz:9c65dd] âœ“ Done 4.92s
2025-12-16 09:15:49,733 - src.llm.client - INFO - [viz:9c65dd] âœ… HTTP 200 in 4.92s
2025-12-16 09:15:49,733 - src.llm.client - INFO - [viz:9c65dd] ğŸ“¡ Stream active (200)
2025-12-16 09:15:49,733 - src.llm.client - INFO - [viz:9c65dd] Starting stream parsing, waiting for first chunk...
2025-12-16 09:15:51,736 - src.llm.client - INFO - [viz:9c65dd] ğŸ“Š 2.0s: 485c @242c/s (129ch, ~121t @61t/s)
2025-12-16 09:15:53,747 - src.llm.client - INFO - [viz:9c65dd] ğŸ“Š 4.0s: 1064c @265c/s (252ch, ~266t @66t/s)
2025-12-16 09:15:55,429 - src.llm.client - INFO - [viz:9c65dd] âœ“ Done 10.61s: 1432c (~213w @135c/s)
2025-12-16 09:15:55,429 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:15:55,430 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 09:15:55,430 - generate_secondary - INFO -     - Length: 295 chars (cleaned: 295 chars)
2025-12-16 09:15:55,430 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:15:55,430 - generate_secondary - INFO - [CRITICAL] Elements: 18 total (nodes: 6, connections: 12) ğŸ”´
2025-12-16 09:15:55,430 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 09:15:55,430 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-16 09:15:55,430 - generate_secondary - WARNING - [WARNING] Only 6 nodes found (require at least 10, need 4 more - add more nodes to the diagram) âš ï¸
2025-12-16 09:15:55,430 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:15:55,430 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:15:55,430 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_10_predictive_coding_neural_basis/session_10/visualization.mmd
2025-12-16 09:15:55,430 - generate_secondary - INFO - Generating integration for session 10: Encoder-Decoder Model...
2025-12-16 09:15:55,430 - src.llm.client - INFO - [int:49cd45] ğŸš€ int | m=gemma3:4b | p=28226c | t=150s
2025-12-16 09:15:55,430 - src.llm.client - INFO - [int:49cd45] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:15:55,430 - src.llm.client - INFO - [int:49cd45] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:15:55,432 - src.llm.client - INFO - [int:49cd45] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=33519 bytes, prompt=28226 chars
2025-12-16 09:15:55,432 - src.llm.client - INFO - [int:49cd45] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:16:00,387 - src.llm.request_handler - INFO - [int:49cd45] âœ“ Done 4.95s
2025-12-16 09:16:00,387 - src.llm.client - INFO - [int:49cd45] âœ… HTTP 200 in 4.96s
2025-12-16 09:16:00,387 - src.llm.client - INFO - [int:49cd45] ğŸ“¡ Stream active (200)
2025-12-16 09:16:00,387 - src.llm.client - INFO - [int:49cd45] Starting stream parsing, waiting for first chunk...
2025-12-16 09:16:02,391 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 2.0s: 717c @358c/s (124ch, ~179t @89t/s)
2025-12-16 09:16:04,405 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 4.0s: 1447c @360c/s (251ch, ~362t @90t/s)
2025-12-16 09:16:06,414 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 6.0s: 2240c @372c/s (378ch, ~560t @93t/s)
2025-12-16 09:16:08,430 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 8.0s: 2970c @369c/s (504ch, ~742t @92t/s)
2025-12-16 09:16:10,434 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 10.0s: 3750c @373c/s (629ch, ~938t @93t/s)
2025-12-16 09:16:12,442 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 12.1s: 4569c @379c/s (753ch, ~1142t @95t/s)
2025-12-16 09:16:14,466 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 14.1s: 5369c @381c/s (874ch, ~1342t @95t/s)
2025-12-16 09:16:16,467 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 16.1s: 6196c @385c/s (994ch, ~1549t @96t/s)
2025-12-16 09:16:18,478 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 18.1s: 6987c @386c/s (1117ch, ~1747t @97t/s)
2025-12-16 09:16:20,486 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 20.1s: 7688c @383c/s (1218ch, ~1922t @96t/s)
2025-12-16 09:16:22,494 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 22.1s: 8461c @383c/s (1336ch, ~2115t @96t/s)
2025-12-16 09:16:24,501 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 24.1s: 9295c @385c/s (1461ch, ~2324t @96t/s)
2025-12-16 09:16:26,505 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 26.1s: 10156c @389c/s (1586ch, ~2539t @97t/s)
2025-12-16 09:16:28,511 - src.llm.client - INFO - [int:49cd45] ğŸ“Š 28.1s: 10893c @387c/s (1698ch, ~2723t @97t/s)
2025-12-16 09:16:29,773 - src.llm.client - INFO - [int:49cd45] âœ“ Done 34.34s: 11257c (~1474w @328c/s)
2025-12-16 09:16:29,777 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:16:29,778 - generate_secondary - INFO - [NEEDS REVIEW] Integration generated âš ï¸
2025-12-16 09:16:29,778 - generate_secondary - INFO -     - Length: 11256 chars, 1474 words
2025-12-16 09:16:29,778 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:16:29,778 - generate_secondary - INFO -     - Connections: 10
2025-12-16 09:16:29,778 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:16:29,778 - generate_secondary - WARNING - [WARNING] Total word count (1474) exceeds maximum 1000 (exceeds by 474 words - condense content) âš ï¸
2025-12-16 09:16:29,778 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:16:29,778 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:16:29,779 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_10_predictive_coding_neural_basis/session_10/integration.md
2025-12-16 09:16:29,779 - generate_secondary - INFO - Generating investigation for session 10: Encoder-Decoder Model...
2025-12-16 09:16:29,779 - src.llm.client - INFO - [inv:103e4b] ğŸš€ inv | m=gemma3:4b | p=27139c | t=150s
2025-12-16 09:16:29,779 - src.llm.client - INFO - [inv:103e4b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:16:29,779 - src.llm.client - INFO - [inv:103e4b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:16:29,780 - src.llm.client - INFO - [inv:103e4b] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=31375 bytes, prompt=27139 chars
2025-12-16 09:16:29,780 - src.llm.client - INFO - [inv:103e4b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:16:34,765 - src.llm.request_handler - INFO - [inv:103e4b] âœ“ Done 4.98s
2025-12-16 09:16:34,765 - src.llm.client - INFO - [inv:103e4b] âœ… HTTP 200 in 4.98s
2025-12-16 09:16:34,765 - src.llm.client - INFO - [inv:103e4b] ğŸ“¡ Stream active (200)
2025-12-16 09:16:34,765 - src.llm.client - INFO - [inv:103e4b] Starting stream parsing, waiting for first chunk...
2025-12-16 09:16:36,776 - src.llm.client - INFO - [inv:103e4b] ğŸ“Š 2.0s: 608c @302c/s (128ch, ~152t @76t/s)
2025-12-16 09:16:38,780 - src.llm.client - INFO - [inv:103e4b] ğŸ“Š 4.0s: 1299c @324c/s (257ch, ~325t @81t/s)
2025-12-16 09:16:40,796 - src.llm.client - INFO - [inv:103e4b] ğŸ“Š 6.0s: 2011c @333c/s (386ch, ~503t @83t/s)
2025-12-16 09:16:42,797 - src.llm.client - INFO - [inv:103e4b] ğŸ“Š 8.0s: 2713c @338c/s (513ch, ~678t @84t/s)
2025-12-16 09:16:44,798 - src.llm.client - INFO - [inv:103e4b] ğŸ“Š 10.0s: 3453c @344c/s (641ch, ~863t @86t/s)
2025-12-16 09:16:46,806 - src.llm.client - INFO - [inv:103e4b] ğŸ“Š 12.0s: 4097c @340c/s (769ch, ~1024t @85t/s)
2025-12-16 09:16:48,552 - src.llm.client - INFO - [inv:103e4b] âœ“ Done 18.77s: 4642c (~670w @247c/s)
2025-12-16 09:16:48,554 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:16:48,554 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:16:48,554 - generate_secondary - INFO -     - Length: 4637 chars, 670 words
2025-12-16 09:16:48,554 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:16:48,554 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:16:48,554 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:16:48,554 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_10_predictive_coding_neural_basis/session_10/investigation.md
2025-12-16 09:16:48,554 - generate_secondary - INFO - Generating open_questions for session 10: Encoder-Decoder Model...
2025-12-16 09:16:48,554 - src.llm.client - INFO - [opq:ef1236] ğŸš€ opq | m=gemma3:4b | p=27225c | t=150s
2025-12-16 09:16:48,555 - src.llm.client - INFO - [opq:ef1236] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:16:48,555 - src.llm.client - INFO - [opq:ef1236] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:16:48,556 - src.llm.client - INFO - [opq:ef1236] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=31472 bytes, prompt=27225 chars
2025-12-16 09:16:48,556 - src.llm.client - INFO - [opq:ef1236] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:16:53,550 - src.llm.request_handler - INFO - [opq:ef1236] âœ“ Done 4.99s
2025-12-16 09:16:53,550 - src.llm.client - INFO - [opq:ef1236] âœ… HTTP 200 in 4.99s
2025-12-16 09:16:53,550 - src.llm.client - INFO - [opq:ef1236] ğŸ“¡ Stream active (200)
2025-12-16 09:16:53,550 - src.llm.client - INFO - [opq:ef1236] Starting stream parsing, waiting for first chunk...
2025-12-16 09:16:55,565 - src.llm.client - INFO - [opq:ef1236] ğŸ“Š 2.0s: 704c @349c/s (119ch, ~176t @87t/s)
2025-12-16 09:16:57,576 - src.llm.client - INFO - [opq:ef1236] ğŸ“Š 4.0s: 1404c @349c/s (246ch, ~351t @87t/s)
2025-12-16 09:16:59,579 - src.llm.client - INFO - [opq:ef1236] ğŸ“Š 6.0s: 2124c @352c/s (371ch, ~531t @88t/s)
2025-12-16 09:17:00,029 - src.llm.client - INFO - [opq:ef1236] âœ“ Done 11.47s: 2212c (~298w @193c/s)
2025-12-16 09:17:00,030 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:17:00,030 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:17:00,030 - generate_secondary - INFO -     - Length: 2211 chars, 298 words
2025-12-16 09:17:00,030 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:17:00,030 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:17:00,030 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:17:00,030 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_10_predictive_coding_neural_basis/session_10/open_questions.md
2025-12-16 09:17:00,031 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:17:00,031 - generate_secondary - INFO - 
============================================================
2025-12-16 09:17:00,031 - generate_secondary - INFO - [11/15] Module 11: Model Learning & Adaptation (1 sessions)
2025-12-16 09:17:00,031 - generate_secondary - INFO - ============================================================
2025-12-16 09:17:00,031 - generate_secondary - INFO - 
  Session 11/15: Parameter Estimation
2025-12-16 09:17:00,032 - generate_secondary - INFO - Generating application for session 11: Parameter Estimation...
2025-12-16 09:17:00,032 - src.llm.client - INFO - [app:d21ceb] ğŸš€ app | m=gemma3:4b | p=31700c | t=150s
2025-12-16 09:17:00,032 - src.llm.client - INFO - [app:d21ceb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:17:00,032 - src.llm.client - INFO - [app:d21ceb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:17:00,033 - src.llm.client - INFO - [app:d21ceb] Sending request to Ollama: model=gemma3:4b, operation=application, payload=33749 bytes, prompt=31700 chars
2025-12-16 09:17:00,033 - src.llm.client - INFO - [app:d21ceb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:17:05,080 - src.llm.request_handler - INFO - [app:d21ceb] âœ“ Done 5.05s
2025-12-16 09:17:05,080 - src.llm.client - INFO - [app:d21ceb] âœ… HTTP 200 in 5.05s
2025-12-16 09:17:05,080 - src.llm.client - INFO - [app:d21ceb] ğŸ“¡ Stream active (200)
2025-12-16 09:17:05,080 - src.llm.client - INFO - [app:d21ceb] Starting stream parsing, waiting for first chunk...
2025-12-16 09:17:07,092 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 2.0s: 748c @372c/s (127ch, ~187t @93t/s)
2025-12-16 09:17:09,096 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 4.0s: 1528c @381c/s (254ch, ~382t @95t/s)
2025-12-16 09:17:11,106 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 6.0s: 2354c @391c/s (381ch, ~588t @98t/s)
2025-12-16 09:17:13,117 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 8.0s: 3210c @399c/s (510ch, ~802t @100t/s)
2025-12-16 09:17:15,118 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 10.0s: 3988c @397c/s (636ch, ~997t @99t/s)
2025-12-16 09:17:17,119 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 12.0s: 4753c @395c/s (764ch, ~1188t @99t/s)
2025-12-16 09:17:19,121 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 14.0s: 5454c @388c/s (892ch, ~1364t @97t/s)
2025-12-16 09:17:21,133 - src.llm.client - INFO - [app:d21ceb] ğŸ“Š 16.1s: 6237c @389c/s (1020ch, ~1559t @97t/s)
2025-12-16 09:17:22,470 - src.llm.client - INFO - [app:d21ceb] âœ“ Done 22.44s: 6652c (~904w @296c/s)
2025-12-16 09:17:22,473 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:17:22,473 - generate_secondary - INFO - [COMPLIANT] Application generated âœ“
2025-12-16 09:17:22,473 - generate_secondary - INFO -     - Length: 6641 chars, 902 words
2025-12-16 09:17:22,473 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:17:22,474 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:17:22,474 - generate_secondary - INFO -     - Avg words per application: 176
2025-12-16 09:17:22,474 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_11_model_learning_adaptation/session_11/application.md
2025-12-16 09:17:22,475 - generate_secondary - INFO - Generating extension for session 11: Parameter Estimation...
2025-12-16 09:17:22,475 - src.llm.client - INFO - [ext:e865ef] ğŸš€ ext | m=gemma3:4b | p=24923c | t=120s
2025-12-16 09:17:22,475 - src.llm.client - INFO - [ext:e865ef] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:17:22,475 - src.llm.client - INFO - [ext:e865ef] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:17:22,478 - src.llm.client - INFO - [ext:e865ef] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=29827 bytes, prompt=24923 chars
2025-12-16 09:17:22,478 - src.llm.client - INFO - [ext:e865ef] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:17:27,419 - src.llm.request_handler - INFO - [ext:e865ef] âœ“ Done 4.94s
2025-12-16 09:17:27,420 - src.llm.client - INFO - [ext:e865ef] âœ… HTTP 200 in 4.94s
2025-12-16 09:17:27,420 - src.llm.client - INFO - [ext:e865ef] ğŸ“¡ Stream active (200)
2025-12-16 09:17:27,420 - src.llm.client - INFO - [ext:e865ef] Starting stream parsing, waiting for first chunk...
2025-12-16 09:17:29,429 - src.llm.client - INFO - [ext:e865ef] ğŸ“Š 2.0s: 824c @410c/s (129ch, ~206t @103t/s)
2025-12-16 09:17:31,441 - src.llm.client - INFO - [ext:e865ef] ğŸ“Š 4.0s: 1611c @401c/s (259ch, ~403t @100t/s)
2025-12-16 09:17:33,444 - src.llm.client - INFO - [ext:e865ef] ğŸ“Š 6.0s: 2406c @399c/s (389ch, ~602t @100t/s)
2025-12-16 09:17:35,456 - src.llm.client - INFO - [ext:e865ef] ğŸ“Š 8.0s: 3193c @397c/s (519ch, ~798t @99t/s)
2025-12-16 09:17:37,461 - src.llm.client - INFO - [ext:e865ef] ğŸ“Š 10.0s: 4002c @399c/s (644ch, ~1000t @100t/s)
2025-12-16 09:17:39,466 - src.llm.client - INFO - [ext:e865ef] ğŸ“Š 12.0s: 4506c @374c/s (773ch, ~1126t @94t/s)
2025-12-16 09:17:39,888 - src.llm.client - INFO - [ext:e865ef] âœ“ Done 17.41s: 4574c (~621w @263c/s)
2025-12-16 09:17:39,889 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:17:39,890 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - INFO -     - Length: 4573 chars, 621 words
2025-12-16 09:17:39,890 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:17:39,890 - generate_secondary - INFO -     - Topics: 6
2025-12-16 09:17:39,890 - generate_secondary - INFO -     - Avg words per topic: 97
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Too many topics (6, maximum 4, 2 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Topic 1 has 171 words (exceeds 150 by 21 words - consider condensing) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Topic 2 has 176 words (exceeds 150 by 26 words - consider condensing) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Topic 3 has 208 words (exceeds 150 by 58 words - consider condensing) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Topic 6 has 25 words (require 100-150, need 75 more words) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - WARNING - [WARNING] Total word count (621) exceeds maximum 600 (exceeds by 21 words - condense content) âš ï¸
2025-12-16 09:17:39,890 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:17:39,890 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:17:39,891 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_11_model_learning_adaptation/session_11/extension.md
2025-12-16 09:17:39,891 - generate_secondary - INFO - Generating visualization for session 11: Parameter Estimation...
2025-12-16 09:17:39,891 - src.llm.client - INFO - [viz:243197] ğŸš€ viz | m=gemma3:4b | p=23883c | t=120s
2025-12-16 09:17:39,891 - src.llm.client - INFO - [viz:243197] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:17:39,891 - src.llm.client - INFO - [viz:243197] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:17:39,893 - src.llm.client - INFO - [viz:243197] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=28109 bytes, prompt=23883 chars
2025-12-16 09:17:39,893 - src.llm.client - INFO - [viz:243197] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:17:44,816 - src.llm.request_handler - INFO - [viz:243197] âœ“ Done 4.92s
2025-12-16 09:17:44,816 - src.llm.client - INFO - [viz:243197] âœ… HTTP 200 in 4.92s
2025-12-16 09:17:44,816 - src.llm.client - INFO - [viz:243197] ğŸ“¡ Stream active (200)
2025-12-16 09:17:44,816 - src.llm.client - INFO - [viz:243197] Starting stream parsing, waiting for first chunk...
2025-12-16 09:17:46,826 - src.llm.client - INFO - [viz:243197] ğŸ“Š 2.0s: 435c @216c/s (128ch, ~109t @54t/s)
2025-12-16 09:17:47,920 - src.llm.client - INFO - [viz:243197] âœ“ Done 8.03s: 633c (~97w @79c/s)
2025-12-16 09:17:47,920 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:17:47,920 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:17:47,920 - generate_secondary - INFO -     - Length: 617 chars (cleaned: 617 chars)
2025-12-16 09:17:47,920 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:17:47,920 - generate_secondary - INFO - [OK] Elements: 37 total (nodes: 14, connections: 23) âœ“
2025-12-16 09:17:47,921 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_11_model_learning_adaptation/session_11/visualization.mmd
2025-12-16 09:17:47,921 - generate_secondary - INFO - Generating integration for session 11: Parameter Estimation...
2025-12-16 09:17:47,921 - src.llm.client - INFO - [int:bb75ad] ğŸš€ int | m=gemma3:4b | p=25232c | t=150s
2025-12-16 09:17:47,921 - src.llm.client - INFO - [int:bb75ad] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:17:47,921 - src.llm.client - INFO - [int:bb75ad] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:17:47,923 - src.llm.client - INFO - [int:bb75ad] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=30475 bytes, prompt=25232 chars
2025-12-16 09:17:47,923 - src.llm.client - INFO - [int:bb75ad] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:17:52,795 - src.llm.request_handler - INFO - [int:bb75ad] âœ“ Done 4.87s
2025-12-16 09:17:52,796 - src.llm.client - INFO - [int:bb75ad] âœ… HTTP 200 in 4.87s
2025-12-16 09:17:52,796 - src.llm.client - INFO - [int:bb75ad] ğŸ“¡ Stream active (200)
2025-12-16 09:17:52,796 - src.llm.client - INFO - [int:bb75ad] Starting stream parsing, waiting for first chunk...
2025-12-16 09:17:54,800 - src.llm.client - INFO - [int:bb75ad] ğŸ“Š 2.0s: 754c @376c/s (127ch, ~188t @94t/s)
2025-12-16 09:17:56,802 - src.llm.client - INFO - [int:bb75ad] ğŸ“Š 4.0s: 1487c @371c/s (252ch, ~372t @93t/s)
2025-12-16 09:17:58,812 - src.llm.client - INFO - [int:bb75ad] ğŸ“Š 6.0s: 2307c @383c/s (381ch, ~577t @96t/s)
2025-12-16 09:18:00,813 - src.llm.client - INFO - [int:bb75ad] ğŸ“Š 8.0s: 2768c @345c/s (510ch, ~692t @86t/s)
2025-12-16 09:18:02,814 - src.llm.client - INFO - [int:bb75ad] ğŸ“Š 10.0s: 3220c @321c/s (639ch, ~805t @80t/s)
2025-12-16 09:18:04,815 - src.llm.client - INFO - [int:bb75ad] ğŸ“Š 12.0s: 3720c @310c/s (768ch, ~930t @77t/s)
2025-12-16 09:18:06,124 - src.llm.client - INFO - [int:bb75ad] âœ“ Done 18.20s: 4105c (~564w @226c/s)
2025-12-16 09:18:06,126 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:18:06,126 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:18:06,126 - generate_secondary - INFO -     - Length: 4091 chars, 562 words
2025-12-16 09:18:06,126 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:18:06,126 - generate_secondary - INFO -     - Connections: 27
2025-12-16 09:18:06,126 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:18:06,126 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_11_model_learning_adaptation/session_11/integration.md
2025-12-16 09:18:06,126 - generate_secondary - INFO - Generating investigation for session 11: Parameter Estimation...
2025-12-16 09:18:06,127 - src.llm.client - INFO - [inv:47a296] ğŸš€ inv | m=gemma3:4b | p=24145c | t=150s
2025-12-16 09:18:06,127 - src.llm.client - INFO - [inv:47a296] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:18:06,127 - src.llm.client - INFO - [inv:47a296] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:18:06,128 - src.llm.client - INFO - [inv:47a296] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=28331 bytes, prompt=24145 chars
2025-12-16 09:18:06,128 - src.llm.client - INFO - [inv:47a296] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:18:11,039 - src.llm.request_handler - INFO - [inv:47a296] âœ“ Done 4.91s
2025-12-16 09:18:11,040 - src.llm.client - INFO - [inv:47a296] âœ… HTTP 200 in 4.91s
2025-12-16 09:18:11,040 - src.llm.client - INFO - [inv:47a296] ğŸ“¡ Stream active (200)
2025-12-16 09:18:11,040 - src.llm.client - INFO - [inv:47a296] Starting stream parsing, waiting for first chunk...
2025-12-16 09:18:13,041 - src.llm.client - INFO - [inv:47a296] ğŸ“Š 2.0s: 563c @281c/s (128ch, ~141t @70t/s)
2025-12-16 09:18:15,049 - src.llm.client - INFO - [inv:47a296] ğŸ“Š 4.0s: 1191c @297c/s (254ch, ~298t @74t/s)
2025-12-16 09:18:17,054 - src.llm.client - INFO - [inv:47a296] ğŸ“Š 6.0s: 1836c @305c/s (374ch, ~459t @76t/s)
2025-12-16 09:18:19,062 - src.llm.client - INFO - [inv:47a296] ğŸ“Š 8.0s: 2314c @288c/s (495ch, ~578t @72t/s)
2025-12-16 09:18:21,070 - src.llm.client - INFO - [inv:47a296] ğŸ“Š 10.0s: 3051c @304c/s (623ch, ~763t @76t/s)
2025-12-16 09:18:23,083 - src.llm.client - INFO - [inv:47a296] ğŸ“Š 12.0s: 3716c @309c/s (749ch, ~929t @77t/s)
2025-12-16 09:18:25,092 - src.llm.client - INFO - [inv:47a296] ğŸ“Š 14.1s: 4405c @313c/s (873ch, ~1101t @78t/s)
2025-12-16 09:18:26,220 - src.llm.client - INFO - [inv:47a296] âœ“ Done 20.09s: 4737c (~686w @236c/s)
2025-12-16 09:18:26,222 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:18:26,223 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:18:26,223 - generate_secondary - INFO -     - Length: 4735 chars, 686 words
2025-12-16 09:18:26,223 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:18:26,223 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:18:26,223 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:18:26,223 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_11_model_learning_adaptation/session_11/investigation.md
2025-12-16 09:18:26,223 - generate_secondary - INFO - Generating open_questions for session 11: Parameter Estimation...
2025-12-16 09:18:26,224 - src.llm.client - INFO - [opq:cb40e1] ğŸš€ opq | m=gemma3:4b | p=24231c | t=150s
2025-12-16 09:18:26,224 - src.llm.client - INFO - [opq:cb40e1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:18:26,224 - src.llm.client - INFO - [opq:cb40e1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:18:26,225 - src.llm.client - INFO - [opq:cb40e1] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=28428 bytes, prompt=24231 chars
2025-12-16 09:18:26,226 - src.llm.client - INFO - [opq:cb40e1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:18:31,173 - src.llm.request_handler - INFO - [opq:cb40e1] âœ“ Done 4.95s
2025-12-16 09:18:31,174 - src.llm.client - INFO - [opq:cb40e1] âœ… HTTP 200 in 4.95s
2025-12-16 09:18:31,174 - src.llm.client - INFO - [opq:cb40e1] ğŸ“¡ Stream active (200)
2025-12-16 09:18:31,174 - src.llm.client - INFO - [opq:cb40e1] Starting stream parsing, waiting for first chunk...
2025-12-16 09:18:33,187 - src.llm.client - INFO - [opq:cb40e1] ğŸ“Š 2.0s: 746c @371c/s (129ch, ~186t @93t/s)
2025-12-16 09:18:35,194 - src.llm.client - INFO - [opq:cb40e1] ğŸ“Š 4.0s: 1499c @373c/s (259ch, ~375t @93t/s)
2025-12-16 09:18:37,089 - src.llm.client - INFO - [opq:cb40e1] âœ“ Done 10.87s: 2202c (~288w @203c/s)
2025-12-16 09:18:37,091 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:18:37,091 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:18:37,091 - generate_secondary - INFO -     - Length: 2190 chars, 286 words
2025-12-16 09:18:37,091 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:18:37,091 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:18:37,091 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:18:37,092 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_11_model_learning_adaptation/session_11/open_questions.md
2025-12-16 09:18:37,092 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:18:37,092 - generate_secondary - INFO - 
============================================================
2025-12-16 09:18:37,092 - generate_secondary - INFO - [12/15] Module 12: Reinforcement Learning â€“ Policy Optimization (1 sessions)
2025-12-16 09:18:37,092 - generate_secondary - INFO - ============================================================
2025-12-16 09:18:37,092 - generate_secondary - INFO - 
  Session 12/15: Markov Decision Processes
2025-12-16 09:18:37,093 - generate_secondary - INFO - Generating application for session 12: Markov Decision Processes...
2025-12-16 09:18:37,093 - src.llm.client - INFO - [app:f7a6bb] ğŸš€ app | m=gemma3:4b | p=33487c | t=150s
2025-12-16 09:18:37,093 - src.llm.client - INFO - [app:f7a6bb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:18:37,093 - src.llm.client - INFO - [app:f7a6bb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:18:37,095 - src.llm.client - INFO - [app:f7a6bb] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35852 bytes, prompt=33487 chars
2025-12-16 09:18:37,095 - src.llm.client - INFO - [app:f7a6bb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:18:41,991 - src.llm.request_handler - INFO - [app:f7a6bb] âœ“ Done 4.90s
2025-12-16 09:18:41,991 - src.llm.client - INFO - [app:f7a6bb] âœ… HTTP 200 in 4.90s
2025-12-16 09:18:41,991 - src.llm.client - INFO - [app:f7a6bb] ğŸ“¡ Stream active (200)
2025-12-16 09:18:41,991 - src.llm.client - INFO - [app:f7a6bb] Starting stream parsing, waiting for first chunk...
2025-12-16 09:18:43,997 - src.llm.client - INFO - [app:f7a6bb] ğŸ“Š 2.0s: 790c @394c/s (130ch, ~198t @98t/s)
2025-12-16 09:18:46,008 - src.llm.client - INFO - [app:f7a6bb] ğŸ“Š 4.0s: 1564c @389c/s (249ch, ~391t @97t/s)
2025-12-16 09:18:48,013 - src.llm.client - INFO - [app:f7a6bb] ğŸ“Š 6.0s: 2292c @381c/s (374ch, ~573t @95t/s)
2025-12-16 09:18:50,027 - src.llm.client - INFO - [app:f7a6bb] ğŸ“Š 8.0s: 3066c @382c/s (500ch, ~766t @95t/s)
2025-12-16 09:18:52,029 - src.llm.client - INFO - [app:f7a6bb] ğŸ“Š 10.0s: 3866c @385c/s (627ch, ~966t @96t/s)
2025-12-16 09:18:54,331 - src.llm.client - INFO - [app:f7a6bb] ğŸ“Š 12.3s: 4482c @363c/s (743ch, ~1120t @91t/s)
2025-12-16 09:18:54,332 - src.llm.client - INFO - [app:f7a6bb] âœ“ Done 17.24s: 4482c (~576w @260c/s)
2025-12-16 09:18:54,334 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:18:54,334 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:18:54,334 - generate_secondary - INFO -     - Length: 4479 chars, 576 words
2025-12-16 09:18:54,334 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:18:54,334 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:18:54,334 - generate_secondary - INFO -     - Avg words per application: 109
2025-12-16 09:18:54,334 - generate_secondary - WARNING - [WARNING] Application 1 has 112 words (require 150-200, need 38 more words) âš ï¸
2025-12-16 09:18:54,334 - generate_secondary - WARNING - [WARNING] Application 2 has 115 words (require 150-200, need 35 more words) âš ï¸
2025-12-16 09:18:54,334 - generate_secondary - WARNING - [WARNING] Application 3 has 104 words (require 150-200, need 46 more words) âš ï¸
2025-12-16 09:18:54,334 - generate_secondary - WARNING - [WARNING] Application 4 has 102 words (require 150-200, need 48 more words) âš ï¸
2025-12-16 09:18:54,334 - generate_secondary - WARNING - [WARNING] Application 5 has 112 words (require 150-200, need 38 more words) âš ï¸
2025-12-16 09:18:54,335 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_12_reinforcement_learning_policy_optimization/session_12/application.md
2025-12-16 09:18:54,335 - generate_secondary - INFO - Generating extension for session 12: Markov Decision Processes...
2025-12-16 09:18:54,335 - src.llm.client - INFO - [ext:0009f8] ğŸš€ ext | m=gemma3:4b | p=26710c | t=120s
2025-12-16 09:18:54,335 - src.llm.client - INFO - [ext:0009f8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:18:54,335 - src.llm.client - INFO - [ext:0009f8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:18:54,337 - src.llm.client - INFO - [ext:0009f8] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=31930 bytes, prompt=26710 chars
2025-12-16 09:18:54,337 - src.llm.client - INFO - [ext:0009f8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:18:59,293 - src.llm.request_handler - INFO - [ext:0009f8] âœ“ Done 4.96s
2025-12-16 09:18:59,293 - src.llm.client - INFO - [ext:0009f8] âœ… HTTP 200 in 4.96s
2025-12-16 09:18:59,293 - src.llm.client - INFO - [ext:0009f8] ğŸ“¡ Stream active (200)
2025-12-16 09:18:59,293 - src.llm.client - INFO - [ext:0009f8] Starting stream parsing, waiting for first chunk...
2025-12-16 09:19:01,305 - src.llm.client - INFO - [ext:0009f8] ğŸ“Š 2.0s: 747c @371c/s (129ch, ~187t @93t/s)
2025-12-16 09:19:03,307 - src.llm.client - INFO - [ext:0009f8] ğŸ“Š 4.0s: 1508c @376c/s (253ch, ~377t @94t/s)
2025-12-16 09:19:05,318 - src.llm.client - INFO - [ext:0009f8] ğŸ“Š 6.0s: 2279c @378c/s (385ch, ~570t @95t/s)
2025-12-16 09:19:07,332 - src.llm.client - INFO - [ext:0009f8] ğŸ“Š 8.0s: 2994c @372c/s (517ch, ~748t @93t/s)
2025-12-16 09:19:09,334 - src.llm.client - INFO - [ext:0009f8] ğŸ“Š 10.0s: 3654c @364c/s (649ch, ~914t @91t/s)
2025-12-16 09:19:11,347 - src.llm.client - INFO - [ext:0009f8] ğŸ“Š 12.1s: 4296c @356c/s (782ch, ~1074t @89t/s)
2025-12-16 09:19:13,353 - src.llm.client - INFO - [ext:0009f8] ğŸ“Š 14.1s: 4777c @340c/s (914ch, ~1194t @85t/s)
2025-12-16 09:19:14,831 - src.llm.client - INFO - [ext:0009f8] âœ“ Done 20.50s: 5052c (~680w @246c/s)
2025-12-16 09:19:14,833 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:19:14,834 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - INFO -     - Length: 5049 chars, 680 words
2025-12-16 09:19:14,834 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:19:14,834 - generate_secondary - INFO -     - Topics: 9
2025-12-16 09:19:14,834 - generate_secondary - INFO -     - Avg words per topic: 72
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Too many topics (9, maximum 4, 5 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 1 has 184 words (exceeds 150 by 34 words - consider condensing) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 2 has 171 words (exceeds 150 by 21 words - consider condensing) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 3 has 199 words (exceeds 150 by 49 words - consider condensing) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 6 has 62 words (require 100-150, need 38 more words) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 7 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 8 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Topic 9 has 27 words (require 100-150, need 73 more words) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - WARNING - [WARNING] Total word count (680) exceeds maximum 600 (exceeds by 80 words - condense content) âš ï¸
2025-12-16 09:19:14,834 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:19:14,834 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:19:14,834 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_12_reinforcement_learning_policy_optimization/session_12/extension.md
2025-12-16 09:19:14,834 - generate_secondary - INFO - Generating visualization for session 12: Markov Decision Processes...
2025-12-16 09:19:14,834 - src.llm.client - INFO - [viz:ad3e2c] ğŸš€ viz | m=gemma3:4b | p=25670c | t=120s
2025-12-16 09:19:14,834 - src.llm.client - INFO - [viz:ad3e2c] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:19:14,834 - src.llm.client - INFO - [viz:ad3e2c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:19:14,836 - src.llm.client - INFO - [viz:ad3e2c] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30212 bytes, prompt=25670 chars
2025-12-16 09:19:14,836 - src.llm.client - INFO - [viz:ad3e2c] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:19:19,730 - src.llm.request_handler - INFO - [viz:ad3e2c] âœ“ Done 4.89s
2025-12-16 09:19:19,730 - src.llm.client - INFO - [viz:ad3e2c] âœ… HTTP 200 in 4.89s
2025-12-16 09:19:19,730 - src.llm.client - INFO - [viz:ad3e2c] ğŸ“¡ Stream active (200)
2025-12-16 09:19:19,730 - src.llm.client - INFO - [viz:ad3e2c] Starting stream parsing, waiting for first chunk...
2025-12-16 09:19:21,739 - src.llm.client - INFO - [viz:ad3e2c] ğŸ“Š 2.0s: 508c @253c/s (131ch, ~127t @63t/s)
2025-12-16 09:19:23,749 - src.llm.client - INFO - [viz:ad3e2c] ğŸ“Š 4.0s: 1104c @275c/s (261ch, ~276t @69t/s)
2025-12-16 09:19:25,034 - src.llm.client - INFO - [viz:ad3e2c] âœ“ Done 10.20s: 1403c (~203w @138c/s)
2025-12-16 09:19:25,034 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:19:25,034 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:19:25,034 - generate_secondary - INFO -     - Length: 287 chars (cleaned: 287 chars)
2025-12-16 09:19:25,034 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:19:25,034 - generate_secondary - INFO - [OK] Elements: 23 total (nodes: 14, connections: 9) âœ“
2025-12-16 09:19:25,035 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_12_reinforcement_learning_policy_optimization/session_12/visualization.mmd
2025-12-16 09:19:25,035 - generate_secondary - INFO - Generating integration for session 12: Markov Decision Processes...
2025-12-16 09:19:25,035 - src.llm.client - INFO - [int:c799de] ğŸš€ int | m=gemma3:4b | p=27019c | t=150s
2025-12-16 09:19:25,035 - src.llm.client - INFO - [int:c799de] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:19:25,035 - src.llm.client - INFO - [int:c799de] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:19:25,037 - src.llm.client - INFO - [int:c799de] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=32578 bytes, prompt=27019 chars
2025-12-16 09:19:25,037 - src.llm.client - INFO - [int:c799de] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:19:29,877 - src.llm.request_handler - INFO - [int:c799de] âœ“ Done 4.84s
2025-12-16 09:19:29,878 - src.llm.client - INFO - [int:c799de] âœ… HTTP 200 in 4.84s
2025-12-16 09:19:29,878 - src.llm.client - INFO - [int:c799de] ğŸ“¡ Stream active (200)
2025-12-16 09:19:29,878 - src.llm.client - INFO - [int:c799de] Starting stream parsing, waiting for first chunk...
2025-12-16 09:19:31,892 - src.llm.client - INFO - [int:c799de] ğŸ“Š 2.0s: 753c @374c/s (132ch, ~188t @93t/s)
2025-12-16 09:19:33,901 - src.llm.client - INFO - [int:c799de] ğŸ“Š 4.0s: 1577c @392c/s (265ch, ~394t @98t/s)
2025-12-16 09:19:35,903 - src.llm.client - INFO - [int:c799de] ğŸ“Š 6.0s: 2369c @393c/s (397ch, ~592t @98t/s)
2025-12-16 09:19:37,907 - src.llm.client - INFO - [int:c799de] ğŸ“Š 8.0s: 3176c @396c/s (529ch, ~794t @99t/s)
2025-12-16 09:19:39,917 - src.llm.client - INFO - [int:c799de] ğŸ“Š 10.0s: 4007c @399c/s (661ch, ~1002t @100t/s)
2025-12-16 09:19:41,927 - src.llm.client - INFO - [int:c799de] ğŸ“Š 12.0s: 4758c @395c/s (792ch, ~1190t @99t/s)
2025-12-16 09:19:43,941 - src.llm.client - INFO - [int:c799de] ğŸ“Š 14.1s: 5309c @378c/s (925ch, ~1327t @94t/s)
2025-12-16 09:19:44,953 - src.llm.client - INFO - [int:c799de] âœ“ Done 19.92s: 5594c (~751w @281c/s)
2025-12-16 09:19:44,955 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:19:44,956 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:19:44,956 - generate_secondary - INFO -     - Length: 5573 chars, 749 words
2025-12-16 09:19:44,956 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:19:44,956 - generate_secondary - INFO -     - Connections: 47
2025-12-16 09:19:44,956 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:19:44,956 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_12_reinforcement_learning_policy_optimization/session_12/integration.md
2025-12-16 09:19:44,956 - generate_secondary - INFO - Generating investigation for session 12: Markov Decision Processes...
2025-12-16 09:19:44,956 - src.llm.client - INFO - [inv:2795a8] ğŸš€ inv | m=gemma3:4b | p=25932c | t=150s
2025-12-16 09:19:44,957 - src.llm.client - INFO - [inv:2795a8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:19:44,957 - src.llm.client - INFO - [inv:2795a8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:19:44,958 - src.llm.client - INFO - [inv:2795a8] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30434 bytes, prompt=25932 chars
2025-12-16 09:19:44,958 - src.llm.client - INFO - [inv:2795a8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:19:49,808 - src.llm.request_handler - INFO - [inv:2795a8] âœ“ Done 4.85s
2025-12-16 09:19:49,809 - src.llm.client - INFO - [inv:2795a8] âœ… HTTP 200 in 4.85s
2025-12-16 09:19:49,809 - src.llm.client - INFO - [inv:2795a8] ğŸ“¡ Stream active (200)
2025-12-16 09:19:49,809 - src.llm.client - INFO - [inv:2795a8] Starting stream parsing, waiting for first chunk...
2025-12-16 09:19:51,810 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 2.0s: 705c @352c/s (131ch, ~176t @88t/s)
2025-12-16 09:19:53,825 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 4.0s: 1112c @277c/s (259ch, ~278t @69t/s)
2025-12-16 09:19:55,839 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 6.0s: 1746c @290c/s (383ch, ~436t @72t/s)
2025-12-16 09:19:57,844 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 8.0s: 2395c @298c/s (512ch, ~599t @75t/s)
2025-12-16 09:19:59,850 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 10.0s: 2891c @288c/s (641ch, ~723t @72t/s)
2025-12-16 09:20:01,854 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 12.0s: 3598c @299c/s (772ch, ~900t @75t/s)
2025-12-16 09:20:03,855 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 14.0s: 4326c @308c/s (902ch, ~1082t @77t/s)
2025-12-16 09:20:05,868 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 16.1s: 4803c @299c/s (1034ch, ~1201t @75t/s)
2025-12-16 09:20:07,882 - src.llm.client - INFO - [inv:2795a8] ğŸ“Š 18.1s: 5496c @304c/s (1165ch, ~1374t @76t/s)
2025-12-16 09:20:08,869 - src.llm.client - INFO - [inv:2795a8] âœ“ Done 23.91s: 5768c (~891w @241c/s)
2025-12-16 09:20:08,871 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:20:08,871 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:20:08,871 - generate_secondary - INFO -     - Length: 5765 chars, 891 words
2025-12-16 09:20:08,871 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:20:08,871 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:20:08,871 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:20:08,871 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_12_reinforcement_learning_policy_optimization/session_12/investigation.md
2025-12-16 09:20:08,872 - generate_secondary - INFO - Generating open_questions for session 12: Markov Decision Processes...
2025-12-16 09:20:08,872 - src.llm.client - INFO - [opq:09cf80] ğŸš€ opq | m=gemma3:4b | p=26018c | t=150s
2025-12-16 09:20:08,872 - src.llm.client - INFO - [opq:09cf80] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:20:08,872 - src.llm.client - INFO - [opq:09cf80] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:20:08,873 - src.llm.client - INFO - [opq:09cf80] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=30531 bytes, prompt=26018 chars
2025-12-16 09:20:08,873 - src.llm.client - INFO - [opq:09cf80] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:20:13,806 - src.llm.request_handler - INFO - [opq:09cf80] âœ“ Done 4.93s
2025-12-16 09:20:13,807 - src.llm.client - INFO - [opq:09cf80] âœ… HTTP 200 in 4.93s
2025-12-16 09:20:13,807 - src.llm.client - INFO - [opq:09cf80] ğŸ“¡ Stream active (200)
2025-12-16 09:20:13,807 - src.llm.client - INFO - [opq:09cf80] Starting stream parsing, waiting for first chunk...
2025-12-16 09:20:15,810 - src.llm.client - INFO - [opq:09cf80] ğŸ“Š 2.0s: 827c @413c/s (131ch, ~207t @103t/s)
2025-12-16 09:20:17,821 - src.llm.client - INFO - [opq:09cf80] ğŸ“Š 4.0s: 1660c @414c/s (261ch, ~415t @103t/s)
2025-12-16 09:20:19,168 - src.llm.client - INFO - [opq:09cf80] âœ“ Done 10.30s: 2136c (~273w @207c/s)
2025-12-16 09:20:19,169 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:20:19,169 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:20:19,169 - generate_secondary - INFO -     - Length: 2135 chars, 273 words
2025-12-16 09:20:19,169 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:20:19,169 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:20:19,169 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:20:19,170 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_12_reinforcement_learning_policy_optimization/session_12/open_questions.md
2025-12-16 09:20:19,170 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:20:19,170 - generate_secondary - INFO - 
============================================================
2025-12-16 09:20:19,170 - generate_secondary - INFO - [13/15] Module 13: Generative Models â€“ Hierarchical Structures (1 sessions)
2025-12-16 09:20:19,170 - generate_secondary - INFO - ============================================================
2025-12-16 09:20:19,170 - generate_secondary - INFO - 
  Session 13/15: Multi-Level Models
2025-12-16 09:20:19,172 - generate_secondary - INFO - Generating application for session 13: Multi-Level Models...
2025-12-16 09:20:19,172 - src.llm.client - INFO - [app:507844] ğŸš€ app | m=gemma3:4b | p=32880c | t=150s
2025-12-16 09:20:19,172 - src.llm.client - INFO - [app:507844] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:20:19,172 - src.llm.client - INFO - [app:507844] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:20:19,173 - src.llm.client - INFO - [app:507844] Sending request to Ollama: model=gemma3:4b, operation=application, payload=34927 bytes, prompt=32880 chars
2025-12-16 09:20:19,174 - src.llm.client - INFO - [app:507844] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:20:24,054 - src.llm.request_handler - INFO - [app:507844] âœ“ Done 4.88s
2025-12-16 09:20:24,054 - src.llm.client - INFO - [app:507844] âœ… HTTP 200 in 4.88s
2025-12-16 09:20:24,054 - src.llm.client - INFO - [app:507844] ğŸ“¡ Stream active (200)
2025-12-16 09:20:24,054 - src.llm.client - INFO - [app:507844] Starting stream parsing, waiting for first chunk...
2025-12-16 09:20:26,057 - src.llm.client - INFO - [app:507844] ğŸ“Š 2.0s: 784c @391c/s (129ch, ~196t @98t/s)
2025-12-16 09:20:28,067 - src.llm.client - INFO - [app:507844] ğŸ“Š 4.0s: 1572c @392c/s (258ch, ~393t @98t/s)
2025-12-16 09:20:30,075 - src.llm.client - INFO - [app:507844] ğŸ“Š 6.0s: 2370c @394c/s (389ch, ~592t @98t/s)
2025-12-16 09:20:32,076 - src.llm.client - INFO - [app:507844] ğŸ“Š 8.0s: 3098c @386c/s (520ch, ~774t @97t/s)
2025-12-16 09:20:34,089 - src.llm.client - INFO - [app:507844] ğŸ“Š 10.0s: 3871c @386c/s (652ch, ~968t @96t/s)
2025-12-16 09:20:36,316 - src.llm.client - INFO - [app:507844] ğŸ“Š 12.3s: 4610c @376c/s (777ch, ~1152t @94t/s)
2025-12-16 09:20:36,317 - src.llm.client - INFO - [app:507844] âœ“ Done 17.15s: 4610c (~609w @269c/s)
2025-12-16 09:20:36,319 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:20:36,319 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:20:36,319 - generate_secondary - INFO -     - Length: 4598 chars, 607 words
2025-12-16 09:20:36,319 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:20:36,319 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:20:36,319 - generate_secondary - INFO -     - Avg words per application: 117
2025-12-16 09:20:36,319 - generate_secondary - WARNING - [WARNING] Application 1 has 123 words (require 150-200, need 27 more words) âš ï¸
2025-12-16 09:20:36,319 - generate_secondary - WARNING - [WARNING] Application 2 has 120 words (require 150-200, need 30 more words) âš ï¸
2025-12-16 09:20:36,319 - generate_secondary - WARNING - [WARNING] Application 3 has 121 words (require 150-200, need 29 more words) âš ï¸
2025-12-16 09:20:36,319 - generate_secondary - WARNING - [WARNING] Application 4 has 117 words (require 150-200, need 33 more words) âš ï¸
2025-12-16 09:20:36,319 - generate_secondary - WARNING - [WARNING] Application 5 has 102 words (require 150-200, need 48 more words) âš ï¸
2025-12-16 09:20:36,320 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_13_generative_models_hierarchical_structures/session_13/application.md
2025-12-16 09:20:36,320 - generate_secondary - INFO - Generating extension for session 13: Multi-Level Models...
2025-12-16 09:20:36,321 - src.llm.client - INFO - [ext:e7ac1e] ğŸš€ ext | m=gemma3:4b | p=26103c | t=120s
2025-12-16 09:20:36,321 - src.llm.client - INFO - [ext:e7ac1e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:20:36,321 - src.llm.client - INFO - [ext:e7ac1e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:20:36,324 - src.llm.client - INFO - [ext:e7ac1e] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=31005 bytes, prompt=26103 chars
2025-12-16 09:20:36,324 - src.llm.client - INFO - [ext:e7ac1e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:20:41,201 - src.llm.request_handler - INFO - [ext:e7ac1e] âœ“ Done 4.88s
2025-12-16 09:20:41,202 - src.llm.client - INFO - [ext:e7ac1e] âœ… HTTP 200 in 4.88s
2025-12-16 09:20:41,202 - src.llm.client - INFO - [ext:e7ac1e] ğŸ“¡ Stream active (200)
2025-12-16 09:20:41,202 - src.llm.client - INFO - [ext:e7ac1e] Starting stream parsing, waiting for first chunk...
2025-12-16 09:20:43,205 - src.llm.client - INFO - [ext:e7ac1e] ğŸ“Š 2.0s: 786c @393c/s (132ch, ~196t @98t/s)
2025-12-16 09:20:45,219 - src.llm.client - INFO - [ext:e7ac1e] ğŸ“Š 4.0s: 1579c @393c/s (265ch, ~395t @98t/s)
2025-12-16 09:20:47,229 - src.llm.client - INFO - [ext:e7ac1e] ğŸ“Š 6.0s: 2381c @395c/s (399ch, ~595t @99t/s)
2025-12-16 09:20:49,239 - src.llm.client - INFO - [ext:e7ac1e] ğŸ“Š 8.0s: 3146c @391c/s (528ch, ~786t @98t/s)
2025-12-16 09:20:50,339 - src.llm.client - INFO - [ext:e7ac1e] âœ“ Done 14.02s: 3510c (~456w @250c/s)
2025-12-16 09:20:50,340 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:20:50,340 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:20:50,340 - generate_secondary - INFO -     - Length: 3496 chars, 454 words
2025-12-16 09:20:50,340 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:20:50,340 - generate_secondary - INFO -     - Topics: 3
2025-12-16 09:20:50,340 - generate_secondary - INFO -     - Avg words per topic: 146
2025-12-16 09:20:50,340 - generate_secondary - WARNING - [WARNING] Topic 1 has 160 words (exceeds 150 by 10 words - consider condensing) âš ï¸
2025-12-16 09:20:50,341 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_13_generative_models_hierarchical_structures/session_13/extension.md
2025-12-16 09:20:50,341 - generate_secondary - INFO - Generating visualization for session 13: Multi-Level Models...
2025-12-16 09:20:50,341 - src.llm.client - INFO - [viz:5789fe] ğŸš€ viz | m=gemma3:4b | p=25063c | t=120s
2025-12-16 09:20:50,341 - src.llm.client - INFO - [viz:5789fe] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:20:50,341 - src.llm.client - INFO - [viz:5789fe] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:20:50,342 - src.llm.client - INFO - [viz:5789fe] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=29287 bytes, prompt=25063 chars
2025-12-16 09:20:50,342 - src.llm.client - INFO - [viz:5789fe] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:20:55,232 - src.llm.request_handler - INFO - [viz:5789fe] âœ“ Done 4.89s
2025-12-16 09:20:55,232 - src.llm.client - INFO - [viz:5789fe] âœ… HTTP 200 in 4.89s
2025-12-16 09:20:55,232 - src.llm.client - INFO - [viz:5789fe] ğŸ“¡ Stream active (200)
2025-12-16 09:20:55,232 - src.llm.client - INFO - [viz:5789fe] Starting stream parsing, waiting for first chunk...
2025-12-16 09:20:57,334 - src.llm.client - INFO - [viz:5789fe] ğŸ“Š 2.1s: 401c @191c/s (123ch, ~100t @48t/s)
2025-12-16 09:20:57,335 - src.llm.client - INFO - [viz:5789fe] âœ“ Done 6.99s: 401c (~54w @57c/s)
2025-12-16 09:20:57,335 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:20:57,336 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:20:57,336 - generate_secondary - INFO -     - Length: 385 chars (cleaned: 385 chars)
2025-12-16 09:20:57,336 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:20:57,336 - generate_secondary - INFO - [OK] Elements: 20 total (nodes: 10, connections: 10) âœ“
2025-12-16 09:20:57,336 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_13_generative_models_hierarchical_structures/session_13/visualization.mmd
2025-12-16 09:20:57,336 - generate_secondary - INFO - Generating integration for session 13: Multi-Level Models...
2025-12-16 09:20:57,336 - src.llm.client - INFO - [int:46403f] ğŸš€ int | m=gemma3:4b | p=26412c | t=150s
2025-12-16 09:20:57,336 - src.llm.client - INFO - [int:46403f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:20:57,336 - src.llm.client - INFO - [int:46403f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:20:57,338 - src.llm.client - INFO - [int:46403f] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=31653 bytes, prompt=26412 chars
2025-12-16 09:20:57,338 - src.llm.client - INFO - [int:46403f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:21:02,201 - src.llm.request_handler - INFO - [int:46403f] âœ“ Done 4.86s
2025-12-16 09:21:02,202 - src.llm.client - INFO - [int:46403f] âœ… HTTP 200 in 4.86s
2025-12-16 09:21:02,202 - src.llm.client - INFO - [int:46403f] ğŸ“¡ Stream active (200)
2025-12-16 09:21:02,202 - src.llm.client - INFO - [int:46403f] Starting stream parsing, waiting for first chunk...
2025-12-16 09:21:04,213 - src.llm.client - INFO - [int:46403f] ğŸ“Š 2.0s: 762c @379c/s (130ch, ~190t @95t/s)
2025-12-16 09:21:06,227 - src.llm.client - INFO - [int:46403f] ğŸ“Š 4.0s: 1584c @393c/s (262ch, ~396t @98t/s)
2025-12-16 09:21:08,237 - src.llm.client - INFO - [int:46403f] ğŸ“Š 6.0s: 2355c @390c/s (393ch, ~589t @98t/s)
2025-12-16 09:21:10,253 - src.llm.client - INFO - [int:46403f] ğŸ“Š 8.1s: 3166c @393c/s (522ch, ~792t @98t/s)
2025-12-16 09:21:12,269 - src.llm.client - INFO - [int:46403f] ğŸ“Š 10.1s: 3759c @373c/s (650ch, ~940t @93t/s)
2025-12-16 09:21:12,515 - src.llm.client - INFO - [int:46403f] âœ“ Done 15.18s: 3761c (~526w @248c/s)
2025-12-16 09:21:12,516 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:21:12,517 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:21:12,517 - generate_secondary - INFO -     - Length: 3747 chars, 524 words
2025-12-16 09:21:12,517 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:21:12,517 - generate_secondary - INFO -     - Connections: 10
2025-12-16 09:21:12,517 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:21:12,517 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_13_generative_models_hierarchical_structures/session_13/integration.md
2025-12-16 09:21:12,517 - generate_secondary - INFO - Generating investigation for session 13: Multi-Level Models...
2025-12-16 09:21:12,517 - src.llm.client - INFO - [inv:439257] ğŸš€ inv | m=gemma3:4b | p=25325c | t=150s
2025-12-16 09:21:12,517 - src.llm.client - INFO - [inv:439257] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:21:12,517 - src.llm.client - INFO - [inv:439257] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:21:12,519 - src.llm.client - INFO - [inv:439257] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=29509 bytes, prompt=25325 chars
2025-12-16 09:21:12,519 - src.llm.client - INFO - [inv:439257] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:21:17,462 - src.llm.request_handler - INFO - [inv:439257] âœ“ Done 4.94s
2025-12-16 09:21:17,463 - src.llm.client - INFO - [inv:439257] âœ… HTTP 200 in 4.94s
2025-12-16 09:21:17,463 - src.llm.client - INFO - [inv:439257] ğŸ“¡ Stream active (200)
2025-12-16 09:21:17,463 - src.llm.client - INFO - [inv:439257] Starting stream parsing, waiting for first chunk...
2025-12-16 09:21:19,469 - src.llm.client - INFO - [inv:439257] ğŸ“Š 2.0s: 624c @311c/s (124ch, ~156t @78t/s)
2025-12-16 09:21:21,481 - src.llm.client - INFO - [inv:439257] ğŸ“Š 4.0s: 1353c @337c/s (250ch, ~338t @84t/s)
2025-12-16 09:21:23,490 - src.llm.client - INFO - [inv:439257] ğŸ“Š 6.0s: 2082c @345c/s (375ch, ~520t @86t/s)
2025-12-16 09:21:25,491 - src.llm.client - INFO - [inv:439257] ğŸ“Š 8.0s: 2689c @335c/s (502ch, ~672t @84t/s)
2025-12-16 09:21:27,497 - src.llm.client - INFO - [inv:439257] ğŸ“Š 10.0s: 3311c @330c/s (628ch, ~828t @82t/s)
2025-12-16 09:21:29,498 - src.llm.client - INFO - [inv:439257] ğŸ“Š 12.0s: 3950c @328c/s (756ch, ~988t @82t/s)
2025-12-16 09:21:31,503 - src.llm.client - INFO - [inv:439257] ğŸ“Š 14.0s: 4550c @324c/s (878ch, ~1138t @81t/s)
2025-12-16 09:21:33,516 - src.llm.client - INFO - [inv:439257] ğŸ“Š 16.1s: 5314c @331c/s (1005ch, ~1328t @83t/s)
2025-12-16 09:21:35,524 - src.llm.client - INFO - [inv:439257] ğŸ“Š 18.1s: 5799c @321c/s (1122ch, ~1450t @80t/s)
2025-12-16 09:21:37,536 - src.llm.client - INFO - [inv:439257] ğŸ“Š 20.1s: 6185c @308c/s (1247ch, ~1546t @77t/s)
2025-12-16 09:21:37,860 - src.llm.client - INFO - [inv:439257] âœ“ Done 25.34s: 6197c (~898w @245c/s)
2025-12-16 09:21:37,863 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:21:37,863 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:21:37,863 - generate_secondary - INFO -     - Length: 6183 chars, 896 words
2025-12-16 09:21:37,863 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:21:37,864 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:21:37,864 - generate_secondary - INFO -     - Structure: 4 sections
2025-12-16 09:21:37,864 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_13_generative_models_hierarchical_structures/session_13/investigation.md
2025-12-16 09:21:37,864 - generate_secondary - INFO - Generating open_questions for session 13: Multi-Level Models...
2025-12-16 09:21:37,864 - src.llm.client - INFO - [opq:608635] ğŸš€ opq | m=gemma3:4b | p=25411c | t=150s
2025-12-16 09:21:37,864 - src.llm.client - INFO - [opq:608635] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:21:37,864 - src.llm.client - INFO - [opq:608635] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:21:37,866 - src.llm.client - INFO - [opq:608635] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=29606 bytes, prompt=25411 chars
2025-12-16 09:21:37,866 - src.llm.client - INFO - [opq:608635] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:21:42,804 - src.llm.request_handler - INFO - [opq:608635] âœ“ Done 4.94s
2025-12-16 09:21:42,805 - src.llm.client - INFO - [opq:608635] âœ… HTTP 200 in 4.94s
2025-12-16 09:21:42,805 - src.llm.client - INFO - [opq:608635] ğŸ“¡ Stream active (200)
2025-12-16 09:21:42,805 - src.llm.client - INFO - [opq:608635] Starting stream parsing, waiting for first chunk...
2025-12-16 09:21:44,813 - src.llm.client - INFO - [opq:608635] ğŸ“Š 2.0s: 768c @382c/s (129ch, ~192t @96t/s)
2025-12-16 09:21:46,821 - src.llm.client - INFO - [opq:608635] ğŸ“Š 4.0s: 1493c @372c/s (253ch, ~373t @93t/s)
2025-12-16 09:21:49,005 - src.llm.client - INFO - [opq:608635] ğŸ“Š 6.2s: 2216c @357c/s (382ch, ~554t @89t/s)
2025-12-16 09:21:49,005 - src.llm.client - INFO - [opq:608635] âœ“ Done 11.14s: 2216c (~298w @199c/s)
2025-12-16 09:21:49,006 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:21:49,007 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:21:49,007 - generate_secondary - INFO -     - Length: 2202 chars, 296 words
2025-12-16 09:21:49,007 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:21:49,007 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:21:49,007 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:21:49,007 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_13_generative_models_hierarchical_structures/session_13/open_questions.md
2025-12-16 09:21:49,007 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:21:49,007 - generate_secondary - INFO - 
============================================================
2025-12-16 09:21:49,007 - generate_secondary - INFO - [14/15] Module 14: Model Selection & Validation (1 sessions)
2025-12-16 09:21:49,007 - generate_secondary - INFO - ============================================================
2025-12-16 09:21:49,007 - generate_secondary - INFO - 
  Session 14/15: Cross-Validation
2025-12-16 09:21:49,009 - generate_secondary - INFO - Generating application for session 14: Cross-Validation...
2025-12-16 09:21:49,009 - src.llm.client - INFO - [app:95544e] ğŸš€ app | m=gemma3:4b | p=29234c | t=150s
2025-12-16 09:21:49,009 - src.llm.client - INFO - [app:95544e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:21:49,009 - src.llm.client - INFO - [app:95544e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:21:49,010 - src.llm.client - INFO - [app:95544e] Sending request to Ollama: model=gemma3:4b, operation=application, payload=31219 bytes, prompt=29234 chars
2025-12-16 09:21:49,011 - src.llm.client - INFO - [app:95544e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:21:53,959 - src.llm.request_handler - INFO - [app:95544e] âœ“ Done 4.95s
2025-12-16 09:21:53,959 - src.llm.client - INFO - [app:95544e] âœ… HTTP 200 in 4.95s
2025-12-16 09:21:53,959 - src.llm.client - INFO - [app:95544e] ğŸ“¡ Stream active (200)
2025-12-16 09:21:53,959 - src.llm.client - INFO - [app:95544e] Starting stream parsing, waiting for first chunk...
2025-12-16 09:21:55,961 - src.llm.client - INFO - [app:95544e] ğŸ“Š 2.0s: 647c @323c/s (109ch, ~162t @81t/s)
2025-12-16 09:21:57,974 - src.llm.client - INFO - [app:95544e] ğŸ“Š 4.0s: 1366c @340c/s (229ch, ~342t @85t/s)
2025-12-16 09:21:59,984 - src.llm.client - INFO - [app:95544e] ğŸ“Š 6.0s: 2180c @362c/s (355ch, ~545t @90t/s)
2025-12-16 09:22:01,989 - src.llm.client - INFO - [app:95544e] ğŸ“Š 8.0s: 2896c @361c/s (469ch, ~724t @90t/s)
2025-12-16 09:22:04,001 - src.llm.client - INFO - [app:95544e] ğŸ“Š 10.0s: 3677c @366c/s (591ch, ~919t @92t/s)
2025-12-16 09:22:06,012 - src.llm.client - INFO - [app:95544e] ğŸ“Š 12.1s: 4427c @367c/s (706ch, ~1107t @92t/s)
2025-12-16 09:22:08,018 - src.llm.client - INFO - [app:95544e] ğŸ“Š 14.1s: 5196c @370c/s (827ch, ~1299t @92t/s)
2025-12-16 09:22:10,019 - src.llm.client - INFO - [app:95544e] ğŸ“Š 16.1s: 5869c @365c/s (951ch, ~1467t @91t/s)
2025-12-16 09:22:10,592 - src.llm.client - INFO - [app:95544e] âœ“ Done 21.58s: 5982c (~817w @277c/s)
2025-12-16 09:22:10,594 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:22:10,594 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:22:10,594 - generate_secondary - INFO -     - Length: 5970 chars, 815 words
2025-12-16 09:22:10,594 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:22:10,594 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:22:10,594 - generate_secondary - INFO -     - Avg words per application: 158
2025-12-16 09:22:10,594 - generate_secondary - WARNING - [WARNING] Application 4 has 141 words (require 150-200, need 9 more words) âš ï¸
2025-12-16 09:22:10,595 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_14_model_selection_validation/session_14/application.md
2025-12-16 09:22:10,595 - generate_secondary - INFO - Generating extension for session 14: Cross-Validation...
2025-12-16 09:22:10,595 - src.llm.client - INFO - [ext:d683f4] ğŸš€ ext | m=gemma3:4b | p=22457c | t=120s
2025-12-16 09:22:10,595 - src.llm.client - INFO - [ext:d683f4] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:22:10,595 - src.llm.client - INFO - [ext:d683f4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:22:10,597 - src.llm.client - INFO - [ext:d683f4] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=27297 bytes, prompt=22457 chars
2025-12-16 09:22:10,597 - src.llm.client - INFO - [ext:d683f4] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:22:15,472 - src.llm.request_handler - INFO - [ext:d683f4] âœ“ Done 4.87s
2025-12-16 09:22:15,472 - src.llm.client - INFO - [ext:d683f4] âœ… HTTP 200 in 4.87s
2025-12-16 09:22:15,472 - src.llm.client - INFO - [ext:d683f4] ğŸ“¡ Stream active (200)
2025-12-16 09:22:15,472 - src.llm.client - INFO - [ext:d683f4] Starting stream parsing, waiting for first chunk...
2025-12-16 09:22:17,488 - src.llm.client - INFO - [ext:d683f4] ğŸ“Š 2.0s: 810c @402c/s (129ch, ~202t @100t/s)
2025-12-16 09:22:19,493 - src.llm.client - INFO - [ext:d683f4] ğŸ“Š 4.0s: 1518c @378c/s (255ch, ~380t @94t/s)
2025-12-16 09:22:21,506 - src.llm.client - INFO - [ext:d683f4] ğŸ“Š 6.0s: 2228c @369c/s (386ch, ~557t @92t/s)
2025-12-16 09:22:23,508 - src.llm.client - INFO - [ext:d683f4] ğŸ“Š 8.0s: 3022c @376c/s (515ch, ~756t @94t/s)
2025-12-16 09:22:25,517 - src.llm.client - INFO - [ext:d683f4] ğŸ“Š 10.0s: 3755c @374c/s (647ch, ~939t @93t/s)
2025-12-16 09:22:26,283 - src.llm.client - INFO - [ext:d683f4] âœ“ Done 15.69s: 3997c (~533w @255c/s)
2025-12-16 09:22:26,284 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:22:26,285 - generate_secondary - INFO - [COMPLIANT] Extension generated âœ“
2025-12-16 09:22:26,285 - generate_secondary - INFO -     - Length: 3997 chars, 533 words
2025-12-16 09:22:26,285 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:22:26,285 - generate_secondary - INFO -     - Topics: 4
2025-12-16 09:22:26,285 - generate_secondary - INFO -     - Avg words per topic: 128
2025-12-16 09:22:26,285 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_14_model_selection_validation/session_14/extension.md
2025-12-16 09:22:26,285 - generate_secondary - INFO - Generating visualization for session 14: Cross-Validation...
2025-12-16 09:22:26,285 - src.llm.client - INFO - [viz:00336f] ğŸš€ viz | m=gemma3:4b | p=21417c | t=120s
2025-12-16 09:22:26,285 - src.llm.client - INFO - [viz:00336f] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:22:26,285 - src.llm.client - INFO - [viz:00336f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:22:26,287 - src.llm.client - INFO - [viz:00336f] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=25579 bytes, prompt=21417 chars
2025-12-16 09:22:26,287 - src.llm.client - INFO - [viz:00336f] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:22:31,096 - src.llm.request_handler - INFO - [viz:00336f] âœ“ Done 4.81s
2025-12-16 09:22:31,097 - src.llm.client - INFO - [viz:00336f] âœ… HTTP 200 in 4.81s
2025-12-16 09:22:31,097 - src.llm.client - INFO - [viz:00336f] ğŸ“¡ Stream active (200)
2025-12-16 09:22:31,097 - src.llm.client - INFO - [viz:00336f] Starting stream parsing, waiting for first chunk...
2025-12-16 09:22:32,906 - src.llm.client - INFO - [viz:00336f] âœ“ Done 6.62s: 341c (~47w @52c/s)
2025-12-16 09:22:32,907 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:22:32,907 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 09:22:32,907 - generate_secondary - INFO -     - Length: 325 chars (cleaned: 325 chars)
2025-12-16 09:22:32,907 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:22:32,907 - generate_secondary - INFO - [OK] Elements: 23 total (nodes: 12, connections: 11) âœ“
2025-12-16 09:22:32,907 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_14_model_selection_validation/session_14/visualization.mmd
2025-12-16 09:22:32,907 - generate_secondary - INFO - Generating integration for session 14: Cross-Validation...
2025-12-16 09:22:32,908 - src.llm.client - INFO - [int:44eb38] ğŸš€ int | m=gemma3:4b | p=22766c | t=150s
2025-12-16 09:22:32,908 - src.llm.client - INFO - [int:44eb38] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:22:32,908 - src.llm.client - INFO - [int:44eb38] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:22:32,909 - src.llm.client - INFO - [int:44eb38] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=27945 bytes, prompt=22766 chars
2025-12-16 09:22:32,909 - src.llm.client - INFO - [int:44eb38] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:22:37,697 - src.llm.request_handler - INFO - [int:44eb38] âœ“ Done 4.79s
2025-12-16 09:22:37,697 - src.llm.client - INFO - [int:44eb38] âœ… HTTP 200 in 4.79s
2025-12-16 09:22:37,697 - src.llm.client - INFO - [int:44eb38] ğŸ“¡ Stream active (200)
2025-12-16 09:22:37,697 - src.llm.client - INFO - [int:44eb38] Starting stream parsing, waiting for first chunk...
2025-12-16 09:22:39,706 - src.llm.client - INFO - [int:44eb38] ğŸ“Š 2.0s: 760c @378c/s (135ch, ~190t @95t/s)
2025-12-16 09:22:41,720 - src.llm.client - INFO - [int:44eb38] ğŸ“Š 4.0s: 1586c @394c/s (270ch, ~396t @99t/s)
2025-12-16 09:22:43,725 - src.llm.client - INFO - [int:44eb38] ğŸ“Š 6.0s: 2379c @395c/s (402ch, ~595t @99t/s)
2025-12-16 09:22:45,726 - src.llm.client - INFO - [int:44eb38] ğŸ“Š 8.0s: 3212c @400c/s (535ch, ~803t @100t/s)
2025-12-16 09:22:47,732 - src.llm.client - INFO - [int:44eb38] ğŸ“Š 10.0s: 3983c @397c/s (669ch, ~996t @99t/s)
2025-12-16 09:22:49,647 - src.llm.client - INFO - [int:44eb38] âœ“ Done 16.74s: 4422c (~597w @264c/s)
2025-12-16 09:22:49,649 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:22:49,649 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:22:49,649 - generate_secondary - INFO -     - Length: 4419 chars, 597 words
2025-12-16 09:22:49,649 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:22:49,650 - generate_secondary - INFO -     - Connections: 24
2025-12-16 09:22:49,651 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:22:49,651 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_14_model_selection_validation/session_14/integration.md
2025-12-16 09:22:49,652 - generate_secondary - INFO - Generating investigation for session 14: Cross-Validation...
2025-12-16 09:22:49,652 - src.llm.client - INFO - [inv:c85ff1] ğŸš€ inv | m=gemma3:4b | p=21679c | t=150s
2025-12-16 09:22:49,652 - src.llm.client - INFO - [inv:c85ff1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:22:49,652 - src.llm.client - INFO - [inv:c85ff1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:22:49,653 - src.llm.client - INFO - [inv:c85ff1] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=25801 bytes, prompt=21679 chars
2025-12-16 09:22:49,653 - src.llm.client - INFO - [inv:c85ff1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:22:54,441 - src.llm.request_handler - INFO - [inv:c85ff1] âœ“ Done 4.79s
2025-12-16 09:22:54,442 - src.llm.client - INFO - [inv:c85ff1] âœ… HTTP 200 in 4.79s
2025-12-16 09:22:54,442 - src.llm.client - INFO - [inv:c85ff1] ğŸ“¡ Stream active (200)
2025-12-16 09:22:54,442 - src.llm.client - INFO - [inv:c85ff1] Starting stream parsing, waiting for first chunk...
2025-12-16 09:22:56,456 - src.llm.client - INFO - [inv:c85ff1] ğŸ“Š 2.0s: 689c @342c/s (132ch, ~172t @86t/s)
2025-12-16 09:22:58,464 - src.llm.client - INFO - [inv:c85ff1] ğŸ“Š 4.0s: 1418c @353c/s (268ch, ~354t @88t/s)
2025-12-16 09:23:00,472 - src.llm.client - INFO - [inv:c85ff1] ğŸ“Š 6.0s: 2202c @365c/s (401ch, ~550t @91t/s)
2025-12-16 09:23:02,481 - src.llm.client - INFO - [inv:c85ff1] ğŸ“Š 8.0s: 2859c @356c/s (535ch, ~715t @89t/s)
2025-12-16 09:23:04,489 - src.llm.client - INFO - [inv:c85ff1] ğŸ“Š 10.0s: 3604c @359c/s (669ch, ~901t @90t/s)
2025-12-16 09:23:06,498 - src.llm.client - INFO - [inv:c85ff1] ğŸ“Š 12.1s: 4296c @356c/s (805ch, ~1074t @89t/s)
2025-12-16 09:23:08,508 - src.llm.client - INFO - [inv:c85ff1] ğŸ“Š 14.1s: 5027c @357c/s (939ch, ~1257t @89t/s)
2025-12-16 09:23:10,311 - src.llm.client - INFO - [inv:c85ff1] âœ“ Done 20.66s: 5667c (~810w @274c/s)
2025-12-16 09:23:10,313 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:23:10,313 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:23:10,313 - generate_secondary - INFO -     - Length: 5651 chars, 808 words
2025-12-16 09:23:10,313 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:23:10,313 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:23:10,313 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:23:10,314 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_14_model_selection_validation/session_14/investigation.md
2025-12-16 09:23:10,314 - generate_secondary - INFO - Generating open_questions for session 14: Cross-Validation...
2025-12-16 09:23:10,314 - src.llm.client - INFO - [opq:dd161e] ğŸš€ opq | m=gemma3:4b | p=21765c | t=150s
2025-12-16 09:23:10,314 - src.llm.client - INFO - [opq:dd161e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:23:10,314 - src.llm.client - INFO - [opq:dd161e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:23:10,315 - src.llm.client - INFO - [opq:dd161e] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=25898 bytes, prompt=21765 chars
2025-12-16 09:23:10,315 - src.llm.client - INFO - [opq:dd161e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:23:15,126 - src.llm.request_handler - INFO - [opq:dd161e] âœ“ Done 4.81s
2025-12-16 09:23:15,126 - src.llm.client - INFO - [opq:dd161e] âœ… HTTP 200 in 4.81s
2025-12-16 09:23:15,126 - src.llm.client - INFO - [opq:dd161e] ğŸ“¡ Stream active (200)
2025-12-16 09:23:15,126 - src.llm.client - INFO - [opq:dd161e] Starting stream parsing, waiting for first chunk...
2025-12-16 09:23:17,134 - src.llm.client - INFO - [opq:dd161e] ğŸ“Š 2.0s: 781c @389c/s (134ch, ~195t @97t/s)
2025-12-16 09:23:19,144 - src.llm.client - INFO - [opq:dd161e] ğŸ“Š 4.0s: 1584c @394c/s (267ch, ~396t @99t/s)
2025-12-16 09:23:20,875 - src.llm.client - INFO - [opq:dd161e] âœ“ Done 10.56s: 2218c (~302w @210c/s)
2025-12-16 09:23:20,876 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:23:20,876 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:23:20,876 - generate_secondary - INFO -     - Length: 2206 chars, 300 words
2025-12-16 09:23:20,876 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:23:20,876 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:23:20,876 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:23:20,876 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_14_model_selection_validation/session_14/open_questions.md
2025-12-16 09:23:20,876 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:23:20,877 - generate_secondary - INFO - 
============================================================
2025-12-16 09:23:20,877 - generate_secondary - INFO - [15/15] Module 15: Applications & Future Directions (1 sessions)
2025-12-16 09:23:20,877 - generate_secondary - INFO - ============================================================
2025-12-16 09:23:20,877 - generate_secondary - INFO - 
  Session 15/15: Concluding Remarks
2025-12-16 09:23:20,878 - generate_secondary - INFO - Generating application for session 15: Concluding Remarks...
2025-12-16 09:23:20,878 - src.llm.client - INFO - [app:0cad2d] ğŸš€ app | m=gemma3:4b | p=31475c | t=150s
2025-12-16 09:23:20,879 - src.llm.client - INFO - [app:0cad2d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:23:20,879 - src.llm.client - INFO - [app:0cad2d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:23:20,880 - src.llm.client - INFO - [app:0cad2d] Sending request to Ollama: model=gemma3:4b, operation=application, payload=33440 bytes, prompt=31475 chars
2025-12-16 09:23:20,880 - src.llm.client - INFO - [app:0cad2d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:23:25,703 - src.llm.request_handler - INFO - [app:0cad2d] âœ“ Done 4.82s
2025-12-16 09:23:25,703 - src.llm.client - INFO - [app:0cad2d] âœ… HTTP 200 in 4.82s
2025-12-16 09:23:25,703 - src.llm.client - INFO - [app:0cad2d] ğŸ“¡ Stream active (200)
2025-12-16 09:23:25,703 - src.llm.client - INFO - [app:0cad2d] Starting stream parsing, waiting for first chunk...
2025-12-16 09:23:27,705 - src.llm.client - INFO - [app:0cad2d] ğŸ“Š 2.0s: 852c @426c/s (132ch, ~213t @106t/s)
2025-12-16 09:23:29,714 - src.llm.client - INFO - [app:0cad2d] ğŸ“Š 4.0s: 1678c @418c/s (263ch, ~420t @105t/s)
2025-12-16 09:23:31,718 - src.llm.client - INFO - [app:0cad2d] ğŸ“Š 6.0s: 2560c @426c/s (396ch, ~640t @106t/s)
2025-12-16 09:23:33,729 - src.llm.client - INFO - [app:0cad2d] ğŸ“Š 8.0s: 3461c @431c/s (529ch, ~865t @108t/s)
2025-12-16 09:23:35,732 - src.llm.client - INFO - [app:0cad2d] ğŸ“Š 10.0s: 4315c @430c/s (661ch, ~1079t @108t/s)
2025-12-16 09:23:37,737 - src.llm.client - INFO - [app:0cad2d] ğŸ“Š 12.0s: 5168c @429c/s (793ch, ~1292t @107t/s)
2025-12-16 09:23:38,912 - src.llm.client - INFO - [app:0cad2d] âœ“ Done 18.03s: 5579c (~716w @309c/s)
2025-12-16 09:23:38,915 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:23:38,916 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-16 09:23:38,916 - generate_secondary - INFO -     - Length: 5567 chars, 714 words
2025-12-16 09:23:38,916 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-16 09:23:38,916 - generate_secondary - INFO -     - Applications: 5
2025-12-16 09:23:38,916 - generate_secondary - INFO -     - Avg words per application: 138
2025-12-16 09:23:38,916 - generate_secondary - WARNING - [WARNING] Application 1 has 146 words (require 150-200, need 4 more words) âš ï¸
2025-12-16 09:23:38,916 - generate_secondary - WARNING - [WARNING] Application 2 has 141 words (require 150-200, need 9 more words) âš ï¸
2025-12-16 09:23:38,916 - generate_secondary - WARNING - [WARNING] Application 3 has 138 words (require 150-200, need 12 more words) âš ï¸
2025-12-16 09:23:38,916 - generate_secondary - WARNING - [WARNING] Application 4 has 130 words (require 150-200, need 20 more words) âš ï¸
2025-12-16 09:23:38,916 - generate_secondary - WARNING - [WARNING] Application 5 has 135 words (require 150-200, need 15 more words) âš ï¸
2025-12-16 09:23:38,917 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_15_applications_future_directions/session_15/application.md
2025-12-16 09:23:38,917 - generate_secondary - INFO - Generating extension for session 15: Concluding Remarks...
2025-12-16 09:23:38,917 - src.llm.client - INFO - [ext:53a72c] ğŸš€ ext | m=gemma3:4b | p=24698c | t=120s
2025-12-16 09:23:38,917 - src.llm.client - INFO - [ext:53a72c] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:23:38,917 - src.llm.client - INFO - [ext:53a72c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:23:38,918 - src.llm.client - INFO - [ext:53a72c] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=29518 bytes, prompt=24698 chars
2025-12-16 09:23:38,918 - src.llm.client - INFO - [ext:53a72c] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:23:43,701 - src.llm.request_handler - INFO - [ext:53a72c] âœ“ Done 4.78s
2025-12-16 09:23:43,701 - src.llm.client - INFO - [ext:53a72c] âœ… HTTP 200 in 4.78s
2025-12-16 09:23:43,702 - src.llm.client - INFO - [ext:53a72c] ğŸ“¡ Stream active (200)
2025-12-16 09:23:43,702 - src.llm.client - INFO - [ext:53a72c] Starting stream parsing, waiting for first chunk...
2025-12-16 09:23:45,713 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 2.0s: 810c @403c/s (137ch, ~202t @101t/s)
2025-12-16 09:23:47,724 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 4.0s: 1624c @404c/s (271ch, ~406t @101t/s)
2025-12-16 09:23:49,735 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 6.0s: 2491c @413c/s (405ch, ~623t @103t/s)
2025-12-16 09:23:51,744 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 8.0s: 3295c @410c/s (539ch, ~824t @102t/s)
2025-12-16 09:23:53,747 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 10.0s: 4016c @400c/s (672ch, ~1004t @100t/s)
2025-12-16 09:23:55,749 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 12.0s: 4521c @375c/s (798ch, ~1130t @94t/s)
2025-12-16 09:23:57,750 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 14.0s: 4925c @351c/s (930ch, ~1231t @88t/s)
2025-12-16 09:23:59,762 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 16.1s: 5368c @334c/s (1064ch, ~1342t @84t/s)
2025-12-16 09:24:01,772 - src.llm.client - INFO - [ext:53a72c] ğŸ“Š 18.1s: 5895c @326c/s (1198ch, ~1474t @82t/s)
2025-12-16 09:24:02,279 - src.llm.client - INFO - [ext:53a72c] âœ“ Done 23.36s: 5937c (~786w @254c/s)
2025-12-16 09:24:02,282 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:24:02,282 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - INFO -     - Length: 5273 chars, 687 words
2025-12-16 09:24:02,282 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-16 09:24:02,282 - generate_secondary - INFO -     - Topics: 10
2025-12-16 09:24:02,282 - generate_secondary - INFO -     - Avg words per topic: 65
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Too many topics (10, maximum 4, 6 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 1 has 165 words (exceeds 150 by 15 words - consider condensing) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 2 has 163 words (exceeds 150 by 13 words - consider condensing) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 3 has 219 words (exceeds 150 by 69 words - consider condensing) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 4 has 8 words (require 100-150, need 92 more words) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 5 has 9 words (require 100-150, need 91 more words) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 6 has 22 words (require 100-150, need 78 more words) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 7 has 47 words (require 100-150, need 53 more words) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 8 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 9 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Topic 10 has 14 words (require 100-150, need 86 more words) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - WARNING - [WARNING] Total word count (687) exceeds maximum 600 (exceeds by 87 words - condense content) âš ï¸
2025-12-16 09:24:02,282 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:24:02,282 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:24:02,283 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_15_applications_future_directions/session_15/extension.md
2025-12-16 09:24:02,283 - generate_secondary - INFO - Generating visualization for session 15: Concluding Remarks...
2025-12-16 09:24:02,283 - src.llm.client - INFO - [viz:5b9469] ğŸš€ viz | m=gemma3:4b | p=23658c | t=120s
2025-12-16 09:24:02,283 - src.llm.client - INFO - [viz:5b9469] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 09:24:02,283 - src.llm.client - INFO - [viz:5b9469] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:24:02,285 - src.llm.client - INFO - [viz:5b9469] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=27800 bytes, prompt=23658 chars
2025-12-16 09:24:02,285 - src.llm.client - INFO - [viz:5b9469] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 09:24:07,096 - src.llm.request_handler - INFO - [viz:5b9469] âœ“ Done 4.81s
2025-12-16 09:24:07,096 - src.llm.client - INFO - [viz:5b9469] âœ… HTTP 200 in 4.81s
2025-12-16 09:24:07,096 - src.llm.client - INFO - [viz:5b9469] ğŸ“¡ Stream active (200)
2025-12-16 09:24:07,096 - src.llm.client - INFO - [viz:5b9469] Starting stream parsing, waiting for first chunk...
2025-12-16 09:24:09,099 - src.llm.client - INFO - [viz:5b9469] ğŸ“Š 2.0s: 521c @260c/s (133ch, ~130t @65t/s)
2025-12-16 09:24:11,267 - src.llm.client - INFO - [viz:5b9469] ğŸ“Š 4.2s: 1127c @270c/s (266ch, ~282t @68t/s)
2025-12-16 09:24:11,268 - src.llm.client - INFO - [viz:5b9469] âœ“ Done 8.99s: 1127c (~158w @125c/s)
2025-12-16 09:24:11,268 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:24:11,268 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 09:24:11,268 - generate_secondary - INFO -     - Length: 279 chars (cleaned: 279 chars)
2025-12-16 09:24:11,268 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-16 09:24:11,268 - generate_secondary - INFO - [CRITICAL] Elements: 16 total (nodes: 8, connections: 8) ğŸ”´
2025-12-16 09:24:11,268 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 09:24:11,268 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-16 09:24:11,268 - generate_secondary - WARNING - [WARNING] Only 8 nodes found (require at least 10, need 2 more - add more nodes to the diagram) âš ï¸
2025-12-16 09:24:11,268 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 09:24:11,268 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 09:24:11,269 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_15_applications_future_directions/session_15/visualization.mmd
2025-12-16 09:24:11,269 - generate_secondary - INFO - Generating integration for session 15: Concluding Remarks...
2025-12-16 09:24:11,269 - src.llm.client - INFO - [int:32a12c] ğŸš€ int | m=gemma3:4b | p=25007c | t=150s
2025-12-16 09:24:11,269 - src.llm.client - INFO - [int:32a12c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:24:11,269 - src.llm.client - INFO - [int:32a12c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:24:11,270 - src.llm.client - INFO - [int:32a12c] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=30166 bytes, prompt=25007 chars
2025-12-16 09:24:11,270 - src.llm.client - INFO - [int:32a12c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:24:16,074 - src.llm.request_handler - INFO - [int:32a12c] âœ“ Done 4.80s
2025-12-16 09:24:16,074 - src.llm.client - INFO - [int:32a12c] âœ… HTTP 200 in 4.80s
2025-12-16 09:24:16,074 - src.llm.client - INFO - [int:32a12c] ğŸ“¡ Stream active (200)
2025-12-16 09:24:16,075 - src.llm.client - INFO - [int:32a12c] Starting stream parsing, waiting for first chunk...
2025-12-16 09:24:18,079 - src.llm.client - INFO - [int:32a12c] ğŸ“Š 2.0s: 715c @357c/s (134ch, ~179t @89t/s)
2025-12-16 09:24:20,079 - src.llm.client - INFO - [int:32a12c] ğŸ“Š 4.0s: 1504c @376c/s (267ch, ~376t @94t/s)
2025-12-16 09:24:22,085 - src.llm.client - INFO - [int:32a12c] ğŸ“Š 6.0s: 2128c @354c/s (401ch, ~532t @89t/s)
2025-12-16 09:24:24,090 - src.llm.client - INFO - [int:32a12c] ğŸ“Š 8.0s: 2638c @329c/s (535ch, ~660t @82t/s)
2025-12-16 09:24:26,098 - src.llm.client - INFO - [int:32a12c] ğŸ“Š 10.0s: 3115c @311c/s (669ch, ~779t @78t/s)
2025-12-16 09:24:28,103 - src.llm.client - INFO - [int:32a12c] ğŸ“Š 12.0s: 3672c @305c/s (803ch, ~918t @76t/s)
2025-12-16 09:24:29,411 - src.llm.client - INFO - [int:32a12c] âœ“ Done 18.14s: 4003c (~581w @221c/s)
2025-12-16 09:24:29,413 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 09:24:29,413 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-16 09:24:29,413 - generate_secondary - INFO -     - Length: 4001 chars, 581 words
2025-12-16 09:24:29,413 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-16 09:24:29,413 - generate_secondary - INFO -     - Connections: 15
2025-12-16 09:24:29,413 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-16 09:24:29,413 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_15_applications_future_directions/session_15/integration.md
2025-12-16 09:24:29,413 - generate_secondary - INFO - Generating investigation for session 15: Concluding Remarks...
2025-12-16 09:24:29,414 - src.llm.client - INFO - [inv:4cfd45] ğŸš€ inv | m=gemma3:4b | p=23920c | t=150s
2025-12-16 09:24:29,414 - src.llm.client - INFO - [inv:4cfd45] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:24:29,414 - src.llm.client - INFO - [inv:4cfd45] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:24:29,415 - src.llm.client - INFO - [inv:4cfd45] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=28022 bytes, prompt=23920 chars
2025-12-16 09:24:29,415 - src.llm.client - INFO - [inv:4cfd45] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:24:34,223 - src.llm.request_handler - INFO - [inv:4cfd45] âœ“ Done 4.81s
2025-12-16 09:24:34,224 - src.llm.client - INFO - [inv:4cfd45] âœ… HTTP 200 in 4.81s
2025-12-16 09:24:34,224 - src.llm.client - INFO - [inv:4cfd45] ğŸ“¡ Stream active (200)
2025-12-16 09:24:34,224 - src.llm.client - INFO - [inv:4cfd45] Starting stream parsing, waiting for first chunk...
2025-12-16 09:24:36,235 - src.llm.client - INFO - [inv:4cfd45] ğŸ“Š 2.0s: 629c @313c/s (134ch, ~157t @78t/s)
2025-12-16 09:24:38,247 - src.llm.client - INFO - [inv:4cfd45] ğŸ“Š 4.0s: 1339c @333c/s (267ch, ~335t @83t/s)
2025-12-16 09:24:40,255 - src.llm.client - INFO - [inv:4cfd45] ğŸ“Š 6.0s: 2085c @346c/s (401ch, ~521t @86t/s)
2025-12-16 09:24:42,265 - src.llm.client - INFO - [inv:4cfd45] ğŸ“Š 8.0s: 2797c @348c/s (535ch, ~699t @87t/s)
2025-12-16 09:24:44,273 - src.llm.client - INFO - [inv:4cfd45] ğŸ“Š 10.0s: 3585c @357c/s (666ch, ~896t @89t/s)
2025-12-16 09:24:46,283 - src.llm.client - INFO - [inv:4cfd45] ğŸ“Š 12.1s: 4265c @354c/s (796ch, ~1066t @88t/s)
2025-12-16 09:24:48,001 - src.llm.client - INFO - [inv:4cfd45] âœ“ Done 18.59s: 4887c (~687w @263c/s)
2025-12-16 09:24:48,002 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:24:48,002 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-16 09:24:48,002 - generate_secondary - INFO -     - Length: 4873 chars, 685 words
2025-12-16 09:24:48,002 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:24:48,002 - generate_secondary - INFO -     - Research questions: 3
2025-12-16 09:24:48,002 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:24:48,003 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_15_applications_future_directions/session_15/investigation.md
2025-12-16 09:24:48,003 - generate_secondary - INFO - Generating open_questions for session 15: Concluding Remarks...
2025-12-16 09:24:48,003 - src.llm.client - INFO - [opq:b5d604] ğŸš€ opq | m=gemma3:4b | p=24006c | t=150s
2025-12-16 09:24:48,003 - src.llm.client - INFO - [opq:b5d604] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 09:24:48,003 - src.llm.client - INFO - [opq:b5d604] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 09:24:48,004 - src.llm.client - INFO - [opq:b5d604] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=28119 bytes, prompt=24006 chars
2025-12-16 09:24:48,004 - src.llm.client - INFO - [opq:b5d604] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 09:24:52,838 - src.llm.request_handler - INFO - [opq:b5d604] âœ“ Done 4.83s
2025-12-16 09:24:52,838 - src.llm.client - INFO - [opq:b5d604] âœ… HTTP 200 in 4.83s
2025-12-16 09:24:52,839 - src.llm.client - INFO - [opq:b5d604] ğŸ“¡ Stream active (200)
2025-12-16 09:24:52,839 - src.llm.client - INFO - [opq:b5d604] Starting stream parsing, waiting for first chunk...
2025-12-16 09:24:54,844 - src.llm.client - INFO - [opq:b5d604] ğŸ“Š 2.0s: 795c @396c/s (130ch, ~199t @99t/s)
2025-12-16 09:24:56,858 - src.llm.client - INFO - [opq:b5d604] ğŸ“Š 4.0s: 1621c @403c/s (256ch, ~405t @101t/s)
2025-12-16 09:24:58,867 - src.llm.client - INFO - [opq:b5d604] ğŸ“Š 6.0s: 2410c @400c/s (386ch, ~602t @100t/s)
2025-12-16 09:24:59,820 - src.llm.client - INFO - [opq:b5d604] âœ“ Done 11.82s: 2697c (~352w @228c/s)
2025-12-16 09:24:59,821 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-16 09:24:59,821 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-16 09:24:59,821 - generate_secondary - INFO -     - Length: 2684 chars, 350 words
2025-12-16 09:24:59,821 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-16 09:24:59,821 - generate_secondary - INFO -     - Open questions: 3
2025-12-16 09:24:59,821 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-16 09:24:59,821 - generate_secondary - INFO -   â†’ Saved to: output/active_inference_college/modules/module_15_applications_future_directions/session_15/open_questions.md
2025-12-16 09:24:59,821 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-16 09:24:59,822 - generate_secondary - INFO - 
2025-12-16 09:24:59,822 - generate_secondary - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 09:24:59,822 - generate_secondary - INFO - [ALL COMPLIANT] Secondary Materials Generation - Summary âœ…
2025-12-16 09:24:59,822 - generate_secondary - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 09:24:59,822 - generate_secondary - INFO -   Items Processed: 15
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - [COMPLIANT] Successful: 14
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - [ERROR] Failed: 1
2025-12-16 09:24:59,822 - generate_secondary - INFO - 
2025-12-16 09:24:59,822 - generate_secondary - INFO -   Compliance Breakdown:
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - [COMPLIANT]: 15
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - [NEEDS REVIEW]: 0
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - [CRITICAL]: 0
2025-12-16 09:24:59,822 - generate_secondary - INFO - 
2025-12-16 09:24:59,822 - generate_secondary - INFO -   Issue Statistics:
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - Total Issues: 0
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - Critical Errors: 0
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - Warnings: 0
2025-12-16 09:24:59,822 - generate_secondary - INFO - 
2025-12-16 09:24:59,822 - generate_secondary - INFO -   Recommendations:
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - All content generated successfully
2025-12-16 09:24:59,822 - generate_secondary - INFO -     - No issues detected
2025-12-16 09:24:59,822 - generate_secondary - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 09:24:59,822 - generate_secondary - ERROR - 
================================================================================
2025-12-16 09:24:59,822 - generate_secondary - ERROR - EXIT CODE: 1 (FAILURE)
2025-12-16 09:24:59,822 - generate_secondary - ERROR - ================================================================================
2025-12-16 09:24:59,822 - generate_secondary - ERROR - Reason: 1 session(s) failed during generation
2025-12-16 09:24:59,822 - generate_secondary - ERROR - 
2025-12-16 09:24:59,822 - generate_secondary - ERROR - Failed sessions details:
2025-12-16 09:24:59,822 - generate_secondary - ERROR -   (Error details not captured - check log file for full traceback)
2025-12-16 09:24:59,822 - generate_secondary - ERROR - 
2025-12-16 09:24:59,822 - generate_secondary - ERROR - Troubleshooting:
2025-12-16 09:24:59,822 - generate_secondary - ERROR -   1. Check log file for detailed error messages and request IDs
2025-12-16 09:24:59,822 - generate_secondary - ERROR -   2. Review docs/TROUBLESHOOTING.md for common issues
2025-12-16 09:24:59,822 - generate_secondary - ERROR -   3. For timeout errors, see timeout troubleshooting section
2025-12-16 09:24:59,822 - generate_secondary - ERROR -   4. Use request IDs to filter logs: grep '[REQUEST_ID]' output/logs/*.log
2025-12-16 09:24:59,822 - generate_secondary - ERROR - ================================================================================
