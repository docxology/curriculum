2025-12-16 12:07:40,136 - root - INFO - Logging to file: /Users/4d/Documents/GitHub/curriculum/scripts/output/logs/04_generate_primary_20251216_120740.log
2025-12-16 12:07:40,136 - generate_primary - INFO - 
2025-12-16 12:07:40,136 - generate_primary - INFO - ğŸ“š STAGE 04: PRIMARY MATERIALS (Session-Based)
2025-12-16 12:07:40,136 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 12:07:40,136 - generate_primary - INFO - Generating materials PER SESSION (not per module)
2025-12-16 12:07:40,136 - generate_primary - INFO - Output structure: output/modules/module_XX/session_YY/[material].md
2025-12-16 12:07:40,136 - generate_primary - INFO - 
2025-12-16 12:07:40,136 - src.config.loader - INFO - Initialized ConfigLoader with directory: /Users/4d/Documents/GitHub/curriculum/config
2025-12-16 12:07:40,137 - src.config.loader - INFO - Course configuration validated successfully
2025-12-16 12:07:40,151 - src.config.loader - INFO - All configurations validated successfully
2025-12-16 12:07:40,151 - generate_primary - INFO - PRIMARY ARTIFACTS GENERATED PER SESSION:
2025-12-16 12:07:40,151 - generate_primary - INFO -   1. lecture.md - Comprehensive instructional content
2025-12-16 12:07:40,151 - generate_primary - INFO -   2. lab.md - Laboratory exercise with procedures
2025-12-16 12:07:40,151 - generate_primary - INFO -   3. study_notes.md - Concise session summary
2025-12-16 12:07:40,151 - generate_primary - INFO -   4. diagram_1.mmd, diagram_2.mmd, ... (up to 4 diagrams)
2025-12-16 12:07:40,151 - generate_primary - INFO -   5. questions.md - Comprehension assessment questions
2025-12-16 12:07:40,151 - generate_primary - INFO - 
2025-12-16 12:07:40,151 - generate_primary - INFO - 
2025-12-16 12:07:40,151 - generate_primary - INFO - âš™ï¸ CONFIGURATION
2025-12-16 12:07:40,151 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 12:07:40,151 - generate_primary - INFO -   â€¢ Diagrams per Session: 4
2025-12-16 12:07:40,151 - generate_primary - INFO -   â€¢ Log File: output/logs/04_generate_primary_20251216_120740.log
2025-12-16 12:07:40,151 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 12:07:40,151 - generate_primary - INFO - Using specified outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/active_inference_ai_short/outlines/course_outline_20251216_120739.json
2025-12-16 12:07:40,151 - generate_primary - INFO - 
2025-12-16 12:07:40,151 - generate_primary - INFO - Processing ALL modules from outline
2025-12-16 12:07:40,151 - src.generate.orchestration.pipeline - INFO - Initializing Educational Course Generator pipeline...
2025-12-16 12:07:40,152 - src.llm.client - INFO - Initialized OllamaClient: model=gemma3:4b, url=http://localhost:11434/api/generate
2025-12-16 12:07:40,152 - src.generate.stages.stage1_outline - INFO - Initialized OutlineGenerator
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - Pipeline initialized successfully
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - STAGE 2: Generating Primary Content (Session-Based)
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - Using explicit outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/active_inference_ai_short/outlines/course_outline_20251216_120739.json
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - Processing 3 modules with session-based generation
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - Using course-specific output directory: output/active_inference_ai_short/
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - Module 1: Foundations of Active Inference (1 sessions)
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:07:40,152 - src.generate.orchestration.pipeline - INFO - 
[1/3] Session 1: Introduction to Active Inference
2025-12-16 12:07:40,153 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 12:07:40,153 - src.generate.formats.lectures - INFO - Generating lecture for: Foundations of Active Inference (Session 1/3)
2025-12-16 12:07:40,153 - src.llm.client - INFO - [lec:429cc4] ğŸš€ lec | m=gemma3:4b | p=3505c | t=180s
2025-12-16 12:07:40,153 - src.llm.client - INFO - [lec:429cc4] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 12:07:40,153 - src.llm.client - INFO - [lec:429cc4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:07:40,162 - src.llm.client - INFO - [lec:429cc4] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=7142 bytes, prompt=3505 chars
2025-12-16 12:07:40,163 - src.llm.client - INFO - [lec:429cc4] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 12:07:41,374 - src.llm.request_handler - INFO - [lec:429cc4] âœ“ Done 1.21s
2025-12-16 12:07:41,374 - src.llm.client - INFO - [lec:429cc4] âœ… HTTP 200 in 1.21s
2025-12-16 12:07:41,374 - src.llm.client - INFO - [lec:429cc4] ğŸ“¡ Stream active (200)
2025-12-16 12:07:41,374 - src.llm.client - INFO - [lec:429cc4] Starting stream parsing, waiting for first chunk...
2025-12-16 12:07:43,381 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 2.0s: 782c @390c/s (122ch, ~196t @97t/s)
2025-12-16 12:07:45,390 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 4.0s: 1385c @345c/s (237ch, ~346t @86t/s)
2025-12-16 12:07:47,402 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 6.0s: 1908c @317c/s (349ch, ~477t @79t/s)
2025-12-16 12:07:49,405 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 8.0s: 2570c @320c/s (473ch, ~642t @80t/s)
2025-12-16 12:07:51,408 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 10.0s: 3122c @311c/s (578ch, ~780t @78t/s)
2025-12-16 12:07:53,408 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 12.0s: 3762c @313c/s (700ch, ~940t @78t/s)
2025-12-16 12:07:55,417 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 14.0s: 4378c @312c/s (819ch, ~1094t @78t/s)
2025-12-16 12:07:57,425 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 16.0s: 5025c @313c/s (936ch, ~1256t @78t/s)
2025-12-16 12:07:59,427 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 18.1s: 5650c @313c/s (1053ch, ~1412t @78t/s)
2025-12-16 12:08:01,440 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 20.1s: 6296c @314c/s (1175ch, ~1574t @78t/s)
2025-12-16 12:08:03,448 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 22.1s: 6896c @312c/s (1282ch, ~1724t @78t/s)
2025-12-16 12:08:05,448 - src.llm.client - INFO - [lec:429cc4] ğŸ“Š 24.1s: 7528c @313c/s (1399ch, ~1882t @78t/s)
2025-12-16 12:08:07,212 - src.llm.client - INFO - [lec:429cc4] âœ“ Done 27.06s: 8152c (~1212w @301c/s)
2025-12-16 12:08:07,214 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 12:08:07,214 - src.generate.formats.lectures - INFO -     - Length: 8408 chars, 1251 words
2025-12-16 12:08:07,214 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 12:08:07,214 - src.generate.formats.lectures - INFO -     - Structure: 7 sections, 0 subsections
2025-12-16 12:08:07,214 - src.generate.formats.lectures - INFO -     - Content: 11 examples, 5 terms defined
2025-12-16 12:08:07,214 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 12:08:07,219 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:08:07,220 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 12:08:07,220 - src.generate.formats.labs - INFO - Generating lab 1 for: Foundations of Active Inference (Session 1)
2025-12-16 12:08:07,220 - src.llm.client - INFO - [lab:72fd58] ğŸš€ lab | m=gemma3:4b | p=3556c | t=150s
2025-12-16 12:08:07,220 - src.llm.client - INFO - [lab:72fd58] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 12:08:07,220 - src.llm.client - INFO - [lab:72fd58] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:08:07,223 - src.llm.client - INFO - [lab:72fd58] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3998 bytes, prompt=3556 chars
2025-12-16 12:08:07,223 - src.llm.client - INFO - [lab:72fd58] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 12:08:08,223 - src.llm.request_handler - INFO - [lab:72fd58] âœ“ Done 1.00s
2025-12-16 12:08:08,223 - src.llm.client - INFO - [lab:72fd58] âœ… HTTP 200 in 1.00s
2025-12-16 12:08:08,223 - src.llm.client - INFO - [lab:72fd58] ğŸ“¡ Stream active (200)
2025-12-16 12:08:08,223 - src.llm.client - INFO - [lab:72fd58] Starting stream parsing, waiting for first chunk...
2025-12-16 12:08:10,225 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 2.0s: 633c @316c/s (126ch, ~158t @79t/s)
2025-12-16 12:08:12,253 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 4.0s: 1218c @304c/s (237ch, ~304t @76t/s)
2025-12-16 12:08:14,241 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 6.0s: 1613c @268c/s (348ch, ~403t @67t/s)
2025-12-16 12:08:16,258 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 8.0s: 2095c @261c/s (458ch, ~524t @65t/s)
2025-12-16 12:08:18,272 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 10.0s: 2742c @273c/s (580ch, ~686t @68t/s)
2025-12-16 12:08:20,284 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 12.1s: 3231c @268c/s (697ch, ~808t @67t/s)
2025-12-16 12:08:22,290 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 14.1s: 3775c @268c/s (818ch, ~944t @67t/s)
2025-12-16 12:08:24,299 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 16.1s: 4102c @255c/s (935ch, ~1026t @64t/s)
2025-12-16 12:08:26,300 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 18.1s: 4538c @251c/s (1052ch, ~1134t @63t/s)
2025-12-16 12:08:28,308 - src.llm.client - INFO - [lab:72fd58] ğŸ“Š 20.1s: 5079c @253c/s (1165ch, ~1270t @63t/s)
2025-12-16 12:08:29,481 - src.llm.client - INFO - [lab:72fd58] âœ“ Done 22.26s: 5443c (~792w @245c/s)
2025-12-16 12:08:29,484 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 12:08:29,484 - src.generate.formats.labs - INFO -     - Length: 5547 chars, 808 words
2025-12-16 12:08:29,484 - src.generate.formats.labs - INFO -     - Procedure: 9 steps
2025-12-16 12:08:29,484 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-16 12:08:29,484 - src.generate.formats.labs - INFO -     - Data tables: 9
2025-12-16 12:08:29,486 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:08:29,488 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 12:08:29,488 - src.generate.formats.study_notes - INFO - Generating study notes for: Foundations of Active Inference (Session 1)
2025-12-16 12:08:29,488 - src.llm.client - INFO - [stu:6a8833] ğŸš€ stu | m=gemma3:4b | p=4705c | t=120s
2025-12-16 12:08:29,488 - src.llm.client - INFO - [stu:6a8833] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:08:29,488 - src.llm.client - INFO - [stu:6a8833] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:08:29,493 - src.llm.client - INFO - [stu:6a8833] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8364 bytes, prompt=4705 chars
2025-12-16 12:08:29,493 - src.llm.client - INFO - [stu:6a8833] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:08:30,973 - src.llm.request_handler - INFO - [stu:6a8833] âœ“ Done 1.48s
2025-12-16 12:08:30,974 - src.llm.client - INFO - [stu:6a8833] âœ… HTTP 200 in 1.48s
2025-12-16 12:08:30,979 - src.llm.client - INFO - [stu:6a8833] ğŸ“¡ Stream active (200)
2025-12-16 12:08:30,982 - src.llm.client - INFO - [stu:6a8833] Starting stream parsing, waiting for first chunk...
2025-12-16 12:08:32,996 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 2.0s: 768c @381c/s (120ch, ~192t @95t/s)
2025-12-16 12:08:34,999 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 4.0s: 1385c @345c/s (237ch, ~346t @86t/s)
2025-12-16 12:08:37,005 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 6.0s: 1970c @327c/s (348ch, ~492t @82t/s)
2025-12-16 12:08:39,013 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 8.0s: 2589c @322c/s (464ch, ~647t @81t/s)
2025-12-16 12:08:41,016 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 10.0s: 3190c @318c/s (580ch, ~798t @79t/s)
2025-12-16 12:08:43,033 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 12.1s: 3796c @315c/s (695ch, ~949t @79t/s)
2025-12-16 12:08:45,044 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 14.1s: 4344c @309c/s (797ch, ~1086t @77t/s)
2025-12-16 12:08:47,047 - src.llm.client - INFO - [stu:6a8833] ğŸ“Š 16.1s: 4828c @301c/s (894ch, ~1207t @75t/s)
2025-12-16 12:08:47,966 - src.llm.client - INFO - [stu:6a8833] âœ“ Done 18.48s: 5060c (~726w @274c/s)
2025-12-16 12:08:47,968 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 12:08:47,968 - src.generate.formats.study_notes - INFO -     - Length: 5126 chars, 737 words
2025-12-16 12:08:47,968 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 12:08:47,968 - src.generate.formats.study_notes - INFO -     - Key concepts: 3
2025-12-16 12:08:47,968 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 3 bullets
2025-12-16 12:08:47,968 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 12:08:47,970 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:08:47,970 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 12:08:47,971 - src.generate.formats.diagrams - INFO - Generating diagram for: Free Energy Minimization (Foundations of Active Inference)
2025-12-16 12:08:47,971 - src.llm.client - INFO - [dia:39b5ca] ğŸš€ dia | m=gemma3:4b | p=5778c | t=120s
2025-12-16 12:08:47,971 - src.generate.formats.diagrams - INFO - Generating diagram for: Predictive Coding (Foundations of Active Inference)
2025-12-16 12:08:47,971 - src.generate.formats.diagrams - INFO - Generating diagram for: Bayesian Inference (Foundations of Active Inference)
2025-12-16 12:08:47,971 - src.generate.formats.diagrams - INFO - Generating diagram for: Perception-Action Loops (Foundations of Active Inference)
2025-12-16 12:08:47,972 - src.llm.client - INFO - [dia:39b5ca] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:08:47,972 - src.llm.client - INFO - [dia:0b0d07] ğŸš€ dia | m=gemma3:4b | p=5764c | t=120s
2025-12-16 12:08:47,972 - src.llm.client - INFO - [dia:55d293] ğŸš€ dia | m=gemma3:4b | p=5766c | t=120s
2025-12-16 12:08:47,973 - src.llm.client - INFO - [dia:d11161] ğŸš€ dia | m=gemma3:4b | p=5776c | t=120s
2025-12-16 12:08:47,973 - src.llm.client - INFO - [dia:39b5ca] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:08:47,973 - src.llm.client - INFO - [dia:0b0d07] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:08:47,973 - src.llm.client - INFO - [dia:55d293] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:08:47,973 - src.llm.client - INFO - [dia:d11161] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:08:47,973 - src.llm.client - INFO - [dia:0b0d07] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:08:47,974 - src.llm.client - INFO - [dia:55d293] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:08:47,974 - src.llm.client - INFO - [dia:d11161] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:08:47,977 - src.llm.client - INFO - [dia:d11161] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11085 bytes, prompt=5776 chars
2025-12-16 12:08:47,977 - src.llm.client - INFO - [dia:d11161] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:08:47,978 - src.llm.client - INFO - [dia:55d293] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11075 bytes, prompt=5766 chars
2025-12-16 12:08:47,978 - src.llm.client - INFO - [dia:55d293] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:08:47,978 - src.llm.client - INFO - [dia:0b0d07] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11073 bytes, prompt=5764 chars
2025-12-16 12:08:47,978 - src.llm.client - INFO - [dia:0b0d07] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:08:47,979 - src.llm.client - INFO - [dia:39b5ca] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11087 bytes, prompt=5778 chars
2025-12-16 12:08:47,979 - src.llm.client - INFO - [dia:39b5ca] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:08:50,264 - src.llm.request_handler - INFO - [dia:39b5ca] âœ“ Done 2.28s
2025-12-16 12:08:50,269 - src.llm.client - INFO - [dia:39b5ca] âœ… HTTP 200 in 2.29s
2025-12-16 12:08:50,269 - src.llm.client - INFO - [dia:39b5ca] ğŸ“¡ Stream active (200)
2025-12-16 12:08:50,269 - src.llm.client - INFO - [dia:39b5ca] Starting stream parsing, waiting for first chunk...
2025-12-16 12:08:52,273 - src.llm.client - INFO - [dia:39b5ca] ğŸ“Š 2.0s: 360c @180c/s (110ch, ~90t @45t/s)
2025-12-16 12:08:54,284 - src.llm.client - INFO - [dia:39b5ca] ğŸ“Š 4.0s: 625c @156c/s (203ch, ~156t @39t/s)
2025-12-16 12:08:55,597 - src.llm.client - INFO - [dia:39b5ca] âœ“ Done 7.63s: 768c (~99w @101c/s)
2025-12-16 12:08:55,601 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Free Energy Minimization (Foundations of Active Inference):
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO -     - Length: 515 chars (cleaned: 515 chars)
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO - [OK] Elements: 35 total (nodes: 15, connections: 20) âœ“
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 12:08:55,602 - src.generate.formats.diagrams - INFO - Generated diagram: 515 characters
2025-12-16 12:08:57,330 - src.llm.request_handler - INFO - [dia:d11161] âœ“ Done 9.35s
2025-12-16 12:08:57,333 - src.llm.client - INFO - [dia:d11161] âœ… HTTP 200 in 9.36s
2025-12-16 12:08:57,335 - src.llm.client - INFO - [dia:d11161] ğŸ“¡ Stream active (200)
2025-12-16 12:08:57,336 - src.llm.client - INFO - [dia:d11161] Starting stream parsing, waiting for first chunk...
2025-12-16 12:08:59,350 - src.llm.client - INFO - [dia:d11161] ğŸ“Š 2.0s: 339c @168c/s (93ch, ~85t @42t/s)
2025-12-16 12:09:01,350 - src.llm.client - INFO - [dia:d11161] ğŸ“Š 4.0s: 610c @152c/s (203ch, ~152t @38t/s)
2025-12-16 12:09:03,294 - src.llm.client - INFO - [dia:d11161] âœ“ Done 15.32s: 810c (~97w @53c/s)
2025-12-16 12:09:03,298 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Perception-Action Loops (Foundations of Active Inference):
2025-12-16 12:09:03,298 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:09:03,299 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:09:03,299 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:09:03,300 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 12:09:03,300 - src.generate.formats.diagrams - INFO -     - Length: 479 chars (cleaned: 479 chars)
2025-12-16 12:09:03,300 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:09:03,300 - src.generate.formats.diagrams - INFO - [OK] Elements: 23 total (nodes: 9, connections: 14) âœ“
2025-12-16 12:09:03,300 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 12:09:03,301 - src.generate.formats.diagrams - INFO - Generated diagram: 479 characters
2025-12-16 12:09:04,967 - src.llm.request_handler - INFO - [dia:0b0d07] âœ“ Done 16.99s
2025-12-16 12:09:04,968 - src.llm.client - INFO - [dia:0b0d07] âœ… HTTP 200 in 16.99s
2025-12-16 12:09:04,968 - src.llm.client - INFO - [dia:0b0d07] ğŸ“¡ Stream active (200)
2025-12-16 12:09:04,968 - src.llm.client - INFO - [dia:0b0d07] Starting stream parsing, waiting for first chunk...
2025-12-16 12:09:07,042 - src.llm.client - INFO - [dia:0b0d07] ğŸ“Š 2.0s: 393c @194c/s (115ch, ~98t @48t/s)
2025-12-16 12:09:08,701 - src.llm.client - INFO - [dia:0b0d07] âœ“ Done 20.73s: 644c (~99w @31c/s)
2025-12-16 12:09:08,702 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Predictive Coding (Foundations of Active Inference):
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO -     - Length: 558 chars (cleaned: 558 chars)
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO - [OK] Elements: 40 total (nodes: 10, connections: 30) âœ“
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 12:09:08,703 - src.generate.formats.diagrams - INFO - Generated diagram: 558 characters
2025-12-16 12:09:10,491 - src.llm.request_handler - INFO - [dia:55d293] âœ“ Done 22.51s
2025-12-16 12:09:10,491 - src.llm.client - INFO - [dia:55d293] âœ… HTTP 200 in 22.51s
2025-12-16 12:09:10,491 - src.llm.client - INFO - [dia:55d293] ğŸ“¡ Stream active (200)
2025-12-16 12:09:10,492 - src.llm.client - INFO - [dia:55d293] Starting stream parsing, waiting for first chunk...
2025-12-16 12:09:12,508 - src.llm.client - INFO - [dia:55d293] ğŸ“Š 2.0s: 314c @156c/s (102ch, ~78t @39t/s)
2025-12-16 12:09:14,545 - src.llm.client - INFO - [dia:55d293] ğŸ“Š 4.1s: 628c @155c/s (216ch, ~157t @39t/s)
2025-12-16 12:09:14,546 - src.llm.client - INFO - [dia:55d293] âœ“ Done 26.57s: 628c (~87w @24c/s)
2025-12-16 12:09:14,546 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Bayesian Inference (Foundations of Active Inference):
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO - [FIXED] Removed classDef command (not supported in all renderers) âœ“
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO -     - Length: 423 chars (cleaned: 423 chars)
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO - [OK] Elements: 30 total (nodes: 11, connections: 19) âœ“
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 12:09:14,547 - src.generate.formats.diagrams - INFO - Generated diagram: 423 characters
2025-12-16 12:09:14,548 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 12:09:14,548 - src.generate.formats.questions - INFO - Generating 10 questions for: Foundations of Active Inference (Session 1)
2025-12-16 12:09:14,548 - src.llm.client - INFO - [qst:aeb971] ğŸš€ qst | m=gemma3:4b | p=7551c | t=150s
2025-12-16 12:09:14,548 - src.llm.client - INFO - [qst:aeb971] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 12:09:14,548 - src.llm.client - INFO - [qst:aeb971] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:09:14,553 - src.llm.client - INFO - [qst:aeb971] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11269 bytes, prompt=7551 chars
2025-12-16 12:09:14,553 - src.llm.client - INFO - [qst:aeb971] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 12:09:17,225 - src.llm.request_handler - INFO - [qst:aeb971] âœ“ Done 2.67s
2025-12-16 12:09:17,227 - src.llm.client - INFO - [qst:aeb971] âœ… HTTP 200 in 2.67s
2025-12-16 12:09:17,227 - src.llm.client - INFO - [qst:aeb971] ğŸ“¡ Stream active (200)
2025-12-16 12:09:17,227 - src.llm.client - INFO - [qst:aeb971] Starting stream parsing, waiting for first chunk...
2025-12-16 12:09:19,242 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 2.0s: 544c @270c/s (111ch, ~136t @68t/s)
2025-12-16 12:09:21,247 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 4.0s: 1086c @270c/s (220ch, ~272t @68t/s)
2025-12-16 12:09:23,259 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 6.0s: 1654c @274c/s (338ch, ~414t @69t/s)
2025-12-16 12:09:25,272 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 8.0s: 2210c @275c/s (459ch, ~552t @69t/s)
2025-12-16 12:09:27,275 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 10.0s: 2850c @284c/s (583ch, ~712t @71t/s)
2025-12-16 12:09:29,295 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 12.1s: 3435c @285c/s (693ch, ~859t @71t/s)
2025-12-16 12:09:31,301 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 14.1s: 3978c @283c/s (801ch, ~994t @71t/s)
2025-12-16 12:09:33,305 - src.llm.client - INFO - [qst:aeb971] ğŸ“Š 16.1s: 4524c @281c/s (902ch, ~1131t @70t/s)
2025-12-16 12:09:34,879 - src.llm.client - INFO - [qst:aeb971] âœ“ Done 20.33s: 4936c (~717w @243c/s)
2025-12-16 12:09:34,881 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 4 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 1, 'total_fixes': 4}
2025-12-16 12:09:34,881 - src.generate.formats.questions - INFO - Applied 4 auto-fixes to questions
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -     Context: Module 1 Session 1
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 5 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -     Context: Module 1 Session 1
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 12:09:34,882 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Foundations of Active Inference (Session 1)
2025-12-16 12:09:34,883 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 12:09:34,885 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:09:34,889 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 1 completed
2025-12-16 12:09:34,889 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:09:34,889 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:09:34,889 - src.generate.orchestration.pipeline - INFO - Module 2: Active Inference in Generative AI (1 sessions)
2025-12-16 12:09:34,889 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:09:34,889 - src.generate.orchestration.pipeline - INFO - 
[2/3] Session 2: Active Inference and Large Language Models
2025-12-16 12:09:34,889 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 12:09:34,889 - src.generate.formats.lectures - INFO - Generating lecture for: Active Inference in Generative AI (Session 2/3)
2025-12-16 12:09:34,890 - src.llm.client - INFO - [lec:26ed4e] ğŸš€ lec | m=gemma3:4b | p=3514c | t=180s
2025-12-16 12:09:34,890 - src.llm.client - INFO - [lec:26ed4e] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 12:09:34,890 - src.llm.client - INFO - [lec:26ed4e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:09:34,893 - src.llm.client - INFO - [lec:26ed4e] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=7151 bytes, prompt=3514 chars
2025-12-16 12:09:34,893 - src.llm.client - INFO - [lec:26ed4e] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 12:09:36,403 - src.llm.request_handler - INFO - [lec:26ed4e] âœ“ Done 1.51s
2025-12-16 12:09:36,404 - src.llm.client - INFO - [lec:26ed4e] âœ… HTTP 200 in 1.51s
2025-12-16 12:09:36,404 - src.llm.client - INFO - [lec:26ed4e] ğŸ“¡ Stream active (200)
2025-12-16 12:09:36,405 - src.llm.client - INFO - [lec:26ed4e] Starting stream parsing, waiting for first chunk...
2025-12-16 12:09:38,429 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 2.0s: 564c @279c/s (99ch, ~141t @70t/s)
2025-12-16 12:09:40,438 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 4.0s: 1109c @275c/s (203ch, ~277t @69t/s)
2025-12-16 12:09:42,459 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 6.0s: 1600c @265c/s (296ch, ~400t @66t/s)
2025-12-16 12:09:44,480 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 8.1s: 2123c @263c/s (399ch, ~531t @66t/s)
2025-12-16 12:09:46,531 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 10.1s: 2661c @264c/s (500ch, ~665t @66t/s)
2025-12-16 12:09:48,488 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 12.1s: 3192c @264c/s (600ch, ~798t @66t/s)
2025-12-16 12:09:50,533 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 14.1s: 3747c @265c/s (711ch, ~937t @66t/s)
2025-12-16 12:09:52,536 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 16.1s: 4300c @267c/s (821ch, ~1075t @67t/s)
2025-12-16 12:09:54,539 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 18.1s: 4803c @265c/s (927ch, ~1201t @66t/s)
2025-12-16 12:09:56,548 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 20.1s: 5449c @271c/s (1048ch, ~1362t @68t/s)
2025-12-16 12:09:58,560 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 22.2s: 6124c @276c/s (1171ch, ~1531t @69t/s)
2025-12-16 12:10:00,574 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 24.2s: 6733c @279c/s (1299ch, ~1683t @70t/s)
2025-12-16 12:10:02,590 - src.llm.client - INFO - [lec:26ed4e] ğŸ“Š 26.2s: 7407c @283c/s (1426ch, ~1852t @71t/s)
2025-12-16 12:10:02,693 - src.llm.client - INFO - [lec:26ed4e] âœ“ Done 27.80s: 7419c (~1127w @267c/s)
2025-12-16 12:10:02,695 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 12:10:02,695 - src.generate.formats.lectures - INFO -     - Length: 7640 chars, 1161 words
2025-12-16 12:10:02,695 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 12:10:02,695 - src.generate.formats.lectures - INFO -     - Structure: 7 sections, 0 subsections
2025-12-16 12:10:02,695 - src.generate.formats.lectures - INFO -     - Content: 8 examples, 0 terms defined
2025-12-16 12:10:02,695 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 12:10:02,698 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:10:02,698 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 12:10:02,698 - src.generate.formats.labs - INFO - Generating lab 2 for: Active Inference in Generative AI (Session 2)
2025-12-16 12:10:02,698 - src.llm.client - INFO - [lab:6fad36] ğŸš€ lab | m=gemma3:4b | p=3543c | t=150s
2025-12-16 12:10:02,698 - src.llm.client - INFO - [lab:6fad36] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 12:10:02,698 - src.llm.client - INFO - [lab:6fad36] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:10:02,700 - src.llm.client - INFO - [lab:6fad36] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3985 bytes, prompt=3543 chars
2025-12-16 12:10:02,700 - src.llm.client - INFO - [lab:6fad36] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 12:10:03,711 - src.llm.request_handler - INFO - [lab:6fad36] âœ“ Done 1.01s
2025-12-16 12:10:03,711 - src.llm.client - INFO - [lab:6fad36] âœ… HTTP 200 in 1.01s
2025-12-16 12:10:03,711 - src.llm.client - INFO - [lab:6fad36] ğŸ“¡ Stream active (200)
2025-12-16 12:10:03,711 - src.llm.client - INFO - [lab:6fad36] Starting stream parsing, waiting for first chunk...
2025-12-16 12:10:05,719 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 2.0s: 662c @330c/s (129ch, ~166t @82t/s)
2025-12-16 12:10:07,723 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 4.0s: 1328c @331c/s (255ch, ~332t @83t/s)
2025-12-16 12:10:09,750 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 6.0s: 1776c @295c/s (354ch, ~444t @74t/s)
2025-12-16 12:10:11,737 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 8.0s: 2233c @278c/s (470ch, ~558t @70t/s)
2025-12-16 12:10:13,745 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 10.0s: 2822c @281c/s (598ch, ~706t @70t/s)
2025-12-16 12:10:15,761 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 12.0s: 3404c @283c/s (728ch, ~851t @71t/s)
2025-12-16 12:10:17,763 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 14.1s: 4054c @289c/s (857ch, ~1014t @72t/s)
2025-12-16 12:10:19,770 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 16.1s: 4660c @290c/s (985ch, ~1165t @73t/s)
2025-12-16 12:10:21,819 - src.llm.client - INFO - [lab:6fad36] ğŸ“Š 18.1s: 5207c @288c/s (1097ch, ~1302t @72t/s)
2025-12-16 12:10:21,820 - src.llm.client - INFO - [lab:6fad36] âœ“ Done 19.12s: 5207c (~702w @272c/s)
2025-12-16 12:10:21,820 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 12:10:21,820 - src.generate.formats.labs - INFO -     - Length: 5308 chars, 718 words
2025-12-16 12:10:21,820 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-16 12:10:21,820 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-16 12:10:21,820 - src.generate.formats.labs - INFO -     - Data tables: 5
2025-12-16 12:10:21,823 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:10:21,823 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 12:10:21,823 - src.generate.formats.study_notes - INFO - Generating study notes for: Active Inference in Generative AI (Session 2)
2025-12-16 12:10:21,823 - src.llm.client - INFO - [stu:ddcbbd] ğŸš€ stu | m=gemma3:4b | p=4686c | t=120s
2025-12-16 12:10:21,823 - src.llm.client - INFO - [stu:ddcbbd] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:10:21,823 - src.llm.client - INFO - [stu:ddcbbd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:10:21,825 - src.llm.client - INFO - [stu:ddcbbd] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8345 bytes, prompt=4686 chars
2025-12-16 12:10:21,825 - src.llm.client - INFO - [stu:ddcbbd] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:10:23,247 - src.llm.request_handler - INFO - [stu:ddcbbd] âœ“ Done 1.42s
2025-12-16 12:10:23,247 - src.llm.client - INFO - [stu:ddcbbd] âœ… HTTP 200 in 1.42s
2025-12-16 12:10:23,247 - src.llm.client - INFO - [stu:ddcbbd] ğŸ“¡ Stream active (200)
2025-12-16 12:10:23,247 - src.llm.client - INFO - [stu:ddcbbd] Starting stream parsing, waiting for first chunk...
2025-12-16 12:10:25,251 - src.llm.client - INFO - [stu:ddcbbd] ğŸ“Š 2.0s: 698c @348c/s (129ch, ~174t @87t/s)
2025-12-16 12:10:27,266 - src.llm.client - INFO - [stu:ddcbbd] ğŸ“Š 4.0s: 1369c @341c/s (259ch, ~342t @85t/s)
2025-12-16 12:10:29,274 - src.llm.client - INFO - [stu:ddcbbd] ğŸ“Š 6.0s: 2070c @343c/s (385ch, ~518t @86t/s)
2025-12-16 12:10:31,275 - src.llm.client - INFO - [stu:ddcbbd] ğŸ“Š 8.0s: 2785c @347c/s (513ch, ~696t @87t/s)
2025-12-16 12:10:33,279 - src.llm.client - INFO - [stu:ddcbbd] ğŸ“Š 10.0s: 3504c @349c/s (642ch, ~876t @87t/s)
2025-12-16 12:10:35,291 - src.llm.client - INFO - [stu:ddcbbd] ğŸ“Š 12.0s: 4209c @349c/s (772ch, ~1052t @87t/s)
2025-12-16 12:10:35,975 - src.llm.client - INFO - [stu:ddcbbd] âœ“ Done 14.15s: 4398c (~627w @311c/s)
2025-12-16 12:10:35,976 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 12:10:35,976 - src.generate.formats.study_notes - INFO -     - Length: 4466 chars, 639 words
2025-12-16 12:10:35,976 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 12:10:35,976 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-16 12:10:35,976 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 0 bullets
2025-12-16 12:10:35,976 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 12:10:35,978 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:10:35,978 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 12:10:35,979 - src.generate.formats.diagrams - INFO - Generating diagram for: Attention Mechanisms as Predictive Coding (Active Inference in Generative AI)
2025-12-16 12:10:35,979 - src.generate.formats.diagrams - INFO - Generating diagram for: Precision Weighting (Active Inference in Generative AI)
2025-12-16 12:10:35,979 - src.llm.client - INFO - [dia:d4868b] ğŸš€ dia | m=gemma3:4b | p=5824c | t=120s
2025-12-16 12:10:35,979 - src.generate.formats.diagrams - INFO - Generating diagram for: RLHF as an Active Inference Process (Active Inference in Generative AI)
2025-12-16 12:10:35,979 - src.generate.formats.diagrams - INFO - Generating diagram for: Model-Based RL (Active Inference in Generative AI)
2025-12-16 12:10:35,979 - src.llm.client - INFO - [dia:a90bad] ğŸš€ dia | m=gemma3:4b | p=5780c | t=120s
2025-12-16 12:10:35,979 - src.llm.client - INFO - [dia:d4868b] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:771e20] ğŸš€ dia | m=gemma3:4b | p=5812c | t=120s
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:060d17] ğŸš€ dia | m=gemma3:4b | p=5770c | t=120s
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:a90bad] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:d4868b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:771e20] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:060d17] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:a90bad] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:10:35,980 - src.llm.client - INFO - [dia:771e20] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:10:35,981 - src.llm.client - INFO - [dia:060d17] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:771e20] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11121 bytes, prompt=5812 chars
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:d4868b] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11133 bytes, prompt=5824 chars
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:060d17] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11079 bytes, prompt=5770 chars
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:771e20] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:a90bad] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11089 bytes, prompt=5780 chars
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:d4868b] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:060d17] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:10:35,983 - src.llm.client - INFO - [dia:a90bad] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:10:37,850 - src.llm.request_handler - INFO - [dia:d4868b] âœ“ Done 1.87s
2025-12-16 12:10:37,853 - src.llm.client - INFO - [dia:d4868b] âœ… HTTP 200 in 1.87s
2025-12-16 12:10:37,853 - src.llm.client - INFO - [dia:d4868b] ğŸ“¡ Stream active (200)
2025-12-16 12:10:37,853 - src.llm.client - INFO - [dia:d4868b] Starting stream parsing, waiting for first chunk...
2025-12-16 12:10:39,857 - src.llm.client - INFO - [dia:d4868b] ğŸ“Š 2.0s: 472c @235c/s (130ch, ~118t @59t/s)
2025-12-16 12:10:41,866 - src.llm.client - INFO - [dia:d4868b] ğŸ“Š 4.0s: 876c @218c/s (260ch, ~219t @55t/s)
2025-12-16 12:10:42,471 - src.llm.client - INFO - [dia:d4868b] âœ“ Done 6.49s: 953c (~119w @147c/s)
2025-12-16 12:10:42,471 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Attention Mechanisms as Predictive Coding (Active Inference in Generative AI):
2025-12-16 12:10:42,471 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:10:42,471 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:10:42,471 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:10:42,471 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - INFO -     - Length: 705 chars (cleaned: 705 chars)
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 36 total (nodes: 15, connections: 21) âš ï¸
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 12:10:42,472 - src.generate.formats.diagrams - INFO - Generated diagram: 705 characters
2025-12-16 12:10:44,118 - src.llm.request_handler - INFO - [dia:060d17] âœ“ Done 8.13s
2025-12-16 12:10:44,120 - src.llm.client - INFO - [dia:060d17] âœ… HTTP 200 in 8.14s
2025-12-16 12:10:44,120 - src.llm.client - INFO - [dia:060d17] ğŸ“¡ Stream active (200)
2025-12-16 12:10:44,120 - src.llm.client - INFO - [dia:060d17] Starting stream parsing, waiting for first chunk...
2025-12-16 12:10:46,133 - src.llm.client - INFO - [dia:060d17] ğŸ“Š 2.0s: 449c @223c/s (129ch, ~112t @56t/s)
2025-12-16 12:10:48,138 - src.llm.client - INFO - [dia:060d17] ğŸ“Š 4.0s: 887c @221c/s (259ch, ~222t @55t/s)
2025-12-16 12:10:50,152 - src.llm.client - INFO - [dia:060d17] ğŸ“Š 6.0s: 1193c @198c/s (385ch, ~298t @49t/s)
2025-12-16 12:10:50,343 - src.llm.client - INFO - [dia:060d17] âœ“ Done 14.36s: 1210c (~167w @84c/s)
2025-12-16 12:10:50,345 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Model-Based RL (Active Inference in Generative AI):
2025-12-16 12:10:50,345 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:10:50,345 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:10:50,345 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:10:50,345 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - INFO -     - Length: 1127 chars (cleaned: 1127 chars)
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 55 total (nodes: 26, connections: 29) âš ï¸
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 12:10:50,346 - src.generate.formats.diagrams - INFO - Generated diagram: 1127 characters
2025-12-16 12:10:52,019 - src.llm.request_handler - INFO - [dia:771e20] âœ“ Done 16.04s
2025-12-16 12:10:52,019 - src.llm.client - INFO - [dia:771e20] âœ… HTTP 200 in 16.04s
2025-12-16 12:10:52,019 - src.llm.client - INFO - [dia:771e20] ğŸ“¡ Stream active (200)
2025-12-16 12:10:52,019 - src.llm.client - INFO - [dia:771e20] Starting stream parsing, waiting for first chunk...
2025-12-16 12:10:54,030 - src.llm.client - INFO - [dia:771e20] ğŸ“Š 2.0s: 407c @202c/s (120ch, ~102t @51t/s)
2025-12-16 12:10:55,810 - src.llm.client - INFO - [dia:771e20] âœ“ Done 19.83s: 647c (~82w @33c/s)
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for RLHF as an Active Inference Process (Active Inference in Generative AI):
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING - [WARNING] Only 8 nodes found (require at least 10, need 2 more - add more nodes to the diagram) âš ï¸
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 8 nodes found (require at least 10, need 2 more - add more nodes to the diagram) ğŸ”´
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING -     Context: Module 2 Session 2
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 8 nodes found (require at least 10, need 2 more - add more nodes to the diagram) ğŸ”´
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING -     Context: Module 2 Session 2
2025-12-16 12:10:55,811 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-16 12:10:55,812 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-16 12:10:55,812 - src.generate.formats.diagrams - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 12:10:55,812 - src.generate.formats.diagrams - WARNING -   Retry attempt 1/1 for diagram: RLHF as an Active Inference Process (Active Inference in Generative AI)
2025-12-16 12:10:55,812 - src.generate.formats.diagrams - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 12:10:57,497 - src.llm.request_handler - INFO - [dia:a90bad] âœ“ Done 21.51s
2025-12-16 12:10:57,497 - src.llm.client - INFO - [dia:a90bad] âœ… HTTP 200 in 21.51s
2025-12-16 12:10:57,497 - src.llm.client - INFO - [dia:a90bad] ğŸ“¡ Stream active (200)
2025-12-16 12:10:57,497 - src.llm.client - INFO - [dia:a90bad] Starting stream parsing, waiting for first chunk...
2025-12-16 12:10:59,506 - src.llm.client - INFO - [dia:a90bad] ğŸ“Š 2.0s: 415c @207c/s (128ch, ~104t @52t/s)
2025-12-16 12:11:01,507 - src.llm.client - INFO - [dia:a90bad] ğŸ“Š 4.0s: 720c @180c/s (256ch, ~180t @45t/s)
2025-12-16 12:11:02,636 - src.llm.client - INFO - [dia:a90bad] âœ“ Done 26.66s: 892c (~125w @33c/s)
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Precision Weighting (Active Inference in Generative AI):
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - INFO -     - Length: 857 chars (cleaned: 857 chars)
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - INFO - [OK] Elements: 45 total (nodes: 15, connections: 30) âœ“
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - INFO -   Cleanup summary: 1 issues fixed (code fences, style commands, etc.)
2025-12-16 12:11:02,637 - src.generate.formats.diagrams - INFO - Generated diagram: 857 characters
2025-12-16 12:11:02,638 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 12:11:02,638 - src.generate.formats.questions - INFO - Generating 10 questions for: Active Inference in Generative AI (Session 2)
2025-12-16 12:11:02,638 - src.llm.client - INFO - [qst:fb1042] ğŸš€ qst | m=gemma3:4b | p=7543c | t=150s
2025-12-16 12:11:02,638 - src.llm.client - INFO - [qst:fb1042] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 12:11:02,638 - src.llm.client - INFO - [qst:fb1042] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:11:02,640 - src.llm.client - INFO - [qst:fb1042] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11237 bytes, prompt=7543 chars
2025-12-16 12:11:02,640 - src.llm.client - INFO - [qst:fb1042] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 12:11:04,751 - src.llm.request_handler - INFO - [qst:fb1042] âœ“ Done 2.11s
2025-12-16 12:11:04,751 - src.llm.client - INFO - [qst:fb1042] âœ… HTTP 200 in 2.11s
2025-12-16 12:11:04,751 - src.llm.client - INFO - [qst:fb1042] ğŸ“¡ Stream active (200)
2025-12-16 12:11:04,751 - src.llm.client - INFO - [qst:fb1042] Starting stream parsing, waiting for first chunk...
2025-12-16 12:11:06,767 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 2.0s: 562c @279c/s (120ch, ~140t @70t/s)
2025-12-16 12:11:08,778 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 4.0s: 1149c @285c/s (234ch, ~287t @71t/s)
2025-12-16 12:11:10,783 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 6.0s: 1798c @298c/s (356ch, ~450t @75t/s)
2025-12-16 12:11:12,795 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 8.0s: 2351c @292c/s (461ch, ~588t @73t/s)
2025-12-16 12:11:14,798 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 10.0s: 2913c @290c/s (576ch, ~728t @72t/s)
2025-12-16 12:11:16,802 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 12.1s: 3478c @289c/s (679ch, ~870t @72t/s)
2025-12-16 12:11:18,816 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 14.1s: 4026c @286c/s (782ch, ~1006t @72t/s)
2025-12-16 12:11:20,832 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 16.1s: 4586c @285c/s (882ch, ~1146t @71t/s)
2025-12-16 12:11:22,841 - src.llm.client - INFO - [qst:fb1042] ğŸ“Š 18.1s: 5258c @291c/s (1001ch, ~1314t @73t/s)
2025-12-16 12:11:23,156 - src.llm.client - INFO - [qst:fb1042] âœ“ Done 20.52s: 5355c (~760w @261c/s)
2025-12-16 12:11:23,157 - src.generate.formats.questions - WARNING - Only 5 questions detected (expected 10) for Active Inference in Generative AI (Session 2)
2025-12-16 12:11:23,157 - src.generate.formats.questions - INFO - [COMPLIANT] Questions generated âœ“
2025-12-16 12:11:23,157 - src.generate.formats.questions - INFO -     - Total: 5 questions
2025-12-16 12:11:23,157 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-16 12:11:23,157 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-16 12:11:23,157 - src.generate.formats.questions - INFO - [OK] Question marks: 6 total, 5 questions with '?' âœ“
2025-12-16 12:11:23,157 - src.generate.formats.questions - INFO -     - Question length: avg 11.0 words (range: 7-15)
2025-12-16 12:11:23,158 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-16 12:11:23,160 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:11:23,162 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 2 completed
2025-12-16 12:11:23,162 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:11:23,162 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:11:23,162 - src.generate.orchestration.pipeline - INFO - Module 3: Advanced Applications & Future Directions (1 sessions)
2025-12-16 12:11:23,162 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:11:23,162 - src.generate.orchestration.pipeline - INFO - 
[3/3] Session 3: Multi-Agent Coordination & Embodied AI
2025-12-16 12:11:23,162 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 12:11:23,162 - src.generate.formats.lectures - INFO - Generating lecture for: Advanced Applications & Future Directions (Session 3/3)
2025-12-16 12:11:23,163 - src.llm.client - INFO - [lec:ea4a02] ğŸš€ lec | m=gemma3:4b | p=3670c | t=180s
2025-12-16 12:11:23,163 - src.llm.client - INFO - [lec:ea4a02] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 12:11:23,163 - src.llm.client - INFO - [lec:ea4a02] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:11:23,164 - src.llm.client - INFO - [lec:ea4a02] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=7307 bytes, prompt=3670 chars
2025-12-16 12:11:23,164 - src.llm.client - INFO - [lec:ea4a02] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 12:11:24,372 - src.llm.request_handler - INFO - [lec:ea4a02] âœ“ Done 1.21s
2025-12-16 12:11:24,372 - src.llm.client - INFO - [lec:ea4a02] âœ… HTTP 200 in 1.21s
2025-12-16 12:11:24,372 - src.llm.client - INFO - [lec:ea4a02] ğŸ“¡ Stream active (200)
2025-12-16 12:11:24,372 - src.llm.client - INFO - [lec:ea4a02] Starting stream parsing, waiting for first chunk...
2025-12-16 12:11:26,387 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 2.0s: 675c @335c/s (119ch, ~169t @84t/s)
2025-12-16 12:11:28,401 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 4.0s: 1361c @338c/s (241ch, ~340t @84t/s)
2025-12-16 12:11:30,420 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 6.0s: 1985c @328c/s (355ch, ~496t @82t/s)
2025-12-16 12:11:32,422 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 8.0s: 2637c @328c/s (473ch, ~659t @82t/s)
2025-12-16 12:11:34,427 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 10.1s: 3281c @326c/s (595ch, ~820t @82t/s)
2025-12-16 12:11:36,440 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 12.1s: 3980c @330c/s (721ch, ~995t @82t/s)
2025-12-16 12:11:38,443 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 14.1s: 4672c @332c/s (846ch, ~1168t @83t/s)
2025-12-16 12:11:40,454 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 16.1s: 5326c @331c/s (971ch, ~1332t @83t/s)
2025-12-16 12:11:42,458 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 18.1s: 5997c @332c/s (1095ch, ~1499t @83t/s)
2025-12-16 12:11:44,458 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 20.1s: 6695c @333c/s (1216ch, ~1674t @83t/s)
2025-12-16 12:11:46,474 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 22.1s: 7359c @333c/s (1338ch, ~1840t @83t/s)
2025-12-16 12:11:48,481 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 24.1s: 8015c @332c/s (1455ch, ~2004t @83t/s)
2025-12-16 12:11:50,513 - src.llm.client - INFO - [lec:ea4a02] ğŸ“Š 26.1s: 8762c @335c/s (1576ch, ~2190t @84t/s)
2025-12-16 12:11:52,480 - src.llm.client - INFO - [lec:ea4a02] âœ“ Done 29.32s: 9313c (~1381w @318c/s)
2025-12-16 12:11:52,482 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 12:11:52,482 - src.generate.formats.lectures - INFO -     - Length: 9596 chars, 1420 words
2025-12-16 12:11:52,482 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 12:11:52,482 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 12:11:52,482 - src.generate.formats.lectures - INFO -     - Content: 13 examples, 1 terms defined
2025-12-16 12:11:52,482 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 12:11:52,485 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:11:52,486 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 12:11:52,486 - src.generate.formats.labs - INFO - Generating lab 3 for: Advanced Applications & Future Directions (Session 3)
2025-12-16 12:11:52,486 - src.llm.client - INFO - [lab:22d96e] ğŸš€ lab | m=gemma3:4b | p=3638c | t=150s
2025-12-16 12:11:52,486 - src.llm.client - INFO - [lab:22d96e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 12:11:52,486 - src.llm.client - INFO - [lab:22d96e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:11:52,488 - src.llm.client - INFO - [lab:22d96e] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=4070 bytes, prompt=3638 chars
2025-12-16 12:11:52,488 - src.llm.client - INFO - [lab:22d96e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 12:11:53,706 - src.llm.request_handler - INFO - [lab:22d96e] âœ“ Done 1.22s
2025-12-16 12:11:53,706 - src.llm.client - INFO - [lab:22d96e] âœ… HTTP 200 in 1.22s
2025-12-16 12:11:53,706 - src.llm.client - INFO - [lab:22d96e] ğŸ“¡ Stream active (200)
2025-12-16 12:11:53,706 - src.llm.client - INFO - [lab:22d96e] Starting stream parsing, waiting for first chunk...
2025-12-16 12:11:55,721 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 2.0s: 684c @340c/s (124ch, ~171t @85t/s)
2025-12-16 12:11:57,737 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 4.0s: 1255c @312c/s (246ch, ~314t @78t/s)
2025-12-16 12:11:59,742 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 6.0s: 1703c @282c/s (370ch, ~426t @71t/s)
2025-12-16 12:12:01,746 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 8.0s: 2285c @284c/s (493ch, ~571t @71t/s)
2025-12-16 12:12:03,761 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 10.1s: 2830c @281c/s (616ch, ~708t @70t/s)
2025-12-16 12:12:05,763 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 12.1s: 3362c @279c/s (736ch, ~840t @70t/s)
2025-12-16 12:12:07,766 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 14.1s: 3771c @268c/s (850ch, ~943t @67t/s)
2025-12-16 12:12:09,781 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 16.1s: 4443c @276c/s (965ch, ~1111t @69t/s)
2025-12-16 12:12:11,783 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 18.1s: 5214c @288c/s (1087ch, ~1304t @72t/s)
2025-12-16 12:12:13,797 - src.llm.client - INFO - [lab:22d96e] ğŸ“Š 20.1s: 5806c @289c/s (1203ch, ~1452t @72t/s)
2025-12-16 12:12:15,447 - src.llm.client - INFO - [lab:22d96e] âœ“ Done 22.96s: 6407c (~844w @279c/s)
2025-12-16 12:12:15,448 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 12:12:15,448 - src.generate.formats.labs - INFO -     - Length: 6526 chars, 862 words
2025-12-16 12:12:15,448 - src.generate.formats.labs - INFO -     - Procedure: 10 steps
2025-12-16 12:12:15,448 - src.generate.formats.labs - INFO -     - Safety: 5 warnings
2025-12-16 12:12:15,448 - src.generate.formats.labs - INFO -     - Data tables: 9
2025-12-16 12:12:15,451 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:12:15,452 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 12:12:15,452 - src.generate.formats.study_notes - INFO - Generating study notes for: Advanced Applications & Future Directions (Session 3)
2025-12-16 12:12:15,452 - src.llm.client - INFO - [stu:326963] ğŸš€ stu | m=gemma3:4b | p=4818c | t=120s
2025-12-16 12:12:15,452 - src.llm.client - INFO - [stu:326963] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:12:15,452 - src.llm.client - INFO - [stu:326963] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:12:15,459 - src.llm.client - INFO - [stu:326963] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8467 bytes, prompt=4818 chars
2025-12-16 12:12:15,459 - src.llm.client - INFO - [stu:326963] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:12:16,885 - src.llm.request_handler - INFO - [stu:326963] âœ“ Done 1.43s
2025-12-16 12:12:16,886 - src.llm.client - INFO - [stu:326963] âœ… HTTP 200 in 1.43s
2025-12-16 12:12:16,886 - src.llm.client - INFO - [stu:326963] ğŸ“¡ Stream active (200)
2025-12-16 12:12:16,886 - src.llm.client - INFO - [stu:326963] Starting stream parsing, waiting for first chunk...
2025-12-16 12:12:18,888 - src.llm.client - INFO - [stu:326963] ğŸ“Š 2.0s: 690c @345c/s (122ch, ~172t @86t/s)
2025-12-16 12:12:20,897 - src.llm.client - INFO - [stu:326963] ğŸ“Š 4.0s: 1396c @348c/s (246ch, ~349t @87t/s)
2025-12-16 12:12:22,899 - src.llm.client - INFO - [stu:326963] ğŸ“Š 6.0s: 2082c @346c/s (366ch, ~520t @87t/s)
2025-12-16 12:12:24,902 - src.llm.client - INFO - [stu:326963] ğŸ“Š 8.0s: 2779c @347c/s (490ch, ~695t @87t/s)
2025-12-16 12:12:26,909 - src.llm.client - INFO - [stu:326963] ğŸ“Š 10.0s: 3517c @351c/s (616ch, ~879t @88t/s)
2025-12-16 12:12:28,922 - src.llm.client - INFO - [stu:326963] ğŸ“Š 12.0s: 4210c @350c/s (736ch, ~1052t @87t/s)
2025-12-16 12:12:30,233 - src.llm.client - INFO - [stu:326963] âœ“ Done 14.78s: 4677c (~662w @316c/s)
2025-12-16 12:12:30,234 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 12:12:30,234 - src.generate.formats.study_notes - INFO -     - Length: 4753 chars, 674 words
2025-12-16 12:12:30,234 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 12:12:30,234 - src.generate.formats.study_notes - INFO -     - Key concepts: 5
2025-12-16 12:12:30,234 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 0 bullets
2025-12-16 12:12:30,234 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 12:12:30,236 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:12:30,236 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 12:12:30,237 - src.generate.formats.diagrams - INFO - Generating diagram for: Free Energy Minimization in Multi-Agent Systems (Advanced Applications & Future Directions)
2025-12-16 12:12:30,237 - src.llm.client - INFO - [dia:1811ae] ğŸš€ dia | m=gemma3:4b | p=5840c | t=120s
2025-12-16 12:12:30,237 - src.llm.client - INFO - [dia:1811ae] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:12:30,237 - src.generate.formats.diagrams - INFO - Generating diagram for: Spatial Intelligence & World Models (Advanced Applications & Future Directions)
2025-12-16 12:12:30,237 - src.generate.formats.diagrams - INFO - Generating diagram for: Robotics and Active Inference (Advanced Applications & Future Directions)
2025-12-16 12:12:30,237 - src.llm.client - INFO - [dia:1811ae] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:12:30,237 - src.generate.formats.diagrams - INFO - Generating diagram for: Emerging Applications (Advanced Applications & Future Directions)
2025-12-16 12:12:30,238 - src.llm.client - INFO - [dia:3d85e3] ğŸš€ dia | m=gemma3:4b | p=5816c | t=120s
2025-12-16 12:12:30,238 - src.llm.client - INFO - [dia:14ab49] ğŸš€ dia | m=gemma3:4b | p=5804c | t=120s
2025-12-16 12:12:30,238 - src.llm.client - INFO - [dia:a21aaf] ğŸš€ dia | m=gemma3:4b | p=5788c | t=120s
2025-12-16 12:12:30,238 - src.llm.client - INFO - [dia:3d85e3] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:12:30,239 - src.llm.client - INFO - [dia:14ab49] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:12:30,239 - src.llm.client - INFO - [dia:a21aaf] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 12:12:30,239 - src.llm.client - INFO - [dia:3d85e3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:12:30,239 - src.llm.client - INFO - [dia:14ab49] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:12:30,239 - src.llm.client - INFO - [dia:a21aaf] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:12:30,242 - src.llm.client - INFO - [dia:1811ae] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11149 bytes, prompt=5840 chars
2025-12-16 12:12:30,242 - src.llm.client - INFO - [dia:a21aaf] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11097 bytes, prompt=5788 chars
2025-12-16 12:12:30,242 - src.llm.client - INFO - [dia:3d85e3] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11125 bytes, prompt=5816 chars
2025-12-16 12:12:30,242 - src.llm.client - INFO - [dia:1811ae] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:12:30,243 - src.llm.client - INFO - [dia:a21aaf] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:12:30,243 - src.llm.client - INFO - [dia:3d85e3] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:12:30,243 - src.llm.client - INFO - [dia:14ab49] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11113 bytes, prompt=5804 chars
2025-12-16 12:12:30,244 - src.llm.client - INFO - [dia:14ab49] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 12:12:32,192 - src.llm.request_handler - INFO - [dia:1811ae] âœ“ Done 1.95s
2025-12-16 12:12:32,192 - src.llm.client - INFO - [dia:1811ae] âœ… HTTP 200 in 1.95s
2025-12-16 12:12:32,192 - src.llm.client - INFO - [dia:1811ae] ğŸ“¡ Stream active (200)
2025-12-16 12:12:32,192 - src.llm.client - INFO - [dia:1811ae] Starting stream parsing, waiting for first chunk...
2025-12-16 12:12:34,202 - src.llm.client - INFO - [dia:1811ae] ğŸ“Š 2.0s: 434c @216c/s (115ch, ~108t @54t/s)
2025-12-16 12:12:36,216 - src.llm.client - INFO - [dia:1811ae] ğŸ“Š 4.0s: 844c @210c/s (240ch, ~211t @52t/s)
2025-12-16 12:12:37,636 - src.llm.client - INFO - [dia:1811ae] âœ“ Done 7.40s: 1101c (~160w @149c/s)
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Free Energy Minimization in Multi-Agent Systems (Advanced Applications & Future Directions):
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO -     - Length: 1052 chars (cleaned: 1052 chars)
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO - [OK] Elements: 68 total (nodes: 21, connections: 47) âœ“
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 12:12:37,637 - src.generate.formats.diagrams - INFO - Generated diagram: 1052 characters
2025-12-16 12:12:39,343 - src.llm.request_handler - INFO - [dia:3d85e3] âœ“ Done 9.10s
2025-12-16 12:12:39,343 - src.llm.client - INFO - [dia:3d85e3] âœ… HTTP 200 in 9.10s
2025-12-16 12:12:39,343 - src.llm.client - INFO - [dia:3d85e3] ğŸ“¡ Stream active (200)
2025-12-16 12:12:39,343 - src.llm.client - INFO - [dia:3d85e3] Starting stream parsing, waiting for first chunk...
2025-12-16 12:12:41,356 - src.llm.client - INFO - [dia:3d85e3] ğŸ“Š 2.0s: 408c @203c/s (122ch, ~102t @51t/s)
2025-12-16 12:12:43,371 - src.llm.client - INFO - [dia:3d85e3] ğŸ“Š 4.0s: 775c @192c/s (248ch, ~194t @48t/s)
2025-12-16 12:12:45,377 - src.llm.client - INFO - [dia:3d85e3] ğŸ“Š 6.0s: 1039c @172c/s (368ch, ~260t @43t/s)
2025-12-16 12:12:46,789 - src.llm.client - INFO - [dia:3d85e3] âœ“ Done 16.55s: 1223c (~149w @74c/s)
2025-12-16 12:12:46,789 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Spatial Intelligence & World Models (Advanced Applications & Future Directions):
2025-12-16 12:12:46,789 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:12:46,789 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:12:46,789 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:12:46,790 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 12:12:46,790 - src.generate.formats.diagrams - INFO -     - Length: 690 chars (cleaned: 690 chars)
2025-12-16 12:12:46,790 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:12:46,790 - src.generate.formats.diagrams - INFO - [OK] Elements: 40 total (nodes: 16, connections: 24) âœ“
2025-12-16 12:12:46,790 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 12:12:46,790 - src.generate.formats.diagrams - INFO - Generated diagram: 690 characters
2025-12-16 12:12:48,492 - src.llm.request_handler - INFO - [dia:a21aaf] âœ“ Done 18.25s
2025-12-16 12:12:48,492 - src.llm.client - INFO - [dia:a21aaf] âœ… HTTP 200 in 18.25s
2025-12-16 12:12:48,492 - src.llm.client - INFO - [dia:a21aaf] ğŸ“¡ Stream active (200)
2025-12-16 12:12:48,492 - src.llm.client - INFO - [dia:a21aaf] Starting stream parsing, waiting for first chunk...
2025-12-16 12:12:50,495 - src.llm.client - INFO - [dia:a21aaf] ğŸ“Š 2.0s: 490c @245c/s (124ch, ~122t @61t/s)
2025-12-16 12:12:52,503 - src.llm.client - INFO - [dia:a21aaf] ğŸ“Š 4.0s: 834c @208c/s (244ch, ~208t @52t/s)
2025-12-16 12:12:54,519 - src.llm.client - INFO - [dia:a21aaf] ğŸ“Š 6.0s: 1106c @184c/s (351ch, ~276t @46t/s)
2025-12-16 12:12:55,500 - src.llm.client - INFO - [dia:a21aaf] âœ“ Done 25.26s: 1246c (~138w @49c/s)
2025-12-16 12:12:55,500 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Emerging Applications (Advanced Applications & Future Directions):
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO -     - Length: 721 chars (cleaned: 721 chars)
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 37 total (nodes: 11, connections: 26) âš ï¸
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 12:12:55,501 - src.generate.formats.diagrams - INFO - Generated diagram: 721 characters
2025-12-16 12:12:57,217 - src.llm.request_handler - INFO - [dia:14ab49] âœ“ Done 26.97s
2025-12-16 12:12:57,217 - src.llm.client - INFO - [dia:14ab49] âœ… HTTP 200 in 26.97s
2025-12-16 12:12:57,217 - src.llm.client - INFO - [dia:14ab49] ğŸ“¡ Stream active (200)
2025-12-16 12:12:57,217 - src.llm.client - INFO - [dia:14ab49] Starting stream parsing, waiting for first chunk...
2025-12-16 12:12:59,219 - src.llm.client - INFO - [dia:14ab49] ğŸ“Š 2.0s: 428c @214c/s (118ch, ~107t @53t/s)
2025-12-16 12:13:01,229 - src.llm.client - INFO - [dia:14ab49] ğŸ“Š 4.0s: 845c @211c/s (243ch, ~211t @53t/s)
2025-12-16 12:13:01,848 - src.llm.client - INFO - [dia:14ab49] âœ“ Done 31.61s: 929c (~125w @29c/s)
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Robotics and Active Inference (Advanced Applications & Future Directions):
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING - [WARNING] Only 7 nodes found (require at least 10, need 3 more - add more nodes to the diagram) âš ï¸
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 7 nodes found (require at least 10, need 3 more - add more nodes to the diagram) ğŸ”´
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -     Context: Module 3 Session 3
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 7 nodes found (require at least 10, need 3 more - add more nodes to the diagram) ğŸ”´
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -     Context: Module 3 Session 3
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-16 12:13:01,848 - src.generate.formats.diagrams - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 12:13:01,849 - src.generate.formats.diagrams - WARNING -   Retry attempt 1/1 for diagram: Robotics and Active Inference (Advanced Applications & Future Directions)
2025-12-16 12:13:01,849 - src.generate.formats.diagrams - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 12:13:01,849 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 12:13:01,849 - src.generate.formats.questions - INFO - Generating 10 questions for: Advanced Applications & Future Directions (Session 3)
2025-12-16 12:13:01,849 - src.llm.client - INFO - [qst:ac3c1e] ğŸš€ qst | m=gemma3:4b | p=7628c | t=150s
2025-12-16 12:13:01,849 - src.llm.client - INFO - [qst:ac3c1e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 12:13:01,849 - src.llm.client - INFO - [qst:ac3c1e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 12:13:01,851 - src.llm.client - INFO - [qst:ac3c1e] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11307 bytes, prompt=7628 chars
2025-12-16 12:13:01,851 - src.llm.client - INFO - [qst:ac3c1e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 12:13:04,192 - src.llm.request_handler - INFO - [qst:ac3c1e] âœ“ Done 2.34s
2025-12-16 12:13:04,192 - src.llm.client - INFO - [qst:ac3c1e] âœ… HTTP 200 in 2.34s
2025-12-16 12:13:04,192 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“¡ Stream active (200)
2025-12-16 12:13:04,192 - src.llm.client - INFO - [qst:ac3c1e] Starting stream parsing, waiting for first chunk...
2025-12-16 12:13:06,194 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“Š 2.0s: 598c @299c/s (126ch, ~150t @75t/s)
2025-12-16 12:13:08,209 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“Š 4.0s: 1223c @305c/s (244ch, ~306t @76t/s)
2025-12-16 12:13:10,209 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“Š 6.0s: 1827c @304c/s (366ch, ~457t @76t/s)
2025-12-16 12:13:12,212 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“Š 8.0s: 2440c @304c/s (486ch, ~610t @76t/s)
2025-12-16 12:13:14,224 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“Š 10.0s: 3029c @302c/s (607ch, ~757t @75t/s)
2025-12-16 12:13:16,228 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“Š 12.0s: 3671c @305c/s (728ch, ~918t @76t/s)
2025-12-16 12:13:18,238 - src.llm.client - INFO - [qst:ac3c1e] ğŸ“Š 14.0s: 4364c @311c/s (851ch, ~1091t @78t/s)
2025-12-16 12:13:19,902 - src.llm.client - INFO - [qst:ac3c1e] âœ“ Done 18.05s: 4962c (~691w @275c/s)
2025-12-16 12:13:19,905 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 0, 'total_fixes': 1}
2025-12-16 12:13:19,905 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-16 12:13:19,906 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 2 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 12:13:19,906 - src.generate.formats.questions - WARNING -     Context: Module 3 Session 3
2025-12-16 12:13:19,906 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 12:13:19,906 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 12:13:19,907 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 12:13:19,907 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Advanced Applications & Future Directions (Session 3)
2025-12-16 12:13:19,907 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 12:13:19,909 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 3 completed
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - QUALITY SCORE SUMMARY
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - Average Quality Score: 100.0/100
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - Overall Quality: excellent
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - Quality Distribution: {'excellent': 3}
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - [ALL COMPLIANT] Primary Materials Generation - Summary âœ…
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -   Items Processed: 3
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT] Successful: 3
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - [ERROR] Failed: 0
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -   Compliance Breakdown:
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT]: 3
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - [NEEDS REVIEW]: 0
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - [CRITICAL]: 0
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -   Issue Statistics:
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - Total Issues: 0
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - Critical Errors: 0
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - Warnings: 0
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -   Recommendations:
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - All content generated successfully
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO -     - No issues detected
2025-12-16 12:13:19,912 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 12:13:19,912 - generate_primary - INFO - 
================================================================================
2025-12-16 12:13:19,912 - generate_primary - INFO - PRIMARY MATERIALS COMPLETE
2025-12-16 12:13:19,912 - generate_primary - INFO - ================================================================================
2025-12-16 12:13:19,912 - generate_primary - INFO - Total sessions processed: 3
2025-12-16 12:13:19,912 - generate_primary - INFO - Successful: 3
2025-12-16 12:13:19,912 - generate_primary - INFO - Failed: 0
2025-12-16 12:13:19,912 - generate_primary - INFO - 
================================================================================
2025-12-16 12:13:19,912 - generate_primary - INFO - EXIT CODE: 0 (SUCCESS)
2025-12-16 12:13:19,912 - generate_primary - INFO - ================================================================================
2025-12-16 12:13:19,912 - generate_primary - INFO - All sessions processed successfully with no critical issues
2025-12-16 12:13:19,912 - generate_primary - INFO - ================================================================================
