2025-12-16 11:27:02,412 - root - INFO - Logging to file: /Users/4d/Documents/GitHub/curriculum/scripts/output/logs/04_generate_primary_20251216_112702.log
2025-12-16 11:27:02,412 - generate_primary - INFO - 
2025-12-16 11:27:02,412 - generate_primary - INFO - ğŸ“š STAGE 04: PRIMARY MATERIALS (Session-Based)
2025-12-16 11:27:02,412 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 11:27:02,412 - generate_primary - INFO - Generating materials PER SESSION (not per module)
2025-12-16 11:27:02,412 - generate_primary - INFO - Output structure: output/modules/module_XX/session_YY/[material].md
2025-12-16 11:27:02,412 - generate_primary - INFO - 
2025-12-16 11:27:02,412 - src.config.loader - INFO - Initialized ConfigLoader with directory: /Users/4d/Documents/GitHub/curriculum/config
2025-12-16 11:27:02,413 - src.config.loader - INFO - Course configuration validated successfully
2025-12-16 11:27:02,426 - src.config.loader - INFO - All configurations validated successfully
2025-12-16 11:27:02,427 - generate_primary - INFO - PRIMARY ARTIFACTS GENERATED PER SESSION:
2025-12-16 11:27:02,427 - generate_primary - INFO -   1. lecture.md - Comprehensive instructional content
2025-12-16 11:27:02,427 - generate_primary - INFO -   2. lab.md - Laboratory exercise with procedures
2025-12-16 11:27:02,427 - generate_primary - INFO -   3. study_notes.md - Concise session summary
2025-12-16 11:27:02,427 - generate_primary - INFO -   4. diagram_1.mmd, diagram_2.mmd, ... (up to 4 diagrams)
2025-12-16 11:27:02,427 - generate_primary - INFO -   5. questions.md - Comprehension assessment questions
2025-12-16 11:27:02,427 - generate_primary - INFO - 
2025-12-16 11:27:02,427 - generate_primary - INFO - 
2025-12-16 11:27:02,427 - generate_primary - INFO - âš™ï¸ CONFIGURATION
2025-12-16 11:27:02,427 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 11:27:02,427 - generate_primary - INFO -   â€¢ Diagrams per Session: 4
2025-12-16 11:27:02,427 - generate_primary - INFO -   â€¢ Log File: output/logs/04_generate_primary_20251216_112702.log
2025-12-16 11:27:02,427 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 11:27:02,427 - generate_primary - INFO - Using specified outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/tree_grafting/outlines/course_outline_20251216_112702.json
2025-12-16 11:27:02,427 - generate_primary - INFO - 
2025-12-16 11:27:02,427 - generate_primary - INFO - Processing ALL modules from outline
2025-12-16 11:27:02,427 - src.generate.orchestration.pipeline - INFO - Initializing Educational Course Generator pipeline...
2025-12-16 11:27:02,427 - src.llm.client - INFO - Initialized OllamaClient: model=gemma3:4b, url=http://localhost:11434/api/generate
2025-12-16 11:27:02,427 - src.generate.stages.stage1_outline - INFO - Initialized OutlineGenerator
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - Pipeline initialized successfully
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - STAGE 2: Generating Primary Content (Session-Based)
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - Using explicit outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/tree_grafting/outlines/course_outline_20251216_112702.json
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - Processing 10 modules with session-based generation
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - Using course-specific output directory: output/tree_grafting/
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - Module 1: Grafting Theory & Biological Principles (1 sessions)
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO - 
[1/10] Session 1: Cambial Activity & Wound Response
2025-12-16 11:27:02,428 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:27:02,429 - src.generate.formats.lectures - INFO - Generating lecture for: Grafting Theory & Biological Principles (Session 1/10)
2025-12-16 11:27:02,429 - src.llm.client - INFO - [lec:8fa8e9] ğŸš€ lec | m=gemma3:4b | p=3250c | t=180s
2025-12-16 11:27:02,429 - src.llm.client - INFO - [lec:8fa8e9] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:27:02,429 - src.llm.client - INFO - [lec:8fa8e9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:27:02,437 - src.llm.client - INFO - [lec:8fa8e9] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6885 bytes, prompt=3250 chars
2025-12-16 11:27:02,437 - src.llm.client - INFO - [lec:8fa8e9] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:27:03,654 - src.llm.request_handler - INFO - [lec:8fa8e9] âœ“ Done 1.22s
2025-12-16 11:27:03,654 - src.llm.client - INFO - [lec:8fa8e9] âœ… HTTP 200 in 1.22s
2025-12-16 11:27:03,654 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“¡ Stream active (200)
2025-12-16 11:27:03,654 - src.llm.client - INFO - [lec:8fa8e9] Starting stream parsing, waiting for first chunk...
2025-12-16 11:27:05,660 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 2.0s: 690c @344c/s (114ch, ~172t @86t/s)
2025-12-16 11:27:07,661 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 4.0s: 1300c @324c/s (230ch, ~325t @81t/s)
2025-12-16 11:27:09,666 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 6.0s: 1864c @310c/s (350ch, ~466t @78t/s)
2025-12-16 11:27:11,669 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 8.0s: 2502c @312c/s (470ch, ~626t @78t/s)
2025-12-16 11:27:13,684 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 10.0s: 3108c @310c/s (588ch, ~777t @77t/s)
2025-12-16 11:27:15,699 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 12.0s: 3805c @316c/s (708ch, ~951t @79t/s)
2025-12-16 11:27:17,710 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 14.1s: 4417c @314c/s (827ch, ~1104t @79t/s)
2025-12-16 11:27:19,711 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 16.1s: 5040c @314c/s (940ch, ~1260t @78t/s)
2025-12-16 11:27:21,722 - src.llm.client - INFO - [lec:8fa8e9] ğŸ“Š 18.1s: 5774c @320c/s (1061ch, ~1444t @80t/s)
2025-12-16 11:27:23,420 - src.llm.client - INFO - [lec:8fa8e9] âœ“ Done 20.99s: 6394c (~936w @305c/s)
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO -     - Length: 6527 chars, 955 words
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO -     - Content: 11 examples, 1 terms defined
2025-12-16 11:27:23,424 - src.generate.formats.lectures - WARNING - [WARNING] Word count (955) below minimum 1000 (need 45 more words - consider regenerating or expanding content) âš ï¸
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:27:23,424 - src.generate.formats.lectures - INFO - Quality score: 90.5/100 (excellent)
2025-12-16 11:27:23,430 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:27:23,430 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:27:23,430 - src.generate.formats.labs - INFO - Generating lab 1 for: Grafting Theory & Biological Principles (Session 1)
2025-12-16 11:27:23,431 - src.llm.client - INFO - [lab:54856d] ğŸš€ lab | m=gemma3:4b | p=3409c | t=150s
2025-12-16 11:27:23,431 - src.llm.client - INFO - [lab:54856d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:27:23,431 - src.llm.client - INFO - [lab:54856d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:27:23,432 - src.llm.client - INFO - [lab:54856d] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3834 bytes, prompt=3409 chars
2025-12-16 11:27:23,432 - src.llm.client - INFO - [lab:54856d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:27:24,479 - src.llm.request_handler - INFO - [lab:54856d] âœ“ Done 1.05s
2025-12-16 11:27:24,480 - src.llm.client - INFO - [lab:54856d] âœ… HTTP 200 in 1.05s
2025-12-16 11:27:24,480 - src.llm.client - INFO - [lab:54856d] ğŸ“¡ Stream active (200)
2025-12-16 11:27:24,480 - src.llm.client - INFO - [lab:54856d] Starting stream parsing, waiting for first chunk...
2025-12-16 11:27:26,488 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 2.0s: 541c @269c/s (112ch, ~135t @67t/s)
2025-12-16 11:27:28,494 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 4.0s: 1118c @279c/s (228ch, ~280t @70t/s)
2025-12-16 11:27:30,510 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 6.0s: 1500c @249c/s (349ch, ~375t @62t/s)
2025-12-16 11:27:32,512 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 8.0s: 1906c @237c/s (466ch, ~476t @59t/s)
2025-12-16 11:27:34,526 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 10.0s: 2499c @249c/s (586ch, ~625t @62t/s)
2025-12-16 11:27:36,536 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 12.1s: 3030c @251c/s (705ch, ~758t @63t/s)
2025-12-16 11:27:38,549 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 14.1s: 3542c @252c/s (826ch, ~886t @63t/s)
2025-12-16 11:27:40,554 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 16.1s: 4046c @252c/s (948ch, ~1012t @63t/s)
2025-12-16 11:27:42,556 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 18.1s: 4754c @263c/s (1071ch, ~1188t @66t/s)
2025-12-16 11:27:44,569 - src.llm.client - INFO - [lab:54856d] ğŸ“Š 20.1s: 5361c @267c/s (1194ch, ~1340t @67t/s)
2025-12-16 11:27:45,351 - src.llm.client - INFO - [lab:54856d] âœ“ Done 21.92s: 5544c (~771w @253c/s)
2025-12-16 11:27:45,352 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:27:45,352 - src.generate.formats.labs - INFO -     - Length: 5655 chars, 788 words
2025-12-16 11:27:45,352 - src.generate.formats.labs - INFO -     - Procedure: 7 steps
2025-12-16 11:27:45,352 - src.generate.formats.labs - INFO -     - Safety: 10 warnings
2025-12-16 11:27:45,352 - src.generate.formats.labs - INFO -     - Data tables: 5
2025-12-16 11:27:45,355 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:27:45,356 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:27:45,356 - src.generate.formats.study_notes - INFO - Generating study notes for: Grafting Theory & Biological Principles (Session 1)
2025-12-16 11:27:45,357 - src.llm.client - INFO - [stu:cf29ec] ğŸš€ stu | m=gemma3:4b | p=4532c | t=120s
2025-12-16 11:27:45,358 - src.llm.client - INFO - [stu:cf29ec] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:27:45,358 - src.llm.client - INFO - [stu:cf29ec] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:27:45,360 - src.llm.client - INFO - [stu:cf29ec] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8174 bytes, prompt=4532 chars
2025-12-16 11:27:45,360 - src.llm.client - INFO - [stu:cf29ec] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:27:46,781 - src.llm.request_handler - INFO - [stu:cf29ec] âœ“ Done 1.42s
2025-12-16 11:27:46,782 - src.llm.client - INFO - [stu:cf29ec] âœ… HTTP 200 in 1.42s
2025-12-16 11:27:46,782 - src.llm.client - INFO - [stu:cf29ec] ğŸ“¡ Stream active (200)
2025-12-16 11:27:46,782 - src.llm.client - INFO - [stu:cf29ec] Starting stream parsing, waiting for first chunk...
2025-12-16 11:27:48,786 - src.llm.client - INFO - [stu:cf29ec] ğŸ“Š 2.0s: 650c @324c/s (108ch, ~162t @81t/s)
2025-12-16 11:27:50,787 - src.llm.client - INFO - [stu:cf29ec] ğŸ“Š 4.0s: 1305c @326c/s (231ch, ~326t @81t/s)
2025-12-16 11:27:52,795 - src.llm.client - INFO - [stu:cf29ec] ğŸ“Š 6.0s: 1950c @324c/s (350ch, ~488t @81t/s)
2025-12-16 11:27:54,808 - src.llm.client - INFO - [stu:cf29ec] ğŸ“Š 8.0s: 2553c @318c/s (473ch, ~638t @80t/s)
2025-12-16 11:27:56,810 - src.llm.client - INFO - [stu:cf29ec] ğŸ“Š 10.0s: 3134c @313c/s (595ch, ~784t @78t/s)
2025-12-16 11:27:58,822 - src.llm.client - INFO - [stu:cf29ec] ğŸ“Š 12.0s: 3751c @312c/s (719ch, ~938t @78t/s)
2025-12-16 11:28:00,823 - src.llm.client - INFO - [stu:cf29ec] ğŸ“Š 14.0s: 4356c @310c/s (840ch, ~1089t @78t/s)
2025-12-16 11:28:01,264 - src.llm.client - INFO - [stu:cf29ec] âœ“ Done 15.91s: 4462c (~665w @281c/s)
2025-12-16 11:28:01,265 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 11:28:01,265 - src.generate.formats.study_notes - INFO -     - Length: 4536 chars, 677 words
2025-12-16 11:28:01,265 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:28:01,265 - src.generate.formats.study_notes - INFO -     - Key concepts: 6
2025-12-16 11:28:01,265 - src.generate.formats.study_notes - INFO -     - Structure: 6 sections, 3 bullets
2025-12-16 11:28:01,265 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:28:01,267 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:28:01,268 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:28:01,269 - src.generate.formats.diagrams - INFO - Generating diagram for: Cambial Zone Definition (Grafting Theory & Biological Principles)
2025-12-16 11:28:01,269 - src.generate.formats.diagrams - INFO - Generating diagram for: Wound Healing Stages (Grafting Theory & Biological Principles)
2025-12-16 11:28:01,269 - src.generate.formats.diagrams - INFO - Generating diagram for: Cellular Mechanisms (Grafting Theory & Biological Principles)
2025-12-16 11:28:01,269 - src.llm.client - INFO - [dia:1e1cc6] ğŸš€ dia | m=gemma3:4b | p=5785c | t=120s
2025-12-16 11:28:01,269 - src.llm.client - INFO - [dia:e0a573] ğŸš€ dia | m=gemma3:4b | p=5779c | t=120s
2025-12-16 11:28:01,269 - src.llm.client - INFO - [dia:21109d] ğŸš€ dia | m=gemma3:4b | p=5777c | t=120s
2025-12-16 11:28:01,270 - src.llm.client - INFO - [dia:1e1cc6] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:28:01,270 - src.llm.client - INFO - [dia:e0a573] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:28:01,270 - src.llm.client - INFO - [dia:21109d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:28:01,270 - src.llm.client - INFO - [dia:1e1cc6] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:28:01,270 - src.llm.client - INFO - [dia:e0a573] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:28:01,270 - src.llm.client - INFO - [dia:21109d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:28:01,273 - src.llm.client - INFO - [dia:e0a573] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11088 bytes, prompt=5779 chars
2025-12-16 11:28:01,273 - src.llm.client - INFO - [dia:21109d] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11086 bytes, prompt=5777 chars
2025-12-16 11:28:01,273 - src.llm.client - INFO - [dia:1e1cc6] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11094 bytes, prompt=5785 chars
2025-12-16 11:28:01,273 - src.llm.client - INFO - [dia:e0a573] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:28:01,273 - src.llm.client - INFO - [dia:21109d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:28:01,273 - src.llm.client - INFO - [dia:1e1cc6] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:28:03,172 - src.llm.request_handler - INFO - [dia:1e1cc6] âœ“ Done 1.90s
2025-12-16 11:28:03,172 - src.llm.client - INFO - [dia:1e1cc6] âœ… HTTP 200 in 1.90s
2025-12-16 11:28:03,172 - src.llm.client - INFO - [dia:1e1cc6] ğŸ“¡ Stream active (200)
2025-12-16 11:28:03,172 - src.llm.client - INFO - [dia:1e1cc6] Starting stream parsing, waiting for first chunk...
2025-12-16 11:28:05,178 - src.llm.client - INFO - [dia:1e1cc6] ğŸ“Š 2.0s: 426c @212c/s (115ch, ~106t @53t/s)
2025-12-16 11:28:07,192 - src.llm.client - INFO - [dia:1e1cc6] ğŸ“Š 4.0s: 873c @217c/s (235ch, ~218t @54t/s)
2025-12-16 11:28:09,192 - src.llm.client - INFO - [dia:1e1cc6] ğŸ“Š 6.0s: 1265c @210c/s (359ch, ~316t @53t/s)
2025-12-16 11:28:11,207 - src.llm.client - INFO - [dia:1e1cc6] ğŸ“Š 8.0s: 1559c @194c/s (482ch, ~390t @49t/s)
2025-12-16 11:28:13,217 - src.llm.client - INFO - [dia:1e1cc6] ğŸ“Š 10.0s: 1857c @185c/s (605ch, ~464t @46t/s)
2025-12-16 11:28:13,361 - src.llm.client - INFO - [dia:1e1cc6] âœ“ Done 12.09s: 1864c (~207w @154c/s)
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Cambial Zone Definition (Grafting Theory & Biological Principles):
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO -     - Length: 1101 chars (cleaned: 1101 chars)
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 57 total (nodes: 25, connections: 32) âš ï¸
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:28:13,362 - src.generate.formats.diagrams - INFO - Generated diagram: 1101 characters
2025-12-16 11:28:15,031 - src.llm.request_handler - INFO - [dia:21109d] âœ“ Done 13.76s
2025-12-16 11:28:15,031 - src.llm.client - INFO - [dia:21109d] âœ… HTTP 200 in 13.76s
2025-12-16 11:28:15,031 - src.llm.client - INFO - [dia:21109d] ğŸ“¡ Stream active (200)
2025-12-16 11:28:15,032 - src.llm.client - INFO - [dia:21109d] Starting stream parsing, waiting for first chunk...
2025-12-16 11:28:17,037 - src.llm.client - INFO - [dia:21109d] ğŸ“Š 2.0s: 560c @279c/s (120ch, ~140t @70t/s)
2025-12-16 11:28:19,039 - src.llm.client - INFO - [dia:21109d] ğŸ“Š 4.0s: 1113c @278c/s (241ch, ~278t @69t/s)
2025-12-16 11:28:21,047 - src.llm.client - INFO - [dia:21109d] ğŸ“Š 6.0s: 1695c @282c/s (367ch, ~424t @70t/s)
2025-12-16 11:28:22,740 - src.llm.client - INFO - [dia:21109d] âœ“ Done 21.47s: 2156c (~175w @100c/s)
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Cellular Mechanisms (Grafting Theory & Biological Principles):
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - INFO -     - Length: 420 chars (cleaned: 420 chars)
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 11 total (nodes: 6, connections: 5) âš ï¸
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - INFO -   Cleanup summary: 2 issues fixed (code fences, style commands, etc.)
2025-12-16 11:28:22,741 - src.generate.formats.diagrams - INFO - Generated diagram: 420 characters
2025-12-16 11:28:24,448 - src.llm.request_handler - INFO - [dia:e0a573] âœ“ Done 23.17s
2025-12-16 11:28:24,448 - src.llm.client - INFO - [dia:e0a573] âœ… HTTP 200 in 23.18s
2025-12-16 11:28:24,448 - src.llm.client - INFO - [dia:e0a573] ğŸ“¡ Stream active (200)
2025-12-16 11:28:24,448 - src.llm.client - INFO - [dia:e0a573] Starting stream parsing, waiting for first chunk...
2025-12-16 11:28:26,450 - src.llm.client - INFO - [dia:e0a573] ğŸ“Š 2.0s: 425c @212c/s (123ch, ~106t @53t/s)
2025-12-16 11:28:28,450 - src.llm.client - INFO - [dia:e0a573] ğŸ“Š 4.0s: 870c @217c/s (247ch, ~218t @54t/s)
2025-12-16 11:28:30,456 - src.llm.client - INFO - [dia:e0a573] ğŸ“Š 6.0s: 1182c @197c/s (366ch, ~296t @49t/s)
2025-12-16 11:28:32,160 - src.llm.client - INFO - [dia:e0a573] âœ“ Done 30.89s: 1425c (~173w @46c/s)
2025-12-16 11:28:32,160 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Wound Healing Stages (Grafting Theory & Biological Principles):
2025-12-16 11:28:32,160 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:28:32,160 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:28:32,160 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 5 long nodes) âš ï¸
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - INFO -     - Length: 913 chars (cleaned: 913 chars)
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 39 total (nodes: 16, connections: 23) âš ï¸
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 5 long nodes) âš ï¸
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:28:32,161 - src.generate.formats.diagrams - INFO - Generated diagram: 913 characters
2025-12-16 11:28:32,162 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:28:32,162 - src.generate.formats.questions - INFO - Generating 10 questions for: Grafting Theory & Biological Principles (Session 1)
2025-12-16 11:28:32,163 - src.llm.client - INFO - [qst:079abb] ğŸš€ qst | m=gemma3:4b | p=7405c | t=150s
2025-12-16 11:28:32,163 - src.llm.client - INFO - [qst:079abb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:28:32,163 - src.llm.client - INFO - [qst:079abb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:28:32,166 - src.llm.client - INFO - [qst:079abb] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11091 bytes, prompt=7405 chars
2025-12-16 11:28:32,166 - src.llm.client - INFO - [qst:079abb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:28:34,407 - src.llm.request_handler - INFO - [qst:079abb] âœ“ Done 2.24s
2025-12-16 11:28:34,408 - src.llm.client - INFO - [qst:079abb] âœ… HTTP 200 in 2.24s
2025-12-16 11:28:34,408 - src.llm.client - INFO - [qst:079abb] ğŸ“¡ Stream active (200)
2025-12-16 11:28:34,408 - src.llm.client - INFO - [qst:079abb] Starting stream parsing, waiting for first chunk...
2025-12-16 11:28:36,415 - src.llm.client - INFO - [qst:079abb] ğŸ“Š 2.0s: 558c @278c/s (120ch, ~140t @70t/s)
2025-12-16 11:28:38,428 - src.llm.client - INFO - [qst:079abb] ğŸ“Š 4.0s: 1138c @283c/s (244ch, ~284t @71t/s)
2025-12-16 11:28:40,438 - src.llm.client - INFO - [qst:079abb] ğŸ“Š 6.0s: 1725c @286c/s (368ch, ~431t @72t/s)
2025-12-16 11:28:42,441 - src.llm.client - INFO - [qst:079abb] ğŸ“Š 8.0s: 2341c @291c/s (490ch, ~585t @73t/s)
2025-12-16 11:28:44,446 - src.llm.client - INFO - [qst:079abb] ğŸ“Š 10.0s: 2898c @289c/s (610ch, ~724t @72t/s)
2025-12-16 11:28:46,458 - src.llm.client - INFO - [qst:079abb] ğŸ“Š 12.0s: 3476c @288c/s (723ch, ~869t @72t/s)
2025-12-16 11:28:48,458 - src.llm.client - INFO - [qst:079abb] ğŸ“Š 14.0s: 4047c @288c/s (835ch, ~1012t @72t/s)
2025-12-16 11:28:49,977 - src.llm.client - INFO - [qst:079abb] âœ“ Done 17.81s: 4501c (~649w @253c/s)
2025-12-16 11:28:49,979 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 4 question format issues: {'format_standardized': 0, 'question_marks_added': 4, 'mc_options_fixed': 0, 'total_fixes': 4}
2025-12-16 11:28:49,979 - src.generate.formats.questions - INFO - Applied 4 auto-fixes to questions
2025-12-16 11:28:49,980 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 11:28:49,980 - src.generate.formats.questions - WARNING -     Context: Module 1 Session 1
2025-12-16 11:28:49,980 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 11:28:49,980 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 11:28:49,980 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 11:28:49,980 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Grafting Theory & Biological Principles (Session 1)
2025-12-16 11:28:49,980 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:28:49,982 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:28:49,984 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 1 completed
2025-12-16 11:28:49,984 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:28:49,984 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:28:49,984 - src.generate.orchestration.pipeline - INFO - Module 2: Basic Grafting Techniques: Whip & Tongue (1 sessions)
2025-12-16 11:28:49,984 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:28:49,984 - src.generate.orchestration.pipeline - INFO - 
[2/10] Session 2: Whip-and-Tongue Grafting â€“ Demonstration & Practice
2025-12-16 11:28:49,984 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:28:49,984 - src.generate.formats.lectures - INFO - Generating lecture for: Basic Grafting Techniques: Whip & Tongue (Session 2/10)
2025-12-16 11:28:49,984 - src.llm.client - INFO - [lec:a614c0] ğŸš€ lec | m=gemma3:4b | p=3222c | t=180s
2025-12-16 11:28:49,985 - src.llm.client - INFO - [lec:a614c0] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:28:49,985 - src.llm.client - INFO - [lec:a614c0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:28:49,987 - src.llm.client - INFO - [lec:a614c0] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6865 bytes, prompt=3222 chars
2025-12-16 11:28:49,987 - src.llm.client - INFO - [lec:a614c0] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:28:51,149 - src.llm.request_handler - INFO - [lec:a614c0] âœ“ Done 1.16s
2025-12-16 11:28:51,150 - src.llm.client - INFO - [lec:a614c0] âœ… HTTP 200 in 1.16s
2025-12-16 11:28:51,150 - src.llm.client - INFO - [lec:a614c0] ğŸ“¡ Stream active (200)
2025-12-16 11:28:51,151 - src.llm.client - INFO - [lec:a614c0] Starting stream parsing, waiting for first chunk...
2025-12-16 11:28:53,155 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 2.0s: 679c @339c/s (120ch, ~170t @85t/s)
2025-12-16 11:28:55,169 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 4.0s: 1283c @320c/s (242ch, ~321t @80t/s)
2025-12-16 11:28:57,181 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 6.0s: 1837c @305c/s (358ch, ~459t @76t/s)
2025-12-16 11:28:59,192 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 8.0s: 2297c @286c/s (478ch, ~574t @71t/s)
2025-12-16 11:29:01,207 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 10.1s: 2800c @278c/s (595ch, ~700t @70t/s)
2025-12-16 11:29:03,218 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 12.1s: 3400c @282c/s (713ch, ~850t @70t/s)
2025-12-16 11:29:05,230 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 14.1s: 4027c @286c/s (838ch, ~1007t @72t/s)
2025-12-16 11:29:07,233 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 16.1s: 4608c @287c/s (960ch, ~1152t @72t/s)
2025-12-16 11:29:09,234 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 18.1s: 5210c @288c/s (1082ch, ~1302t @72t/s)
2025-12-16 11:29:11,243 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 20.1s: 5840c @291c/s (1206ch, ~1460t @73t/s)
2025-12-16 11:29:13,246 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 22.1s: 6397c @290c/s (1329ch, ~1599t @72t/s)
2025-12-16 11:29:15,262 - src.llm.client - INFO - [lec:a614c0] ğŸ“Š 24.1s: 7100c @294c/s (1450ch, ~1775t @74t/s)
2025-12-16 11:29:15,378 - src.llm.client - INFO - [lec:a614c0] âœ“ Done 25.39s: 7123c (~1105w @281c/s)
2025-12-16 11:29:15,379 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 11:29:15,379 - src.generate.formats.lectures - INFO -     - Length: 7232 chars, 1120 words
2025-12-16 11:29:15,379 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:29:15,379 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 11:29:15,379 - src.generate.formats.lectures - INFO -     - Content: 11 examples, 2 terms defined
2025-12-16 11:29:15,379 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:29:15,382 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:29:15,382 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:29:15,382 - src.generate.formats.labs - INFO - Generating lab 2 for: Basic Grafting Techniques: Whip & Tongue (Session 2)
2025-12-16 11:29:15,383 - src.llm.client - INFO - [lab:e82947] ğŸš€ lab | m=gemma3:4b | p=3353c | t=150s
2025-12-16 11:29:15,383 - src.llm.client - INFO - [lab:e82947] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:29:15,383 - src.llm.client - INFO - [lab:e82947] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:29:15,384 - src.llm.client - INFO - [lab:e82947] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3773 bytes, prompt=3353 chars
2025-12-16 11:29:15,384 - src.llm.client - INFO - [lab:e82947] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:29:16,402 - src.llm.request_handler - INFO - [lab:e82947] âœ“ Done 1.02s
2025-12-16 11:29:16,402 - src.llm.client - INFO - [lab:e82947] âœ… HTTP 200 in 1.02s
2025-12-16 11:29:16,402 - src.llm.client - INFO - [lab:e82947] ğŸ“¡ Stream active (200)
2025-12-16 11:29:16,402 - src.llm.client - INFO - [lab:e82947] Starting stream parsing, waiting for first chunk...
2025-12-16 11:29:18,404 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 2.0s: 525c @262c/s (127ch, ~131t @66t/s)
2025-12-16 11:29:20,417 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 4.0s: 866c @216c/s (243ch, ~216t @54t/s)
2025-12-16 11:29:22,432 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 6.0s: 1310c @217c/s (366ch, ~328t @54t/s)
2025-12-16 11:29:24,434 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 8.0s: 1783c @222c/s (479ch, ~446t @56t/s)
2025-12-16 11:29:26,446 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 10.0s: 2264c @225c/s (602ch, ~566t @56t/s)
2025-12-16 11:29:28,446 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 12.0s: 2757c @229c/s (722ch, ~689t @57t/s)
2025-12-16 11:29:30,460 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 14.1s: 3207c @228c/s (835ch, ~802t @57t/s)
2025-12-16 11:29:32,466 - src.llm.client - INFO - [lab:e82947] ğŸ“Š 16.1s: 3705c @231c/s (952ch, ~926t @58t/s)
2025-12-16 11:29:34,181 - src.llm.client - INFO - [lab:e82947] âœ“ Done 18.80s: 4226c (~648w @225c/s)
2025-12-16 11:29:34,181 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:29:34,181 - src.generate.formats.labs - INFO -     - Length: 4326 chars, 665 words
2025-12-16 11:29:34,181 - src.generate.formats.labs - INFO -     - Procedure: 13 steps
2025-12-16 11:29:34,181 - src.generate.formats.labs - INFO -     - Safety: 4 warnings
2025-12-16 11:29:34,181 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 11:29:34,183 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:29:34,184 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:29:34,184 - src.generate.formats.study_notes - INFO - Generating study notes for: Basic Grafting Techniques: Whip & Tongue (Session 2)
2025-12-16 11:29:34,184 - src.llm.client - INFO - [stu:8a2cdd] ğŸš€ stu | m=gemma3:4b | p=4485c | t=120s
2025-12-16 11:29:34,184 - src.llm.client - INFO - [stu:8a2cdd] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:29:34,184 - src.llm.client - INFO - [stu:8a2cdd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:29:34,188 - src.llm.client - INFO - [stu:8a2cdd] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8121 bytes, prompt=4485 chars
2025-12-16 11:29:34,188 - src.llm.client - INFO - [stu:8a2cdd] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:29:35,699 - src.llm.request_handler - INFO - [stu:8a2cdd] âœ“ Done 1.51s
2025-12-16 11:29:35,700 - src.llm.client - INFO - [stu:8a2cdd] âœ… HTTP 200 in 1.51s
2025-12-16 11:29:35,700 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“¡ Stream active (200)
2025-12-16 11:29:35,700 - src.llm.client - INFO - [stu:8a2cdd] Starting stream parsing, waiting for first chunk...
2025-12-16 11:29:37,711 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“Š 2.0s: 671c @334c/s (119ch, ~168t @83t/s)
2025-12-16 11:29:39,722 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“Š 4.0s: 1254c @312c/s (235ch, ~314t @78t/s)
2025-12-16 11:29:41,731 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“Š 6.0s: 1852c @307c/s (358ch, ~463t @77t/s)
2025-12-16 11:29:43,736 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“Š 8.0s: 2477c @308c/s (481ch, ~619t @77t/s)
2025-12-16 11:29:45,749 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“Š 10.0s: 2960c @295c/s (602ch, ~740t @74t/s)
2025-12-16 11:29:47,762 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“Š 12.1s: 3390c @281c/s (716ch, ~848t @70t/s)
2025-12-16 11:29:49,789 - src.llm.client - INFO - [stu:8a2cdd] ğŸ“Š 14.1s: 3971c @282c/s (829ch, ~993t @70t/s)
2025-12-16 11:29:50,240 - src.llm.client - INFO - [stu:8a2cdd] âœ“ Done 16.06s: 4084c (~619w @254c/s)
2025-12-16 11:29:50,241 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 11:29:50,241 - src.generate.formats.study_notes - INFO -     - Length: 4159 chars, 632 words
2025-12-16 11:29:50,241 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:29:50,241 - src.generate.formats.study_notes - INFO -     - Key concepts: 3
2025-12-16 11:29:50,241 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 3 bullets
2025-12-16 11:29:50,241 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:29:50,243 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:29:50,245 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:29:50,247 - src.generate.formats.diagrams - INFO - Generating diagram for: Tool Selection (Basic Grafting Techniques: Whip & Tongue)
2025-12-16 11:29:50,247 - src.generate.formats.diagrams - INFO - Generating diagram for: Blade Angle (Basic Grafting Techniques: Whip & Tongue)
2025-12-16 11:29:50,247 - src.generate.formats.diagrams - INFO - Generating diagram for: Correct Alignment (Basic Grafting Techniques: Whip & Tongue)
2025-12-16 11:29:50,248 - src.llm.client - INFO - [dia:6dac31] ğŸš€ dia | m=gemma3:4b | p=5786c | t=120s
2025-12-16 11:29:50,248 - src.llm.client - INFO - [dia:c3b9be] ğŸš€ dia | m=gemma3:4b | p=5780c | t=120s
2025-12-16 11:29:50,248 - src.llm.client - INFO - [dia:84fc1d] ğŸš€ dia | m=gemma3:4b | p=5792c | t=120s
2025-12-16 11:29:50,248 - src.llm.client - INFO - [dia:6dac31] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:29:50,248 - src.llm.client - INFO - [dia:c3b9be] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:29:50,248 - src.llm.client - INFO - [dia:84fc1d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:29:50,248 - src.llm.client - INFO - [dia:6dac31] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:29:50,249 - src.llm.client - INFO - [dia:c3b9be] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:29:50,249 - src.llm.client - INFO - [dia:84fc1d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:29:50,252 - src.llm.client - INFO - [dia:6dac31] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11100 bytes, prompt=5786 chars
2025-12-16 11:29:50,252 - src.llm.client - INFO - [dia:6dac31] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:29:50,253 - src.llm.client - INFO - [dia:84fc1d] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11106 bytes, prompt=5792 chars
2025-12-16 11:29:50,253 - src.llm.client - INFO - [dia:84fc1d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:29:50,253 - src.llm.client - INFO - [dia:c3b9be] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11094 bytes, prompt=5780 chars
2025-12-16 11:29:50,253 - src.llm.client - INFO - [dia:c3b9be] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:29:52,238 - src.llm.request_handler - INFO - [dia:c3b9be] âœ“ Done 1.98s
2025-12-16 11:29:52,239 - src.llm.client - INFO - [dia:c3b9be] âœ… HTTP 200 in 1.99s
2025-12-16 11:29:52,239 - src.llm.client - INFO - [dia:c3b9be] ğŸ“¡ Stream active (200)
2025-12-16 11:29:52,239 - src.llm.client - INFO - [dia:c3b9be] Starting stream parsing, waiting for first chunk...
2025-12-16 11:29:54,243 - src.llm.client - INFO - [dia:c3b9be] ğŸ“Š 2.0s: 369c @184c/s (109ch, ~92t @46t/s)
2025-12-16 11:29:56,254 - src.llm.client - INFO - [dia:c3b9be] ğŸ“Š 4.0s: 764c @190c/s (222ch, ~191t @48t/s)
2025-12-16 11:29:58,259 - src.llm.client - INFO - [dia:c3b9be] ğŸ“Š 6.0s: 1045c @174c/s (328ch, ~261t @43t/s)
2025-12-16 11:30:00,272 - src.llm.client - INFO - [dia:c3b9be] ğŸ“Š 8.0s: 1324c @165c/s (442ch, ~331t @41t/s)
2025-12-16 11:30:00,721 - src.llm.client - INFO - [dia:c3b9be] âœ“ Done 10.47s: 1377c (~174w @131c/s)
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Blade Angle (Basic Grafting Techniques: Whip & Tongue):
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO -     - Length: 852 chars (cleaned: 852 chars)
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO - [OK] Elements: 49 total (nodes: 18, connections: 31) âœ“
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:30:00,722 - src.generate.formats.diagrams - INFO - Generated diagram: 852 characters
2025-12-16 11:30:02,418 - src.llm.request_handler - INFO - [dia:6dac31] âœ“ Done 12.17s
2025-12-16 11:30:02,418 - src.llm.client - INFO - [dia:6dac31] âœ… HTTP 200 in 12.17s
2025-12-16 11:30:02,418 - src.llm.client - INFO - [dia:6dac31] ğŸ“¡ Stream active (200)
2025-12-16 11:30:02,418 - src.llm.client - INFO - [dia:6dac31] Starting stream parsing, waiting for first chunk...
2025-12-16 11:30:04,421 - src.llm.client - INFO - [dia:6dac31] ğŸ“Š 2.0s: 383c @191c/s (119ch, ~96t @48t/s)
2025-12-16 11:30:06,427 - src.llm.client - INFO - [dia:6dac31] ğŸ“Š 4.0s: 761c @190c/s (236ch, ~190t @47t/s)
2025-12-16 11:30:08,266 - src.llm.client - INFO - [dia:6dac31] âœ“ Done 18.02s: 1061c (~149w @59c/s)
2025-12-16 11:30:08,269 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Tool Selection (Basic Grafting Techniques: Whip & Tongue):
2025-12-16 11:30:08,269 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:30:08,269 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:30:08,269 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:30:08,269 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:30:08,269 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:30:08,270 - src.generate.formats.diagrams - INFO -     - Length: 901 chars (cleaned: 901 chars)
2025-12-16 11:30:08,270 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:30:08,270 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 54 total (nodes: 21, connections: 33) âš ï¸
2025-12-16 11:30:08,270 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:30:08,270 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:30:08,270 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:30:08,270 - src.generate.formats.diagrams - INFO - Generated diagram: 901 characters
2025-12-16 11:30:09,985 - src.llm.request_handler - INFO - [dia:84fc1d] âœ“ Done 19.73s
2025-12-16 11:30:09,985 - src.llm.client - INFO - [dia:84fc1d] âœ… HTTP 200 in 19.73s
2025-12-16 11:30:09,985 - src.llm.client - INFO - [dia:84fc1d] ğŸ“¡ Stream active (200)
2025-12-16 11:30:09,985 - src.llm.client - INFO - [dia:84fc1d] Starting stream parsing, waiting for first chunk...
2025-12-16 11:30:11,986 - src.llm.client - INFO - [dia:84fc1d] ğŸ“Š 2.0s: 417c @208c/s (123ch, ~104t @52t/s)
2025-12-16 11:30:13,999 - src.llm.client - INFO - [dia:84fc1d] ğŸ“Š 4.0s: 860c @214c/s (246ch, ~215t @54t/s)
2025-12-16 11:30:16,004 - src.llm.client - INFO - [dia:84fc1d] ğŸ“Š 6.0s: 1176c @195c/s (369ch, ~294t @49t/s)
2025-12-16 11:30:16,602 - src.llm.client - INFO - [dia:84fc1d] âœ“ Done 26.35s: 1251c (~161w @47c/s)
2025-12-16 11:30:16,602 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Correct Alignment (Basic Grafting Techniques: Whip & Tongue):
2025-12-16 11:30:16,602 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:30:16,602 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:30:16,602 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:30:16,602 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - INFO -     - Length: 896 chars (cleaned: 896 chars)
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 43 total (nodes: 17, connections: 26) âš ï¸
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:30:16,603 - src.generate.formats.diagrams - INFO - Generated diagram: 896 characters
2025-12-16 11:30:16,603 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:30:16,603 - src.generate.formats.questions - INFO - Generating 10 questions for: Basic Grafting Techniques: Whip & Tongue (Session 2)
2025-12-16 11:30:16,604 - src.llm.client - INFO - [qst:d67f5b] ğŸš€ qst | m=gemma3:4b | p=7361c | t=150s
2025-12-16 11:30:16,604 - src.llm.client - INFO - [qst:d67f5b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:30:16,604 - src.llm.client - INFO - [qst:d67f5b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:30:16,605 - src.llm.client - INFO - [qst:d67f5b] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11035 bytes, prompt=7361 chars
2025-12-16 11:30:16,605 - src.llm.client - INFO - [qst:d67f5b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:30:18,916 - src.llm.request_handler - INFO - [qst:d67f5b] âœ“ Done 2.31s
2025-12-16 11:30:18,917 - src.llm.client - INFO - [qst:d67f5b] âœ… HTTP 200 in 2.31s
2025-12-16 11:30:18,917 - src.llm.client - INFO - [qst:d67f5b] ğŸ“¡ Stream active (200)
2025-12-16 11:30:18,917 - src.llm.client - INFO - [qst:d67f5b] Starting stream parsing, waiting for first chunk...
2025-12-16 11:30:20,922 - src.llm.client - INFO - [qst:d67f5b] ğŸ“Š 2.0s: 585c @292c/s (119ch, ~146t @73t/s)
2025-12-16 11:30:22,927 - src.llm.client - INFO - [qst:d67f5b] ğŸ“Š 4.0s: 1158c @289c/s (238ch, ~290t @72t/s)
2025-12-16 11:30:24,938 - src.llm.client - INFO - [qst:d67f5b] ğŸ“Š 6.0s: 1729c @287c/s (360ch, ~432t @72t/s)
2025-12-16 11:30:26,949 - src.llm.client - INFO - [qst:d67f5b] ğŸ“Š 8.0s: 2330c @290c/s (481ch, ~582t @73t/s)
2025-12-16 11:30:28,983 - src.llm.client - INFO - [qst:d67f5b] ğŸ“Š 10.1s: 2889c @287c/s (602ch, ~722t @72t/s)
2025-12-16 11:30:30,974 - src.llm.client - INFO - [qst:d67f5b] ğŸ“Š 12.1s: 3406c @282c/s (703ch, ~852t @71t/s)
2025-12-16 11:30:32,986 - src.llm.client - INFO - [qst:d67f5b] ğŸ“Š 14.1s: 4031c @287c/s (824ch, ~1008t @72t/s)
2025-12-16 11:30:33,706 - src.llm.client - INFO - [qst:d67f5b] âœ“ Done 17.10s: 4263c (~636w @249c/s)
2025-12-16 11:30:33,707 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 11:30:33,707 - src.generate.formats.questions - WARNING -     Context: Module 2 Session 2
2025-12-16 11:30:33,707 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 11:30:33,707 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 11:30:33,707 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 11:30:33,707 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Basic Grafting Techniques: Whip & Tongue (Session 2)
2025-12-16 11:30:33,707 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:30:33,709 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:30:33,710 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 2 completed
2025-12-16 11:30:33,711 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:30:33,711 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:30:33,711 - src.generate.orchestration.pipeline - INFO - Module 3: Cleft Grafting & Crust Grafting (1 sessions)
2025-12-16 11:30:33,711 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:30:33,711 - src.generate.orchestration.pipeline - INFO - 
[3/10] Session 3: Cleft & Crust Grafting â€“ Technique & Considerations
2025-12-16 11:30:33,711 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:30:33,711 - src.generate.formats.lectures - INFO - Generating lecture for: Cleft Grafting & Crust Grafting (Session 3/10)
2025-12-16 11:30:33,711 - src.llm.client - INFO - [lec:c148ab] ğŸš€ lec | m=gemma3:4b | p=3167c | t=180s
2025-12-16 11:30:33,711 - src.llm.client - INFO - [lec:c148ab] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:30:33,711 - src.llm.client - INFO - [lec:c148ab] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:30:33,712 - src.llm.client - INFO - [lec:c148ab] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6808 bytes, prompt=3167 chars
2025-12-16 11:30:33,712 - src.llm.client - INFO - [lec:c148ab] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:30:34,817 - src.llm.request_handler - INFO - [lec:c148ab] âœ“ Done 1.10s
2025-12-16 11:30:34,818 - src.llm.client - INFO - [lec:c148ab] âœ… HTTP 200 in 1.11s
2025-12-16 11:30:34,818 - src.llm.client - INFO - [lec:c148ab] ğŸ“¡ Stream active (200)
2025-12-16 11:30:34,818 - src.llm.client - INFO - [lec:c148ab] Starting stream parsing, waiting for first chunk...
2025-12-16 11:30:36,825 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 2.0s: 720c @359c/s (122ch, ~180t @90t/s)
2025-12-16 11:30:38,827 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 4.0s: 1427c @356c/s (248ch, ~357t @89t/s)
2025-12-16 11:30:40,836 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 6.0s: 2043c @339c/s (375ch, ~511t @85t/s)
2025-12-16 11:30:42,850 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 8.0s: 2668c @332c/s (497ch, ~667t @83t/s)
2025-12-16 11:30:44,860 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 10.0s: 3308c @329c/s (623ch, ~827t @82t/s)
2025-12-16 11:30:46,862 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 12.0s: 3947c @328c/s (736ch, ~987t @82t/s)
2025-12-16 11:30:48,877 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 14.1s: 4551c @324c/s (852ch, ~1138t @81t/s)
2025-12-16 11:30:50,882 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 16.1s: 5220c @325c/s (974ch, ~1305t @81t/s)
2025-12-16 11:30:52,888 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 18.1s: 5926c @328c/s (1100ch, ~1482t @82t/s)
2025-12-16 11:30:54,903 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 20.1s: 6661c @332c/s (1226ch, ~1665t @83t/s)
2025-12-16 11:30:56,908 - src.llm.client - INFO - [lec:c148ab] ğŸ“Š 22.1s: 7479c @339c/s (1351ch, ~1870t @85t/s)
2025-12-16 11:30:57,776 - src.llm.client - INFO - [lec:c148ab] âœ“ Done 24.06s: 7792c (~1132w @324c/s)
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO -     - Length: 7882 chars, 1146 words
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO -     - Content: 12 examples, 0 terms defined
2025-12-16 11:30:57,777 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:30:57,777 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 11:30:57,780 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:30:57,781 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:30:57,781 - src.generate.formats.labs - INFO - Generating lab 3 for: Cleft Grafting & Crust Grafting (Session 3)
2025-12-16 11:30:57,781 - src.llm.client - INFO - [lab:c6e80c] ğŸš€ lab | m=gemma3:4b | p=3331c | t=150s
2025-12-16 11:30:57,781 - src.llm.client - INFO - [lab:c6e80c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:30:57,781 - src.llm.client - INFO - [lab:c6e80c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:30:57,782 - src.llm.client - INFO - [lab:c6e80c] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3732 bytes, prompt=3331 chars
2025-12-16 11:30:57,782 - src.llm.client - INFO - [lab:c6e80c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:30:58,858 - src.llm.request_handler - INFO - [lab:c6e80c] âœ“ Done 1.08s
2025-12-16 11:30:58,858 - src.llm.client - INFO - [lab:c6e80c] âœ… HTTP 200 in 1.08s
2025-12-16 11:30:58,858 - src.llm.client - INFO - [lab:c6e80c] ğŸ“¡ Stream active (200)
2025-12-16 11:30:58,858 - src.llm.client - INFO - [lab:c6e80c] Starting stream parsing, waiting for first chunk...
2025-12-16 11:31:00,873 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 2.0s: 518c @257c/s (126ch, ~130t @64t/s)
2025-12-16 11:31:02,877 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 4.0s: 1209c @301c/s (248ch, ~302t @75t/s)
2025-12-16 11:31:04,883 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 6.0s: 1581c @262c/s (367ch, ~395t @66t/s)
2025-12-16 11:31:06,888 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 8.0s: 2048c @255c/s (489ch, ~512t @64t/s)
2025-12-16 11:31:08,890 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 10.0s: 2616c @261c/s (611ch, ~654t @65t/s)
2025-12-16 11:31:10,892 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 12.0s: 3171c @264c/s (732ch, ~793t @66t/s)
2025-12-16 11:31:12,908 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 14.1s: 3749c @267c/s (849ch, ~937t @67t/s)
2025-12-16 11:31:14,917 - src.llm.client - INFO - [lab:c6e80c] ğŸ“Š 16.1s: 4327c @269c/s (964ch, ~1082t @67t/s)
2025-12-16 11:31:16,615 - src.llm.client - INFO - [lab:c6e80c] âœ“ Done 18.83s: 4796c (~672w @255c/s)
2025-12-16 11:31:16,615 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:31:16,615 - src.generate.formats.labs - INFO -     - Length: 4895 chars, 689 words
2025-12-16 11:31:16,615 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-16 11:31:16,615 - src.generate.formats.labs - INFO -     - Safety: 8 warnings
2025-12-16 11:31:16,615 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 11:31:16,617 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:31:16,617 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:31:16,617 - src.generate.formats.study_notes - INFO - Generating study notes for: Cleft Grafting & Crust Grafting (Session 3)
2025-12-16 11:31:16,618 - src.llm.client - INFO - [stu:26e854] ğŸš€ stu | m=gemma3:4b | p=4448c | t=120s
2025-12-16 11:31:16,618 - src.llm.client - INFO - [stu:26e854] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:31:16,618 - src.llm.client - INFO - [stu:26e854] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:31:16,619 - src.llm.client - INFO - [stu:26e854] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8064 bytes, prompt=4448 chars
2025-12-16 11:31:16,619 - src.llm.client - INFO - [stu:26e854] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:31:18,012 - src.llm.request_handler - INFO - [stu:26e854] âœ“ Done 1.39s
2025-12-16 11:31:18,012 - src.llm.client - INFO - [stu:26e854] âœ… HTTP 200 in 1.39s
2025-12-16 11:31:18,012 - src.llm.client - INFO - [stu:26e854] ğŸ“¡ Stream active (200)
2025-12-16 11:31:18,012 - src.llm.client - INFO - [stu:26e854] Starting stream parsing, waiting for first chunk...
2025-12-16 11:31:20,027 - src.llm.client - INFO - [stu:26e854] ğŸ“Š 2.0s: 606c @301c/s (111ch, ~152t @75t/s)
2025-12-16 11:31:22,029 - src.llm.client - INFO - [stu:26e854] ğŸ“Š 4.0s: 1091c @272c/s (211ch, ~273t @68t/s)
2025-12-16 11:31:24,031 - src.llm.client - INFO - [stu:26e854] ğŸ“Š 6.0s: 1738c @289c/s (334ch, ~434t @72t/s)
2025-12-16 11:31:26,043 - src.llm.client - INFO - [stu:26e854] ğŸ“Š 8.0s: 2307c @287c/s (456ch, ~577t @72t/s)
2025-12-16 11:31:28,050 - src.llm.client - INFO - [stu:26e854] ğŸ“Š 10.0s: 2989c @298c/s (574ch, ~747t @74t/s)
2025-12-16 11:31:29,333 - src.llm.client - INFO - [stu:26e854] âœ“ Done 12.72s: 3344c (~452w @263c/s)
2025-12-16 11:31:29,335 - src.generate.formats.study_notes - INFO - [NEEDS REVIEW] Study notes generated âš ï¸
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - INFO -     - Length: 3410 chars, 464 words
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - INFO -     - Key concepts: 20
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - WARNING - [WARNING] Too many key concepts (20, maximum 10, 10 excess - consolidate related concepts or remove less critical ones) âš ï¸
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:31:29,336 - src.generate.formats.study_notes - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 11:31:29,337 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:31:29,338 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:31:29,338 - src.generate.formats.diagrams - INFO - Generating diagram for: Preparing the Cleft (Cleft Grafting & Crust Grafting)
2025-12-16 11:31:29,339 - src.generate.formats.diagrams - INFO - Generating diagram for: Binding Methods (Cleft Grafting & Crust Grafting)
2025-12-16 11:31:29,339 - src.llm.client - INFO - [dia:260949] ğŸš€ dia | m=gemma3:4b | p=5787c | t=120s
2025-12-16 11:31:29,339 - src.llm.client - INFO - [dia:be355a] ğŸš€ dia | m=gemma3:4b | p=5779c | t=120s
2025-12-16 11:31:29,339 - src.llm.client - INFO - [dia:260949] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:31:29,340 - src.llm.client - INFO - [dia:be355a] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:31:29,340 - src.llm.client - INFO - [dia:260949] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:31:29,340 - src.llm.client - INFO - [dia:be355a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:31:29,342 - src.llm.client - INFO - [dia:260949] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11101 bytes, prompt=5787 chars
2025-12-16 11:31:29,342 - src.llm.client - INFO - [dia:be355a] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11093 bytes, prompt=5779 chars
2025-12-16 11:31:29,342 - src.llm.client - INFO - [dia:260949] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:31:29,342 - src.llm.client - INFO - [dia:be355a] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:31:31,272 - src.llm.request_handler - INFO - [dia:260949] âœ“ Done 1.93s
2025-12-16 11:31:31,272 - src.llm.client - INFO - [dia:260949] âœ… HTTP 200 in 1.93s
2025-12-16 11:31:31,272 - src.llm.client - INFO - [dia:260949] ğŸ“¡ Stream active (200)
2025-12-16 11:31:31,272 - src.llm.client - INFO - [dia:260949] Starting stream parsing, waiting for first chunk...
2025-12-16 11:31:33,273 - src.llm.client - INFO - [dia:260949] ğŸ“Š 2.0s: 484c @242c/s (118ch, ~121t @60t/s)
2025-12-16 11:31:35,280 - src.llm.client - INFO - [dia:260949] ğŸ“Š 4.0s: 948c @237c/s (243ch, ~237t @59t/s)
2025-12-16 11:31:36,093 - src.llm.client - INFO - [dia:260949] âœ“ Done 6.75s: 1071c (~146w @159c/s)
2025-12-16 11:31:36,093 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Preparing the Cleft (Cleft Grafting & Crust Grafting):
2025-12-16 11:31:36,093 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 3 long nodes) âš ï¸
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO -     - Length: 988 chars (cleaned: 988 chars)
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 43 total (nodes: 16, connections: 27) âš ï¸
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 3 long nodes) âš ï¸
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:31:36,094 - src.generate.formats.diagrams - INFO - Generated diagram: 988 characters
2025-12-16 11:31:37,792 - src.llm.request_handler - INFO - [dia:be355a] âœ“ Done 8.45s
2025-12-16 11:31:37,795 - src.llm.client - INFO - [dia:be355a] âœ… HTTP 200 in 8.45s
2025-12-16 11:31:37,795 - src.llm.client - INFO - [dia:be355a] ğŸ“¡ Stream active (200)
2025-12-16 11:31:37,795 - src.llm.client - INFO - [dia:be355a] Starting stream parsing, waiting for first chunk...
2025-12-16 11:31:39,806 - src.llm.client - INFO - [dia:be355a] ğŸ“Š 2.0s: 463c @230c/s (125ch, ~116t @58t/s)
2025-12-16 11:31:41,817 - src.llm.client - INFO - [dia:be355a] ğŸ“Š 4.0s: 829c @206c/s (243ch, ~207t @52t/s)
2025-12-16 11:31:42,250 - src.llm.client - INFO - [dia:be355a] âœ“ Done 12.91s: 891c (~122w @69c/s)
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Binding Methods (Cleft Grafting & Crust Grafting):
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 5 long nodes) âš ï¸
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - INFO -     - Length: 876 chars (cleaned: 876 chars)
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 29 total (nodes: 12, connections: 17) âš ï¸
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 5 long nodes) âš ï¸
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - INFO -   Cleanup summary: 2 issues fixed (code fences, style commands, etc.)
2025-12-16 11:31:42,251 - src.generate.formats.diagrams - INFO - Generated diagram: 876 characters
2025-12-16 11:31:42,252 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:31:42,252 - src.generate.formats.questions - INFO - Generating 10 questions for: Cleft Grafting & Crust Grafting (Session 3)
2025-12-16 11:31:42,253 - src.llm.client - INFO - [qst:27593a] ğŸš€ qst | m=gemma3:4b | p=7331c | t=150s
2025-12-16 11:31:42,253 - src.llm.client - INFO - [qst:27593a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:31:42,253 - src.llm.client - INFO - [qst:27593a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:31:42,256 - src.llm.client - INFO - [qst:27593a] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11036 bytes, prompt=7331 chars
2025-12-16 11:31:42,256 - src.llm.client - INFO - [qst:27593a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:31:44,392 - src.llm.request_handler - INFO - [qst:27593a] âœ“ Done 2.14s
2025-12-16 11:31:44,392 - src.llm.client - INFO - [qst:27593a] âœ… HTTP 200 in 2.14s
2025-12-16 11:31:44,392 - src.llm.client - INFO - [qst:27593a] ğŸ“¡ Stream active (200)
2025-12-16 11:31:44,392 - src.llm.client - INFO - [qst:27593a] Starting stream parsing, waiting for first chunk...
2025-12-16 11:31:46,396 - src.llm.client - INFO - [qst:27593a] ğŸ“Š 2.0s: 597c @298c/s (125ch, ~149t @75t/s)
2025-12-16 11:31:48,412 - src.llm.client - INFO - [qst:27593a] ğŸ“Š 4.0s: 1102c @274c/s (238ch, ~276t @69t/s)
2025-12-16 11:31:50,422 - src.llm.client - INFO - [qst:27593a] ğŸ“Š 6.0s: 1672c @277c/s (359ch, ~418t @69t/s)
2025-12-16 11:31:52,427 - src.llm.client - INFO - [qst:27593a] ğŸ“Š 8.0s: 2225c @277c/s (481ch, ~556t @69t/s)
2025-12-16 11:31:54,436 - src.llm.client - INFO - [qst:27593a] ğŸ“Š 10.0s: 2846c @283c/s (603ch, ~712t @71t/s)
2025-12-16 11:31:56,452 - src.llm.client - INFO - [qst:27593a] ğŸ“Š 12.1s: 3497c @290c/s (727ch, ~874t @72t/s)
2025-12-16 11:31:58,460 - src.llm.client - INFO - [qst:27593a] ğŸ“Š 14.1s: 4180c @297c/s (851ch, ~1045t @74t/s)
2025-12-16 11:31:59,105 - src.llm.client - INFO - [qst:27593a] âœ“ Done 16.85s: 4397c (~633w @261c/s)
2025-12-16 11:31:59,105 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 4 question format issues: {'format_standardized': 0, 'question_marks_added': 4, 'mc_options_fixed': 0, 'total_fixes': 4}
2025-12-16 11:31:59,105 - src.generate.formats.questions - INFO - Applied 4 auto-fixes to questions
2025-12-16 11:31:59,106 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 2 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 11:31:59,106 - src.generate.formats.questions - WARNING -     Context: Module 3 Session 3
2025-12-16 11:31:59,106 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 11:31:59,106 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 11:31:59,106 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 11:31:59,106 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Cleft Grafting & Crust Grafting (Session 3)
2025-12-16 11:31:59,106 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:31:59,108 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:31:59,111 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 3 completed
2025-12-16 11:31:59,111 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:31:59,111 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:31:59,111 - src.generate.orchestration.pipeline - INFO - Module 4: Bark Grafting (Bark Grafting) (1 sessions)
2025-12-16 11:31:59,111 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:31:59,111 - src.generate.orchestration.pipeline - INFO - 
[4/10] Session 4: Bark Grafting â€“ Procedure & Troubleshooting
2025-12-16 11:31:59,112 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:31:59,112 - src.generate.formats.lectures - INFO - Generating lecture for: Bark Grafting (Bark Grafting) (Session 4/10)
2025-12-16 11:31:59,112 - src.llm.client - INFO - [lec:31ee38] ğŸš€ lec | m=gemma3:4b | p=3107c | t=180s
2025-12-16 11:31:59,112 - src.llm.client - INFO - [lec:31ee38] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:31:59,112 - src.llm.client - INFO - [lec:31ee38] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:31:59,114 - src.llm.client - INFO - [lec:31ee38] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6748 bytes, prompt=3107 chars
2025-12-16 11:31:59,114 - src.llm.client - INFO - [lec:31ee38] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:32:00,217 - src.llm.request_handler - INFO - [lec:31ee38] âœ“ Done 1.10s
2025-12-16 11:32:00,217 - src.llm.client - INFO - [lec:31ee38] âœ… HTTP 200 in 1.10s
2025-12-16 11:32:00,217 - src.llm.client - INFO - [lec:31ee38] ğŸ“¡ Stream active (200)
2025-12-16 11:32:00,217 - src.llm.client - INFO - [lec:31ee38] Starting stream parsing, waiting for first chunk...
2025-12-16 11:32:02,224 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 2.0s: 657c @327c/s (121ch, ~164t @82t/s)
2025-12-16 11:32:04,225 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 4.0s: 1314c @328c/s (249ch, ~328t @82t/s)
2025-12-16 11:32:06,228 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 6.0s: 1823c @303c/s (370ch, ~456t @76t/s)
2025-12-16 11:32:08,232 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 8.0s: 2416c @301c/s (493ch, ~604t @75t/s)
2025-12-16 11:32:10,238 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 10.0s: 3019c @301c/s (620ch, ~755t @75t/s)
2025-12-16 11:32:12,241 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 12.0s: 3498c @291c/s (743ch, ~874t @73t/s)
2025-12-16 11:32:14,244 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 14.0s: 4154c @296c/s (867ch, ~1038t @74t/s)
2025-12-16 11:32:16,249 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 16.0s: 4801c @299c/s (992ch, ~1200t @75t/s)
2025-12-16 11:32:18,265 - src.llm.client - INFO - [lec:31ee38] ğŸ“Š 18.0s: 5406c @300c/s (1111ch, ~1352t @75t/s)
2025-12-16 11:32:20,147 - src.llm.client - INFO - [lec:31ee38] âœ“ Done 21.03s: 6053c (~937w @288c/s)
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO -     - Length: 6140 chars, 950 words
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 1 subsections
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO -     - Content: 10 examples, 3 terms defined
2025-12-16 11:32:20,148 - src.generate.formats.lectures - WARNING - [WARNING] Word count (950) below minimum 1000 (need 50 more words - consider regenerating or expanding content) âš ï¸
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:32:20,148 - src.generate.formats.lectures - INFO - Quality score: 90.0/100 (excellent)
2025-12-16 11:32:20,150 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:32:20,151 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:32:20,151 - src.generate.formats.labs - INFO - Generating lab 4 for: Bark Grafting (Bark Grafting) (Session 4)
2025-12-16 11:32:20,151 - src.llm.client - INFO - [lab:fe374e] ğŸš€ lab | m=gemma3:4b | p=3327c | t=150s
2025-12-16 11:32:20,151 - src.llm.client - INFO - [lab:fe374e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:32:20,151 - src.llm.client - INFO - [lab:fe374e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:32:20,154 - src.llm.client - INFO - [lab:fe374e] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3750 bytes, prompt=3327 chars
2025-12-16 11:32:20,154 - src.llm.client - INFO - [lab:fe374e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:32:21,200 - src.llm.request_handler - INFO - [lab:fe374e] âœ“ Done 1.05s
2025-12-16 11:32:21,200 - src.llm.client - INFO - [lab:fe374e] âœ… HTTP 200 in 1.05s
2025-12-16 11:32:21,200 - src.llm.client - INFO - [lab:fe374e] ğŸ“¡ Stream active (200)
2025-12-16 11:32:21,200 - src.llm.client - INFO - [lab:fe374e] Starting stream parsing, waiting for first chunk...
2025-12-16 11:32:23,206 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 2.0s: 585c @292c/s (123ch, ~146t @73t/s)
2025-12-16 11:32:25,212 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 4.0s: 1230c @307c/s (250ch, ~308t @77t/s)
2025-12-16 11:32:27,222 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 6.0s: 1665c @276c/s (376ch, ~416t @69t/s)
2025-12-16 11:32:29,229 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 8.0s: 2173c @271c/s (502ch, ~543t @68t/s)
2025-12-16 11:32:31,236 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 10.0s: 2669c @266c/s (628ch, ~667t @66t/s)
2025-12-16 11:32:33,250 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 12.1s: 3132c @260c/s (754ch, ~783t @65t/s)
2025-12-16 11:32:35,255 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 14.1s: 3626c @258c/s (880ch, ~906t @64t/s)
2025-12-16 11:32:37,259 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 16.1s: 4017c @250c/s (1004ch, ~1004t @63t/s)
2025-12-16 11:32:39,263 - src.llm.client - INFO - [lab:fe374e] ğŸ“Š 18.1s: 4618c @256c/s (1126ch, ~1154t @64t/s)
2025-12-16 11:32:40,411 - src.llm.client - INFO - [lab:fe374e] âœ“ Done 20.26s: 4943c (~765w @244c/s)
2025-12-16 11:32:40,412 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:32:40,412 - src.generate.formats.labs - INFO -     - Length: 5039 chars, 780 words
2025-12-16 11:32:40,412 - src.generate.formats.labs - INFO -     - Procedure: 13 steps
2025-12-16 11:32:40,412 - src.generate.formats.labs - INFO -     - Safety: 9 warnings
2025-12-16 11:32:40,412 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 11:32:40,415 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:32:40,416 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:32:40,416 - src.generate.formats.study_notes - INFO - Generating study notes for: Bark Grafting (Bark Grafting) (Session 4)
2025-12-16 11:32:40,418 - src.llm.client - INFO - [stu:065533] ğŸš€ stu | m=gemma3:4b | p=4435c | t=120s
2025-12-16 11:32:40,418 - src.llm.client - INFO - [stu:065533] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:32:40,418 - src.llm.client - INFO - [stu:065533] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:32:40,420 - src.llm.client - INFO - [stu:065533] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8073 bytes, prompt=4435 chars
2025-12-16 11:32:40,420 - src.llm.client - INFO - [stu:065533] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:32:41,848 - src.llm.request_handler - INFO - [stu:065533] âœ“ Done 1.43s
2025-12-16 11:32:41,849 - src.llm.client - INFO - [stu:065533] âœ… HTTP 200 in 1.43s
2025-12-16 11:32:41,849 - src.llm.client - INFO - [stu:065533] ğŸ“¡ Stream active (200)
2025-12-16 11:32:41,849 - src.llm.client - INFO - [stu:065533] Starting stream parsing, waiting for first chunk...
2025-12-16 11:32:43,863 - src.llm.client - INFO - [stu:065533] ğŸ“Š 2.0s: 688c @342c/s (127ch, ~172t @85t/s)
2025-12-16 11:32:45,868 - src.llm.client - INFO - [stu:065533] ğŸ“Š 4.0s: 1348c @335c/s (255ch, ~337t @84t/s)
2025-12-16 11:32:47,875 - src.llm.client - INFO - [stu:065533] ğŸ“Š 6.0s: 2012c @334c/s (377ch, ~503t @83t/s)
2025-12-16 11:32:49,880 - src.llm.client - INFO - [stu:065533] ğŸ“Š 8.0s: 2593c @323c/s (490ch, ~648t @81t/s)
2025-12-16 11:32:51,884 - src.llm.client - INFO - [stu:065533] ğŸ“Š 10.0s: 3123c @311c/s (615ch, ~781t @78t/s)
2025-12-16 11:32:53,717 - src.llm.client - INFO - [stu:065533] âœ“ Done 13.30s: 3648c (~518w @274c/s)
2025-12-16 11:32:53,717 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 11:32:53,717 - src.generate.formats.study_notes - INFO -     - Length: 3712 chars, 529 words
2025-12-16 11:32:53,717 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:32:53,717 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-16 11:32:53,717 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 3 bullets
2025-12-16 11:32:53,717 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:32:53,719 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:32:53,720 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:32:53,720 - src.generate.formats.diagrams - INFO - Generating diagram for: Bark Preparation (Bark Grafting (Bark Grafting))
2025-12-16 11:32:53,720 - src.generate.formats.diagrams - INFO - Generating diagram for: Binding Techniques (Bark Grafting (Bark Grafting))
2025-12-16 11:32:53,720 - src.llm.client - INFO - [dia:74825d] ğŸš€ dia | m=gemma3:4b | p=5771c | t=120s
2025-12-16 11:32:53,720 - src.llm.client - INFO - [dia:808b74] ğŸš€ dia | m=gemma3:4b | p=5775c | t=120s
2025-12-16 11:32:53,720 - src.llm.client - INFO - [dia:74825d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:32:53,720 - src.llm.client - INFO - [dia:808b74] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:32:53,720 - src.llm.client - INFO - [dia:74825d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:32:53,720 - src.llm.client - INFO - [dia:808b74] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:32:53,722 - src.llm.client - INFO - [dia:74825d] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11085 bytes, prompt=5771 chars
2025-12-16 11:32:53,722 - src.llm.client - INFO - [dia:808b74] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11089 bytes, prompt=5775 chars
2025-12-16 11:32:53,722 - src.llm.client - INFO - [dia:74825d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:32:53,722 - src.llm.client - INFO - [dia:808b74] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:32:55,667 - src.llm.request_handler - INFO - [dia:808b74] âœ“ Done 1.94s
2025-12-16 11:32:55,668 - src.llm.client - INFO - [dia:808b74] âœ… HTTP 200 in 1.95s
2025-12-16 11:32:55,668 - src.llm.client - INFO - [dia:808b74] ğŸ“¡ Stream active (200)
2025-12-16 11:32:55,668 - src.llm.client - INFO - [dia:808b74] Starting stream parsing, waiting for first chunk...
2025-12-16 11:32:57,677 - src.llm.client - INFO - [dia:808b74] ğŸ“Š 2.0s: 447c @222c/s (123ch, ~112t @56t/s)
2025-12-16 11:32:59,682 - src.llm.client - INFO - [dia:808b74] ğŸ“Š 4.0s: 899c @224c/s (248ch, ~225t @56t/s)
2025-12-16 11:33:00,393 - src.llm.client - INFO - [dia:808b74] âœ“ Done 6.67s: 1021c (~121w @153c/s)
2025-12-16 11:33:00,394 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Binding Techniques (Bark Grafting (Bark Grafting)):
2025-12-16 11:33:00,394 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:33:00,394 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:33:00,394 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:33:00,394 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 6 long nodes) âš ï¸
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - INFO -     - Length: 933 chars (cleaned: 933 chars)
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 33 total (nodes: 22, connections: 11) âš ï¸
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 6 long nodes) âš ï¸
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:33:00,395 - src.generate.formats.diagrams - INFO - Generated diagram: 933 characters
2025-12-16 11:33:02,129 - src.llm.request_handler - INFO - [dia:74825d] âœ“ Done 8.41s
2025-12-16 11:33:02,130 - src.llm.client - INFO - [dia:74825d] âœ… HTTP 200 in 8.41s
2025-12-16 11:33:02,130 - src.llm.client - INFO - [dia:74825d] ğŸ“¡ Stream active (200)
2025-12-16 11:33:02,130 - src.llm.client - INFO - [dia:74825d] Starting stream parsing, waiting for first chunk...
2025-12-16 11:33:04,137 - src.llm.client - INFO - [dia:74825d] ğŸ“Š 2.0s: 383c @191c/s (118ch, ~96t @48t/s)
2025-12-16 11:33:06,139 - src.llm.client - INFO - [dia:74825d] ğŸ“Š 4.0s: 744c @186c/s (237ch, ~186t @46t/s)
2025-12-16 11:33:07,183 - src.llm.client - INFO - [dia:74825d] âœ“ Done 13.46s: 876c (~126w @65c/s)
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Bark Preparation (Bark Grafting (Bark Grafting)):
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO -     - Length: 689 chars (cleaned: 689 chars)
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 42 total (nodes: 14, connections: 28) âš ï¸
2025-12-16 11:33:07,183 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:33:07,184 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:33:07,184 - src.generate.formats.diagrams - INFO -   Cleanup summary: 5 issues fixed (code fences, style commands, etc.)
2025-12-16 11:33:07,184 - src.generate.formats.diagrams - INFO - Generated diagram: 689 characters
2025-12-16 11:33:07,184 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:33:07,184 - src.generate.formats.questions - INFO - Generating 10 questions for: Bark Grafting (Bark Grafting) (Session 4)
2025-12-16 11:33:07,184 - src.llm.client - INFO - [qst:9e0a5f] ğŸš€ qst | m=gemma3:4b | p=7328c | t=150s
2025-12-16 11:33:07,184 - src.llm.client - INFO - [qst:9e0a5f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:33:07,184 - src.llm.client - INFO - [qst:9e0a5f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:33:07,186 - src.llm.client - INFO - [qst:9e0a5f] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11009 bytes, prompt=7328 chars
2025-12-16 11:33:07,186 - src.llm.client - INFO - [qst:9e0a5f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:33:09,375 - src.llm.request_handler - INFO - [qst:9e0a5f] âœ“ Done 2.19s
2025-12-16 11:33:09,376 - src.llm.client - INFO - [qst:9e0a5f] âœ… HTTP 200 in 2.19s
2025-12-16 11:33:09,376 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“¡ Stream active (200)
2025-12-16 11:33:09,376 - src.llm.client - INFO - [qst:9e0a5f] Starting stream parsing, waiting for first chunk...
2025-12-16 11:33:11,379 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 2.0s: 561c @280c/s (120ch, ~140t @70t/s)
2025-12-16 11:33:13,381 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 4.0s: 1064c @266c/s (241ch, ~266t @66t/s)
2025-12-16 11:33:15,388 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 6.0s: 1631c @271c/s (366ch, ~408t @68t/s)
2025-12-16 11:33:17,402 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 8.0s: 2215c @276c/s (492ch, ~554t @69t/s)
2025-12-16 11:33:19,416 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 10.0s: 2868c @286c/s (614ch, ~717t @71t/s)
2025-12-16 11:33:21,424 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 12.0s: 3376c @280c/s (724ch, ~844t @70t/s)
2025-12-16 11:33:23,438 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 14.1s: 4001c @285c/s (848ch, ~1000t @71t/s)
2025-12-16 11:33:25,510 - src.llm.client - INFO - [qst:9e0a5f] ğŸ“Š 16.1s: 4629c @287c/s (966ch, ~1157t @72t/s)
2025-12-16 11:33:25,510 - src.llm.client - INFO - [qst:9e0a5f] âœ“ Done 18.33s: 4629c (~702w @253c/s)
2025-12-16 11:33:25,512 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 1, 'total_fixes': 2}
2025-12-16 11:33:25,512 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -     Context: Module 4 Session 4
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -     Context: Module 4 Session 4
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Bark Grafting (Bark Grafting) (Session 4)
2025-12-16 11:33:25,513 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:33:25,515 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:33:25,517 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 4 completed
2025-12-16 11:33:25,517 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:33:25,517 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:33:25,517 - src.generate.orchestration.pipeline - INFO - Module 5: Budding Techniques (T-Budding) (1 sessions)
2025-12-16 11:33:25,517 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:33:25,517 - src.generate.orchestration.pipeline - INFO - 
[5/10] Session 5: T-Budding â€“ Demonstration & Application
2025-12-16 11:33:25,518 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:33:25,518 - src.generate.formats.lectures - INFO - Generating lecture for: Budding Techniques (T-Budding) (Session 5/10)
2025-12-16 11:33:25,519 - src.llm.client - INFO - [lec:a2db3d] ğŸš€ lec | m=gemma3:4b | p=3103c | t=180s
2025-12-16 11:33:25,519 - src.llm.client - INFO - [lec:a2db3d] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:33:25,519 - src.llm.client - INFO - [lec:a2db3d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:33:25,523 - src.llm.client - INFO - [lec:a2db3d] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6744 bytes, prompt=3103 chars
2025-12-16 11:33:25,523 - src.llm.client - INFO - [lec:a2db3d] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:33:26,641 - src.llm.request_handler - INFO - [lec:a2db3d] âœ“ Done 1.12s
2025-12-16 11:33:26,641 - src.llm.client - INFO - [lec:a2db3d] âœ… HTTP 200 in 1.12s
2025-12-16 11:33:26,641 - src.llm.client - INFO - [lec:a2db3d] ğŸ“¡ Stream active (200)
2025-12-16 11:33:26,641 - src.llm.client - INFO - [lec:a2db3d] Starting stream parsing, waiting for first chunk...
2025-12-16 11:33:28,656 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 2.0s: 677c @336c/s (127ch, ~169t @84t/s)
2025-12-16 11:33:30,663 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 4.0s: 1277c @318c/s (251ch, ~319t @79t/s)
2025-12-16 11:33:32,673 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 6.0s: 1846c @306c/s (372ch, ~462t @77t/s)
2025-12-16 11:33:34,673 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 8.0s: 2363c @294c/s (494ch, ~591t @74t/s)
2025-12-16 11:33:36,678 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 10.0s: 2887c @288c/s (618ch, ~722t @72t/s)
2025-12-16 11:33:38,682 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 12.0s: 3449c @286c/s (742ch, ~862t @72t/s)
2025-12-16 11:33:40,697 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 14.1s: 4080c @290c/s (861ch, ~1020t @73t/s)
2025-12-16 11:33:42,709 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 16.1s: 4650c @289c/s (979ch, ~1162t @72t/s)
2025-12-16 11:33:44,722 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 18.1s: 5243c @290c/s (1102ch, ~1311t @72t/s)
2025-12-16 11:33:46,730 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 20.1s: 5844c @291c/s (1225ch, ~1461t @73t/s)
2025-12-16 11:33:48,735 - src.llm.client - INFO - [lec:a2db3d] ğŸ“Š 22.1s: 6415c @290c/s (1331ch, ~1604t @73t/s)
2025-12-16 11:33:48,854 - src.llm.client - INFO - [lec:a2db3d] âœ“ Done 23.34s: 6424c (~1016w @275c/s)
2025-12-16 11:33:48,857 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 11:33:48,857 - src.generate.formats.lectures - INFO -     - Length: 6508 chars, 1027 words
2025-12-16 11:33:48,857 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:33:48,857 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 11:33:48,857 - src.generate.formats.lectures - INFO -     - Content: 14 examples, 1 terms defined
2025-12-16 11:33:48,857 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:33:48,862 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:33:48,863 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:33:48,863 - src.generate.formats.labs - INFO - Generating lab 5 for: Budding Techniques (T-Budding) (Session 5)
2025-12-16 11:33:48,863 - src.llm.client - INFO - [lab:15003c] ğŸš€ lab | m=gemma3:4b | p=3325c | t=150s
2025-12-16 11:33:48,863 - src.llm.client - INFO - [lab:15003c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:33:48,863 - src.llm.client - INFO - [lab:15003c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:33:48,865 - src.llm.client - INFO - [lab:15003c] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3758 bytes, prompt=3325 chars
2025-12-16 11:33:48,865 - src.llm.client - INFO - [lab:15003c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:33:50,047 - src.llm.request_handler - INFO - [lab:15003c] âœ“ Done 1.18s
2025-12-16 11:33:50,047 - src.llm.client - INFO - [lab:15003c] âœ… HTTP 200 in 1.18s
2025-12-16 11:33:50,047 - src.llm.client - INFO - [lab:15003c] ğŸ“¡ Stream active (200)
2025-12-16 11:33:50,047 - src.llm.client - INFO - [lab:15003c] Starting stream parsing, waiting for first chunk...
2025-12-16 11:33:52,052 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 2.0s: 572c @285c/s (124ch, ~143t @71t/s)
2025-12-16 11:33:54,066 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 4.0s: 1114c @277c/s (248ch, ~278t @69t/s)
2025-12-16 11:33:56,068 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 6.0s: 1493c @248c/s (366ch, ~373t @62t/s)
2025-12-16 11:33:58,071 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 8.0s: 1909c @238c/s (486ch, ~477t @59t/s)
2025-12-16 11:34:00,076 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 10.0s: 2458c @245c/s (611ch, ~614t @61t/s)
2025-12-16 11:34:02,082 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 12.0s: 2951c @245c/s (733ch, ~738t @61t/s)
2025-12-16 11:34:04,087 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 14.0s: 3471c @247c/s (855ch, ~868t @62t/s)
2025-12-16 11:34:06,094 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 16.0s: 4029c @251c/s (978ch, ~1007t @63t/s)
2025-12-16 11:34:08,101 - src.llm.client - INFO - [lab:15003c] ğŸ“Š 18.1s: 4493c @249c/s (1090ch, ~1123t @62t/s)
2025-12-16 11:34:09,555 - src.llm.client - INFO - [lab:15003c] âœ“ Done 20.69s: 4899c (~696w @237c/s)
2025-12-16 11:34:09,556 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:34:09,556 - src.generate.formats.labs - INFO -     - Length: 4995 chars, 710 words
2025-12-16 11:34:09,556 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 11:34:09,556 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-16 11:34:09,556 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 11:34:09,558 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:34:09,558 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:34:09,558 - src.generate.formats.study_notes - INFO - Generating study notes for: Budding Techniques (T-Budding) (Session 5)
2025-12-16 11:34:09,558 - src.llm.client - INFO - [stu:d063ff] ğŸš€ stu | m=gemma3:4b | p=4432c | t=120s
2025-12-16 11:34:09,558 - src.llm.client - INFO - [stu:d063ff] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:34:09,558 - src.llm.client - INFO - [stu:d063ff] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:34:09,560 - src.llm.client - INFO - [stu:d063ff] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8080 bytes, prompt=4432 chars
2025-12-16 11:34:09,560 - src.llm.client - INFO - [stu:d063ff] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:34:10,993 - src.llm.request_handler - INFO - [stu:d063ff] âœ“ Done 1.43s
2025-12-16 11:34:10,993 - src.llm.client - INFO - [stu:d063ff] âœ… HTTP 200 in 1.43s
2025-12-16 11:34:10,993 - src.llm.client - INFO - [stu:d063ff] ğŸ“¡ Stream active (200)
2025-12-16 11:34:10,993 - src.llm.client - INFO - [stu:d063ff] Starting stream parsing, waiting for first chunk...
2025-12-16 11:34:13,006 - src.llm.client - INFO - [stu:d063ff] ğŸ“Š 2.0s: 590c @293c/s (121ch, ~148t @73t/s)
2025-12-16 11:34:15,014 - src.llm.client - INFO - [stu:d063ff] ğŸ“Š 4.0s: 1209c @301c/s (242ch, ~302t @75t/s)
2025-12-16 11:34:17,029 - src.llm.client - INFO - [stu:d063ff] ğŸ“Š 6.0s: 1805c @299c/s (364ch, ~451t @75t/s)
2025-12-16 11:34:19,032 - src.llm.client - INFO - [stu:d063ff] ğŸ“Š 8.0s: 2406c @299c/s (486ch, ~602t @75t/s)
2025-12-16 11:34:21,041 - src.llm.client - INFO - [stu:d063ff] ğŸ“Š 10.0s: 3054c @304c/s (608ch, ~764t @76t/s)
2025-12-16 11:34:22,522 - src.llm.client - INFO - [stu:d063ff] âœ“ Done 12.96s: 3475c (~504w @268c/s)
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO - [NEEDS REVIEW] Study notes generated âš ï¸
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO -     - Length: 3540 chars, 514 words
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO -     - Key concepts: 11
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - WARNING - [WARNING] Too many key concepts (11, maximum 10, 1 excess - consolidate related concepts or remove less critical ones) âš ï¸
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:34:22,523 - src.generate.formats.study_notes - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 11:34:22,525 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:34:22,527 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:34:22,527 - src.generate.formats.diagrams - INFO - Generating diagram for: Scion Preparation (Budding Techniques (T-Budding))
2025-12-16 11:34:22,527 - src.generate.formats.diagrams - INFO - Generating diagram for: Insertion Technique (Budding Techniques (T-Budding))
2025-12-16 11:34:22,527 - src.llm.client - INFO - [dia:2c570b] ğŸš€ dia | m=gemma3:4b | p=5770c | t=120s
2025-12-16 11:34:22,527 - src.llm.client - INFO - [dia:fb7609] ğŸš€ dia | m=gemma3:4b | p=5774c | t=120s
2025-12-16 11:34:22,528 - src.llm.client - INFO - [dia:2c570b] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:34:22,528 - src.llm.client - INFO - [dia:fb7609] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:34:22,528 - src.llm.client - INFO - [dia:2c570b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:34:22,528 - src.llm.client - INFO - [dia:fb7609] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:34:22,534 - src.llm.client - INFO - [dia:2c570b] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11084 bytes, prompt=5770 chars
2025-12-16 11:34:22,534 - src.llm.client - INFO - [dia:fb7609] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11088 bytes, prompt=5774 chars
2025-12-16 11:34:22,534 - src.llm.client - INFO - [dia:2c570b] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:34:22,534 - src.llm.client - INFO - [dia:fb7609] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:34:24,518 - src.llm.request_handler - INFO - [dia:fb7609] âœ“ Done 1.98s
2025-12-16 11:34:24,518 - src.llm.client - INFO - [dia:fb7609] âœ… HTTP 200 in 1.98s
2025-12-16 11:34:24,518 - src.llm.client - INFO - [dia:fb7609] ğŸ“¡ Stream active (200)
2025-12-16 11:34:24,519 - src.llm.client - INFO - [dia:fb7609] Starting stream parsing, waiting for first chunk...
2025-12-16 11:34:26,525 - src.llm.client - INFO - [dia:fb7609] ğŸ“Š 2.0s: 455c @227c/s (122ch, ~114t @57t/s)
2025-12-16 11:34:28,538 - src.llm.client - INFO - [dia:fb7609] ğŸ“Š 4.0s: 824c @205c/s (236ch, ~206t @51t/s)
2025-12-16 11:34:29,212 - src.llm.client - INFO - [dia:fb7609] âœ“ Done 6.68s: 897c (~118w @134c/s)
2025-12-16 11:34:29,212 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Insertion Technique (Budding Techniques (T-Budding)):
2025-12-16 11:34:29,212 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:34:29,212 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:34:29,212 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:34:29,212 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:34:29,212 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:34:29,213 - src.generate.formats.diagrams - INFO -     - Length: 771 chars (cleaned: 771 chars)
2025-12-16 11:34:29,213 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:34:29,213 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 37 total (nodes: 14, connections: 23) âš ï¸
2025-12-16 11:34:29,213 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:34:29,213 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 11:34:29,213 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:34:29,213 - src.generate.formats.diagrams - INFO - Generated diagram: 771 characters
2025-12-16 11:34:30,906 - src.llm.request_handler - INFO - [dia:2c570b] âœ“ Done 8.37s
2025-12-16 11:34:30,906 - src.llm.client - INFO - [dia:2c570b] âœ… HTTP 200 in 8.37s
2025-12-16 11:34:30,906 - src.llm.client - INFO - [dia:2c570b] ğŸ“¡ Stream active (200)
2025-12-16 11:34:30,906 - src.llm.client - INFO - [dia:2c570b] Starting stream parsing, waiting for first chunk...
2025-12-16 11:34:32,910 - src.llm.client - INFO - [dia:2c570b] ğŸ“Š 2.0s: 383c @191c/s (109ch, ~96t @48t/s)
2025-12-16 11:34:34,920 - src.llm.client - INFO - [dia:2c570b] ğŸ“Š 4.0s: 870c @217c/s (230ch, ~218t @54t/s)
2025-12-16 11:34:36,211 - src.llm.client - INFO - [dia:2c570b] âœ“ Done 13.68s: 1163c (~163w @85c/s)
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Scion Preparation (Budding Techniques (T-Budding)):
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - INFO -     - Length: 1148 chars (cleaned: 1148 chars)
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - INFO - [OK] Elements: 49 total (nodes: 21, connections: 28) âœ“
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - INFO -   Cleanup summary: 1 issues fixed (code fences, style commands, etc.)
2025-12-16 11:34:36,212 - src.generate.formats.diagrams - INFO - Generated diagram: 1148 characters
2025-12-16 11:34:36,213 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:34:36,213 - src.generate.formats.questions - INFO - Generating 10 questions for: Budding Techniques (T-Budding) (Session 5)
2025-12-16 11:34:36,213 - src.llm.client - INFO - [qst:b00f15] ğŸš€ qst | m=gemma3:4b | p=7327c | t=150s
2025-12-16 11:34:36,213 - src.llm.client - INFO - [qst:b00f15] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:34:36,213 - src.llm.client - INFO - [qst:b00f15] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:34:36,215 - src.llm.client - INFO - [qst:b00f15] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11062 bytes, prompt=7327 chars
2025-12-16 11:34:36,215 - src.llm.client - INFO - [qst:b00f15] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:34:38,517 - src.llm.request_handler - INFO - [qst:b00f15] âœ“ Done 2.30s
2025-12-16 11:34:38,519 - src.llm.client - INFO - [qst:b00f15] âœ… HTTP 200 in 2.30s
2025-12-16 11:34:38,519 - src.llm.client - INFO - [qst:b00f15] ğŸ“¡ Stream active (200)
2025-12-16 11:34:38,519 - src.llm.client - INFO - [qst:b00f15] Starting stream parsing, waiting for first chunk...
2025-12-16 11:34:40,527 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 2.0s: 571c @284c/s (120ch, ~143t @71t/s)
2025-12-16 11:34:42,539 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 4.0s: 1085c @270c/s (243ch, ~271t @67t/s)
2025-12-16 11:34:44,553 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 6.0s: 1598c @265c/s (366ch, ~400t @66t/s)
2025-12-16 11:34:46,558 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 8.0s: 2125c @264c/s (483ch, ~531t @66t/s)
2025-12-16 11:34:48,564 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 10.0s: 2672c @266c/s (597ch, ~668t @66t/s)
2025-12-16 11:34:50,567 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 12.0s: 3272c @272c/s (710ch, ~818t @68t/s)
2025-12-16 11:34:52,574 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 14.1s: 3860c @275c/s (829ch, ~965t @69t/s)
2025-12-16 11:34:54,585 - src.llm.client - INFO - [qst:b00f15] ğŸ“Š 16.1s: 4452c @277c/s (947ch, ~1113t @69t/s)
2025-12-16 11:34:55,785 - src.llm.client - INFO - [qst:b00f15] âœ“ Done 19.57s: 4801c (~697w @245c/s)
2025-12-16 11:34:55,786 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 0, 'total_fixes': 1}
2025-12-16 11:34:55,786 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-16 11:34:55,787 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 11:34:55,787 - src.generate.formats.questions - WARNING -     Context: Module 5 Session 5
2025-12-16 11:34:55,787 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 11:34:55,787 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 11:34:55,787 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 11:34:55,787 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Budding Techniques (T-Budding) (Session 5)
2025-12-16 11:34:55,787 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:34:55,789 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:34:55,791 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 5 completed
2025-12-16 11:34:55,791 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:34:55,791 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:34:55,791 - src.generate.orchestration.pipeline - INFO - Module 6: Bridge Grafting â€“ Intermediate Technique (1 sessions)
2025-12-16 11:34:55,791 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:34:55,791 - src.generate.orchestration.pipeline - INFO - 
[6/10] Session 6: Bridge Grafting â€“ Technique & Best Practices
2025-12-16 11:34:55,792 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:34:55,792 - src.generate.formats.lectures - INFO - Generating lecture for: Bridge Grafting â€“ Intermediate Technique (Session 6/10)
2025-12-16 11:34:55,792 - src.llm.client - INFO - [lec:592eb0] ğŸš€ lec | m=gemma3:4b | p=3162c | t=180s
2025-12-16 11:34:55,792 - src.llm.client - INFO - [lec:592eb0] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:34:55,792 - src.llm.client - INFO - [lec:592eb0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:34:55,799 - src.llm.client - INFO - [lec:592eb0] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6813 bytes, prompt=3162 chars
2025-12-16 11:34:55,799 - src.llm.client - INFO - [lec:592eb0] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:34:56,930 - src.llm.request_handler - INFO - [lec:592eb0] âœ“ Done 1.13s
2025-12-16 11:34:56,930 - src.llm.client - INFO - [lec:592eb0] âœ… HTTP 200 in 1.13s
2025-12-16 11:34:56,930 - src.llm.client - INFO - [lec:592eb0] ğŸ“¡ Stream active (200)
2025-12-16 11:34:56,930 - src.llm.client - INFO - [lec:592eb0] Starting stream parsing, waiting for first chunk...
2025-12-16 11:34:58,943 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 2.0s: 722c @359c/s (119ch, ~180t @90t/s)
2025-12-16 11:35:00,956 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 4.0s: 1367c @340c/s (242ch, ~342t @85t/s)
2025-12-16 11:35:02,969 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 6.0s: 1995c @330c/s (367ch, ~499t @83t/s)
2025-12-16 11:35:04,984 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 8.1s: 2582c @321c/s (487ch, ~646t @80t/s)
2025-12-16 11:35:06,996 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 10.1s: 3194c @317c/s (609ch, ~798t @79t/s)
2025-12-16 11:35:09,005 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 12.1s: 3753c @311c/s (731ch, ~938t @78t/s)
2025-12-16 11:35:11,006 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 14.1s: 4294c @305c/s (846ch, ~1074t @76t/s)
2025-12-16 11:35:13,010 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 16.1s: 4836c @301c/s (962ch, ~1209t @75t/s)
2025-12-16 11:35:15,022 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 18.1s: 5462c @302c/s (1085ch, ~1366t @75t/s)
2025-12-16 11:35:17,034 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 20.1s: 6168c @307c/s (1210ch, ~1542t @77t/s)
2025-12-16 11:35:19,045 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 22.1s: 6819c @308c/s (1335ch, ~1705t @77t/s)
2025-12-16 11:35:21,055 - src.llm.client - INFO - [lec:592eb0] ğŸ“Š 24.1s: 7504c @311c/s (1451ch, ~1876t @78t/s)
2025-12-16 11:35:21,645 - src.llm.client - INFO - [lec:592eb0] âœ“ Done 25.85s: 7644c (~1129w @296c/s)
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO -     - Length: 7744 chars, 1143 words
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO -     - Content: 10 examples, 2 terms defined
2025-12-16 11:35:21,646 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:35:21,646 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 11:35:21,649 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:35:21,651 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:35:21,651 - src.generate.formats.labs - INFO - Generating lab 6 for: Bridge Grafting â€“ Intermediate Technique (Session 6)
2025-12-16 11:35:21,652 - src.llm.client - INFO - [lab:bbc649] ğŸš€ lab | m=gemma3:4b | p=3339c | t=150s
2025-12-16 11:35:21,652 - src.llm.client - INFO - [lab:bbc649] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:35:21,652 - src.llm.client - INFO - [lab:bbc649] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:35:21,659 - src.llm.client - INFO - [lab:bbc649] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3773 bytes, prompt=3339 chars
2025-12-16 11:35:21,659 - src.llm.client - INFO - [lab:bbc649] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:35:22,630 - src.llm.request_handler - INFO - [lab:bbc649] âœ“ Done 0.97s
2025-12-16 11:35:22,630 - src.llm.client - INFO - [lab:bbc649] âœ… HTTP 200 in 0.97s
2025-12-16 11:35:22,631 - src.llm.client - INFO - [lab:bbc649] ğŸ“¡ Stream active (200)
2025-12-16 11:35:22,631 - src.llm.client - INFO - [lab:bbc649] Starting stream parsing, waiting for first chunk...
2025-12-16 11:35:24,647 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 2.0s: 641c @318c/s (125ch, ~160t @79t/s)
2025-12-16 11:35:26,656 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 4.0s: 1284c @319c/s (253ch, ~321t @80t/s)
2025-12-16 11:35:28,668 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 6.0s: 1715c @284c/s (382ch, ~429t @71t/s)
2025-12-16 11:35:30,678 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 8.0s: 2214c @275c/s (505ch, ~554t @69t/s)
2025-12-16 11:35:32,679 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 10.0s: 2729c @272c/s (628ch, ~682t @68t/s)
2025-12-16 11:35:34,683 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 12.1s: 3244c @269c/s (747ch, ~811t @67t/s)
2025-12-16 11:35:36,691 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 14.1s: 3775c @268c/s (866ch, ~944t @67t/s)
2025-12-16 11:35:38,699 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 16.1s: 4538c @282c/s (985ch, ~1134t @71t/s)
2025-12-16 11:35:40,706 - src.llm.client - INFO - [lab:bbc649] ğŸ“Š 18.1s: 5113c @283c/s (1104ch, ~1278t @71t/s)
2025-12-16 11:35:41,239 - src.llm.client - INFO - [lab:bbc649] âœ“ Done 19.59s: 5276c (~733w @269c/s)
2025-12-16 11:35:41,240 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:35:41,240 - src.generate.formats.labs - INFO -     - Length: 5379 chars, 749 words
2025-12-16 11:35:41,240 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-16 11:35:41,240 - src.generate.formats.labs - INFO -     - Safety: 8 warnings
2025-12-16 11:35:41,240 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 11:35:41,242 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:35:41,243 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:35:41,243 - src.generate.formats.study_notes - INFO - Generating study notes for: Bridge Grafting â€“ Intermediate Technique (Session 6)
2025-12-16 11:35:41,243 - src.llm.client - INFO - [stu:2e8fd6] ğŸš€ stu | m=gemma3:4b | p=4458c | t=120s
2025-12-16 11:35:41,243 - src.llm.client - INFO - [stu:2e8fd6] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:35:41,243 - src.llm.client - INFO - [stu:2e8fd6] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:35:41,246 - src.llm.client - INFO - [stu:2e8fd6] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8107 bytes, prompt=4458 chars
2025-12-16 11:35:41,246 - src.llm.client - INFO - [stu:2e8fd6] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:35:42,616 - src.llm.request_handler - INFO - [stu:2e8fd6] âœ“ Done 1.37s
2025-12-16 11:35:42,616 - src.llm.client - INFO - [stu:2e8fd6] âœ… HTTP 200 in 1.37s
2025-12-16 11:35:42,616 - src.llm.client - INFO - [stu:2e8fd6] ğŸ“¡ Stream active (200)
2025-12-16 11:35:42,616 - src.llm.client - INFO - [stu:2e8fd6] Starting stream parsing, waiting for first chunk...
2025-12-16 11:35:44,631 - src.llm.client - INFO - [stu:2e8fd6] ğŸ“Š 2.0s: 696c @345c/s (120ch, ~174t @86t/s)
2025-12-16 11:35:46,639 - src.llm.client - INFO - [stu:2e8fd6] ğŸ“Š 4.0s: 1376c @342c/s (247ch, ~344t @86t/s)
2025-12-16 11:35:48,641 - src.llm.client - INFO - [stu:2e8fd6] ğŸ“Š 6.0s: 1998c @332c/s (363ch, ~500t @83t/s)
2025-12-16 11:35:50,653 - src.llm.client - INFO - [stu:2e8fd6] ğŸ“Š 8.0s: 2641c @329c/s (479ch, ~660t @82t/s)
2025-12-16 11:35:52,064 - src.llm.client - INFO - [stu:2e8fd6] âœ“ Done 10.82s: 2993c (~422w @277c/s)
2025-12-16 11:35:52,065 - src.generate.formats.study_notes - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 11:35:52,065 - src.generate.formats.study_notes - WARNING -     [CRITICAL] Issue 1: Only 1 key concepts highlighted (require 3-10, need 2 more - format concepts as **Concept Name:** in bullet points)
2025-12-16 11:35:52,065 - src.generate.formats.study_notes - WARNING -   Retry attempt 1/1 for study notes: Bridge Grafting â€“ Intermediate Technique (Session 6)
2025-12-16 11:35:52,065 - src.generate.formats.study_notes - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:35:52,066 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:35:52,066 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:35:52,067 - src.generate.formats.diagrams - INFO - Generating diagram for: Preparing the Rootstock (Bridge Grafting â€“ Intermediate Technique)
2025-12-16 11:35:52,067 - src.generate.formats.diagrams - INFO - Generating diagram for: Binding String (Bridge Grafting â€“ Intermediate Technique)
2025-12-16 11:35:52,067 - src.llm.client - INFO - [dia:329fa5] ğŸš€ dia | m=gemma3:4b | p=5797c | t=120s
2025-12-16 11:35:52,067 - src.llm.client - INFO - [dia:3502fa] ğŸš€ dia | m=gemma3:4b | p=5779c | t=120s
2025-12-16 11:35:52,067 - src.llm.client - INFO - [dia:329fa5] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:35:52,067 - src.llm.client - INFO - [dia:3502fa] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:35:52,067 - src.llm.client - INFO - [dia:329fa5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:35:52,067 - src.llm.client - INFO - [dia:3502fa] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:35:52,069 - src.llm.client - INFO - [dia:329fa5] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11116 bytes, prompt=5797 chars
2025-12-16 11:35:52,069 - src.llm.client - INFO - [dia:3502fa] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11098 bytes, prompt=5779 chars
2025-12-16 11:35:52,069 - src.llm.client - INFO - [dia:329fa5] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:35:52,070 - src.llm.client - INFO - [dia:3502fa] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:35:53,973 - src.llm.request_handler - INFO - [dia:329fa5] âœ“ Done 1.90s
2025-12-16 11:35:53,973 - src.llm.client - INFO - [dia:329fa5] âœ… HTTP 200 in 1.90s
2025-12-16 11:35:53,973 - src.llm.client - INFO - [dia:329fa5] ğŸ“¡ Stream active (200)
2025-12-16 11:35:53,973 - src.llm.client - INFO - [dia:329fa5] Starting stream parsing, waiting for first chunk...
2025-12-16 11:35:55,987 - src.llm.client - INFO - [dia:329fa5] ğŸ“Š 2.0s: 452c @224c/s (125ch, ~113t @56t/s)
2025-12-16 11:35:57,995 - src.llm.client - INFO - [dia:329fa5] ğŸ“Š 4.0s: 870c @216c/s (250ch, ~218t @54t/s)
2025-12-16 11:36:00,005 - src.llm.client - INFO - [dia:329fa5] ğŸ“Š 6.0s: 1142c @189c/s (373ch, ~286t @47t/s)
2025-12-16 11:36:02,005 - src.llm.client - INFO - [dia:329fa5] ğŸ“Š 8.0s: 1429c @178c/s (496ch, ~357t @44t/s)
2025-12-16 11:36:02,977 - src.llm.client - INFO - [dia:329fa5] âœ“ Done 10.91s: 1541c (~162w @141c/s)
2025-12-16 11:36:02,978 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Preparing the Rootstock (Bridge Grafting â€“ Intermediate Technique):
2025-12-16 11:36:02,978 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:36:02,978 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:36:02,978 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:36:02,978 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:36:02,978 - src.generate.formats.diagrams - INFO -     - Length: 824 chars (cleaned: 824 chars)
2025-12-16 11:36:02,979 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:36:02,979 - src.generate.formats.diagrams - INFO - [OK] Elements: 44 total (nodes: 17, connections: 27) âœ“
2025-12-16 11:36:02,979 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:36:02,979 - src.generate.formats.diagrams - INFO - Generated diagram: 824 characters
2025-12-16 11:36:04,713 - src.llm.request_handler - INFO - [dia:3502fa] âœ“ Done 12.64s
2025-12-16 11:36:04,713 - src.llm.client - INFO - [dia:3502fa] âœ… HTTP 200 in 12.64s
2025-12-16 11:36:04,713 - src.llm.client - INFO - [dia:3502fa] ğŸ“¡ Stream active (200)
2025-12-16 11:36:04,713 - src.llm.client - INFO - [dia:3502fa] Starting stream parsing, waiting for first chunk...
2025-12-16 11:36:06,727 - src.llm.client - INFO - [dia:3502fa] ğŸ“Š 2.0s: 394c @196c/s (121ch, ~98t @49t/s)
2025-12-16 11:36:08,740 - src.llm.client - INFO - [dia:3502fa] ğŸ“Š 4.0s: 804c @200c/s (245ch, ~201t @50t/s)
2025-12-16 11:36:10,744 - src.llm.client - INFO - [dia:3502fa] ğŸ“Š 6.0s: 1068c @177c/s (363ch, ~267t @44t/s)
2025-12-16 11:36:11,412 - src.llm.client - INFO - [dia:3502fa] âœ“ Done 19.34s: 1145c (~139w @59c/s)
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Binding String (Bridge Grafting â€“ Intermediate Technique):
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO -     - Length: 749 chars (cleaned: 749 chars)
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO - [OK] Elements: 42 total (nodes: 15, connections: 27) âœ“
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:36:11,413 - src.generate.formats.diagrams - INFO - Generated diagram: 749 characters
2025-12-16 11:36:11,415 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:36:11,415 - src.generate.formats.questions - INFO - Generating 10 questions for: Bridge Grafting â€“ Intermediate Technique (Session 6)
2025-12-16 11:36:11,416 - src.llm.client - INFO - [qst:0eee30] ğŸš€ qst | m=gemma3:4b | p=7344c | t=150s
2025-12-16 11:36:11,416 - src.llm.client - INFO - [qst:0eee30] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:36:11,416 - src.llm.client - INFO - [qst:0eee30] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:36:11,417 - src.llm.client - INFO - [qst:0eee30] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11049 bytes, prompt=7344 chars
2025-12-16 11:36:11,417 - src.llm.client - INFO - [qst:0eee30] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:36:13,591 - src.llm.request_handler - INFO - [qst:0eee30] âœ“ Done 2.17s
2025-12-16 11:36:13,592 - src.llm.client - INFO - [qst:0eee30] âœ… HTTP 200 in 2.17s
2025-12-16 11:36:13,592 - src.llm.client - INFO - [qst:0eee30] ğŸ“¡ Stream active (200)
2025-12-16 11:36:13,593 - src.llm.client - INFO - [qst:0eee30] Starting stream parsing, waiting for first chunk...
2025-12-16 11:36:15,603 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 2.0s: 555c @276c/s (116ch, ~139t @69t/s)
2025-12-16 11:36:17,604 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 4.0s: 1145c @285c/s (235ch, ~286t @71t/s)
2025-12-16 11:36:19,605 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 6.0s: 1711c @285c/s (351ch, ~428t @71t/s)
2025-12-16 11:36:21,614 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 8.0s: 2322c @289c/s (472ch, ~580t @72t/s)
2025-12-16 11:36:23,630 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 10.0s: 2896c @289c/s (591ch, ~724t @72t/s)
2025-12-16 11:36:25,632 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 12.0s: 3486c @290c/s (713ch, ~872t @72t/s)
2025-12-16 11:36:27,646 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 14.1s: 4113c @293c/s (826ch, ~1028t @73t/s)
2025-12-16 11:36:29,649 - src.llm.client - INFO - [qst:0eee30] ğŸ“Š 16.1s: 4816c @300c/s (942ch, ~1204t @75t/s)
2025-12-16 11:36:30,019 - src.llm.client - INFO - [qst:0eee30] âœ“ Done 18.60s: 4915c (~696w @264c/s)
2025-12-16 11:36:30,020 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 1, 'total_fixes': 3}
2025-12-16 11:36:30,020 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -     Context: Module 6 Session 6
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -     Context: Module 6 Session 6
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Bridge Grafting â€“ Intermediate Technique (Session 6)
2025-12-16 11:36:30,021 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:36:30,023 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:36:30,027 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 6 completed
2025-12-16 11:36:30,027 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:36:30,027 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:36:30,027 - src.generate.orchestration.pipeline - INFO - Module 7: Inarching â€“ Expanding Grafting Options (1 sessions)
2025-12-16 11:36:30,027 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:36:30,027 - src.generate.orchestration.pipeline - INFO - 
[7/10] Session 7: Inarching â€“ Procedure & Considerations
2025-12-16 11:36:30,027 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:36:30,027 - src.generate.formats.lectures - INFO - Generating lecture for: Inarching â€“ Expanding Grafting Options (Session 7/10)
2025-12-16 11:36:30,028 - src.llm.client - INFO - [lec:f2ba64] ğŸš€ lec | m=gemma3:4b | p=3134c | t=180s
2025-12-16 11:36:30,028 - src.llm.client - INFO - [lec:f2ba64] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:36:30,028 - src.llm.client - INFO - [lec:f2ba64] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:36:30,030 - src.llm.client - INFO - [lec:f2ba64] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6785 bytes, prompt=3134 chars
2025-12-16 11:36:30,030 - src.llm.client - INFO - [lec:f2ba64] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:36:31,197 - src.llm.request_handler - INFO - [lec:f2ba64] âœ“ Done 1.17s
2025-12-16 11:36:31,198 - src.llm.client - INFO - [lec:f2ba64] âœ… HTTP 200 in 1.17s
2025-12-16 11:36:31,198 - src.llm.client - INFO - [lec:f2ba64] ğŸ“¡ Stream active (200)
2025-12-16 11:36:31,198 - src.llm.client - INFO - [lec:f2ba64] Starting stream parsing, waiting for first chunk...
2025-12-16 11:36:33,212 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 2.0s: 672c @334c/s (122ch, ~168t @83t/s)
2025-12-16 11:36:35,223 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 4.0s: 1332c @331c/s (243ch, ~333t @83t/s)
2025-12-16 11:36:37,227 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 6.0s: 1958c @325c/s (361ch, ~490t @81t/s)
2025-12-16 11:36:39,240 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 8.0s: 2527c @314c/s (474ch, ~632t @79t/s)
2025-12-16 11:36:41,250 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 10.1s: 3136c @312c/s (596ch, ~784t @78t/s)
2025-12-16 11:36:43,257 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 12.1s: 3740c @310c/s (716ch, ~935t @78t/s)
2025-12-16 11:36:45,261 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 14.1s: 4334c @308c/s (833ch, ~1084t @77t/s)
2025-12-16 11:36:47,269 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 16.1s: 4893c @304c/s (954ch, ~1223t @76t/s)
2025-12-16 11:36:49,272 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 18.1s: 5464c @302c/s (1061ch, ~1366t @76t/s)
2025-12-16 11:36:51,274 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 20.1s: 5997c @299c/s (1156ch, ~1499t @75t/s)
2025-12-16 11:36:53,279 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 22.1s: 6596c @299c/s (1273ch, ~1649t @75t/s)
2025-12-16 11:36:55,283 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 24.1s: 7226c @300c/s (1388ch, ~1806t @75t/s)
2025-12-16 11:36:57,295 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 26.1s: 7879c @302c/s (1503ch, ~1970t @75t/s)
2025-12-16 11:36:59,305 - src.llm.client - INFO - [lec:f2ba64] ğŸ“Š 28.1s: 8566c @305c/s (1622ch, ~2142t @76t/s)
2025-12-16 11:36:59,701 - src.llm.client - INFO - [lec:f2ba64] âœ“ Done 29.67s: 8666c (~1269w @292c/s)
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO -     - Length: 8758 chars, 1282 words
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO -     - Content: 11 examples, 0 terms defined
2025-12-16 11:36:59,703 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:36:59,703 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 11:36:59,706 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:36:59,707 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:36:59,707 - src.generate.formats.labs - INFO - Generating lab 7 for: Inarching â€“ Expanding Grafting Options (Session 7)
2025-12-16 11:36:59,708 - src.llm.client - INFO - [lab:7cc8be] ğŸš€ lab | m=gemma3:4b | p=3336c | t=150s
2025-12-16 11:36:59,708 - src.llm.client - INFO - [lab:7cc8be] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:36:59,708 - src.llm.client - INFO - [lab:7cc8be] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:36:59,714 - src.llm.client - INFO - [lab:7cc8be] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3777 bytes, prompt=3336 chars
2025-12-16 11:36:59,714 - src.llm.client - INFO - [lab:7cc8be] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:37:00,723 - src.llm.request_handler - INFO - [lab:7cc8be] âœ“ Done 1.01s
2025-12-16 11:37:00,723 - src.llm.client - INFO - [lab:7cc8be] âœ… HTTP 200 in 1.01s
2025-12-16 11:37:00,723 - src.llm.client - INFO - [lab:7cc8be] ğŸ“¡ Stream active (200)
2025-12-16 11:37:00,724 - src.llm.client - INFO - [lab:7cc8be] Starting stream parsing, waiting for first chunk...
2025-12-16 11:37:02,735 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 2.0s: 568c @282c/s (109ch, ~142t @71t/s)
2025-12-16 11:37:04,736 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 4.0s: 1110c @277c/s (210ch, ~278t @69t/s)
2025-12-16 11:37:06,743 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 6.0s: 1496c @249c/s (330ch, ~374t @62t/s)
2025-12-16 11:37:08,748 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 8.0s: 1843c @230c/s (447ch, ~461t @57t/s)
2025-12-16 11:37:10,747 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 10.0s: 2304c @230c/s (569ch, ~576t @57t/s)
2025-12-16 11:37:12,756 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 12.0s: 2881c @239c/s (693ch, ~720t @60t/s)
2025-12-16 11:37:14,758 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 14.0s: 3349c @239c/s (812ch, ~837t @60t/s)
2025-12-16 11:37:16,759 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 16.0s: 3900c @243c/s (933ch, ~975t @61t/s)
2025-12-16 11:37:18,762 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 18.0s: 4505c @250c/s (1054ch, ~1126t @62t/s)
2025-12-16 11:37:20,765 - src.llm.client - INFO - [lab:7cc8be] ğŸ“Š 20.0s: 5050c @252c/s (1175ch, ~1262t @63t/s)
2025-12-16 11:37:22,518 - src.llm.client - INFO - [lab:7cc8be] âœ“ Done 22.81s: 5525c (~764w @242c/s)
2025-12-16 11:37:22,518 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:37:22,518 - src.generate.formats.labs - INFO -     - Length: 5631 chars, 780 words
2025-12-16 11:37:22,518 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-16 11:37:22,518 - src.generate.formats.labs - INFO -     - Safety: 4 warnings
2025-12-16 11:37:22,518 - src.generate.formats.labs - INFO -     - Data tables: 8
2025-12-16 11:37:22,521 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:37:22,523 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:37:22,524 - src.generate.formats.study_notes - INFO - Generating study notes for: Inarching â€“ Expanding Grafting Options (Session 7)
2025-12-16 11:37:22,526 - src.llm.client - INFO - [stu:972d71] ğŸš€ stu | m=gemma3:4b | p=4449c | t=120s
2025-12-16 11:37:22,527 - src.llm.client - INFO - [stu:972d71] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:37:22,527 - src.llm.client - INFO - [stu:972d71] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:37:22,529 - src.llm.client - INFO - [stu:972d71] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8105 bytes, prompt=4449 chars
2025-12-16 11:37:22,529 - src.llm.client - INFO - [stu:972d71] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:37:23,953 - src.llm.request_handler - INFO - [stu:972d71] âœ“ Done 1.42s
2025-12-16 11:37:23,957 - src.llm.client - INFO - [stu:972d71] âœ… HTTP 200 in 1.43s
2025-12-16 11:37:23,957 - src.llm.client - INFO - [stu:972d71] ğŸ“¡ Stream active (200)
2025-12-16 11:37:23,958 - src.llm.client - INFO - [stu:972d71] Starting stream parsing, waiting for first chunk...
2025-12-16 11:37:25,965 - src.llm.client - INFO - [stu:972d71] ğŸ“Š 2.0s: 586c @292c/s (114ch, ~146t @73t/s)
2025-12-16 11:37:27,970 - src.llm.client - INFO - [stu:972d71] ğŸ“Š 4.0s: 1246c @310c/s (231ch, ~312t @78t/s)
2025-12-16 11:37:29,979 - src.llm.client - INFO - [stu:972d71] ğŸ“Š 6.0s: 1847c @307c/s (343ch, ~462t @77t/s)
2025-12-16 11:37:31,986 - src.llm.client - INFO - [stu:972d71] ğŸ“Š 8.0s: 2404c @299c/s (454ch, ~601t @75t/s)
2025-12-16 11:37:33,986 - src.llm.client - INFO - [stu:972d71] ğŸ“Š 10.0s: 3008c @300c/s (575ch, ~752t @75t/s)
2025-12-16 11:37:35,991 - src.llm.client - INFO - [stu:972d71] ğŸ“Š 12.0s: 3606c @300c/s (690ch, ~902t @75t/s)
2025-12-16 11:37:37,619 - src.llm.client - INFO - [stu:972d71] âœ“ Done 15.09s: 4101c (~590w @272c/s)
2025-12-16 11:37:37,620 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 11:37:37,620 - src.generate.formats.study_notes - INFO -     - Length: 4174 chars, 602 words
2025-12-16 11:37:37,620 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:37:37,620 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-16 11:37:37,620 - src.generate.formats.study_notes - INFO -     - Structure: 5 sections, 4 bullets
2025-12-16 11:37:37,620 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:37:37,622 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:37:37,623 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:37:37,623 - src.generate.formats.diagrams - INFO - Generating diagram for: Rootstock Selection (Inarching â€“ Expanding Grafting Options)
2025-12-16 11:37:37,623 - src.generate.formats.diagrams - INFO - Generating diagram for: Timing of Grafting (Inarching â€“ Expanding Grafting Options)
2025-12-16 11:37:37,623 - src.llm.client - INFO - [dia:81dc9e] ğŸš€ dia | m=gemma3:4b | p=5781c | t=120s
2025-12-16 11:37:37,624 - src.llm.client - INFO - [dia:1f72b4] ğŸš€ dia | m=gemma3:4b | p=5779c | t=120s
2025-12-16 11:37:37,624 - src.llm.client - INFO - [dia:81dc9e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:37:37,624 - src.llm.client - INFO - [dia:1f72b4] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:37:37,624 - src.llm.client - INFO - [dia:81dc9e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:37:37,624 - src.llm.client - INFO - [dia:1f72b4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:37:37,626 - src.llm.client - INFO - [dia:1f72b4] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11098 bytes, prompt=5779 chars
2025-12-16 11:37:37,626 - src.llm.client - INFO - [dia:81dc9e] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11100 bytes, prompt=5781 chars
2025-12-16 11:37:37,626 - src.llm.client - INFO - [dia:1f72b4] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:37:37,626 - src.llm.client - INFO - [dia:81dc9e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:37:39,714 - src.llm.request_handler - INFO - [dia:1f72b4] âœ“ Done 2.09s
2025-12-16 11:37:39,715 - src.llm.client - INFO - [dia:1f72b4] âœ… HTTP 200 in 2.09s
2025-12-16 11:37:39,715 - src.llm.client - INFO - [dia:1f72b4] ğŸ“¡ Stream active (200)
2025-12-16 11:37:39,715 - src.llm.client - INFO - [dia:1f72b4] Starting stream parsing, waiting for first chunk...
2025-12-16 11:37:41,719 - src.llm.client - INFO - [dia:1f72b4] ğŸ“Š 2.0s: 376c @188c/s (96ch, ~94t @47t/s)
2025-12-16 11:37:43,724 - src.llm.client - INFO - [dia:1f72b4] ğŸ“Š 4.0s: 786c @196c/s (198ch, ~196t @49t/s)
2025-12-16 11:37:45,728 - src.llm.client - INFO - [dia:1f72b4] ğŸ“Š 6.0s: 1098c @183c/s (308ch, ~274t @46t/s)
2025-12-16 11:37:45,877 - src.llm.client - INFO - [dia:1f72b4] âœ“ Done 8.25s: 1108c (~149w @134c/s)
2025-12-16 11:37:45,879 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Timing of Grafting (Inarching â€“ Expanding Grafting Options):
2025-12-16 11:37:45,880 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:37:45,880 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:37:45,880 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:37:45,880 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 4 long nodes) âš ï¸
2025-12-16 11:37:45,880 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:37:45,881 - src.generate.formats.diagrams - INFO -     - Length: 944 chars (cleaned: 944 chars)
2025-12-16 11:37:45,881 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:37:45,881 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 41 total (nodes: 14, connections: 27) âš ï¸
2025-12-16 11:37:45,881 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:37:45,881 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 4 long nodes) âš ï¸
2025-12-16 11:37:45,881 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:37:45,881 - src.generate.formats.diagrams - INFO - Generated diagram: 944 characters
2025-12-16 11:37:47,618 - src.llm.request_handler - INFO - [dia:81dc9e] âœ“ Done 9.99s
2025-12-16 11:37:47,618 - src.llm.client - INFO - [dia:81dc9e] âœ… HTTP 200 in 9.99s
2025-12-16 11:37:47,618 - src.llm.client - INFO - [dia:81dc9e] ğŸ“¡ Stream active (200)
2025-12-16 11:37:47,618 - src.llm.client - INFO - [dia:81dc9e] Starting stream parsing, waiting for first chunk...
2025-12-16 11:37:49,622 - src.llm.client - INFO - [dia:81dc9e] ğŸ“Š 2.0s: 322c @161c/s (93ch, ~80t @40t/s)
2025-12-16 11:37:51,631 - src.llm.client - INFO - [dia:81dc9e] ğŸ“Š 4.0s: 665c @166c/s (195ch, ~166t @41t/s)
2025-12-16 11:37:53,632 - src.llm.client - INFO - [dia:81dc9e] ğŸ“Š 6.0s: 1020c @170c/s (309ch, ~255t @42t/s)
2025-12-16 11:37:53,902 - src.llm.client - INFO - [dia:81dc9e] âœ“ Done 16.28s: 1044c (~144w @64c/s)
2025-12-16 11:37:53,904 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Rootstock Selection (Inarching â€“ Expanding Grafting Options):
2025-12-16 11:37:53,904 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:37:53,904 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:37:53,905 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:37:53,905 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:37:53,905 - src.generate.formats.diagrams - INFO -     - Length: 921 chars (cleaned: 921 chars)
2025-12-16 11:37:53,905 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:37:53,905 - src.generate.formats.diagrams - INFO - [OK] Elements: 50 total (nodes: 21, connections: 29) âœ“
2025-12-16 11:37:53,905 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:37:53,905 - src.generate.formats.diagrams - INFO - Generated diagram: 921 characters
2025-12-16 11:37:53,906 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:37:53,906 - src.generate.formats.questions - INFO - Generating 10 questions for: Inarching â€“ Expanding Grafting Options (Session 7)
2025-12-16 11:37:53,906 - src.llm.client - INFO - [qst:9ba05a] ğŸš€ qst | m=gemma3:4b | p=7336c | t=150s
2025-12-16 11:37:53,906 - src.llm.client - INFO - [qst:9ba05a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:37:53,906 - src.llm.client - INFO - [qst:9ba05a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:37:53,909 - src.llm.client - INFO - [qst:9ba05a] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11008 bytes, prompt=7336 chars
2025-12-16 11:37:53,909 - src.llm.client - INFO - [qst:9ba05a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:37:56,187 - src.llm.request_handler - INFO - [qst:9ba05a] âœ“ Done 2.28s
2025-12-16 11:37:56,191 - src.llm.client - INFO - [qst:9ba05a] âœ… HTTP 200 in 2.28s
2025-12-16 11:37:56,191 - src.llm.client - INFO - [qst:9ba05a] ğŸ“¡ Stream active (200)
2025-12-16 11:37:56,191 - src.llm.client - INFO - [qst:9ba05a] Starting stream parsing, waiting for first chunk...
2025-12-16 11:37:58,198 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 2.0s: 546c @272c/s (114ch, ~136t @68t/s)
2025-12-16 11:38:00,201 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 4.0s: 1141c @285c/s (235ch, ~285t @71t/s)
2025-12-16 11:38:02,210 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 6.0s: 1699c @282c/s (354ch, ~425t @71t/s)
2025-12-16 11:38:04,221 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 8.0s: 2295c @286c/s (477ch, ~574t @71t/s)
2025-12-16 11:38:06,233 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 10.0s: 2885c @287c/s (599ch, ~721t @72t/s)
2025-12-16 11:38:08,234 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 12.0s: 3549c @295c/s (720ch, ~887t @74t/s)
2025-12-16 11:38:10,240 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 14.0s: 4173c @297c/s (835ch, ~1043t @74t/s)
2025-12-16 11:38:12,249 - src.llm.client - INFO - [qst:9ba05a] ğŸ“Š 16.1s: 4733c @295c/s (953ch, ~1183t @74t/s)
2025-12-16 11:38:13,523 - src.llm.client - INFO - [qst:9ba05a] âœ“ Done 19.62s: 5126c (~735w @261c/s)
2025-12-16 11:38:13,523 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 0, 'total_fixes': 1}
2025-12-16 11:38:13,523 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -     Context: Module 7 Session 7
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -     Context: Module 7 Session 7
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Inarching â€“ Expanding Grafting Options (Session 7)
2025-12-16 11:38:13,524 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 11:38:13,526 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:38:13,529 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 7 completed
2025-12-16 11:38:13,529 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:38:13,529 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:38:13,529 - src.generate.orchestration.pipeline - INFO - Module 8: Topworking â€“ Advanced Grafting (1 sessions)
2025-12-16 11:38:13,529 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:38:13,529 - src.generate.orchestration.pipeline - INFO - 
[8/10] Session 8: Topworking â€“ Technique & Timing
2025-12-16 11:38:13,529 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:38:13,529 - src.generate.formats.lectures - INFO - Generating lecture for: Topworking â€“ Advanced Grafting (Session 8/10)
2025-12-16 11:38:13,529 - src.llm.client - INFO - [lec:2dd9de] ğŸš€ lec | m=gemma3:4b | p=3102c | t=180s
2025-12-16 11:38:13,529 - src.llm.client - INFO - [lec:2dd9de] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:38:13,529 - src.llm.client - INFO - [lec:2dd9de] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:38:13,531 - src.llm.client - INFO - [lec:2dd9de] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6753 bytes, prompt=3102 chars
2025-12-16 11:38:13,531 - src.llm.client - INFO - [lec:2dd9de] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:38:14,656 - src.llm.request_handler - INFO - [lec:2dd9de] âœ“ Done 1.12s
2025-12-16 11:38:14,656 - src.llm.client - INFO - [lec:2dd9de] âœ… HTTP 200 in 1.13s
2025-12-16 11:38:14,656 - src.llm.client - INFO - [lec:2dd9de] ğŸ“¡ Stream active (200)
2025-12-16 11:38:14,656 - src.llm.client - INFO - [lec:2dd9de] Starting stream parsing, waiting for first chunk...
2025-12-16 11:38:16,666 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 2.0s: 660c @328c/s (118ch, ~165t @82t/s)
2025-12-16 11:38:18,668 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 4.0s: 1314c @328c/s (240ch, ~328t @82t/s)
2025-12-16 11:38:20,683 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 6.0s: 1952c @324c/s (361ch, ~488t @81t/s)
2025-12-16 11:38:22,698 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 8.0s: 2545c @316c/s (479ch, ~636t @79t/s)
2025-12-16 11:38:24,713 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 10.1s: 3181c @316c/s (602ch, ~795t @79t/s)
2025-12-16 11:38:26,726 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 12.1s: 3760c @312c/s (725ch, ~940t @78t/s)
2025-12-16 11:38:28,733 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 14.1s: 4360c @310c/s (842ch, ~1090t @77t/s)
2025-12-16 11:38:30,736 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 16.1s: 4928c @306c/s (960ch, ~1232t @77t/s)
2025-12-16 11:38:32,745 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 18.1s: 5572c @308c/s (1084ch, ~1393t @77t/s)
2025-12-16 11:38:34,756 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 20.1s: 6213c @309c/s (1204ch, ~1553t @77t/s)
2025-12-16 11:38:36,771 - src.llm.client - INFO - [lec:2dd9de] ğŸ“Š 22.1s: 6887c @311c/s (1324ch, ~1722t @78t/s)
2025-12-16 11:38:37,116 - src.llm.client - INFO - [lec:2dd9de] âœ“ Done 23.59s: 6983c (~1047w @296c/s)
2025-12-16 11:38:37,118 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 11:38:37,118 - src.generate.formats.lectures - INFO -     - Length: 7068 chars, 1059 words
2025-12-16 11:38:37,118 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:38:37,118 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 11:38:37,118 - src.generate.formats.lectures - INFO -     - Content: 12 examples, 1 terms defined
2025-12-16 11:38:37,118 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:38:37,121 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:38:37,122 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:38:37,122 - src.generate.formats.labs - INFO - Generating lab 8 for: Topworking â€“ Advanced Grafting (Session 8)
2025-12-16 11:38:37,122 - src.llm.client - INFO - [lab:1ec75b] ğŸš€ lab | m=gemma3:4b | p=3318c | t=150s
2025-12-16 11:38:37,122 - src.llm.client - INFO - [lab:1ec75b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:38:37,122 - src.llm.client - INFO - [lab:1ec75b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:38:37,124 - src.llm.client - INFO - [lab:1ec75b] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3779 bytes, prompt=3318 chars
2025-12-16 11:38:37,124 - src.llm.client - INFO - [lab:1ec75b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:38:38,086 - src.llm.request_handler - INFO - [lab:1ec75b] âœ“ Done 0.96s
2025-12-16 11:38:38,086 - src.llm.client - INFO - [lab:1ec75b] âœ… HTTP 200 in 0.96s
2025-12-16 11:38:38,086 - src.llm.client - INFO - [lab:1ec75b] ğŸ“¡ Stream active (200)
2025-12-16 11:38:38,086 - src.llm.client - INFO - [lab:1ec75b] Starting stream parsing, waiting for first chunk...
2025-12-16 11:38:40,100 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 2.0s: 620c @308c/s (118ch, ~155t @77t/s)
2025-12-16 11:38:42,104 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 4.0s: 1191c @296c/s (239ch, ~298t @74t/s)
2025-12-16 11:38:44,108 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 6.0s: 1505c @250c/s (355ch, ~376t @62t/s)
2025-12-16 11:38:46,113 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 8.0s: 1927c @240c/s (465ch, ~482t @60t/s)
2025-12-16 11:38:48,116 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 10.0s: 2466c @246c/s (584ch, ~616t @61t/s)
2025-12-16 11:38:50,121 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 12.0s: 2851c @237c/s (686ch, ~713t @59t/s)
2025-12-16 11:38:52,129 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 14.0s: 3318c @236c/s (801ch, ~830t @59t/s)
2025-12-16 11:38:54,135 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 16.0s: 3886c @242c/s (918ch, ~972t @61t/s)
2025-12-16 11:38:56,143 - src.llm.client - INFO - [lab:1ec75b] ğŸ“Š 18.1s: 4372c @242c/s (1029ch, ~1093t @61t/s)
2025-12-16 11:38:57,090 - src.llm.client - INFO - [lab:1ec75b] âœ“ Done 19.97s: 4649c (~675w @233c/s)
2025-12-16 11:38:57,091 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:38:57,091 - src.generate.formats.labs - INFO -     - Length: 4743 chars, 690 words
2025-12-16 11:38:57,091 - src.generate.formats.labs - INFO -     - Procedure: 10 steps
2025-12-16 11:38:57,091 - src.generate.formats.labs - INFO -     - Safety: 10 warnings
2025-12-16 11:38:57,091 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 11:38:57,093 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:38:57,093 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:38:57,093 - src.generate.formats.study_notes - INFO - Generating study notes for: Topworking â€“ Advanced Grafting (Session 8)
2025-12-16 11:38:57,094 - src.llm.client - INFO - [stu:3e7e72] ğŸš€ stu | m=gemma3:4b | p=4435c | t=120s
2025-12-16 11:38:57,094 - src.llm.client - INFO - [stu:3e7e72] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:38:57,094 - src.llm.client - INFO - [stu:3e7e72] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:38:57,100 - src.llm.client - INFO - [stu:3e7e72] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8111 bytes, prompt=4435 chars
2025-12-16 11:38:57,100 - src.llm.client - INFO - [stu:3e7e72] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:38:58,507 - src.llm.request_handler - INFO - [stu:3e7e72] âœ“ Done 1.41s
2025-12-16 11:38:58,507 - src.llm.client - INFO - [stu:3e7e72] âœ… HTTP 200 in 1.41s
2025-12-16 11:38:58,507 - src.llm.client - INFO - [stu:3e7e72] ğŸ“¡ Stream active (200)
2025-12-16 11:38:58,507 - src.llm.client - INFO - [stu:3e7e72] Starting stream parsing, waiting for first chunk...
2025-12-16 11:39:00,514 - src.llm.client - INFO - [stu:3e7e72] ğŸ“Š 2.0s: 708c @353c/s (124ch, ~177t @88t/s)
2025-12-16 11:39:02,523 - src.llm.client - INFO - [stu:3e7e72] ğŸ“Š 4.0s: 1298c @323c/s (251ch, ~324t @81t/s)
2025-12-16 11:39:04,533 - src.llm.client - INFO - [stu:3e7e72] ğŸ“Š 6.0s: 1958c @325c/s (374ch, ~490t @81t/s)
2025-12-16 11:39:06,539 - src.llm.client - INFO - [stu:3e7e72] ğŸ“Š 8.0s: 2617c @326c/s (499ch, ~654t @81t/s)
2025-12-16 11:39:08,202 - src.llm.client - INFO - [stu:3e7e72] âœ“ Done 11.11s: 3103c (~422w @279c/s)
2025-12-16 11:39:08,202 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 11:39:08,202 - src.generate.formats.study_notes - INFO -     - Length: 3168 chars, 433 words
2025-12-16 11:39:08,202 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:39:08,202 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-16 11:39:08,202 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 3 bullets
2025-12-16 11:39:08,202 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:39:08,206 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:39:08,206 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:39:08,207 - src.generate.formats.diagrams - INFO - Generating diagram for: Scion Selection (Topworking â€“ Advanced Grafting)
2025-12-16 11:39:08,207 - src.llm.client - INFO - [dia:0c05d4] ğŸš€ dia | m=gemma3:4b | p=5758c | t=120s
2025-12-16 11:39:08,207 - src.llm.client - INFO - [dia:0c05d4] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:39:08,207 - src.llm.client - INFO - [dia:0c05d4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:39:08,208 - src.generate.formats.diagrams - INFO - Generating diagram for: Graft Placement (Topworking â€“ Advanced Grafting)
2025-12-16 11:39:08,209 - src.llm.client - INFO - [dia:a0f618] ğŸš€ dia | m=gemma3:4b | p=5758c | t=120s
2025-12-16 11:39:08,209 - src.llm.client - INFO - [dia:a0f618] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:39:08,209 - src.llm.client - INFO - [dia:a0f618] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:39:08,213 - src.llm.client - INFO - [dia:0c05d4] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11077 bytes, prompt=5758 chars
2025-12-16 11:39:08,213 - src.llm.client - INFO - [dia:a0f618] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11077 bytes, prompt=5758 chars
2025-12-16 11:39:08,213 - src.llm.client - INFO - [dia:0c05d4] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:39:08,213 - src.llm.client - INFO - [dia:a0f618] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:39:10,145 - src.llm.request_handler - INFO - [dia:0c05d4] âœ“ Done 1.93s
2025-12-16 11:39:10,146 - src.llm.client - INFO - [dia:0c05d4] âœ… HTTP 200 in 1.93s
2025-12-16 11:39:10,146 - src.llm.client - INFO - [dia:0c05d4] ğŸ“¡ Stream active (200)
2025-12-16 11:39:10,146 - src.llm.client - INFO - [dia:0c05d4] Starting stream parsing, waiting for first chunk...
2025-12-16 11:39:12,156 - src.llm.client - INFO - [dia:0c05d4] ğŸ“Š 2.0s: 441c @219c/s (124ch, ~110t @55t/s)
2025-12-16 11:39:14,130 - src.llm.client - INFO - [dia:0c05d4] âœ“ Done 5.92s: 808c (~110w @136c/s)
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Scion Selection (Topworking â€“ Advanced Grafting):
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO -     - Length: 685 chars (cleaned: 685 chars)
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO - [OK] Elements: 36 total (nodes: 14, connections: 22) âœ“
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:39:14,131 - src.generate.formats.diagrams - INFO - Generated diagram: 685 characters
2025-12-16 11:39:15,773 - src.llm.request_handler - INFO - [dia:a0f618] âœ“ Done 7.56s
2025-12-16 11:39:15,773 - src.llm.client - INFO - [dia:a0f618] âœ… HTTP 200 in 7.56s
2025-12-16 11:39:15,773 - src.llm.client - INFO - [dia:a0f618] ğŸ“¡ Stream active (200)
2025-12-16 11:39:15,773 - src.llm.client - INFO - [dia:a0f618] Starting stream parsing, waiting for first chunk...
2025-12-16 11:39:17,782 - src.llm.client - INFO - [dia:a0f618] ğŸ“Š 2.0s: 464c @231c/s (121ch, ~116t @58t/s)
2025-12-16 11:39:19,787 - src.llm.client - INFO - [dia:a0f618] ğŸ“Š 4.0s: 914c @228c/s (244ch, ~228t @57t/s)
2025-12-16 11:39:20,977 - src.llm.client - INFO - [dia:a0f618] âœ“ Done 12.77s: 1082c (~145w @85c/s)
2025-12-16 11:39:20,979 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Graft Placement (Topworking â€“ Advanced Grafting):
2025-12-16 11:39:20,979 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:39:20,979 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:39:20,979 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:39:20,979 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 5 long nodes) âš ï¸
2025-12-16 11:39:20,979 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:39:20,980 - src.generate.formats.diagrams - INFO -     - Length: 922 chars (cleaned: 922 chars)
2025-12-16 11:39:20,980 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:39:20,980 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 41 total (nodes: 15, connections: 26) âš ï¸
2025-12-16 11:39:20,980 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:39:20,980 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 5 long nodes) âš ï¸
2025-12-16 11:39:20,980 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 11:39:20,980 - src.generate.formats.diagrams - INFO - Generated diagram: 922 characters
2025-12-16 11:39:20,981 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:39:20,981 - src.generate.formats.questions - INFO - Generating 10 questions for: Topworking â€“ Advanced Grafting (Session 8)
2025-12-16 11:39:20,982 - src.llm.client - INFO - [qst:b52dc5] ğŸš€ qst | m=gemma3:4b | p=7322c | t=150s
2025-12-16 11:39:20,982 - src.llm.client - INFO - [qst:b52dc5] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:39:20,982 - src.llm.client - INFO - [qst:b52dc5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:39:20,984 - src.llm.client - INFO - [qst:b52dc5] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11103 bytes, prompt=7322 chars
2025-12-16 11:39:20,984 - src.llm.client - INFO - [qst:b52dc5] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:39:23,251 - src.llm.request_handler - INFO - [qst:b52dc5] âœ“ Done 2.27s
2025-12-16 11:39:23,252 - src.llm.client - INFO - [qst:b52dc5] âœ… HTTP 200 in 2.27s
2025-12-16 11:39:23,252 - src.llm.client - INFO - [qst:b52dc5] ğŸ“¡ Stream active (200)
2025-12-16 11:39:23,252 - src.llm.client - INFO - [qst:b52dc5] Starting stream parsing, waiting for first chunk...
2025-12-16 11:39:25,257 - src.llm.client - INFO - [qst:b52dc5] ğŸ“Š 2.0s: 566c @282c/s (118ch, ~142t @71t/s)
2025-12-16 11:39:27,273 - src.llm.client - INFO - [qst:b52dc5] ğŸ“Š 4.0s: 1139c @283c/s (239ch, ~285t @71t/s)
2025-12-16 11:39:29,280 - src.llm.client - INFO - [qst:b52dc5] ğŸ“Š 6.0s: 1687c @280c/s (355ch, ~422t @70t/s)
2025-12-16 11:39:31,294 - src.llm.client - INFO - [qst:b52dc5] ğŸ“Š 8.0s: 2237c @278c/s (473ch, ~559t @70t/s)
2025-12-16 11:39:33,309 - src.llm.client - INFO - [qst:b52dc5] ğŸ“Š 10.1s: 2874c @286c/s (593ch, ~718t @71t/s)
2025-12-16 11:39:35,325 - src.llm.client - INFO - [qst:b52dc5] ğŸ“Š 12.1s: 3501c @290c/s (713ch, ~875t @72t/s)
2025-12-16 11:39:37,335 - src.llm.client - INFO - [qst:b52dc5] ğŸ“Š 14.1s: 4141c @294c/s (831ch, ~1035t @74t/s)
2025-12-16 11:39:38,665 - src.llm.client - INFO - [qst:b52dc5] âœ“ Done 17.68s: 4519c (~650w @256c/s)
2025-12-16 11:39:38,666 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 0, 'total_fixes': 3}
2025-12-16 11:39:38,666 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 11:39:38,667 - src.generate.formats.questions - INFO - [COMPLIANT] Questions generated âœ“
2025-12-16 11:39:38,667 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-16 11:39:38,667 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-16 11:39:38,667 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-16 11:39:38,667 - src.generate.formats.questions - INFO - [OK] Question marks: 10 total, 10 questions with '?' âœ“
2025-12-16 11:39:38,667 - src.generate.formats.questions - INFO -     - Question length: avg 10.5 words (range: 7-19)
2025-12-16 11:39:38,667 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-16 11:39:38,669 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:39:38,672 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 8 completed
2025-12-16 11:39:38,672 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:39:38,672 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:39:38,672 - src.generate.orchestration.pipeline - INFO - Module 9: Frameworking â€“ Enhancing Tree Structure (1 sessions)
2025-12-16 11:39:38,672 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:39:38,672 - src.generate.orchestration.pipeline - INFO - 
[9/10] Session 9: Frameworking â€“ Grafting for Stability
2025-12-16 11:39:38,672 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:39:38,672 - src.generate.formats.lectures - INFO - Generating lecture for: Frameworking â€“ Enhancing Tree Structure (Session 9/10)
2025-12-16 11:39:38,673 - src.llm.client - INFO - [lec:e09cd7] ğŸš€ lec | m=gemma3:4b | p=3165c | t=180s
2025-12-16 11:39:38,673 - src.llm.client - INFO - [lec:e09cd7] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:39:38,673 - src.llm.client - INFO - [lec:e09cd7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:39:38,675 - src.llm.client - INFO - [lec:e09cd7] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6816 bytes, prompt=3165 chars
2025-12-16 11:39:38,675 - src.llm.client - INFO - [lec:e09cd7] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:39:39,883 - src.llm.request_handler - INFO - [lec:e09cd7] âœ“ Done 1.21s
2025-12-16 11:39:39,884 - src.llm.client - INFO - [lec:e09cd7] âœ… HTTP 200 in 1.21s
2025-12-16 11:39:39,884 - src.llm.client - INFO - [lec:e09cd7] ğŸ“¡ Stream active (200)
2025-12-16 11:39:39,884 - src.llm.client - INFO - [lec:e09cd7] Starting stream parsing, waiting for first chunk...
2025-12-16 11:39:41,885 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 2.0s: 673c @336c/s (124ch, ~168t @84t/s)
2025-12-16 11:39:43,899 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 4.0s: 1309c @326c/s (250ch, ~327t @82t/s)
2025-12-16 11:39:45,914 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 6.0s: 1930c @320c/s (372ch, ~482t @80t/s)
2025-12-16 11:39:47,919 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 8.0s: 2529c @315c/s (497ch, ~632t @79t/s)
2025-12-16 11:39:49,920 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 10.0s: 3076c @307c/s (602ch, ~769t @77t/s)
2025-12-16 11:39:51,925 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 12.0s: 3668c @305c/s (719ch, ~917t @76t/s)
2025-12-16 11:39:53,939 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 14.1s: 4281c @305c/s (840ch, ~1070t @76t/s)
2025-12-16 11:39:55,951 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 16.1s: 4972c @309c/s (963ch, ~1243t @77t/s)
2025-12-16 11:39:57,963 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 18.1s: 5567c @308c/s (1085ch, ~1392t @77t/s)
2025-12-16 11:39:59,988 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 20.1s: 6083c @303c/s (1202ch, ~1521t @76t/s)
2025-12-16 11:40:02,029 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 22.1s: 6649c @300c/s (1311ch, ~1662t @75t/s)
2025-12-16 11:40:04,042 - src.llm.client - INFO - [lec:e09cd7] ğŸ“Š 24.2s: 7255c @300c/s (1426ch, ~1814t @75t/s)
2025-12-16 11:40:06,008 - src.llm.client - INFO - [lec:e09cd7] âœ“ Done 27.33s: 7887c (~1183w @289c/s)
2025-12-16 11:40:06,013 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 11:40:06,013 - src.generate.formats.lectures - INFO -     - Length: 7989 chars, 1197 words
2025-12-16 11:40:06,013 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:40:06,013 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 11:40:06,013 - src.generate.formats.lectures - INFO -     - Content: 7 examples, 2 terms defined
2025-12-16 11:40:06,013 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:40:06,016 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:40:06,018 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:40:06,018 - src.generate.formats.labs - INFO - Generating lab 9 for: Frameworking â€“ Enhancing Tree Structure (Session 9)
2025-12-16 11:40:06,018 - src.llm.client - INFO - [lab:6d8bae] ğŸš€ lab | m=gemma3:4b | p=3350c | t=150s
2025-12-16 11:40:06,018 - src.llm.client - INFO - [lab:6d8bae] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:40:06,018 - src.llm.client - INFO - [lab:6d8bae] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:40:06,021 - src.llm.client - INFO - [lab:6d8bae] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3800 bytes, prompt=3350 chars
2025-12-16 11:40:06,021 - src.llm.client - INFO - [lab:6d8bae] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:40:07,038 - src.llm.request_handler - INFO - [lab:6d8bae] âœ“ Done 1.02s
2025-12-16 11:40:07,038 - src.llm.client - INFO - [lab:6d8bae] âœ… HTTP 200 in 1.02s
2025-12-16 11:40:07,038 - src.llm.client - INFO - [lab:6d8bae] ğŸ“¡ Stream active (200)
2025-12-16 11:40:07,038 - src.llm.client - INFO - [lab:6d8bae] Starting stream parsing, waiting for first chunk...
2025-12-16 11:40:09,051 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 2.0s: 580c @288c/s (119ch, ~145t @72t/s)
2025-12-16 11:40:11,056 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 4.0s: 1150c @286c/s (236ch, ~288t @72t/s)
2025-12-16 11:40:13,062 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 6.0s: 1470c @244c/s (352ch, ~368t @61t/s)
2025-12-16 11:40:15,070 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 8.0s: 1837c @229c/s (464ch, ~459t @57t/s)
2025-12-16 11:40:17,086 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 10.0s: 2261c @225c/s (580ch, ~565t @56t/s)
2025-12-16 11:40:19,094 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 12.1s: 2801c @232c/s (698ch, ~700t @58t/s)
2025-12-16 11:40:21,107 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 14.1s: 3281c @233c/s (819ch, ~820t @58t/s)
2025-12-16 11:40:23,120 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 16.1s: 3789c @236c/s (940ch, ~947t @59t/s)
2025-12-16 11:40:25,130 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 18.1s: 4432c @245c/s (1057ch, ~1108t @61t/s)
2025-12-16 11:40:27,133 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 20.1s: 5083c @253c/s (1173ch, ~1271t @63t/s)
2025-12-16 11:40:29,141 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 22.1s: 5676c @257c/s (1294ch, ~1419t @64t/s)
2025-12-16 11:40:31,146 - src.llm.client - INFO - [lab:6d8bae] ğŸ“Š 24.1s: 6273c @260c/s (1410ch, ~1568t @65t/s)
2025-12-16 11:40:32,201 - src.llm.client - INFO - [lab:6d8bae] âœ“ Done 26.18s: 6556c (~851w @250c/s)
2025-12-16 11:40:32,202 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:40:32,202 - src.generate.formats.labs - INFO -     - Length: 6666 chars, 869 words
2025-12-16 11:40:32,202 - src.generate.formats.labs - INFO -     - Procedure: 10 steps
2025-12-16 11:40:32,202 - src.generate.formats.labs - INFO -     - Safety: 8 warnings
2025-12-16 11:40:32,202 - src.generate.formats.labs - INFO -     - Data tables: 11
2025-12-16 11:40:32,205 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:40:32,206 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:40:32,206 - src.generate.formats.study_notes - INFO - Generating study notes for: Frameworking â€“ Enhancing Tree Structure (Session 9)
2025-12-16 11:40:32,206 - src.llm.client - INFO - [stu:4fa2b2] ğŸš€ stu | m=gemma3:4b | p=4457c | t=120s
2025-12-16 11:40:32,206 - src.llm.client - INFO - [stu:4fa2b2] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:40:32,206 - src.llm.client - INFO - [stu:4fa2b2] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:40:32,208 - src.llm.client - INFO - [stu:4fa2b2] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8122 bytes, prompt=4457 chars
2025-12-16 11:40:32,208 - src.llm.client - INFO - [stu:4fa2b2] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:40:33,626 - src.llm.request_handler - INFO - [stu:4fa2b2] âœ“ Done 1.42s
2025-12-16 11:40:33,626 - src.llm.client - INFO - [stu:4fa2b2] âœ… HTTP 200 in 1.42s
2025-12-16 11:40:33,626 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“¡ Stream active (200)
2025-12-16 11:40:33,626 - src.llm.client - INFO - [stu:4fa2b2] Starting stream parsing, waiting for first chunk...
2025-12-16 11:40:35,643 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“Š 2.0s: 655c @325c/s (121ch, ~164t @81t/s)
2025-12-16 11:40:37,646 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“Š 4.0s: 1277c @318c/s (240ch, ~319t @79t/s)
2025-12-16 11:40:39,651 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“Š 6.0s: 1895c @315c/s (359ch, ~474t @79t/s)
2025-12-16 11:40:41,664 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“Š 8.0s: 2502c @311c/s (478ch, ~626t @78t/s)
2025-12-16 11:40:43,679 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“Š 10.1s: 3192c @318c/s (599ch, ~798t @79t/s)
2025-12-16 11:40:45,680 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“Š 12.1s: 3783c @314c/s (710ch, ~946t @78t/s)
2025-12-16 11:40:47,689 - src.llm.client - INFO - [stu:4fa2b2] ğŸ“Š 14.1s: 4376c @311c/s (827ch, ~1094t @78t/s)
2025-12-16 11:40:48,959 - src.llm.client - INFO - [stu:4fa2b2] âœ“ Done 16.75s: 4752c (~680w @284c/s)
2025-12-16 11:40:48,960 - src.generate.formats.study_notes - INFO - [NEEDS REVIEW] Study notes generated âš ï¸
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - INFO -     - Length: 4826 chars, 692 words
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - INFO -     - Key concepts: 12
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - INFO -     - Structure: 4 sections, 3 bullets
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - WARNING - [WARNING] Too many key concepts (12, maximum 10, 2 excess - consolidate related concepts or remove less critical ones) âš ï¸
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 11:40:48,961 - src.generate.formats.study_notes - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 11:40:48,962 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:40:48,963 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:40:48,963 - src.generate.formats.diagrams - INFO - Generating diagram for: Scion Size & Placement (Frameworking â€“ Enhancing Tree Structure)
2025-12-16 11:40:48,963 - src.generate.formats.diagrams - INFO - Generating diagram for: Binding Strength (Frameworking â€“ Enhancing Tree Structure)
2025-12-16 11:40:48,963 - src.llm.client - INFO - [dia:8e0356] ğŸš€ dia | m=gemma3:4b | p=5787c | t=120s
2025-12-16 11:40:48,964 - src.llm.client - INFO - [dia:a15052] ğŸš€ dia | m=gemma3:4b | p=5775c | t=120s
2025-12-16 11:40:48,964 - src.llm.client - INFO - [dia:8e0356] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:40:48,964 - src.llm.client - INFO - [dia:a15052] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:40:48,964 - src.llm.client - INFO - [dia:8e0356] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:40:48,964 - src.llm.client - INFO - [dia:a15052] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:40:48,966 - src.llm.client - INFO - [dia:8e0356] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11106 bytes, prompt=5787 chars
2025-12-16 11:40:48,966 - src.llm.client - INFO - [dia:a15052] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11094 bytes, prompt=5775 chars
2025-12-16 11:40:48,966 - src.llm.client - INFO - [dia:8e0356] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:40:48,966 - src.llm.client - INFO - [dia:a15052] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:40:50,923 - src.llm.request_handler - INFO - [dia:8e0356] âœ“ Done 1.96s
2025-12-16 11:40:50,923 - src.llm.client - INFO - [dia:8e0356] âœ… HTTP 200 in 1.96s
2025-12-16 11:40:50,923 - src.llm.client - INFO - [dia:8e0356] ğŸ“¡ Stream active (200)
2025-12-16 11:40:50,923 - src.llm.client - INFO - [dia:8e0356] Starting stream parsing, waiting for first chunk...
2025-12-16 11:40:52,929 - src.llm.client - INFO - [dia:8e0356] ğŸ“Š 2.0s: 426c @212c/s (113ch, ~106t @53t/s)
2025-12-16 11:40:54,942 - src.llm.client - INFO - [dia:8e0356] ğŸ“Š 4.0s: 823c @205c/s (235ch, ~206t @51t/s)
2025-12-16 11:40:56,946 - src.llm.client - INFO - [dia:8e0356] ğŸ“Š 6.0s: 1132c @188c/s (358ch, ~283t @47t/s)
2025-12-16 11:40:57,222 - src.llm.client - INFO - [dia:8e0356] âœ“ Done 8.26s: 1170c (~145w @142c/s)
2025-12-16 11:40:57,222 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Scion Size & Placement (Frameworking â€“ Enhancing Tree Structure):
2025-12-16 11:40:57,222 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:40:57,222 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:40:57,222 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:40:57,223 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:40:57,223 - src.generate.formats.diagrams - INFO -     - Length: 747 chars (cleaned: 747 chars)
2025-12-16 11:40:57,223 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:40:57,223 - src.generate.formats.diagrams - INFO - [OK] Elements: 36 total (nodes: 13, connections: 23) âœ“
2025-12-16 11:40:57,223 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:40:57,223 - src.generate.formats.diagrams - INFO - Generated diagram: 747 characters
2025-12-16 11:40:58,944 - src.llm.request_handler - INFO - [dia:a15052] âœ“ Done 9.98s
2025-12-16 11:40:58,944 - src.llm.client - INFO - [dia:a15052] âœ… HTTP 200 in 9.98s
2025-12-16 11:40:58,945 - src.llm.client - INFO - [dia:a15052] ğŸ“¡ Stream active (200)
2025-12-16 11:40:58,945 - src.llm.client - INFO - [dia:a15052] Starting stream parsing, waiting for first chunk...
2025-12-16 11:41:00,949 - src.llm.client - INFO - [dia:a15052] ğŸ“Š 2.0s: 441c @220c/s (120ch, ~110t @55t/s)
2025-12-16 11:41:02,963 - src.llm.client - INFO - [dia:a15052] ğŸ“Š 4.0s: 813c @202c/s (231ch, ~203t @51t/s)
2025-12-16 11:41:03,542 - src.llm.client - INFO - [dia:a15052] âœ“ Done 14.58s: 865c (~120w @59c/s)
2025-12-16 11:41:03,556 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Binding Strength (Frameworking â€“ Enhancing Tree Structure):
2025-12-16 11:41:03,558 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:41:03,558 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:41:03,558 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:41:03,573 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:41:03,573 - src.generate.formats.diagrams - INFO -     - Length: 748 chars (cleaned: 748 chars)
2025-12-16 11:41:03,573 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:41:03,573 - src.generate.formats.diagrams - INFO - [OK] Elements: 40 total (nodes: 15, connections: 25) âœ“
2025-12-16 11:41:03,573 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:41:03,573 - src.generate.formats.diagrams - INFO - Generated diagram: 748 characters
2025-12-16 11:41:03,580 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:41:03,581 - src.generate.formats.questions - INFO - Generating 10 questions for: Frameworking â€“ Enhancing Tree Structure (Session 9)
2025-12-16 11:41:03,584 - src.llm.client - INFO - [qst:4f9459] ğŸš€ qst | m=gemma3:4b | p=7347c | t=150s
2025-12-16 11:41:03,584 - src.llm.client - INFO - [qst:4f9459] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:41:03,585 - src.llm.client - INFO - [qst:4f9459] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:41:03,600 - src.llm.client - INFO - [qst:4f9459] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11098 bytes, prompt=7347 chars
2025-12-16 11:41:03,600 - src.llm.client - INFO - [qst:4f9459] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:41:06,257 - src.llm.request_handler - INFO - [qst:4f9459] âœ“ Done 2.66s
2025-12-16 11:41:06,260 - src.llm.client - INFO - [qst:4f9459] âœ… HTTP 200 in 2.66s
2025-12-16 11:41:06,260 - src.llm.client - INFO - [qst:4f9459] ğŸ“¡ Stream active (200)
2025-12-16 11:41:06,260 - src.llm.client - INFO - [qst:4f9459] Starting stream parsing, waiting for first chunk...
2025-12-16 11:41:08,267 - src.llm.client - INFO - [qst:4f9459] ğŸ“Š 2.0s: 494c @246c/s (110ch, ~124t @62t/s)
2025-12-16 11:41:10,277 - src.llm.client - INFO - [qst:4f9459] ğŸ“Š 4.0s: 1065c @265c/s (230ch, ~266t @66t/s)
2025-12-16 11:41:12,286 - src.llm.client - INFO - [qst:4f9459] ğŸ“Š 6.0s: 1607c @267c/s (348ch, ~402t @67t/s)
2025-12-16 11:41:14,298 - src.llm.client - INFO - [qst:4f9459] ğŸ“Š 8.0s: 2142c @266c/s (468ch, ~536t @67t/s)
2025-12-16 11:41:16,307 - src.llm.client - INFO - [qst:4f9459] ğŸ“Š 10.0s: 2717c @270c/s (595ch, ~679t @68t/s)
2025-12-16 11:41:18,315 - src.llm.client - INFO - [qst:4f9459] ğŸ“Š 12.1s: 3368c @279c/s (719ch, ~842t @70t/s)
2025-12-16 11:41:20,317 - src.llm.client - INFO - [qst:4f9459] ğŸ“Š 14.1s: 4017c @286c/s (841ch, ~1004t @71t/s)
2025-12-16 11:41:22,288 - src.llm.client - INFO - [qst:4f9459] âœ“ Done 18.70s: 4660c (~693w @249c/s)
2025-12-16 11:41:22,289 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 0, 'total_fixes': 2}
2025-12-16 11:41:22,289 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-16 11:41:22,289 - src.generate.formats.questions - INFO - [COMPLIANT] Questions generated âœ“
2025-12-16 11:41:22,289 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-16 11:41:22,289 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-16 11:41:22,290 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-16 11:41:22,290 - src.generate.formats.questions - INFO - [OK] Question marks: 13 total, 10 questions with '?' âœ“
2025-12-16 11:41:22,290 - src.generate.formats.questions - INFO -     - Question length: avg 13.4 words (range: 7-20)
2025-12-16 11:41:22,290 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-16 11:41:22,292 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:41:22,294 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 9 completed
2025-12-16 11:41:22,294 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:41:22,294 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:41:22,294 - src.generate.orchestration.pipeline - INFO - Module 10: Grafting Troubleshooting & Maintenance (1 sessions)
2025-12-16 11:41:22,294 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:41:22,294 - src.generate.orchestration.pipeline - INFO - 
[10/10] Session 10: Graft Failure Analysis & Remediation
2025-12-16 11:41:22,295 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 11:41:22,295 - src.generate.formats.lectures - INFO - Generating lecture for: Grafting Troubleshooting & Maintenance (Session 10/10)
2025-12-16 11:41:22,295 - src.llm.client - INFO - [lec:9de768] ğŸš€ lec | m=gemma3:4b | p=3157c | t=180s
2025-12-16 11:41:22,295 - src.llm.client - INFO - [lec:9de768] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 11:41:22,295 - src.llm.client - INFO - [lec:9de768] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:41:22,297 - src.llm.client - INFO - [lec:9de768] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6789 bytes, prompt=3157 chars
2025-12-16 11:41:22,297 - src.llm.client - INFO - [lec:9de768] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 11:41:23,398 - src.llm.request_handler - INFO - [lec:9de768] âœ“ Done 1.10s
2025-12-16 11:41:23,398 - src.llm.client - INFO - [lec:9de768] âœ… HTTP 200 in 1.10s
2025-12-16 11:41:23,398 - src.llm.client - INFO - [lec:9de768] ğŸ“¡ Stream active (200)
2025-12-16 11:41:23,399 - src.llm.client - INFO - [lec:9de768] Starting stream parsing, waiting for first chunk...
2025-12-16 11:41:25,408 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 2.0s: 665c @331c/s (125ch, ~166t @83t/s)
2025-12-16 11:41:27,422 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 4.0s: 1354c @337c/s (247ch, ~338t @84t/s)
2025-12-16 11:41:29,426 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 6.0s: 1975c @328c/s (365ch, ~494t @82t/s)
2025-12-16 11:41:31,444 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 8.0s: 2615c @325c/s (480ch, ~654t @81t/s)
2025-12-16 11:41:33,448 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 10.0s: 3216c @320c/s (596ch, ~804t @80t/s)
2025-12-16 11:41:35,459 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 12.1s: 3823c @317c/s (708ch, ~956t @79t/s)
2025-12-16 11:41:37,462 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 14.1s: 4476c @318c/s (830ch, ~1119t @80t/s)
2025-12-16 11:41:39,475 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 16.1s: 5057c @315c/s (955ch, ~1264t @79t/s)
2025-12-16 11:41:41,490 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 18.1s: 5627c @311c/s (1067ch, ~1407t @78t/s)
2025-12-16 11:41:43,494 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 20.1s: 6223c @310c/s (1185ch, ~1556t @77t/s)
2025-12-16 11:41:45,502 - src.llm.client - INFO - [lec:9de768] ğŸ“Š 22.1s: 6869c @311c/s (1299ch, ~1717t @78t/s)
2025-12-16 11:41:46,167 - src.llm.client - INFO - [lec:9de768] âœ“ Done 23.87s: 7110c (~1022w @298c/s)
2025-12-16 11:41:46,169 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 11:41:46,169 - src.generate.formats.lectures - INFO -     - Length: 7208 chars, 1035 words
2025-12-16 11:41:46,169 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 11:41:46,169 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 11:41:46,169 - src.generate.formats.lectures - INFO -     - Content: 9 examples, 3 terms defined
2025-12-16 11:41:46,169 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:41:46,172 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:41:46,173 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 11:41:46,173 - src.generate.formats.labs - INFO - Generating lab 10 for: Grafting Troubleshooting & Maintenance (Session 10)
2025-12-16 11:41:46,173 - src.llm.client - INFO - [lab:f7efad] ğŸš€ lab | m=gemma3:4b | p=3352c | t=150s
2025-12-16 11:41:46,174 - src.llm.client - INFO - [lab:f7efad] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:41:46,174 - src.llm.client - INFO - [lab:f7efad] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:41:46,181 - src.llm.client - INFO - [lab:f7efad] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3778 bytes, prompt=3352 chars
2025-12-16 11:41:46,181 - src.llm.client - INFO - [lab:f7efad] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:41:47,147 - src.llm.request_handler - INFO - [lab:f7efad] âœ“ Done 0.97s
2025-12-16 11:41:47,147 - src.llm.client - INFO - [lab:f7efad] âœ… HTTP 200 in 0.97s
2025-12-16 11:41:47,147 - src.llm.client - INFO - [lab:f7efad] ğŸ“¡ Stream active (200)
2025-12-16 11:41:47,147 - src.llm.client - INFO - [lab:f7efad] Starting stream parsing, waiting for first chunk...
2025-12-16 11:41:49,148 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 2.0s: 604c @302c/s (119ch, ~151t @75t/s)
2025-12-16 11:41:51,167 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 4.0s: 1096c @273c/s (222ch, ~274t @68t/s)
2025-12-16 11:41:53,177 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 6.0s: 1565c @260c/s (346ch, ~391t @65t/s)
2025-12-16 11:41:55,190 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 8.0s: 1925c @239c/s (468ch, ~481t @60t/s)
2025-12-16 11:41:57,195 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 10.0s: 2440c @243c/s (587ch, ~610t @61t/s)
2025-12-16 11:41:59,204 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 12.1s: 2949c @245c/s (713ch, ~737t @61t/s)
2025-12-16 11:42:01,206 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 14.1s: 3457c @246c/s (835ch, ~864t @61t/s)
2025-12-16 11:42:03,207 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 16.1s: 3878c @241c/s (957ch, ~970t @60t/s)
2025-12-16 11:42:05,212 - src.llm.client - INFO - [lab:f7efad] ğŸ“Š 18.1s: 4426c @245c/s (1081ch, ~1106t @61t/s)
2025-12-16 11:42:06,265 - src.llm.client - INFO - [lab:f7efad] âœ“ Done 20.09s: 4697c (~687w @234c/s)
2025-12-16 11:42:06,267 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 11:42:06,267 - src.generate.formats.labs - INFO -     - Length: 4796 chars, 701 words
2025-12-16 11:42:06,268 - src.generate.formats.labs - INFO -     - Procedure: 13 steps
2025-12-16 11:42:06,268 - src.generate.formats.labs - INFO -     - Safety: 10 warnings
2025-12-16 11:42:06,268 - src.generate.formats.labs - INFO -     - Data tables: 4
2025-12-16 11:42:06,271 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:42:06,276 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 11:42:06,277 - src.generate.formats.study_notes - INFO - Generating study notes for: Grafting Troubleshooting & Maintenance (Session 10)
2025-12-16 11:42:06,278 - src.llm.client - INFO - [stu:9b8710] ğŸš€ stu | m=gemma3:4b | p=4470c | t=120s
2025-12-16 11:42:06,278 - src.llm.client - INFO - [stu:9b8710] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:42:06,278 - src.llm.client - INFO - [stu:9b8710] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:42:06,281 - src.llm.client - INFO - [stu:9b8710] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8111 bytes, prompt=4470 chars
2025-12-16 11:42:06,281 - src.llm.client - INFO - [stu:9b8710] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:42:07,686 - src.llm.request_handler - INFO - [stu:9b8710] âœ“ Done 1.41s
2025-12-16 11:42:07,686 - src.llm.client - INFO - [stu:9b8710] âœ… HTTP 200 in 1.41s
2025-12-16 11:42:07,686 - src.llm.client - INFO - [stu:9b8710] ğŸ“¡ Stream active (200)
2025-12-16 11:42:07,686 - src.llm.client - INFO - [stu:9b8710] Starting stream parsing, waiting for first chunk...
2025-12-16 11:42:09,695 - src.llm.client - INFO - [stu:9b8710] ğŸ“Š 2.0s: 613c @305c/s (120ch, ~153t @76t/s)
2025-12-16 11:42:11,703 - src.llm.client - INFO - [stu:9b8710] ğŸ“Š 4.0s: 1264c @315c/s (234ch, ~316t @79t/s)
2025-12-16 11:42:13,713 - src.llm.client - INFO - [stu:9b8710] ğŸ“Š 6.0s: 1916c @318c/s (358ch, ~479t @79t/s)
2025-12-16 11:42:15,721 - src.llm.client - INFO - [stu:9b8710] ğŸ“Š 8.0s: 2593c @323c/s (482ch, ~648t @81t/s)
2025-12-16 11:42:17,730 - src.llm.client - INFO - [stu:9b8710] ğŸ“Š 10.0s: 3228c @321c/s (604ch, ~807t @80t/s)
2025-12-16 11:42:19,504 - src.llm.client - INFO - [stu:9b8710] âœ“ Done 13.23s: 3791c (~513w @287c/s)
2025-12-16 11:42:19,504 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 11:42:19,504 - src.generate.formats.study_notes - INFO -     - Length: 3864 chars, 524 words
2025-12-16 11:42:19,504 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 11:42:19,504 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-16 11:42:19,504 - src.generate.formats.study_notes - INFO -     - Structure: 5 sections, 7 bullets
2025-12-16 11:42:19,504 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 11:42:19,506 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:42:19,506 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 11:42:19,506 - src.generate.formats.diagrams - INFO - Generating diagram for: Desiccation (Grafting Troubleshooting & Maintenance)
2025-12-16 11:42:19,506 - src.generate.formats.diagrams - INFO - Generating diagram for: Disease Transmission (Grafting Troubleshooting & Maintenance)
2025-12-16 11:42:19,506 - src.generate.formats.diagrams - INFO - Generating diagram for: Poor Cambial Contact (Grafting Troubleshooting & Maintenance)
2025-12-16 11:42:19,506 - src.llm.client - INFO - [dia:a511f4] ğŸš€ dia | m=gemma3:4b | p=5763c | t=120s
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:ec0977] ğŸš€ dia | m=gemma3:4b | p=5781c | t=120s
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:cc6878] ğŸš€ dia | m=gemma3:4b | p=5781c | t=120s
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:a511f4] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:ec0977] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:cc6878] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:a511f4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:ec0977] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:42:19,507 - src.llm.client - INFO - [dia:cc6878] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:42:19,509 - src.llm.client - INFO - [dia:cc6878] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11090 bytes, prompt=5781 chars
2025-12-16 11:42:19,509 - src.llm.client - INFO - [dia:a511f4] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11072 bytes, prompt=5763 chars
2025-12-16 11:42:19,509 - src.llm.client - INFO - [dia:cc6878] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:42:19,509 - src.llm.client - INFO - [dia:a511f4] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:42:19,510 - src.llm.client - INFO - [dia:ec0977] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11090 bytes, prompt=5781 chars
2025-12-16 11:42:19,510 - src.llm.client - INFO - [dia:ec0977] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 11:42:21,447 - src.llm.request_handler - INFO - [dia:a511f4] âœ“ Done 1.94s
2025-12-16 11:42:21,450 - src.llm.client - INFO - [dia:a511f4] âœ… HTTP 200 in 1.94s
2025-12-16 11:42:21,450 - src.llm.client - INFO - [dia:a511f4] ğŸ“¡ Stream active (200)
2025-12-16 11:42:21,450 - src.llm.client - INFO - [dia:a511f4] Starting stream parsing, waiting for first chunk...
2025-12-16 11:42:23,461 - src.llm.client - INFO - [dia:a511f4] ğŸ“Š 2.0s: 382c @190c/s (114ch, ~96t @48t/s)
2025-12-16 11:42:25,474 - src.llm.client - INFO - [dia:a511f4] ğŸ“Š 4.0s: 763c @190c/s (233ch, ~191t @47t/s)
2025-12-16 11:42:27,476 - src.llm.client - INFO - [dia:a511f4] ğŸ“Š 6.0s: 1120c @186c/s (350ch, ~280t @46t/s)
2025-12-16 11:42:29,479 - src.llm.client - INFO - [dia:a511f4] ğŸ“Š 8.0s: 1408c @175c/s (471ch, ~352t @44t/s)
2025-12-16 11:42:30,144 - src.llm.client - INFO - [dia:a511f4] âœ“ Done 10.64s: 1492c (~201w @140c/s)
2025-12-16 11:42:30,145 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Desiccation (Grafting Troubleshooting & Maintenance):
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO -     - Length: 1092 chars (cleaned: 1092 chars)
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO - [OK] Elements: 74 total (nodes: 26, connections: 48) âœ“
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:42:30,146 - src.generate.formats.diagrams - INFO - Generated diagram: 1092 characters
2025-12-16 11:42:31,860 - src.llm.request_handler - INFO - [dia:cc6878] âœ“ Done 12.35s
2025-12-16 11:42:31,861 - src.llm.client - INFO - [dia:cc6878] âœ… HTTP 200 in 12.35s
2025-12-16 11:42:31,861 - src.llm.client - INFO - [dia:cc6878] ğŸ“¡ Stream active (200)
2025-12-16 11:42:31,861 - src.llm.client - INFO - [dia:cc6878] Starting stream parsing, waiting for first chunk...
2025-12-16 11:42:33,877 - src.llm.client - INFO - [dia:cc6878] ğŸ“Š 2.0s: 429c @213c/s (117ch, ~107t @53t/s)
2025-12-16 11:42:35,881 - src.llm.client - INFO - [dia:cc6878] ğŸ“Š 4.0s: 888c @221c/s (234ch, ~222t @55t/s)
2025-12-16 11:42:37,891 - src.llm.client - INFO - [dia:cc6878] ğŸ“Š 6.0s: 1189c @197c/s (354ch, ~297t @49t/s)
2025-12-16 11:42:39,177 - src.llm.client - INFO - [dia:cc6878] âœ“ Done 19.67s: 1346c (~172w @68c/s)
2025-12-16 11:42:39,180 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Poor Cambial Contact (Grafting Troubleshooting & Maintenance):
2025-12-16 11:42:39,181 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:42:39,181 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 11:42:39,181 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 11:42:39,182 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 11:42:39,182 - src.generate.formats.diagrams - INFO -     - Length: 923 chars (cleaned: 923 chars)
2025-12-16 11:42:39,182 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:42:39,182 - src.generate.formats.diagrams - INFO - [OK] Elements: 46 total (nodes: 17, connections: 29) âœ“
2025-12-16 11:42:39,182 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 11:42:39,182 - src.generate.formats.diagrams - INFO - Generated diagram: 923 characters
2025-12-16 11:42:40,867 - src.llm.request_handler - INFO - [dia:ec0977] âœ“ Done 21.36s
2025-12-16 11:42:40,868 - src.llm.client - INFO - [dia:ec0977] âœ… HTTP 200 in 21.36s
2025-12-16 11:42:40,868 - src.llm.client - INFO - [dia:ec0977] ğŸ“¡ Stream active (200)
2025-12-16 11:42:40,868 - src.llm.client - INFO - [dia:ec0977] Starting stream parsing, waiting for first chunk...
2025-12-16 11:42:42,887 - src.llm.client - INFO - [dia:ec0977] ğŸ“Š 2.0s: 362c @179c/s (106ch, ~90t @45t/s)
2025-12-16 11:42:44,900 - src.llm.client - INFO - [dia:ec0977] ğŸ“Š 4.0s: 790c @196c/s (216ch, ~198t @49t/s)
2025-12-16 11:42:46,242 - src.llm.client - INFO - [dia:ec0977] âœ“ Done 26.73s: 1046c (~147w @39c/s)
2025-12-16 11:42:46,242 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Disease Transmission (Grafting Troubleshooting & Maintenance):
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 3 long nodes) âš ï¸
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - INFO -     - Length: 1031 chars (cleaned: 1031 chars)
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 54 total (nodes: 22, connections: 32) âš ï¸
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 3 long nodes) âš ï¸
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - INFO -   Cleanup summary: 2 issues fixed (code fences, style commands, etc.)
2025-12-16 11:42:46,243 - src.generate.formats.diagrams - INFO - Generated diagram: 1031 characters
2025-12-16 11:42:46,245 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 11:42:46,245 - src.generate.formats.questions - INFO - Generating 10 questions for: Grafting Troubleshooting & Maintenance (Session 10)
2025-12-16 11:42:46,245 - src.llm.client - INFO - [qst:fdd383] ğŸš€ qst | m=gemma3:4b | p=7359c | t=150s
2025-12-16 11:42:46,245 - src.llm.client - INFO - [qst:fdd383] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 11:42:46,245 - src.llm.client - INFO - [qst:fdd383] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 11:42:46,249 - src.llm.client - INFO - [qst:fdd383] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11055 bytes, prompt=7359 chars
2025-12-16 11:42:46,249 - src.llm.client - INFO - [qst:fdd383] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 11:42:48,531 - src.llm.request_handler - INFO - [qst:fdd383] âœ“ Done 2.28s
2025-12-16 11:42:48,543 - src.llm.client - INFO - [qst:fdd383] âœ… HTTP 200 in 2.29s
2025-12-16 11:42:48,545 - src.llm.client - INFO - [qst:fdd383] ğŸ“¡ Stream active (200)
2025-12-16 11:42:48,553 - src.llm.client - INFO - [qst:fdd383] Starting stream parsing, waiting for first chunk...
2025-12-16 11:42:50,560 - src.llm.client - INFO - [qst:fdd383] ğŸ“Š 2.0s: 462c @230c/s (98ch, ~116t @58t/s)
2025-12-16 11:42:52,571 - src.llm.client - INFO - [qst:fdd383] ğŸ“Š 4.0s: 1016c @253c/s (212ch, ~254t @63t/s)
2025-12-16 11:42:54,581 - src.llm.client - INFO - [qst:fdd383] ğŸ“Š 6.0s: 1628c @270c/s (334ch, ~407t @68t/s)
2025-12-16 11:42:56,589 - src.llm.client - INFO - [qst:fdd383] ğŸ“Š 8.0s: 2211c @275c/s (456ch, ~553t @69t/s)
2025-12-16 11:42:58,593 - src.llm.client - INFO - [qst:fdd383] ğŸ“Š 10.0s: 2853c @284c/s (575ch, ~713t @71t/s)
2025-12-16 11:43:00,596 - src.llm.client - INFO - [qst:fdd383] ğŸ“Š 12.0s: 3495c @290c/s (690ch, ~874t @73t/s)
2025-12-16 11:43:02,203 - src.llm.client - INFO - [qst:fdd383] âœ“ Done 15.96s: 4000c (~557w @251c/s)
2025-12-16 11:43:02,204 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 0, 'total_fixes': 1}
2025-12-16 11:43:02,204 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-16 11:43:02,204 - src.generate.formats.questions - INFO - [COMPLIANT] Questions generated âœ“
2025-12-16 11:43:02,204 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-16 11:43:02,204 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-16 11:43:02,204 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-16 11:43:02,204 - src.generate.formats.questions - INFO - [OK] Question marks: 10 total, 10 questions with '?' âœ“
2025-12-16 11:43:02,205 - src.generate.formats.questions - INFO -     - Question length: avg 12.5 words (range: 7-24)
2025-12-16 11:43:02,205 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-16 11:43:02,207 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 11:43:02,209 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 10 completed
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - QUALITY SCORE SUMMARY
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - Average Quality Score: 97.5/100
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - Overall Quality: excellent
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - Quality Distribution: {'excellent': 10}
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - WARNING - Cross-session consistency: 2 issues found
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO -   Recommendation: Consider adding intermediate sessions to bridge 2 concept gaps
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - [ALL COMPLIANT] Primary Materials Generation - Summary âœ…
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 11:43:02,210 - src.generate.orchestration.pipeline - INFO -   Items Processed: 10
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT] Successful: 10
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - [ERROR] Failed: 0
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -   Compliance Breakdown:
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT]: 10
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - [NEEDS REVIEW]: 0
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - [CRITICAL]: 0
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -   Issue Statistics:
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - Total Issues: 0
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - Critical Errors: 0
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - Warnings: 0
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -   Recommendations:
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - All content generated successfully
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO -     - No issues detected
2025-12-16 11:43:02,211 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 11:43:02,211 - generate_primary - INFO - 
================================================================================
2025-12-16 11:43:02,211 - generate_primary - INFO - PRIMARY MATERIALS COMPLETE
2025-12-16 11:43:02,211 - generate_primary - INFO - ================================================================================
2025-12-16 11:43:02,211 - generate_primary - INFO - Total sessions processed: 10
2025-12-16 11:43:02,211 - generate_primary - INFO - Successful: 10
2025-12-16 11:43:02,211 - generate_primary - INFO - Failed: 0
2025-12-16 11:43:02,211 - generate_primary - INFO - 
================================================================================
2025-12-16 11:43:02,211 - generate_primary - INFO - EXIT CODE: 0 (SUCCESS)
2025-12-16 11:43:02,211 - generate_primary - INFO - ================================================================================
2025-12-16 11:43:02,211 - generate_primary - INFO - All sessions processed successfully with no critical issues
2025-12-16 11:43:02,211 - generate_primary - INFO - ================================================================================
