2025-12-15 13:05:43,556 - root - INFO - Logging to file: /Users/mini/Documents/GitHub/curriculum/scripts/output/logs/05_generate_secondary_20251215_130543.log
2025-12-15 13:05:43,556 - generate_secondary - INFO - 
2025-12-15 13:05:43,556 - generate_secondary - INFO - ğŸ”¬ STAGE 05: SECONDARY MATERIALS (Session-Level Synthesis)
2025-12-15 13:05:43,556 - generate_secondary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-15 13:05:43,556 - generate_secondary - INFO - Generating materials PER SESSION (with full session context)
2025-12-15 13:05:43,556 - generate_secondary - INFO - Reading all content from: [course-specific]/modules/module_XX/session_YY/
2025-12-15 13:05:43,556 - generate_secondary - INFO - Output structure: [course-specific]/modules/module_XX/session_YY/[type].md
2025-12-15 13:05:43,556 - generate_secondary - INFO - 
2025-12-15 13:05:43,556 - generate_secondary - INFO - SECONDARY TYPES GENERATED PER SESSION:
2025-12-15 13:05:43,556 - generate_secondary - INFO -   1. application.md - Real-world applications and case studies
2025-12-15 13:05:43,556 - generate_secondary - INFO -   2. extension.md - Advanced topics beyond core curriculum
2025-12-15 13:05:43,556 - generate_secondary - INFO -   3. visualization.mmd - Additional diagrams and concept maps (Mermaid format)
2025-12-15 13:05:43,556 - generate_secondary - INFO -   4. integration.md - Cross-module connections and synthesis
2025-12-15 13:05:43,556 - generate_secondary - INFO -   5. investigation.md - Research questions and experiments
2025-12-15 13:05:43,556 - generate_secondary - INFO -   6. open_questions.md - Current scientific debates and frontiers
2025-12-15 13:05:43,556 - generate_secondary - INFO - 
2025-12-15 13:05:43,556 - generate_secondary - INFO - 
2025-12-15 13:05:43,556 - generate_secondary - INFO - âš™ï¸ CONFIGURATION
2025-12-15 13:05:43,556 - generate_secondary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-15 13:05:43,556 - generate_secondary - INFO -   â€¢ Content Validation: DISABLED
2025-12-15 13:05:43,556 - generate_secondary - INFO -   â€¢ Dry Run: DISABLED
2025-12-15 13:05:43,556 - generate_secondary - INFO -   â€¢ Log File: output/logs/05_generate_secondary_20251215_130543.log
2025-12-15 13:05:43,556 - generate_secondary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-15 13:05:43,556 - src.config.loader - INFO - Initialized ConfigLoader with directory: /Users/mini/Documents/GitHub/curriculum/config
2025-12-15 13:05:43,557 - src.config.loader - INFO - Course configuration validated successfully
2025-12-15 13:05:43,568 - src.config.loader - INFO - All configurations validated successfully
2025-12-15 13:05:43,568 - generate_secondary - INFO - Using most recent outline from output/outlines/ or scripts/output/outlines/
2025-12-15 13:05:43,569 - src.config.loader - INFO - Found most recent outline: /Users/mini/Documents/GitHub/curriculum/scripts/output/active_inference/outlines/course_outline_20251215_115426.json
2025-12-15 13:05:43,569 - src.config.loader - INFO - Found most recent outline: /Users/mini/Documents/GitHub/curriculum/scripts/output/active_inference/outlines/course_outline_20251215_115426.json
2025-12-15 13:05:43,569 - src.config.loader - INFO - Loaded 10 modules from outline: course_outline_20251215_115426.json
2025-12-15 13:05:43,570 - generate_secondary - INFO - Using course-specific output directory: output/active_inference/
2025-12-15 13:05:43,570 - generate_secondary - INFO - Processing ALL modules
2025-12-15 13:05:43,570 - generate_secondary - INFO - Processing 10 modules (20 total sessions)
2025-12-15 13:05:43,570 - generate_secondary - INFO - Secondary types: application, extension, visualization, integration, investigation, open_questions
2025-12-15 13:05:43,570 - src.llm.client - INFO - Initialized OllamaClient: model=gemma3:4b, url=http://localhost:11434/api/generate
2025-12-15 13:05:43,570 - generate_secondary - INFO - 
============================================================
2025-12-15 13:05:43,570 - generate_secondary - INFO - [1/10] Module 1: Introduction to Active Inference (1 sessions)
2025-12-15 13:05:43,570 - generate_secondary - INFO - ============================================================
2025-12-15 13:05:43,570 - generate_secondary - INFO - 
  Session 1/20: Perception & Action Basics
2025-12-15 13:05:43,571 - generate_secondary - INFO - Generating application for session 1: Perception & Action Basics...
2025-12-15 13:05:43,571 - src.llm.client - INFO - [app:6a7b32] ğŸš€ app | m=gemma3:4b | p=32677c | t=150s
2025-12-15 13:05:43,571 - src.llm.client - INFO - [app:6a7b32] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:05:43,571 - src.llm.client - INFO - [app:6a7b32] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:05:43,575 - src.llm.client - INFO - [app:6a7b32] Sending request to Ollama: model=gemma3:4b, operation=application, payload=34833 bytes, prompt=32677 chars
2025-12-15 13:05:43,575 - src.llm.client - INFO - [app:6a7b32] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:05:53,970 - src.llm.request_handler - INFO - [app:6a7b32] âœ“ Done 10.40s
2025-12-15 13:05:53,971 - src.llm.client - INFO - [app:6a7b32] âœ… HTTP 200 in 10.40s
2025-12-15 13:05:53,971 - src.llm.client - INFO - [app:6a7b32] ğŸ“¡ Stream active (200)
2025-12-15 13:05:53,971 - src.llm.client - INFO - [app:6a7b32] Starting stream parsing, waiting for first chunk...
2025-12-15 13:05:55,984 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 2.0s: 381c @189c/s (65ch, ~95t @47t/s)
2025-12-15 13:05:57,994 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 4.0s: 773c @192c/s (130ch, ~193t @48t/s)
2025-12-15 13:06:00,000 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 6.0s: 1164c @193c/s (195ch, ~291t @48t/s)
2025-12-15 13:06:02,006 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 8.0s: 1567c @195c/s (260ch, ~392t @49t/s)
2025-12-15 13:06:04,010 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 10.0s: 1972c @196c/s (325ch, ~493t @49t/s)
2025-12-15 13:06:06,015 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 12.0s: 2351c @195c/s (390ch, ~588t @49t/s)
2025-12-15 13:06:08,024 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 14.1s: 2712c @193c/s (455ch, ~678t @48t/s)
2025-12-15 13:06:10,029 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 16.1s: 3114c @194c/s (520ch, ~778t @48t/s)
2025-12-15 13:06:12,033 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 18.1s: 3508c @194c/s (585ch, ~877t @49t/s)
2025-12-15 13:06:14,042 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 20.1s: 3895c @194c/s (650ch, ~974t @49t/s)
2025-12-15 13:06:16,050 - src.llm.client - INFO - [app:6a7b32] ğŸ“Š 22.1s: 4264c @193c/s (715ch, ~1066t @48t/s)
2025-12-15 13:06:17,982 - src.llm.client - INFO - [app:6a7b32] âœ“ Done 34.41s: 4561c (~610w @133c/s)
2025-12-15 13:06:17,987 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:06:17,987 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:06:17,987 - generate_secondary - INFO -     - Length: 4560 chars, 610 words
2025-12-15 13:06:17,987 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:06:17,987 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:06:17,987 - generate_secondary - INFO -     - Avg words per application: 117
2025-12-15 13:06:17,987 - generate_secondary - WARNING - [WARNING] Application 1 has 121 words (require 150-200, need 29 more words) âš ï¸
2025-12-15 13:06:17,987 - generate_secondary - WARNING - [WARNING] Application 2 has 116 words (require 150-200, need 34 more words) âš ï¸
2025-12-15 13:06:17,987 - generate_secondary - WARNING - [WARNING] Application 3 has 114 words (require 150-200, need 36 more words) âš ï¸
2025-12-15 13:06:17,987 - generate_secondary - WARNING - [WARNING] Application 4 has 108 words (require 150-200, need 42 more words) âš ï¸
2025-12-15 13:06:17,987 - generate_secondary - WARNING - [WARNING] Application 5 has 126 words (require 150-200, need 24 more words) âš ï¸
2025-12-15 13:06:17,988 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_01_introduction_to_active_inference/session_01/application.md
2025-12-15 13:06:17,988 - generate_secondary - INFO - Generating extension for session 1: Perception & Action Basics...
2025-12-15 13:06:17,988 - src.llm.client - INFO - [ext:295b72] ğŸš€ ext | m=gemma3:4b | p=26563c | t=120s
2025-12-15 13:06:17,988 - src.llm.client - INFO - [ext:295b72] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:06:17,988 - src.llm.client - INFO - [ext:295b72] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:06:17,989 - src.llm.client - INFO - [ext:295b72] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=31590 bytes, prompt=26563 chars
2025-12-15 13:06:17,989 - src.llm.client - INFO - [ext:295b72] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:06:28,185 - src.llm.request_handler - INFO - [ext:295b72] âœ“ Done 10.20s
2025-12-15 13:06:28,185 - src.llm.client - INFO - [ext:295b72] âœ… HTTP 200 in 10.20s
2025-12-15 13:06:28,185 - src.llm.client - INFO - [ext:295b72] ğŸ“¡ Stream active (200)
2025-12-15 13:06:28,185 - src.llm.client - INFO - [ext:295b72] Starting stream parsing, waiting for first chunk...
2025-12-15 13:06:30,200 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 2.0s: 399c @198c/s (65ch, ~100t @50t/s)
2025-12-15 13:06:32,204 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 4.0s: 786c @196c/s (130ch, ~196t @49t/s)
2025-12-15 13:06:34,209 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 6.0s: 1225c @203c/s (195ch, ~306t @51t/s)
2025-12-15 13:06:36,214 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 8.0s: 1620c @202c/s (260ch, ~405t @50t/s)
2025-12-15 13:06:38,223 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 10.0s: 2004c @200c/s (324ch, ~501t @50t/s)
2025-12-15 13:06:40,224 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 12.0s: 2421c @201c/s (389ch, ~605t @50t/s)
2025-12-15 13:06:42,226 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 14.0s: 2807c @200c/s (454ch, ~702t @50t/s)
2025-12-15 13:06:44,229 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 16.0s: 3212c @200c/s (519ch, ~803t @50t/s)
2025-12-15 13:06:46,235 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 18.0s: 3625c @201c/s (584ch, ~906t @50t/s)
2025-12-15 13:06:48,243 - src.llm.client - INFO - [ext:295b72] ğŸ“Š 20.1s: 4082c @204c/s (649ch, ~1020t @51t/s)
2025-12-15 13:06:49,780 - src.llm.client - INFO - [ext:295b72] âœ“ Done 31.79s: 4353c (~579w @137c/s)
2025-12-15 13:06:49,782 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:06:49,782 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:06:49,782 - generate_secondary - INFO -     - Length: 4353 chars, 579 words
2025-12-15 13:06:49,782 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:06:49,782 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:06:49,782 - generate_secondary - INFO -     - Avg words per topic: 187
2025-12-15 13:06:49,782 - generate_secondary - WARNING - [WARNING] Topic 1 has 196 words (exceeds 150 by 46 words - consider condensing) âš ï¸
2025-12-15 13:06:49,782 - generate_secondary - WARNING - [WARNING] Topic 2 has 185 words (exceeds 150 by 35 words - consider condensing) âš ï¸
2025-12-15 13:06:49,782 - generate_secondary - WARNING - [WARNING] Topic 3 has 181 words (exceeds 150 by 31 words - consider condensing) âš ï¸
2025-12-15 13:06:49,782 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_01_introduction_to_active_inference/session_01/extension.md
2025-12-15 13:06:49,782 - generate_secondary - INFO - Generating visualization for session 1: Perception & Action Basics...
2025-12-15 13:06:49,783 - src.llm.client - INFO - [viz:82c707] ğŸš€ viz | m=gemma3:4b | p=25523c | t=120s
2025-12-15 13:06:49,783 - src.llm.client - INFO - [viz:82c707] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:06:49,783 - src.llm.client - INFO - [viz:82c707] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:06:49,784 - src.llm.client - INFO - [viz:82c707] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=29872 bytes, prompt=25523 chars
2025-12-15 13:06:49,784 - src.llm.client - INFO - [viz:82c707] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:06:59,785 - src.llm.request_handler - INFO - [viz:82c707] âœ“ Done 10.00s
2025-12-15 13:06:59,785 - src.llm.client - INFO - [viz:82c707] âœ… HTTP 200 in 10.00s
2025-12-15 13:06:59,785 - src.llm.client - INFO - [viz:82c707] ğŸ“¡ Stream active (200)
2025-12-15 13:06:59,786 - src.llm.client - INFO - [viz:82c707] Starting stream parsing, waiting for first chunk...
2025-12-15 13:07:01,793 - src.llm.client - INFO - [viz:82c707] ğŸ“Š 2.0s: 260c @129c/s (65ch, ~65t @32t/s)
2025-12-15 13:07:03,798 - src.llm.client - INFO - [viz:82c707] ğŸ“Š 4.0s: 462c @115c/s (130ch, ~116t @29t/s)
2025-12-15 13:07:05,803 - src.llm.client - INFO - [viz:82c707] ğŸ“Š 6.0s: 680c @113c/s (195ch, ~170t @28t/s)
2025-12-15 13:07:07,808 - src.llm.client - INFO - [viz:82c707] ğŸ“Š 8.0s: 953c @119c/s (260ch, ~238t @30t/s)
2025-12-15 13:07:09,567 - src.llm.client - INFO - [viz:82c707] âœ“ Done 19.78s: 1195c (~180w @60c/s)
2025-12-15 13:07:09,568 - src.generate.processors.cleanup - INFO - Cleanup complete: 4 issues before, 0 issues after
2025-12-15 13:07:09,568 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:07:09,568 - generate_secondary - INFO -     - Length: 362 chars (cleaned: 362 chars)
2025-12-15 13:07:09,568 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:07:09,568 - generate_secondary - INFO - [OK] Elements: 26 total (nodes: 11, connections: 15) âœ“
2025-12-15 13:07:09,569 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_01_introduction_to_active_inference/session_01/visualization.mmd
2025-12-15 13:07:09,569 - generate_secondary - INFO - Generating integration for session 1: Perception & Action Basics...
2025-12-15 13:07:09,569 - src.llm.client - INFO - [int:0a0b11] ğŸš€ int | m=gemma3:4b | p=26872c | t=150s
2025-12-15 13:07:09,569 - src.llm.client - INFO - [int:0a0b11] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:07:09,569 - src.llm.client - INFO - [int:0a0b11] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:07:09,570 - src.llm.client - INFO - [int:0a0b11] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=32238 bytes, prompt=26872 chars
2025-12-15 13:07:09,570 - src.llm.client - INFO - [int:0a0b11] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:07:19,381 - src.llm.request_handler - INFO - [int:0a0b11] âœ“ Done 9.81s
2025-12-15 13:07:19,381 - src.llm.client - INFO - [int:0a0b11] âœ… HTTP 200 in 9.81s
2025-12-15 13:07:19,381 - src.llm.client - INFO - [int:0a0b11] ğŸ“¡ Stream active (200)
2025-12-15 13:07:19,382 - src.llm.client - INFO - [int:0a0b11] Starting stream parsing, waiting for first chunk...
2025-12-15 13:07:21,398 - src.llm.client - INFO - [int:0a0b11] ğŸ“Š 2.0s: 355c @176c/s (65ch, ~89t @44t/s)
2025-12-15 13:07:23,407 - src.llm.client - INFO - [int:0a0b11] ğŸ“Š 4.0s: 731c @182c/s (130ch, ~183t @45t/s)
2025-12-15 13:07:25,413 - src.llm.client - INFO - [int:0a0b11] ğŸ“Š 6.0s: 1148c @190c/s (195ch, ~287t @48t/s)
2025-12-15 13:07:27,416 - src.llm.client - INFO - [int:0a0b11] ğŸ“Š 8.0s: 1567c @195c/s (260ch, ~392t @49t/s)
2025-12-15 13:07:29,421 - src.llm.client - INFO - [int:0a0b11] ğŸ“Š 10.0s: 1854c @185c/s (325ch, ~464t @46t/s)
2025-12-15 13:07:31,639 - src.llm.client - INFO - [int:0a0b11] ğŸ“Š 12.3s: 2171c @177c/s (388ch, ~543t @44t/s)
2025-12-15 13:07:31,640 - src.llm.client - INFO - [int:0a0b11] âœ“ Done 22.07s: 2171c (~288w @98c/s)
2025-12-15 13:07:31,641 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:07:31,641 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:07:31,641 - generate_secondary - INFO -     - Length: 2170 chars, 288 words
2025-12-15 13:07:31,641 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:07:31,642 - generate_secondary - INFO -     - Connections: 10
2025-12-15 13:07:31,642 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:07:31,642 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_01_introduction_to_active_inference/session_01/integration.md
2025-12-15 13:07:31,642 - generate_secondary - INFO - Generating investigation for session 1: Perception & Action Basics...
2025-12-15 13:07:31,642 - src.llm.client - INFO - [inv:7ade18] ğŸš€ inv | m=gemma3:4b | p=25785c | t=150s
2025-12-15 13:07:31,642 - src.llm.client - INFO - [inv:7ade18] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:07:31,642 - src.llm.client - INFO - [inv:7ade18] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:07:31,643 - src.llm.client - INFO - [inv:7ade18] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30094 bytes, prompt=25785 chars
2025-12-15 13:07:31,643 - src.llm.client - INFO - [inv:7ade18] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:07:41,998 - src.llm.request_handler - INFO - [inv:7ade18] âœ“ Done 10.36s
2025-12-15 13:07:41,999 - src.llm.client - INFO - [inv:7ade18] âœ… HTTP 200 in 10.36s
2025-12-15 13:07:41,999 - src.llm.client - INFO - [inv:7ade18] ğŸ“¡ Stream active (200)
2025-12-15 13:07:41,999 - src.llm.client - INFO - [inv:7ade18] Starting stream parsing, waiting for first chunk...
2025-12-15 13:07:44,012 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 2.0s: 335c @166c/s (65ch, ~84t @42t/s)
2025-12-15 13:07:46,014 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 4.0s: 698c @174c/s (130ch, ~174t @43t/s)
2025-12-15 13:07:48,016 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 6.0s: 1030c @171c/s (195ch, ~258t @43t/s)
2025-12-15 13:07:50,018 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 8.0s: 1422c @177c/s (260ch, ~356t @44t/s)
2025-12-15 13:07:52,018 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 10.0s: 1833c @183c/s (325ch, ~458t @46t/s)
2025-12-15 13:07:54,021 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 12.0s: 2153c @179c/s (390ch, ~538t @45t/s)
2025-12-15 13:07:56,021 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 14.0s: 2488c @177c/s (455ch, ~622t @44t/s)
2025-12-15 13:07:58,026 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 16.0s: 2814c @176c/s (520ch, ~704t @44t/s)
2025-12-15 13:08:00,036 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 18.0s: 3166c @176c/s (585ch, ~792t @44t/s)
2025-12-15 13:08:02,047 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 20.0s: 3563c @178c/s (650ch, ~891t @44t/s)
2025-12-15 13:08:04,057 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 22.1s: 3928c @178c/s (715ch, ~982t @45t/s)
2025-12-15 13:08:06,064 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 24.1s: 4292c @178c/s (780ch, ~1073t @45t/s)
2025-12-15 13:08:08,080 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 26.1s: 4599c @176c/s (845ch, ~1150t @44t/s)
2025-12-15 13:08:10,095 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 28.1s: 4953c @176c/s (910ch, ~1238t @44t/s)
2025-12-15 13:08:12,115 - src.llm.client - INFO - [inv:7ade18] ğŸ“Š 30.1s: 5348c @178c/s (975ch, ~1337t @44t/s)
2025-12-15 13:08:14,009 - src.llm.client - INFO - [inv:7ade18] âœ“ Done 42.37s: 5678c (~816w @134c/s)
2025-12-15 13:08:14,012 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:08:14,012 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:08:14,012 - generate_secondary - INFO -     - Length: 5673 chars, 816 words
2025-12-15 13:08:14,012 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:08:14,012 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:08:14,012 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:08:14,013 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_01_introduction_to_active_inference/session_01/investigation.md
2025-12-15 13:08:14,013 - generate_secondary - INFO - Generating open_questions for session 1: Perception & Action Basics...
2025-12-15 13:08:14,013 - src.llm.client - INFO - [opq:342ea7] ğŸš€ opq | m=gemma3:4b | p=25871c | t=150s
2025-12-15 13:08:14,013 - src.llm.client - INFO - [opq:342ea7] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:08:14,013 - src.llm.client - INFO - [opq:342ea7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:08:14,014 - src.llm.client - INFO - [opq:342ea7] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=30191 bytes, prompt=25871 chars
2025-12-15 13:08:14,014 - src.llm.client - INFO - [opq:342ea7] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:08:24,013 - src.llm.request_handler - INFO - [opq:342ea7] âœ“ Done 10.00s
2025-12-15 13:08:24,013 - src.llm.client - INFO - [opq:342ea7] âœ… HTTP 200 in 10.00s
2025-12-15 13:08:24,013 - src.llm.client - INFO - [opq:342ea7] ğŸ“¡ Stream active (200)
2025-12-15 13:08:24,013 - src.llm.client - INFO - [opq:342ea7] Starting stream parsing, waiting for first chunk...
2025-12-15 13:08:26,029 - src.llm.client - INFO - [opq:342ea7] ğŸ“Š 2.0s: 361c @179c/s (65ch, ~90t @45t/s)
2025-12-15 13:08:28,032 - src.llm.client - INFO - [opq:342ea7] ğŸ“Š 4.0s: 775c @193c/s (130ch, ~194t @48t/s)
2025-12-15 13:08:30,033 - src.llm.client - INFO - [opq:342ea7] ğŸ“Š 6.0s: 1155c @192c/s (195ch, ~289t @48t/s)
2025-12-15 13:08:32,034 - src.llm.client - INFO - [opq:342ea7] ğŸ“Š 8.0s: 1596c @199c/s (260ch, ~399t @50t/s)
2025-12-15 13:08:34,038 - src.llm.client - INFO - [opq:342ea7] ğŸ“Š 10.0s: 1994c @199c/s (325ch, ~498t @50t/s)
2025-12-15 13:08:36,049 - src.llm.client - INFO - [opq:342ea7] ğŸ“Š 12.0s: 2396c @199c/s (390ch, ~599t @50t/s)
2025-12-15 13:08:37,081 - src.llm.client - INFO - [opq:342ea7] âœ“ Done 23.07s: 2574c (~326w @112c/s)
2025-12-15 13:08:37,083 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:08:37,083 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:08:37,083 - generate_secondary - INFO -     - Length: 2573 chars, 326 words
2025-12-15 13:08:37,083 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:08:37,083 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:08:37,083 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:08:37,083 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_01_introduction_to_active_inference/session_01/open_questions.md
2025-12-15 13:08:37,083 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:08:37,083 - generate_secondary - INFO - 
============================================================
2025-12-15 13:08:37,083 - generate_secondary - INFO - [2/10] Module 2: Bayesian Mechanics (2 sessions)
2025-12-15 13:08:37,083 - generate_secondary - INFO - ============================================================
2025-12-15 13:08:37,083 - generate_secondary - INFO - 
  Session 2/20: Probability Theory Review
2025-12-15 13:08:37,084 - generate_secondary - INFO - Generating application for session 2: Probability Theory Review...
2025-12-15 13:08:37,084 - src.llm.client - INFO - [app:16f0a8] ğŸš€ app | m=gemma3:4b | p=26994c | t=150s
2025-12-15 13:08:37,084 - src.llm.client - INFO - [app:16f0a8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:08:37,084 - src.llm.client - INFO - [app:16f0a8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:08:37,085 - src.llm.client - INFO - [app:16f0a8] Sending request to Ollama: model=gemma3:4b, operation=application, payload=28888 bytes, prompt=26994 chars
2025-12-15 13:08:37,085 - src.llm.client - INFO - [app:16f0a8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:08:47,840 - src.llm.request_handler - INFO - [app:16f0a8] âœ“ Done 10.76s
2025-12-15 13:08:47,840 - src.llm.client - INFO - [app:16f0a8] âœ… HTTP 200 in 10.76s
2025-12-15 13:08:47,840 - src.llm.client - INFO - [app:16f0a8] ğŸ“¡ Stream active (200)
2025-12-15 13:08:47,841 - src.llm.client - INFO - [app:16f0a8] Starting stream parsing, waiting for first chunk...
2025-12-15 13:08:49,843 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 2.0s: 363c @181c/s (64ch, ~91t @45t/s)
2025-12-15 13:08:51,846 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 4.0s: 745c @186c/s (128ch, ~186t @46t/s)
2025-12-15 13:08:53,851 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 6.0s: 1144c @190c/s (193ch, ~286t @48t/s)
2025-12-15 13:08:55,864 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 8.0s: 1526c @190c/s (258ch, ~382t @48t/s)
2025-12-15 13:08:57,879 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 10.0s: 1951c @194c/s (323ch, ~488t @49t/s)
2025-12-15 13:08:59,895 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 12.1s: 2381c @198c/s (388ch, ~595t @49t/s)
2025-12-15 13:09:01,914 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 14.1s: 2799c @199c/s (453ch, ~700t @50t/s)
2025-12-15 13:09:03,932 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 16.1s: 3199c @199c/s (518ch, ~800t @50t/s)
2025-12-15 13:09:05,953 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 18.1s: 3626c @200c/s (583ch, ~906t @50t/s)
2025-12-15 13:09:07,977 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 20.1s: 4066c @202c/s (648ch, ~1016t @50t/s)
2025-12-15 13:09:10,000 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 22.2s: 4463c @201c/s (713ch, ~1116t @50t/s)
2025-12-15 13:09:12,023 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 24.2s: 4829c @200c/s (778ch, ~1207t @50t/s)
2025-12-15 13:09:14,046 - src.llm.client - INFO - [app:16f0a8] ğŸ“Š 26.2s: 5201c @198c/s (843ch, ~1300t @50t/s)
2025-12-15 13:09:15,644 - src.llm.client - INFO - [app:16f0a8] âœ“ Done 38.56s: 5494c (~739w @142c/s)
2025-12-15 13:09:15,646 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:09:15,646 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:09:15,646 - generate_secondary - INFO -     - Length: 5483 chars, 737 words
2025-12-15 13:09:15,646 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:09:15,646 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:09:15,646 - generate_secondary - INFO -     - Avg words per application: 143
2025-12-15 13:09:15,646 - generate_secondary - WARNING - [WARNING] Application 3 has 139 words (require 150-200, need 11 more words) âš ï¸
2025-12-15 13:09:15,646 - generate_secondary - WARNING - [WARNING] Application 4 has 130 words (require 150-200, need 20 more words) âš ï¸
2025-12-15 13:09:15,646 - generate_secondary - WARNING - [WARNING] Application 5 has 141 words (require 150-200, need 9 more words) âš ï¸
2025-12-15 13:09:15,647 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_02/application.md
2025-12-15 13:09:15,647 - generate_secondary - INFO - Generating extension for session 2: Probability Theory Review...
2025-12-15 13:09:15,647 - src.llm.client - INFO - [ext:e239ca] ğŸš€ ext | m=gemma3:4b | p=20880c | t=120s
2025-12-15 13:09:15,647 - src.llm.client - INFO - [ext:e239ca] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:09:15,647 - src.llm.client - INFO - [ext:e239ca] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:09:15,648 - src.llm.client - INFO - [ext:e239ca] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=25645 bytes, prompt=20880 chars
2025-12-15 13:09:15,648 - src.llm.client - INFO - [ext:e239ca] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:09:25,984 - src.llm.request_handler - INFO - [ext:e239ca] âœ“ Done 10.34s
2025-12-15 13:09:25,984 - src.llm.client - INFO - [ext:e239ca] âœ… HTTP 200 in 10.34s
2025-12-15 13:09:25,984 - src.llm.client - INFO - [ext:e239ca] ğŸ“¡ Stream active (200)
2025-12-15 13:09:25,984 - src.llm.client - INFO - [ext:e239ca] Starting stream parsing, waiting for first chunk...
2025-12-15 13:09:28,005 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 2.0s: 388c @192c/s (65ch, ~97t @48t/s)
2025-12-15 13:09:30,015 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 4.0s: 778c @193c/s (130ch, ~194t @48t/s)
2025-12-15 13:09:32,025 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 6.0s: 1218c @202c/s (195ch, ~304t @50t/s)
2025-12-15 13:09:34,037 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 8.1s: 1651c @205c/s (260ch, ~413t @51t/s)
2025-12-15 13:09:36,056 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 10.1s: 2121c @211c/s (325ch, ~530t @53t/s)
2025-12-15 13:09:38,080 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 12.1s: 2518c @208c/s (389ch, ~630t @52t/s)
2025-12-15 13:09:40,093 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 14.1s: 2950c @209c/s (454ch, ~738t @52t/s)
2025-12-15 13:09:42,224 - src.llm.client - INFO - [ext:e239ca] ğŸ“Š 16.2s: 3368c @207c/s (516ch, ~842t @52t/s)
2025-12-15 13:09:42,225 - src.llm.client - INFO - [ext:e239ca] âœ“ Done 26.58s: 3368c (~421w @127c/s)
2025-12-15 13:09:42,226 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:09:42,226 - generate_secondary - INFO - [COMPLIANT] Extension generated âœ“
2025-12-15 13:09:42,227 - generate_secondary - INFO -     - Length: 3368 chars, 421 words
2025-12-15 13:09:42,227 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:09:42,227 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:09:42,227 - generate_secondary - INFO -     - Avg words per topic: 133
2025-12-15 13:09:42,227 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_02/extension.md
2025-12-15 13:09:42,228 - generate_secondary - INFO - Generating visualization for session 2: Probability Theory Review...
2025-12-15 13:09:42,228 - src.llm.client - INFO - [viz:cab49d] ğŸš€ viz | m=gemma3:4b | p=19840c | t=120s
2025-12-15 13:09:42,228 - src.llm.client - INFO - [viz:cab49d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:09:42,228 - src.llm.client - INFO - [viz:cab49d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:09:42,229 - src.llm.client - INFO - [viz:cab49d] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=23927 bytes, prompt=19840 chars
2025-12-15 13:09:42,229 - src.llm.client - INFO - [viz:cab49d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:09:52,026 - src.llm.request_handler - INFO - [viz:cab49d] âœ“ Done 9.80s
2025-12-15 13:09:52,026 - src.llm.client - INFO - [viz:cab49d] âœ… HTTP 200 in 9.80s
2025-12-15 13:09:52,026 - src.llm.client - INFO - [viz:cab49d] ğŸ“¡ Stream active (200)
2025-12-15 13:09:52,026 - src.llm.client - INFO - [viz:cab49d] Starting stream parsing, waiting for first chunk...
2025-12-15 13:09:54,045 - src.llm.client - INFO - [viz:cab49d] ğŸ“Š 2.0s: 283c @140c/s (65ch, ~71t @35t/s)
2025-12-15 13:09:56,059 - src.llm.client - INFO - [viz:cab49d] ğŸ“Š 4.0s: 522c @129c/s (130ch, ~130t @32t/s)
2025-12-15 13:09:57,474 - src.llm.client - INFO - [viz:cab49d] âœ“ Done 15.25s: 686c (~96w @45c/s)
2025-12-15 13:09:57,475 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:09:57,475 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:09:57,475 - generate_secondary - INFO -     - Length: 395 chars (cleaned: 395 chars)
2025-12-15 13:09:57,475 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:09:57,475 - generate_secondary - INFO - [OK] Elements: 23 total (nodes: 10, connections: 13) âœ“
2025-12-15 13:09:57,475 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_02/visualization.mmd
2025-12-15 13:09:57,475 - generate_secondary - INFO - Generating integration for session 2: Probability Theory Review...
2025-12-15 13:09:57,475 - src.llm.client - INFO - [int:753fcd] ğŸš€ int | m=gemma3:4b | p=21189c | t=150s
2025-12-15 13:09:57,475 - src.llm.client - INFO - [int:753fcd] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:09:57,475 - src.llm.client - INFO - [int:753fcd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:09:57,476 - src.llm.client - INFO - [int:753fcd] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=26293 bytes, prompt=21189 chars
2025-12-15 13:09:57,476 - src.llm.client - INFO - [int:753fcd] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:10:07,276 - src.llm.request_handler - INFO - [int:753fcd] âœ“ Done 9.80s
2025-12-15 13:10:07,276 - src.llm.client - INFO - [int:753fcd] âœ… HTTP 200 in 9.80s
2025-12-15 13:10:07,276 - src.llm.client - INFO - [int:753fcd] ğŸ“¡ Stream active (200)
2025-12-15 13:10:07,276 - src.llm.client - INFO - [int:753fcd] Starting stream parsing, waiting for first chunk...
2025-12-15 13:10:09,299 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 2.0s: 383c @189c/s (65ch, ~96t @47t/s)
2025-12-15 13:10:11,311 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 4.0s: 745c @185c/s (130ch, ~186t @46t/s)
2025-12-15 13:10:13,323 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 6.0s: 1124c @186c/s (195ch, ~281t @46t/s)
2025-12-15 13:10:15,332 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 8.1s: 1506c @187c/s (260ch, ~376t @47t/s)
2025-12-15 13:10:17,347 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 10.1s: 1919c @191c/s (325ch, ~480t @48t/s)
2025-12-15 13:10:19,364 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 12.1s: 2295c @190c/s (390ch, ~574t @47t/s)
2025-12-15 13:10:21,377 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 14.1s: 2672c @189c/s (455ch, ~668t @47t/s)
2025-12-15 13:10:23,398 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 16.1s: 3052c @189c/s (520ch, ~763t @47t/s)
2025-12-15 13:10:25,516 - src.llm.client - INFO - [int:753fcd] ğŸ“Š 18.2s: 3395c @186c/s (581ch, ~849t @47t/s)
2025-12-15 13:10:25,517 - src.llm.client - INFO - [int:753fcd] âœ“ Done 28.04s: 3395c (~471w @121c/s)
2025-12-15 13:10:25,518 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:10:25,519 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:10:25,519 - generate_secondary - INFO -     - Length: 3395 chars, 471 words
2025-12-15 13:10:25,519 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:10:25,519 - generate_secondary - INFO -     - Connections: 24
2025-12-15 13:10:25,519 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:10:25,519 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_02/integration.md
2025-12-15 13:10:25,519 - generate_secondary - INFO - Generating investigation for session 2: Probability Theory Review...
2025-12-15 13:10:25,519 - src.llm.client - INFO - [inv:1cb2c6] ğŸš€ inv | m=gemma3:4b | p=20102c | t=150s
2025-12-15 13:10:25,519 - src.llm.client - INFO - [inv:1cb2c6] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:10:25,519 - src.llm.client - INFO - [inv:1cb2c6] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:10:25,521 - src.llm.client - INFO - [inv:1cb2c6] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=24149 bytes, prompt=20102 chars
2025-12-15 13:10:25,521 - src.llm.client - INFO - [inv:1cb2c6] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:10:36,152 - src.llm.request_handler - INFO - [inv:1cb2c6] âœ“ Done 10.63s
2025-12-15 13:10:36,153 - src.llm.client - INFO - [inv:1cb2c6] âœ… HTTP 200 in 10.63s
2025-12-15 13:10:36,153 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“¡ Stream active (200)
2025-12-15 13:10:36,153 - src.llm.client - INFO - [inv:1cb2c6] Starting stream parsing, waiting for first chunk...
2025-12-15 13:10:38,176 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 2.0s: 353c @174c/s (64ch, ~88t @44t/s)
2025-12-15 13:10:40,184 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 4.0s: 683c @169c/s (129ch, ~171t @42t/s)
2025-12-15 13:10:42,190 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 6.0s: 1016c @168c/s (194ch, ~254t @42t/s)
2025-12-15 13:10:44,202 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 8.0s: 1415c @176c/s (259ch, ~354t @44t/s)
2025-12-15 13:10:46,215 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 10.1s: 1777c @177c/s (324ch, ~444t @44t/s)
2025-12-15 13:10:48,233 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 12.1s: 2213c @183c/s (389ch, ~553t @46t/s)
2025-12-15 13:10:50,245 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 14.1s: 2582c @183c/s (454ch, ~646t @46t/s)
2025-12-15 13:10:52,263 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 16.1s: 2924c @182c/s (519ch, ~731t @45t/s)
2025-12-15 13:10:54,283 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 18.1s: 3301c @182c/s (584ch, ~825t @46t/s)
2025-12-15 13:10:56,306 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 20.2s: 3683c @183c/s (649ch, ~921t @46t/s)
2025-12-15 13:10:58,330 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 22.2s: 4096c @185c/s (714ch, ~1024t @46t/s)
2025-12-15 13:11:00,349 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 24.2s: 4425c @183c/s (779ch, ~1106t @46t/s)
2025-12-15 13:11:02,369 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 26.2s: 4804c @183c/s (844ch, ~1201t @46t/s)
2025-12-15 13:11:04,397 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 28.2s: 5159c @183c/s (909ch, ~1290t @46t/s)
2025-12-15 13:11:06,428 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 30.3s: 5527c @183c/s (974ch, ~1382t @46t/s)
2025-12-15 13:11:08,435 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 32.3s: 5898c @183c/s (1038ch, ~1474t @46t/s)
2025-12-15 13:11:10,460 - src.llm.client - INFO - [inv:1cb2c6] ğŸ“Š 34.3s: 6304c @184c/s (1103ch, ~1576t @46t/s)
2025-12-15 13:11:11,983 - src.llm.client - INFO - [inv:1cb2c6] âœ“ Done 46.46s: 6540c (~890w @141c/s)
2025-12-15 13:11:11,986 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:11:11,986 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:11:11,986 - generate_secondary - INFO -     - Length: 6527 chars, 888 words
2025-12-15 13:11:11,986 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:11:11,986 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:11:11,986 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:11:11,986 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_02/investigation.md
2025-12-15 13:11:11,986 - generate_secondary - INFO - Generating open_questions for session 2: Probability Theory Review...
2025-12-15 13:11:11,986 - src.llm.client - INFO - [opq:b6cba8] ğŸš€ opq | m=gemma3:4b | p=20188c | t=150s
2025-12-15 13:11:11,986 - src.llm.client - INFO - [opq:b6cba8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:11:11,986 - src.llm.client - INFO - [opq:b6cba8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:11:11,988 - src.llm.client - INFO - [opq:b6cba8] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=24246 bytes, prompt=20188 chars
2025-12-15 13:11:11,988 - src.llm.client - INFO - [opq:b6cba8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:11:21,785 - src.llm.request_handler - INFO - [opq:b6cba8] âœ“ Done 9.80s
2025-12-15 13:11:21,785 - src.llm.client - INFO - [opq:b6cba8] âœ… HTTP 200 in 9.80s
2025-12-15 13:11:21,786 - src.llm.client - INFO - [opq:b6cba8] ğŸ“¡ Stream active (200)
2025-12-15 13:11:21,786 - src.llm.client - INFO - [opq:b6cba8] Starting stream parsing, waiting for first chunk...
2025-12-15 13:11:23,807 - src.llm.client - INFO - [opq:b6cba8] ğŸ“Š 2.0s: 348c @172c/s (65ch, ~87t @43t/s)
2025-12-15 13:11:25,821 - src.llm.client - INFO - [opq:b6cba8] ğŸ“Š 4.0s: 772c @191c/s (130ch, ~193t @48t/s)
2025-12-15 13:11:27,838 - src.llm.client - INFO - [opq:b6cba8] ğŸ“Š 6.1s: 1128c @186c/s (195ch, ~282t @47t/s)
2025-12-15 13:11:29,845 - src.llm.client - INFO - [opq:b6cba8] ğŸ“Š 8.1s: 1555c @193c/s (260ch, ~389t @48t/s)
2025-12-15 13:11:31,857 - src.llm.client - INFO - [opq:b6cba8] ğŸ“Š 10.1s: 1928c @191c/s (325ch, ~482t @48t/s)
2025-12-15 13:11:33,756 - src.llm.client - INFO - [opq:b6cba8] âœ“ Done 21.77s: 2289c (~287w @105c/s)
2025-12-15 13:11:33,757 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:11:33,757 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:11:33,757 - generate_secondary - INFO -     - Length: 2275 chars, 285 words
2025-12-15 13:11:33,757 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:11:33,757 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:11:33,757 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:11:33,758 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_02/open_questions.md
2025-12-15 13:11:33,758 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:11:33,758 - generate_secondary - INFO - 
  Session 3/20: Bayesian Inference
2025-12-15 13:11:33,758 - generate_secondary - INFO - Generating application for session 3: Bayesian Inference...
2025-12-15 13:11:33,758 - src.llm.client - INFO - [app:700f0c] ğŸš€ app | m=gemma3:4b | p=28873c | t=150s
2025-12-15 13:11:33,758 - src.llm.client - INFO - [app:700f0c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:11:33,758 - src.llm.client - INFO - [app:700f0c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:11:33,759 - src.llm.client - INFO - [app:700f0c] Sending request to Ollama: model=gemma3:4b, operation=application, payload=30809 bytes, prompt=28873 chars
2025-12-15 13:11:33,759 - src.llm.client - INFO - [app:700f0c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:11:43,576 - src.llm.request_handler - INFO - [app:700f0c] âœ“ Done 9.82s
2025-12-15 13:11:43,576 - src.llm.client - INFO - [app:700f0c] âœ… HTTP 200 in 9.82s
2025-12-15 13:11:43,576 - src.llm.client - INFO - [app:700f0c] ğŸ“¡ Stream active (200)
2025-12-15 13:11:43,576 - src.llm.client - INFO - [app:700f0c] Starting stream parsing, waiting for first chunk...
2025-12-15 13:11:45,592 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 2.0s: 396c @196c/s (65ch, ~99t @49t/s)
2025-12-15 13:11:47,606 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 4.0s: 774c @192c/s (130ch, ~194t @48t/s)
2025-12-15 13:11:49,618 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 6.0s: 1157c @192c/s (195ch, ~289t @48t/s)
2025-12-15 13:11:51,629 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 8.1s: 1538c @191c/s (260ch, ~384t @48t/s)
2025-12-15 13:11:53,645 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 10.1s: 1927c @191c/s (325ch, ~482t @48t/s)
2025-12-15 13:11:55,662 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 12.1s: 2312c @191c/s (390ch, ~578t @48t/s)
2025-12-15 13:11:57,677 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 14.1s: 2693c @191c/s (455ch, ~673t @48t/s)
2025-12-15 13:11:59,693 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 16.1s: 3035c @188c/s (520ch, ~759t @47t/s)
2025-12-15 13:12:01,708 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 18.1s: 3439c @190c/s (585ch, ~860t @47t/s)
2025-12-15 13:12:03,731 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 20.2s: 3769c @187c/s (650ch, ~942t @47t/s)
2025-12-15 13:12:05,754 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 22.2s: 4110c @185c/s (715ch, ~1028t @46t/s)
2025-12-15 13:12:07,782 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 24.2s: 4502c @186c/s (780ch, ~1126t @46t/s)
2025-12-15 13:12:09,805 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 26.2s: 4851c @185c/s (845ch, ~1213t @46t/s)
2025-12-15 13:12:11,826 - src.llm.client - INFO - [app:700f0c] ğŸ“Š 28.3s: 5242c @186c/s (910ch, ~1310t @46t/s)
2025-12-15 13:12:13,040 - src.llm.client - INFO - [app:700f0c] âœ“ Done 39.28s: 5399c (~743w @137c/s)
2025-12-15 13:12:13,042 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:12:13,042 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:12:13,042 - generate_secondary - INFO -     - Length: 5398 chars, 743 words
2025-12-15 13:12:13,042 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:12:13,042 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:12:13,042 - generate_secondary - INFO -     - Avg words per application: 144
2025-12-15 13:12:13,042 - generate_secondary - WARNING - [WARNING] Application 1 has 148 words (require 150-200, need 2 more words) âš ï¸
2025-12-15 13:12:13,042 - generate_secondary - WARNING - [WARNING] Application 3 has 139 words (require 150-200, need 11 more words) âš ï¸
2025-12-15 13:12:13,042 - generate_secondary - WARNING - [WARNING] Application 4 has 139 words (require 150-200, need 11 more words) âš ï¸
2025-12-15 13:12:13,042 - generate_secondary - WARNING - [WARNING] Application 5 has 137 words (require 150-200, need 13 more words) âš ï¸
2025-12-15 13:12:13,042 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_03/application.md
2025-12-15 13:12:13,042 - generate_secondary - INFO - Generating extension for session 3: Bayesian Inference...
2025-12-15 13:12:13,043 - src.llm.client - INFO - [ext:496467] ğŸš€ ext | m=gemma3:4b | p=22759c | t=120s
2025-12-15 13:12:13,043 - src.llm.client - INFO - [ext:496467] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:12:13,043 - src.llm.client - INFO - [ext:496467] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:12:13,044 - src.llm.client - INFO - [ext:496467] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=27566 bytes, prompt=22759 chars
2025-12-15 13:12:13,044 - src.llm.client - INFO - [ext:496467] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:12:23,057 - src.llm.request_handler - INFO - [ext:496467] âœ“ Done 10.01s
2025-12-15 13:12:23,058 - src.llm.client - INFO - [ext:496467] âœ… HTTP 200 in 10.01s
2025-12-15 13:12:23,058 - src.llm.client - INFO - [ext:496467] ğŸ“¡ Stream active (200)
2025-12-15 13:12:23,058 - src.llm.client - INFO - [ext:496467] Starting stream parsing, waiting for first chunk...
2025-12-15 13:12:25,076 - src.llm.client - INFO - [ext:496467] ğŸ“Š 2.0s: 380c @188c/s (65ch, ~95t @47t/s)
2025-12-15 13:12:27,094 - src.llm.client - INFO - [ext:496467] ğŸ“Š 4.0s: 835c @207c/s (130ch, ~209t @52t/s)
2025-12-15 13:12:29,105 - src.llm.client - INFO - [ext:496467] ğŸ“Š 6.0s: 1213c @201c/s (195ch, ~303t @50t/s)
2025-12-15 13:12:31,113 - src.llm.client - INFO - [ext:496467] ğŸ“Š 8.1s: 1612c @200c/s (260ch, ~403t @50t/s)
2025-12-15 13:12:33,130 - src.llm.client - INFO - [ext:496467] ğŸ“Š 10.1s: 2046c @203c/s (325ch, ~512t @51t/s)
2025-12-15 13:12:35,149 - src.llm.client - INFO - [ext:496467] ğŸ“Š 12.1s: 2445c @202c/s (390ch, ~611t @51t/s)
2025-12-15 13:12:37,171 - src.llm.client - INFO - [ext:496467] ğŸ“Š 14.1s: 2883c @204c/s (455ch, ~721t @51t/s)
2025-12-15 13:12:39,187 - src.llm.client - INFO - [ext:496467] ğŸ“Š 16.1s: 3290c @204c/s (519ch, ~822t @51t/s)
2025-12-15 13:12:41,202 - src.llm.client - INFO - [ext:496467] ğŸ“Š 18.1s: 3756c @207c/s (584ch, ~939t @52t/s)
2025-12-15 13:12:42,393 - src.llm.client - INFO - [ext:496467] âœ“ Done 29.35s: 3974c (~514w @135c/s)
2025-12-15 13:12:42,395 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:12:42,395 - generate_secondary - INFO - [COMPLIANT] Extension generated âœ“
2025-12-15 13:12:42,395 - generate_secondary - INFO -     - Length: 3973 chars, 514 words
2025-12-15 13:12:42,395 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:12:42,395 - generate_secondary - INFO -     - Topics: 4
2025-12-15 13:12:42,395 - generate_secondary - INFO -     - Avg words per topic: 121
2025-12-15 13:12:42,396 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_03/extension.md
2025-12-15 13:12:42,396 - generate_secondary - INFO - Generating visualization for session 3: Bayesian Inference...
2025-12-15 13:12:42,396 - src.llm.client - INFO - [viz:bd6469] ğŸš€ viz | m=gemma3:4b | p=21719c | t=120s
2025-12-15 13:12:42,396 - src.llm.client - INFO - [viz:bd6469] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:12:42,396 - src.llm.client - INFO - [viz:bd6469] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:12:42,397 - src.llm.client - INFO - [viz:bd6469] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=25848 bytes, prompt=21719 chars
2025-12-15 13:12:42,397 - src.llm.client - INFO - [viz:bd6469] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:12:52,270 - src.llm.request_handler - INFO - [viz:bd6469] âœ“ Done 9.87s
2025-12-15 13:12:52,270 - src.llm.client - INFO - [viz:bd6469] âœ… HTTP 200 in 9.87s
2025-12-15 13:12:52,270 - src.llm.client - INFO - [viz:bd6469] ğŸ“¡ Stream active (200)
2025-12-15 13:12:52,270 - src.llm.client - INFO - [viz:bd6469] Starting stream parsing, waiting for first chunk...
2025-12-15 13:12:53,673 - src.llm.client - INFO - [viz:bd6469] âœ“ Done 11.28s: 133c (~19w @12c/s)
2025-12-15 13:12:53,674 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:12:53,674 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:12:53,674 - generate_secondary - INFO -     - Length: 117 chars (cleaned: 117 chars)
2025-12-15 13:12:53,674 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:12:53,674 - generate_secondary - INFO - [CRITICAL] Elements: 7 total (nodes: 3, connections: 4) ğŸ”´
2025-12-15 13:12:53,674 - generate_secondary - WARNING -     - Mermaid syntax warnings: 2 issues fixed (code fences, style commands)
2025-12-15 13:12:53,674 - generate_secondary - WARNING -     - Critical issues: 4 structural problems requiring attention
2025-12-15 13:12:53,674 - generate_secondary - WARNING - [WARNING] Only 3 nodes found (require at least 10, need 7 more - add more nodes to the diagram) âš ï¸
2025-12-15 13:12:53,674 - generate_secondary - WARNING - [WARNING] Only 7 connections found (require at least 8, need 1 more - add more arrows/connections) âš ï¸
2025-12-15 13:12:53,674 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:12:53,674 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:12:53,674 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_03/visualization.mmd
2025-12-15 13:12:53,674 - generate_secondary - INFO - Generating integration for session 3: Bayesian Inference...
2025-12-15 13:12:53,674 - src.llm.client - INFO - [int:fea602] ğŸš€ int | m=gemma3:4b | p=23068c | t=150s
2025-12-15 13:12:53,675 - src.llm.client - INFO - [int:fea602] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:12:53,675 - src.llm.client - INFO - [int:fea602] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:12:53,676 - src.llm.client - INFO - [int:fea602] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=28214 bytes, prompt=23068 chars
2025-12-15 13:12:53,676 - src.llm.client - INFO - [int:fea602] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:13:03,487 - src.llm.request_handler - INFO - [int:fea602] âœ“ Done 9.81s
2025-12-15 13:13:03,488 - src.llm.client - INFO - [int:fea602] âœ… HTTP 200 in 9.81s
2025-12-15 13:13:03,488 - src.llm.client - INFO - [int:fea602] ğŸ“¡ Stream active (200)
2025-12-15 13:13:03,488 - src.llm.client - INFO - [int:fea602] Starting stream parsing, waiting for first chunk...
2025-12-15 13:13:05,509 - src.llm.client - INFO - [int:fea602] ğŸ“Š 2.0s: 367c @182c/s (65ch, ~92t @45t/s)
2025-12-15 13:13:07,528 - src.llm.client - INFO - [int:fea602] ğŸ“Š 4.0s: 749c @185c/s (130ch, ~187t @46t/s)
2025-12-15 13:13:09,542 - src.llm.client - INFO - [int:fea602] ğŸ“Š 6.1s: 1069c @177c/s (195ch, ~267t @44t/s)
2025-12-15 13:13:11,557 - src.llm.client - INFO - [int:fea602] ğŸ“Š 8.1s: 1367c @169c/s (260ch, ~342t @42t/s)
2025-12-15 13:13:13,574 - src.llm.client - INFO - [int:fea602] ğŸ“Š 10.1s: 1712c @170c/s (325ch, ~428t @42t/s)
2025-12-15 13:13:15,594 - src.llm.client - INFO - [int:fea602] ğŸ“Š 12.1s: 1999c @165c/s (390ch, ~500t @41t/s)
2025-12-15 13:13:17,613 - src.llm.client - INFO - [int:fea602] ğŸ“Š 14.1s: 2371c @168c/s (455ch, ~593t @42t/s)
2025-12-15 13:13:19,629 - src.llm.client - INFO - [int:fea602] ğŸ“Š 16.1s: 2735c @169c/s (520ch, ~684t @42t/s)
2025-12-15 13:13:21,645 - src.llm.client - INFO - [int:fea602] ğŸ“Š 18.2s: 3095c @170c/s (585ch, ~774t @43t/s)
2025-12-15 13:13:23,667 - src.llm.client - INFO - [int:fea602] ğŸ“Š 20.2s: 3480c @172c/s (650ch, ~870t @43t/s)
2025-12-15 13:13:25,688 - src.llm.client - INFO - [int:fea602] ğŸ“Š 22.2s: 3832c @173c/s (715ch, ~958t @43t/s)
2025-12-15 13:13:27,711 - src.llm.client - INFO - [int:fea602] ğŸ“Š 24.2s: 4236c @175c/s (780ch, ~1059t @44t/s)
2025-12-15 13:13:29,734 - src.llm.client - INFO - [int:fea602] ğŸ“Š 26.2s: 4492c @171c/s (845ch, ~1123t @43t/s)
2025-12-15 13:13:31,513 - src.llm.client - INFO - [int:fea602] âœ“ Done 37.84s: 4684c (~666w @124c/s)
2025-12-15 13:13:31,515 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:13:31,515 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:13:31,515 - generate_secondary - INFO -     - Length: 4670 chars, 664 words
2025-12-15 13:13:31,515 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:13:31,515 - generate_secondary - INFO -     - Connections: 29
2025-12-15 13:13:31,515 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:13:31,516 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_03/integration.md
2025-12-15 13:13:31,516 - generate_secondary - INFO - Generating investigation for session 3: Bayesian Inference...
2025-12-15 13:13:31,516 - src.llm.client - INFO - [inv:580d6b] ğŸš€ inv | m=gemma3:4b | p=21981c | t=150s
2025-12-15 13:13:31,516 - src.llm.client - INFO - [inv:580d6b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:13:31,516 - src.llm.client - INFO - [inv:580d6b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:13:31,517 - src.llm.client - INFO - [inv:580d6b] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=26070 bytes, prompt=21981 chars
2025-12-15 13:13:31,517 - src.llm.client - INFO - [inv:580d6b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:13:41,545 - src.llm.request_handler - INFO - [inv:580d6b] âœ“ Done 10.03s
2025-12-15 13:13:41,545 - src.llm.client - INFO - [inv:580d6b] âœ… HTTP 200 in 10.03s
2025-12-15 13:13:41,545 - src.llm.client - INFO - [inv:580d6b] ğŸ“¡ Stream active (200)
2025-12-15 13:13:41,545 - src.llm.client - INFO - [inv:580d6b] Starting stream parsing, waiting for first chunk...
2025-12-15 13:13:43,566 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 2.0s: 356c @176c/s (65ch, ~89t @44t/s)
2025-12-15 13:13:45,579 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 4.0s: 631c @156c/s (130ch, ~158t @39t/s)
2025-12-15 13:13:47,586 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 6.0s: 966c @160c/s (195ch, ~242t @40t/s)
2025-12-15 13:13:49,599 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 8.1s: 1339c @166c/s (260ch, ~335t @42t/s)
2025-12-15 13:13:51,611 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 10.1s: 1697c @169c/s (324ch, ~424t @42t/s)
2025-12-15 13:13:53,630 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 12.1s: 2059c @170c/s (389ch, ~515t @43t/s)
2025-12-15 13:13:55,645 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 14.1s: 2410c @171c/s (454ch, ~602t @43t/s)
2025-12-15 13:13:57,646 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 16.1s: 2776c @172c/s (517ch, ~694t @43t/s)
2025-12-15 13:13:59,669 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 18.1s: 3156c @174c/s (582ch, ~789t @44t/s)
2025-12-15 13:14:01,690 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 20.1s: 3450c @171c/s (646ch, ~862t @43t/s)
2025-12-15 13:14:03,720 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 22.2s: 3785c @171c/s (711ch, ~946t @43t/s)
2025-12-15 13:14:05,746 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 24.2s: 4160c @172c/s (776ch, ~1040t @43t/s)
2025-12-15 13:14:07,747 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 26.2s: 4580c @175c/s (840ch, ~1145t @44t/s)
2025-12-15 13:14:09,773 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 28.2s: 4943c @175c/s (905ch, ~1236t @44t/s)
2025-12-15 13:14:11,804 - src.llm.client - INFO - [inv:580d6b] ğŸ“Š 30.3s: 5209c @172c/s (970ch, ~1302t @43t/s)
2025-12-15 13:14:12,996 - src.llm.client - INFO - [inv:580d6b] âœ“ Done 41.48s: 5359c (~777w @129c/s)
2025-12-15 13:14:12,999 - src.generate.processors.cleanup - INFO - Cleanup complete: 2 issues before, 0 issues after
2025-12-15 13:14:12,999 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:14:12,999 - generate_secondary - INFO -     - Length: 5301 chars, 767 words
2025-12-15 13:14:12,999 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:14:12,999 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:14:12,999 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:14:12,999 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_03/investigation.md
2025-12-15 13:14:12,999 - generate_secondary - INFO - Generating open_questions for session 3: Bayesian Inference...
2025-12-15 13:14:12,999 - src.llm.client - INFO - [opq:250125] ğŸš€ opq | m=gemma3:4b | p=22067c | t=150s
2025-12-15 13:14:12,999 - src.llm.client - INFO - [opq:250125] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:14:12,999 - src.llm.client - INFO - [opq:250125] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:14:13,001 - src.llm.client - INFO - [opq:250125] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=26167 bytes, prompt=22067 chars
2025-12-15 13:14:13,001 - src.llm.client - INFO - [opq:250125] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:14:22,800 - src.llm.request_handler - INFO - [opq:250125] âœ“ Done 9.80s
2025-12-15 13:14:22,800 - src.llm.client - INFO - [opq:250125] âœ… HTTP 200 in 9.80s
2025-12-15 13:14:22,800 - src.llm.client - INFO - [opq:250125] ğŸ“¡ Stream active (200)
2025-12-15 13:14:22,800 - src.llm.client - INFO - [opq:250125] Starting stream parsing, waiting for first chunk...
2025-12-15 13:14:24,824 - src.llm.client - INFO - [opq:250125] ğŸ“Š 2.0s: 327c @162c/s (65ch, ~82t @40t/s)
2025-12-15 13:14:26,845 - src.llm.client - INFO - [opq:250125] ğŸ“Š 4.0s: 731c @181c/s (130ch, ~183t @45t/s)
2025-12-15 13:14:28,864 - src.llm.client - INFO - [opq:250125] ğŸ“Š 6.1s: 1068c @176c/s (195ch, ~267t @44t/s)
2025-12-15 13:14:30,883 - src.llm.client - INFO - [opq:250125] ğŸ“Š 8.1s: 1488c @184c/s (260ch, ~372t @46t/s)
2025-12-15 13:14:32,905 - src.llm.client - INFO - [opq:250125] ğŸ“Š 10.1s: 1894c @187c/s (325ch, ~474t @47t/s)
2025-12-15 13:14:34,379 - src.llm.client - INFO - [opq:250125] âœ“ Done 21.38s: 2189c (~277w @102c/s)
2025-12-15 13:14:34,381 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:14:34,381 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:14:34,381 - generate_secondary - INFO -     - Length: 2188 chars, 277 words
2025-12-15 13:14:34,381 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:14:34,381 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:14:34,381 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:14:34,381 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_02_bayesian_mechanics/session_03/open_questions.md
2025-12-15 13:14:34,381 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:14:34,381 - generate_secondary - INFO - 
============================================================
2025-12-15 13:14:34,381 - generate_secondary - INFO - [3/10] Module 3: Variational Inference (2 sessions)
2025-12-15 13:14:34,381 - generate_secondary - INFO - ============================================================
2025-12-15 13:14:34,381 - generate_secondary - INFO - 
  Session 4/20: Approximation Techniques
2025-12-15 13:14:34,382 - generate_secondary - INFO - Generating application for session 4: Approximation Techniques...
2025-12-15 13:14:34,382 - src.llm.client - INFO - [app:decffe] ğŸš€ app | m=gemma3:4b | p=33635c | t=150s
2025-12-15 13:14:34,382 - src.llm.client - INFO - [app:decffe] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:14:34,382 - src.llm.client - INFO - [app:decffe] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:14:34,383 - src.llm.client - INFO - [app:decffe] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35929 bytes, prompt=33635 chars
2025-12-15 13:14:34,383 - src.llm.client - INFO - [app:decffe] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:14:44,195 - src.llm.request_handler - INFO - [app:decffe] âœ“ Done 9.81s
2025-12-15 13:14:44,196 - src.llm.client - INFO - [app:decffe] âœ… HTTP 200 in 9.81s
2025-12-15 13:14:44,196 - src.llm.client - INFO - [app:decffe] ğŸ“¡ Stream active (200)
2025-12-15 13:14:44,196 - src.llm.client - INFO - [app:decffe] Starting stream parsing, waiting for first chunk...
2025-12-15 13:14:46,220 - src.llm.client - INFO - [app:decffe] ğŸ“Š 2.0s: 389c @192c/s (65ch, ~97t @48t/s)
2025-12-15 13:14:48,235 - src.llm.client - INFO - [app:decffe] ğŸ“Š 4.0s: 790c @196c/s (130ch, ~198t @49t/s)
2025-12-15 13:14:50,249 - src.llm.client - INFO - [app:decffe] ğŸ“Š 6.1s: 1177c @194c/s (195ch, ~294t @49t/s)
2025-12-15 13:14:52,265 - src.llm.client - INFO - [app:decffe] ğŸ“Š 8.1s: 1604c @199c/s (260ch, ~401t @50t/s)
2025-12-15 13:14:54,288 - src.llm.client - INFO - [app:decffe] ğŸ“Š 10.1s: 2037c @202c/s (325ch, ~509t @50t/s)
2025-12-15 13:14:56,308 - src.llm.client - INFO - [app:decffe] ğŸ“Š 12.1s: 2403c @198c/s (390ch, ~601t @50t/s)
2025-12-15 13:14:58,329 - src.llm.client - INFO - [app:decffe] ğŸ“Š 14.1s: 2755c @195c/s (455ch, ~689t @49t/s)
2025-12-15 13:15:00,350 - src.llm.client - INFO - [app:decffe] ğŸ“Š 16.2s: 3170c @196c/s (520ch, ~792t @49t/s)
2025-12-15 13:15:02,372 - src.llm.client - INFO - [app:decffe] ğŸ“Š 18.2s: 3536c @195c/s (585ch, ~884t @49t/s)
2025-12-15 13:15:04,398 - src.llm.client - INFO - [app:decffe] ğŸ“Š 20.2s: 3893c @193c/s (650ch, ~973t @48t/s)
2025-12-15 13:15:06,424 - src.llm.client - INFO - [app:decffe] ğŸ“Š 22.2s: 4247c @191c/s (715ch, ~1062t @48t/s)
2025-12-15 13:15:08,454 - src.llm.client - INFO - [app:decffe] ğŸ“Š 24.3s: 4614c @190c/s (780ch, ~1154t @48t/s)
2025-12-15 13:15:09,856 - src.llm.client - INFO - [app:decffe] âœ“ Done 35.47s: 4801c (~656w @135c/s)
2025-12-15 13:15:09,858 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:15:09,858 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:15:09,858 - generate_secondary - INFO -     - Length: 4672 chars, 639 words
2025-12-15 13:15:09,858 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:15:09,858 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:15:09,858 - generate_secondary - INFO -     - Avg words per application: 126
2025-12-15 13:15:09,858 - generate_secondary - WARNING - [WARNING] Application 1 has 131 words (require 150-200, need 19 more words) âš ï¸
2025-12-15 13:15:09,858 - generate_secondary - WARNING - [WARNING] Application 2 has 126 words (require 150-200, need 24 more words) âš ï¸
2025-12-15 13:15:09,858 - generate_secondary - WARNING - [WARNING] Application 3 has 127 words (require 150-200, need 23 more words) âš ï¸
2025-12-15 13:15:09,858 - generate_secondary - WARNING - [WARNING] Application 4 has 122 words (require 150-200, need 28 more words) âš ï¸
2025-12-15 13:15:09,858 - generate_secondary - WARNING - [WARNING] Application 5 has 123 words (require 150-200, need 27 more words) âš ï¸
2025-12-15 13:15:09,859 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_04/application.md
2025-12-15 13:15:09,859 - generate_secondary - INFO - Generating extension for session 4: Approximation Techniques...
2025-12-15 13:15:09,859 - src.llm.client - INFO - [ext:6fece5] ğŸš€ ext | m=gemma3:4b | p=27521c | t=120s
2025-12-15 13:15:09,859 - src.llm.client - INFO - [ext:6fece5] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:15:09,859 - src.llm.client - INFO - [ext:6fece5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:15:09,860 - src.llm.client - INFO - [ext:6fece5] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32686 bytes, prompt=27521 chars
2025-12-15 13:15:09,860 - src.llm.client - INFO - [ext:6fece5] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:15:42,717 - src.llm.request_handler - INFO - [ext:6fece5] âœ“ Done 32.86s
2025-12-15 13:15:42,717 - src.llm.client - INFO - [ext:6fece5] âœ… HTTP 200 in 32.86s
2025-12-15 13:15:42,717 - src.llm.client - INFO - [ext:6fece5] ğŸ“¡ Stream active (200)
2025-12-15 13:15:42,717 - src.llm.client - INFO - [ext:6fece5] Starting stream parsing, waiting for first chunk...
2025-12-15 13:15:44,742 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 2.0s: 335c @165c/s (65ch, ~84t @41t/s)
2025-12-15 13:15:46,756 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 4.0s: 733c @181c/s (130ch, ~183t @45t/s)
2025-12-15 13:15:48,774 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 6.1s: 1129c @186c/s (195ch, ~282t @47t/s)
2025-12-15 13:15:50,787 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 8.1s: 1505c @186c/s (260ch, ~376t @47t/s)
2025-12-15 13:15:52,805 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 10.1s: 1940c @192c/s (325ch, ~485t @48t/s)
2025-12-15 13:15:54,827 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 12.1s: 2252c @186c/s (390ch, ~563t @46t/s)
2025-12-15 13:15:56,848 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 14.1s: 2656c @188c/s (455ch, ~664t @47t/s)
2025-12-15 13:15:58,871 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 16.2s: 3010c @186c/s (520ch, ~752t @47t/s)
2025-12-15 13:16:00,898 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 18.2s: 3468c @191c/s (585ch, ~867t @48t/s)
2025-12-15 13:16:02,924 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 20.2s: 3859c @191c/s (650ch, ~965t @48t/s)
2025-12-15 13:16:04,925 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 22.2s: 4260c @192c/s (714ch, ~1065t @48t/s)
2025-12-15 13:16:06,949 - src.llm.client - INFO - [ext:6fece5] ğŸ“Š 24.2s: 4694c @194c/s (779ch, ~1174t @48t/s)
2025-12-15 13:16:07,611 - src.llm.client - INFO - [ext:6fece5] âœ“ Done 57.75s: 4756c (~618w @82c/s)
2025-12-15 13:16:07,613 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:16:07,614 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:16:07,614 - generate_secondary - INFO -     - Length: 4756 chars, 618 words
2025-12-15 13:16:07,614 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:16:07,614 - generate_secondary - INFO -     - Topics: 4
2025-12-15 13:16:07,614 - generate_secondary - INFO -     - Avg words per topic: 146
2025-12-15 13:16:07,614 - generate_secondary - WARNING - [WARNING] Topic 1 has 153 words (exceeds 150 by 3 words - consider condensing) âš ï¸
2025-12-15 13:16:07,614 - generate_secondary - WARNING - [WARNING] Topic 2 has 152 words (exceeds 150 by 2 words - consider condensing) âš ï¸
2025-12-15 13:16:07,614 - generate_secondary - WARNING - [WARNING] Total word count (618) exceeds maximum 600 (exceeds by 18 words - condense content) âš ï¸
2025-12-15 13:16:07,614 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:16:07,614 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:16:07,614 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_04/extension.md
2025-12-15 13:16:07,614 - generate_secondary - INFO - Generating visualization for session 4: Approximation Techniques...
2025-12-15 13:16:07,614 - src.llm.client - INFO - [viz:85722e] ğŸš€ viz | m=gemma3:4b | p=26481c | t=120s
2025-12-15 13:16:07,614 - src.llm.client - INFO - [viz:85722e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:16:07,614 - src.llm.client - INFO - [viz:85722e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:16:07,616 - src.llm.client - INFO - [viz:85722e] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30968 bytes, prompt=26481 chars
2025-12-15 13:16:07,616 - src.llm.client - INFO - [viz:85722e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:16:17,948 - src.llm.request_handler - INFO - [viz:85722e] âœ“ Done 10.33s
2025-12-15 13:16:17,948 - src.llm.client - INFO - [viz:85722e] âœ… HTTP 200 in 10.33s
2025-12-15 13:16:17,948 - src.llm.client - INFO - [viz:85722e] ğŸ“¡ Stream active (200)
2025-12-15 13:16:17,949 - src.llm.client - INFO - [viz:85722e] Starting stream parsing, waiting for first chunk...
2025-12-15 13:16:19,972 - src.llm.client - INFO - [viz:85722e] ğŸ“Š 2.0s: 293c @145c/s (65ch, ~73t @36t/s)
2025-12-15 13:16:21,989 - src.llm.client - INFO - [viz:85722e] ğŸ“Š 4.0s: 501c @124c/s (130ch, ~125t @31t/s)
2025-12-15 13:16:24,010 - src.llm.client - INFO - [viz:85722e] ğŸ“Š 6.1s: 703c @116c/s (195ch, ~176t @29t/s)
2025-12-15 13:16:26,028 - src.llm.client - INFO - [viz:85722e] ğŸ“Š 8.1s: 953c @118c/s (260ch, ~238t @29t/s)
2025-12-15 13:16:28,050 - src.llm.client - INFO - [viz:85722e] ğŸ“Š 10.1s: 1205c @119c/s (325ch, ~301t @30t/s)
2025-12-15 13:16:29,077 - src.llm.client - INFO - [viz:85722e] âœ“ Done 21.46s: 1312c (~190w @61c/s)
2025-12-15 13:16:29,077 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-15 13:16:29,078 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:16:29,078 - generate_secondary - INFO -     - Length: 944 chars (cleaned: 944 chars)
2025-12-15 13:16:29,078 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:16:29,078 - generate_secondary - INFO - [OK] Elements: 51 total (nodes: 26, connections: 25) âœ“
2025-12-15 13:16:29,078 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_04/visualization.mmd
2025-12-15 13:16:29,078 - generate_secondary - INFO - Generating integration for session 4: Approximation Techniques...
2025-12-15 13:16:29,078 - src.llm.client - INFO - [int:35b5d6] ğŸš€ int | m=gemma3:4b | p=27830c | t=150s
2025-12-15 13:16:29,078 - src.llm.client - INFO - [int:35b5d6] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:16:29,078 - src.llm.client - INFO - [int:35b5d6] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:16:29,079 - src.llm.client - INFO - [int:35b5d6] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=33334 bytes, prompt=27830 chars
2025-12-15 13:16:29,079 - src.llm.client - INFO - [int:35b5d6] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:16:39,159 - src.llm.request_handler - INFO - [int:35b5d6] âœ“ Done 10.08s
2025-12-15 13:16:39,160 - src.llm.client - INFO - [int:35b5d6] âœ… HTTP 200 in 10.08s
2025-12-15 13:16:39,160 - src.llm.client - INFO - [int:35b5d6] ğŸ“¡ Stream active (200)
2025-12-15 13:16:39,160 - src.llm.client - INFO - [int:35b5d6] Starting stream parsing, waiting for first chunk...
2025-12-15 13:16:41,187 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 2.0s: 365c @180c/s (65ch, ~91t @45t/s)
2025-12-15 13:16:43,207 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 4.0s: 820c @203c/s (130ch, ~205t @51t/s)
2025-12-15 13:16:45,233 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 6.1s: 1269c @209c/s (195ch, ~317t @52t/s)
2025-12-15 13:16:47,247 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 8.1s: 1718c @212c/s (260ch, ~430t @53t/s)
2025-12-15 13:16:49,270 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 10.1s: 2027c @200c/s (325ch, ~507t @50t/s)
2025-12-15 13:16:51,289 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 12.1s: 2235c @184c/s (390ch, ~559t @46t/s)
2025-12-15 13:16:53,309 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 14.1s: 2434c @172c/s (455ch, ~608t @43t/s)
2025-12-15 13:16:55,329 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 16.2s: 2665c @165c/s (520ch, ~666t @41t/s)
2025-12-15 13:16:57,348 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 18.2s: 2897c @159c/s (585ch, ~724t @40t/s)
2025-12-15 13:16:59,377 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 20.2s: 3106c @154c/s (650ch, ~776t @38t/s)
2025-12-15 13:17:01,404 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 22.2s: 3310c @149c/s (715ch, ~828t @37t/s)
2025-12-15 13:17:03,430 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 24.3s: 3535c @146c/s (780ch, ~884t @36t/s)
2025-12-15 13:17:05,455 - src.llm.client - INFO - [int:35b5d6] ğŸ“Š 26.3s: 3763c @143c/s (845ch, ~941t @36t/s)
2025-12-15 13:17:06,583 - src.llm.client - INFO - [int:35b5d6] âœ“ Done 37.50s: 3863c (~534w @103c/s)
2025-12-15 13:17:06,585 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:17:06,585 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:17:06,585 - generate_secondary - INFO -     - Length: 3862 chars, 534 words
2025-12-15 13:17:06,585 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:17:06,585 - generate_secondary - INFO -     - Connections: 17
2025-12-15 13:17:06,585 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:17:06,585 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_04/integration.md
2025-12-15 13:17:06,585 - generate_secondary - INFO - Generating investigation for session 4: Approximation Techniques...
2025-12-15 13:17:06,585 - src.llm.client - INFO - [inv:b1ff7a] ğŸš€ inv | m=gemma3:4b | p=26743c | t=150s
2025-12-15 13:17:06,585 - src.llm.client - INFO - [inv:b1ff7a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:17:06,585 - src.llm.client - INFO - [inv:b1ff7a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:17:06,587 - src.llm.client - INFO - [inv:b1ff7a] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=31190 bytes, prompt=26743 chars
2025-12-15 13:17:06,587 - src.llm.client - INFO - [inv:b1ff7a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:17:16,584 - src.llm.request_handler - INFO - [inv:b1ff7a] âœ“ Done 10.00s
2025-12-15 13:17:16,585 - src.llm.client - INFO - [inv:b1ff7a] âœ… HTTP 200 in 10.00s
2025-12-15 13:17:16,585 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“¡ Stream active (200)
2025-12-15 13:17:16,585 - src.llm.client - INFO - [inv:b1ff7a] Starting stream parsing, waiting for first chunk...
2025-12-15 13:17:18,614 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 2.0s: 358c @176c/s (65ch, ~90t @44t/s)
2025-12-15 13:17:20,633 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 4.0s: 615c @152c/s (130ch, ~154t @38t/s)
2025-12-15 13:17:22,646 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 6.1s: 857c @141c/s (195ch, ~214t @35t/s)
2025-12-15 13:17:24,663 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 8.1s: 1159c @143c/s (260ch, ~290t @36t/s)
2025-12-15 13:17:26,683 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 10.1s: 1508c @149c/s (325ch, ~377t @37t/s)
2025-12-15 13:17:28,705 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 12.1s: 1868c @154c/s (390ch, ~467t @39t/s)
2025-12-15 13:17:30,727 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 14.1s: 2180c @154c/s (455ch, ~545t @39t/s)
2025-12-15 13:17:32,750 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 16.2s: 2415c @149c/s (520ch, ~604t @37t/s)
2025-12-15 13:17:34,772 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 18.2s: 2779c @153c/s (585ch, ~695t @38t/s)
2025-12-15 13:17:36,795 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 20.2s: 3018c @149c/s (649ch, ~754t @37t/s)
2025-12-15 13:17:38,802 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 22.2s: 3368c @152c/s (712ch, ~842t @38t/s)
2025-12-15 13:17:40,826 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 24.2s: 3703c @153c/s (777ch, ~926t @38t/s)
2025-12-15 13:17:42,854 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 26.3s: 4000c @152c/s (842ch, ~1000t @38t/s)
2025-12-15 13:17:44,879 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 28.3s: 4293c @152c/s (907ch, ~1073t @38t/s)
2025-12-15 13:17:46,906 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 30.3s: 4612c @152c/s (972ch, ~1153t @38t/s)
2025-12-15 13:17:48,916 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 32.3s: 4946c @153c/s (1036ch, ~1236t @38t/s)
2025-12-15 13:17:51,014 - src.llm.client - INFO - [inv:b1ff7a] ğŸ“Š 34.4s: 5273c @153c/s (1092ch, ~1318t @38t/s)
2025-12-15 13:17:51,015 - src.llm.client - INFO - [inv:b1ff7a] âœ“ Done 44.43s: 5273c (~824w @119c/s)
2025-12-15 13:17:51,017 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:17:51,017 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:17:51,017 - generate_secondary - INFO -     - Length: 5259 chars, 822 words
2025-12-15 13:17:51,017 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:17:51,017 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:17:51,017 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:17:51,017 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_04/investigation.md
2025-12-15 13:17:51,017 - generate_secondary - INFO - Generating open_questions for session 4: Approximation Techniques...
2025-12-15 13:17:51,017 - src.llm.client - INFO - [opq:19720c] ğŸš€ opq | m=gemma3:4b | p=26829c | t=150s
2025-12-15 13:17:51,017 - src.llm.client - INFO - [opq:19720c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:17:51,017 - src.llm.client - INFO - [opq:19720c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:17:51,019 - src.llm.client - INFO - [opq:19720c] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=31287 bytes, prompt=26829 chars
2025-12-15 13:17:51,019 - src.llm.client - INFO - [opq:19720c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:18:00,932 - src.llm.request_handler - INFO - [opq:19720c] âœ“ Done 9.91s
2025-12-15 13:18:00,933 - src.llm.client - INFO - [opq:19720c] âœ… HTTP 200 in 9.91s
2025-12-15 13:18:00,933 - src.llm.client - INFO - [opq:19720c] ğŸ“¡ Stream active (200)
2025-12-15 13:18:00,933 - src.llm.client - INFO - [opq:19720c] Starting stream parsing, waiting for first chunk...
2025-12-15 13:18:02,957 - src.llm.client - INFO - [opq:19720c] ğŸ“Š 2.0s: 399c @197c/s (65ch, ~100t @49t/s)
2025-12-15 13:18:04,972 - src.llm.client - INFO - [opq:19720c] ğŸ“Š 4.0s: 805c @199c/s (130ch, ~201t @50t/s)
2025-12-15 13:18:06,991 - src.llm.client - INFO - [opq:19720c] ğŸ“Š 6.1s: 1229c @203c/s (195ch, ~307t @51t/s)
2025-12-15 13:18:09,011 - src.llm.client - INFO - [opq:19720c] ğŸ“Š 8.1s: 1605c @199c/s (260ch, ~401t @50t/s)
2025-12-15 13:18:11,030 - src.llm.client - INFO - [opq:19720c] ğŸ“Š 10.1s: 2005c @199c/s (325ch, ~501t @50t/s)
2025-12-15 13:18:11,839 - src.llm.client - INFO - [opq:19720c] âœ“ Done 20.82s: 2110c (~279w @101c/s)
2025-12-15 13:18:11,840 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:18:11,840 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:18:11,840 - generate_secondary - INFO -     - Length: 2023 chars, 267 words
2025-12-15 13:18:11,840 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:18:11,840 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:18:11,840 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:18:11,840 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_04/open_questions.md
2025-12-15 13:18:11,840 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:18:11,840 - generate_secondary - INFO - 
  Session 5/20: Variational Free Energy
2025-12-15 13:18:11,841 - generate_secondary - INFO - Generating application for session 5: Variational Free Energy...
2025-12-15 13:18:11,841 - src.llm.client - INFO - [app:512834] ğŸš€ app | m=gemma3:4b | p=32131c | t=150s
2025-12-15 13:18:11,841 - src.llm.client - INFO - [app:512834] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:18:11,841 - src.llm.client - INFO - [app:512834] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:18:11,844 - src.llm.client - INFO - [app:512834] Sending request to Ollama: model=gemma3:4b, operation=application, payload=34449 bytes, prompt=32131 chars
2025-12-15 13:18:11,844 - src.llm.client - INFO - [app:512834] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:18:21,914 - src.llm.request_handler - INFO - [app:512834] âœ“ Done 10.07s
2025-12-15 13:18:21,915 - src.llm.client - INFO - [app:512834] âœ… HTTP 200 in 10.07s
2025-12-15 13:18:21,915 - src.llm.client - INFO - [app:512834] ğŸ“¡ Stream active (200)
2025-12-15 13:18:21,915 - src.llm.client - INFO - [app:512834] Starting stream parsing, waiting for first chunk...
2025-12-15 13:18:23,940 - src.llm.client - INFO - [app:512834] ğŸ“Š 2.0s: 403c @199c/s (65ch, ~101t @50t/s)
2025-12-15 13:18:25,958 - src.llm.client - INFO - [app:512834] ğŸ“Š 4.0s: 831c @206c/s (130ch, ~208t @51t/s)
2025-12-15 13:18:27,980 - src.llm.client - INFO - [app:512834] ğŸ“Š 6.1s: 1227c @202c/s (195ch, ~307t @51t/s)
2025-12-15 13:18:29,995 - src.llm.client - INFO - [app:512834] ğŸ“Š 8.1s: 1622c @201c/s (260ch, ~406t @50t/s)
2025-12-15 13:18:32,019 - src.llm.client - INFO - [app:512834] ğŸ“Š 10.1s: 2052c @203c/s (325ch, ~513t @51t/s)
2025-12-15 13:18:34,040 - src.llm.client - INFO - [app:512834] ğŸ“Š 12.1s: 2460c @203c/s (390ch, ~615t @51t/s)
2025-12-15 13:18:36,063 - src.llm.client - INFO - [app:512834] ğŸ“Š 14.1s: 2864c @202c/s (455ch, ~716t @51t/s)
2025-12-15 13:18:38,065 - src.llm.client - INFO - [app:512834] ğŸ“Š 16.2s: 3272c @203c/s (518ch, ~818t @51t/s)
2025-12-15 13:18:40,087 - src.llm.client - INFO - [app:512834] ğŸ“Š 18.2s: 3650c @201c/s (583ch, ~912t @50t/s)
2025-12-15 13:18:42,112 - src.llm.client - INFO - [app:512834] ğŸ“Š 20.2s: 4070c @202c/s (648ch, ~1018t @50t/s)
2025-12-15 13:18:44,138 - src.llm.client - INFO - [app:512834] ğŸ“Š 22.2s: 4484c @202c/s (713ch, ~1121t @50t/s)
2025-12-15 13:18:46,161 - src.llm.client - INFO - [app:512834] ğŸ“Š 24.2s: 4915c @203c/s (778ch, ~1229t @51t/s)
2025-12-15 13:18:48,191 - src.llm.client - INFO - [app:512834] ğŸ“Š 26.3s: 5354c @204c/s (843ch, ~1338t @51t/s)
2025-12-15 13:18:50,220 - src.llm.client - INFO - [app:512834] ğŸ“Š 28.3s: 5762c @204c/s (908ch, ~1440t @51t/s)
2025-12-15 13:18:52,224 - src.llm.client - INFO - [app:512834] ğŸ“Š 30.3s: 6144c @203c/s (972ch, ~1536t @51t/s)
2025-12-15 13:18:53,215 - src.llm.client - INFO - [app:512834] âœ“ Done 41.37s: 6271c (~851w @152c/s)
2025-12-15 13:18:53,218 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:18:53,218 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:18:53,218 - generate_secondary - INFO -     - Length: 6260 chars, 849 words
2025-12-15 13:18:53,218 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:18:53,218 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:18:53,218 - generate_secondary - INFO -     - Avg words per application: 165
2025-12-15 13:18:53,218 - generate_secondary - WARNING - [WARNING] Application 4 has 141 words (require 150-200, need 9 more words) âš ï¸
2025-12-15 13:18:53,218 - generate_secondary - WARNING - [WARNING] Application 5 has 146 words (require 150-200, need 4 more words) âš ï¸
2025-12-15 13:18:53,218 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_05/application.md
2025-12-15 13:18:53,218 - generate_secondary - INFO - Generating extension for session 5: Variational Free Energy...
2025-12-15 13:18:53,218 - src.llm.client - INFO - [ext:89a0b5] ğŸš€ ext | m=gemma3:4b | p=26017c | t=120s
2025-12-15 13:18:53,218 - src.llm.client - INFO - [ext:89a0b5] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:18:53,218 - src.llm.client - INFO - [ext:89a0b5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:18:53,220 - src.llm.client - INFO - [ext:89a0b5] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=31206 bytes, prompt=26017 chars
2025-12-15 13:18:53,220 - src.llm.client - INFO - [ext:89a0b5] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:19:03,265 - src.llm.request_handler - INFO - [ext:89a0b5] âœ“ Done 10.05s
2025-12-15 13:19:03,265 - src.llm.client - INFO - [ext:89a0b5] âœ… HTTP 200 in 10.05s
2025-12-15 13:19:03,265 - src.llm.client - INFO - [ext:89a0b5] ğŸ“¡ Stream active (200)
2025-12-15 13:19:03,265 - src.llm.client - INFO - [ext:89a0b5] Starting stream parsing, waiting for first chunk...
2025-12-15 13:19:05,289 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 2.0s: 352c @174c/s (65ch, ~88t @43t/s)
2025-12-15 13:19:07,314 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 4.0s: 769c @190c/s (130ch, ~192t @47t/s)
2025-12-15 13:19:09,333 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 6.1s: 1200c @198c/s (195ch, ~300t @49t/s)
2025-12-15 13:19:11,352 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 8.1s: 1593c @197c/s (260ch, ~398t @49t/s)
2025-12-15 13:19:13,376 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 10.1s: 2035c @201c/s (325ch, ~509t @50t/s)
2025-12-15 13:19:15,398 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 12.1s: 2486c @205c/s (390ch, ~622t @51t/s)
2025-12-15 13:19:17,417 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 14.2s: 2853c @202c/s (455ch, ~713t @50t/s)
2025-12-15 13:19:19,438 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 16.2s: 3301c @204c/s (520ch, ~825t @51t/s)
2025-12-15 13:19:21,463 - src.llm.client - INFO - [ext:89a0b5] ğŸ“Š 18.2s: 3701c @203c/s (585ch, ~925t @51t/s)
2025-12-15 13:19:22,506 - src.llm.client - INFO - [ext:89a0b5] âœ“ Done 29.29s: 3863c (~492w @132c/s)
2025-12-15 13:19:22,508 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:19:22,508 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:19:22,508 - generate_secondary - INFO -     - Length: 3688 chars, 468 words
2025-12-15 13:19:22,508 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:19:22,508 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:19:22,508 - generate_secondary - INFO -     - Avg words per topic: 154
2025-12-15 13:19:22,508 - generate_secondary - WARNING - [WARNING] Topic 2 has 151 words (exceeds 150 by 1 words - consider condensing) âš ï¸
2025-12-15 13:19:22,508 - generate_secondary - WARNING - [WARNING] Topic 3 has 170 words (exceeds 150 by 20 words - consider condensing) âš ï¸
2025-12-15 13:19:22,509 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_05/extension.md
2025-12-15 13:19:22,509 - generate_secondary - INFO - Generating visualization for session 5: Variational Free Energy...
2025-12-15 13:19:22,509 - src.llm.client - INFO - [viz:54a69f] ğŸš€ viz | m=gemma3:4b | p=24977c | t=120s
2025-12-15 13:19:22,509 - src.llm.client - INFO - [viz:54a69f] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:19:22,509 - src.llm.client - INFO - [viz:54a69f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:19:22,510 - src.llm.client - INFO - [viz:54a69f] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=29488 bytes, prompt=24977 chars
2025-12-15 13:19:22,510 - src.llm.client - INFO - [viz:54a69f] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:19:32,574 - src.llm.request_handler - INFO - [viz:54a69f] âœ“ Done 10.06s
2025-12-15 13:19:32,574 - src.llm.client - INFO - [viz:54a69f] âœ… HTTP 200 in 10.06s
2025-12-15 13:19:32,574 - src.llm.client - INFO - [viz:54a69f] ğŸ“¡ Stream active (200)
2025-12-15 13:19:32,574 - src.llm.client - INFO - [viz:54a69f] Starting stream parsing, waiting for first chunk...
2025-12-15 13:19:34,595 - src.llm.client - INFO - [viz:54a69f] ğŸ“Š 2.0s: 249c @123c/s (64ch, ~62t @31t/s)
2025-12-15 13:19:36,614 - src.llm.client - INFO - [viz:54a69f] ğŸ“Š 4.0s: 485c @120c/s (129ch, ~121t @30t/s)
2025-12-15 13:19:38,634 - src.llm.client - INFO - [viz:54a69f] ğŸ“Š 6.1s: 741c @122c/s (193ch, ~185t @31t/s)
2025-12-15 13:19:40,733 - src.llm.client - INFO - [viz:54a69f] ğŸ“Š 8.2s: 1002c @123c/s (253ch, ~250t @31t/s)
2025-12-15 13:19:40,734 - src.llm.client - INFO - [viz:54a69f] âœ“ Done 18.22s: 1002c (~146w @55c/s)
2025-12-15 13:19:40,734 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-15 13:19:40,734 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:19:40,734 - generate_secondary - INFO -     - Length: 192 chars (cleaned: 192 chars)
2025-12-15 13:19:40,734 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:19:40,734 - generate_secondary - INFO - [CRITICAL] Elements: 15 total (nodes: 6, connections: 9) ğŸ”´
2025-12-15 13:19:40,734 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:19:40,734 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-15 13:19:40,734 - generate_secondary - WARNING - [WARNING] Only 6 nodes found (require at least 10, need 4 more - add more nodes to the diagram) âš ï¸
2025-12-15 13:19:40,734 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:19:40,734 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:19:40,735 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_05/visualization.mmd
2025-12-15 13:19:40,735 - generate_secondary - INFO - Generating integration for session 5: Variational Free Energy...
2025-12-15 13:19:40,735 - src.llm.client - INFO - [int:205e43] ğŸš€ int | m=gemma3:4b | p=26326c | t=150s
2025-12-15 13:19:40,735 - src.llm.client - INFO - [int:205e43] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:19:40,735 - src.llm.client - INFO - [int:205e43] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:19:40,737 - src.llm.client - INFO - [int:205e43] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=31854 bytes, prompt=26326 chars
2025-12-15 13:19:40,737 - src.llm.client - INFO - [int:205e43] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:19:50,551 - src.llm.request_handler - INFO - [int:205e43] âœ“ Done 9.81s
2025-12-15 13:19:50,551 - src.llm.client - INFO - [int:205e43] âœ… HTTP 200 in 9.81s
2025-12-15 13:19:50,551 - src.llm.client - INFO - [int:205e43] ğŸ“¡ Stream active (200)
2025-12-15 13:19:50,551 - src.llm.client - INFO - [int:205e43] Starting stream parsing, waiting for first chunk...
2025-12-15 13:19:52,579 - src.llm.client - INFO - [int:205e43] ğŸ“Š 2.0s: 365c @180c/s (65ch, ~91t @45t/s)
2025-12-15 13:19:54,599 - src.llm.client - INFO - [int:205e43] ğŸ“Š 4.0s: 739c @183c/s (130ch, ~185t @46t/s)
2025-12-15 13:19:56,618 - src.llm.client - INFO - [int:205e43] ğŸ“Š 6.1s: 1140c @188c/s (195ch, ~285t @47t/s)
2025-12-15 13:19:58,635 - src.llm.client - INFO - [int:205e43] ğŸ“Š 8.1s: 1505c @186c/s (260ch, ~376t @47t/s)
2025-12-15 13:20:00,654 - src.llm.client - INFO - [int:205e43] ğŸ“Š 10.1s: 1903c @188c/s (325ch, ~476t @47t/s)
2025-12-15 13:20:02,680 - src.llm.client - INFO - [int:205e43] ğŸ“Š 12.1s: 2310c @190c/s (390ch, ~578t @48t/s)
2025-12-15 13:20:04,703 - src.llm.client - INFO - [int:205e43] ğŸ“Š 14.2s: 2726c @193c/s (455ch, ~682t @48t/s)
2025-12-15 13:20:06,723 - src.llm.client - INFO - [int:205e43] ğŸ“Š 16.2s: 3117c @193c/s (520ch, ~779t @48t/s)
2025-12-15 13:20:08,753 - src.llm.client - INFO - [int:205e43] ğŸ“Š 18.2s: 3450c @190c/s (585ch, ~862t @47t/s)
2025-12-15 13:20:10,772 - src.llm.client - INFO - [int:205e43] ğŸ“Š 20.2s: 3712c @184c/s (650ch, ~928t @46t/s)
2025-12-15 13:20:11,567 - src.llm.client - INFO - [int:205e43] âœ“ Done 30.83s: 3783c (~518w @123c/s)
2025-12-15 13:20:11,569 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:20:11,569 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:20:11,571 - generate_secondary - INFO -     - Length: 3769 chars, 516 words
2025-12-15 13:20:11,571 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:20:11,571 - generate_secondary - INFO -     - Connections: 30
2025-12-15 13:20:11,571 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:20:11,572 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_05/integration.md
2025-12-15 13:20:11,572 - generate_secondary - INFO - Generating investigation for session 5: Variational Free Energy...
2025-12-15 13:20:11,572 - src.llm.client - INFO - [inv:3fb68d] ğŸš€ inv | m=gemma3:4b | p=25239c | t=150s
2025-12-15 13:20:11,572 - src.llm.client - INFO - [inv:3fb68d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:20:11,572 - src.llm.client - INFO - [inv:3fb68d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:20:11,574 - src.llm.client - INFO - [inv:3fb68d] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=29710 bytes, prompt=25239 chars
2025-12-15 13:20:11,574 - src.llm.client - INFO - [inv:3fb68d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:20:47,224 - src.llm.request_handler - INFO - [inv:3fb68d] âœ“ Done 35.65s
2025-12-15 13:20:47,224 - src.llm.client - INFO - [inv:3fb68d] âœ… HTTP 200 in 35.65s
2025-12-15 13:20:47,224 - src.llm.client - INFO - [inv:3fb68d] ğŸ“¡ Stream active (200)
2025-12-15 13:20:47,225 - src.llm.client - INFO - [inv:3fb68d] Starting stream parsing, waiting for first chunk...
2025-12-15 13:20:49,249 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 2.0s: 387c @191c/s (65ch, ~97t @48t/s)
2025-12-15 13:20:51,267 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 4.0s: 692c @171c/s (130ch, ~173t @43t/s)
2025-12-15 13:20:53,287 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 6.1s: 1011c @167c/s (195ch, ~253t @42t/s)
2025-12-15 13:20:55,304 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 8.1s: 1374c @170c/s (260ch, ~344t @43t/s)
2025-12-15 13:20:57,320 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 10.1s: 1763c @175c/s (325ch, ~441t @44t/s)
2025-12-15 13:20:59,340 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 12.1s: 2070c @171c/s (390ch, ~518t @43t/s)
2025-12-15 13:21:01,362 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 14.1s: 2410c @170c/s (455ch, ~602t @43t/s)
2025-12-15 13:21:03,389 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 16.2s: 2762c @171c/s (520ch, ~690t @43t/s)
2025-12-15 13:21:05,410 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 18.2s: 3162c @174c/s (585ch, ~790t @43t/s)
2025-12-15 13:21:07,437 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 20.2s: 3492c @173c/s (650ch, ~873t @43t/s)
2025-12-15 13:21:09,463 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 22.2s: 3864c @174c/s (715ch, ~966t @43t/s)
2025-12-15 13:21:11,489 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 24.3s: 4238c @175c/s (780ch, ~1060t @44t/s)
2025-12-15 13:21:13,519 - src.llm.client - INFO - [inv:3fb68d] ğŸ“Š 26.3s: 4527c @172c/s (845ch, ~1132t @43t/s)
2025-12-15 13:21:13,836 - src.llm.client - INFO - [inv:3fb68d] âœ“ Done 62.26s: 4528c (~650w @73c/s)
2025-12-15 13:21:13,839 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:21:13,839 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:21:13,839 - generate_secondary - INFO -     - Length: 4514 chars, 648 words
2025-12-15 13:21:13,839 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:21:13,839 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:21:13,839 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:21:13,839 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_05/investigation.md
2025-12-15 13:21:13,840 - generate_secondary - INFO - Generating open_questions for session 5: Variational Free Energy...
2025-12-15 13:21:13,840 - src.llm.client - INFO - [opq:149392] ğŸš€ opq | m=gemma3:4b | p=25325c | t=150s
2025-12-15 13:21:13,840 - src.llm.client - INFO - [opq:149392] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:21:13,840 - src.llm.client - INFO - [opq:149392] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:21:13,841 - src.llm.client - INFO - [opq:149392] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=29807 bytes, prompt=25325 chars
2025-12-15 13:21:13,841 - src.llm.client - INFO - [opq:149392] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:21:23,905 - src.llm.request_handler - INFO - [opq:149392] âœ“ Done 10.06s
2025-12-15 13:21:23,905 - src.llm.client - INFO - [opq:149392] âœ… HTTP 200 in 10.06s
2025-12-15 13:21:23,905 - src.llm.client - INFO - [opq:149392] ğŸ“¡ Stream active (200)
2025-12-15 13:21:23,905 - src.llm.client - INFO - [opq:149392] Starting stream parsing, waiting for first chunk...
2025-12-15 13:21:25,927 - src.llm.client - INFO - [opq:149392] ğŸ“Š 2.0s: 327c @162c/s (65ch, ~82t @40t/s)
2025-12-15 13:21:27,947 - src.llm.client - INFO - [opq:149392] ğŸ“Š 4.0s: 745c @184c/s (130ch, ~186t @46t/s)
2025-12-15 13:21:29,960 - src.llm.client - INFO - [opq:149392] ğŸ“Š 6.1s: 1034c @171c/s (195ch, ~258t @43t/s)
2025-12-15 13:21:31,978 - src.llm.client - INFO - [opq:149392] ğŸ“Š 8.1s: 1460c @181c/s (260ch, ~365t @45t/s)
2025-12-15 13:21:33,999 - src.llm.client - INFO - [opq:149392] ğŸ“Š 10.1s: 1876c @186c/s (325ch, ~469t @46t/s)
2025-12-15 13:21:35,262 - src.llm.client - INFO - [opq:149392] âœ“ Done 21.42s: 2111c (~281w @99c/s)
2025-12-15 13:21:35,263 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:21:35,263 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:21:35,263 - generate_secondary - INFO -     - Length: 1941 chars, 259 words
2025-12-15 13:21:35,263 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:21:35,263 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:21:35,263 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:21:35,263 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_03_variational_inference/session_05/open_questions.md
2025-12-15 13:21:35,263 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:21:35,264 - generate_secondary - INFO - 
============================================================
2025-12-15 13:21:35,264 - generate_secondary - INFO - [4/10] Module 4: Hierarchical Generative Models (2 sessions)
2025-12-15 13:21:35,264 - generate_secondary - INFO - ============================================================
2025-12-15 13:21:35,264 - generate_secondary - INFO - 
  Session 6/20: Recurrent Predictive Models
2025-12-15 13:21:35,264 - generate_secondary - INFO - Generating application for session 6: Recurrent Predictive Models...
2025-12-15 13:21:35,264 - src.llm.client - INFO - [app:8d3231] ğŸš€ app | m=gemma3:4b | p=33819c | t=150s
2025-12-15 13:21:35,264 - src.llm.client - INFO - [app:8d3231] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:21:35,264 - src.llm.client - INFO - [app:8d3231] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:21:35,266 - src.llm.client - INFO - [app:8d3231] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35982 bytes, prompt=33819 chars
2025-12-15 13:21:35,266 - src.llm.client - INFO - [app:8d3231] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:21:45,080 - src.llm.request_handler - INFO - [app:8d3231] âœ“ Done 9.81s
2025-12-15 13:21:45,080 - src.llm.client - INFO - [app:8d3231] âœ… HTTP 200 in 9.81s
2025-12-15 13:21:45,080 - src.llm.client - INFO - [app:8d3231] ğŸ“¡ Stream active (200)
2025-12-15 13:21:45,081 - src.llm.client - INFO - [app:8d3231] Starting stream parsing, waiting for first chunk...
2025-12-15 13:21:47,109 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 2.0s: 403c @199c/s (65ch, ~101t @50t/s)
2025-12-15 13:21:49,127 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 4.0s: 805c @199c/s (130ch, ~201t @50t/s)
2025-12-15 13:21:51,147 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 6.1s: 1139c @188c/s (195ch, ~285t @47t/s)
2025-12-15 13:21:53,165 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 8.1s: 1583c @196c/s (260ch, ~396t @49t/s)
2025-12-15 13:21:55,185 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 10.1s: 1972c @195c/s (325ch, ~493t @49t/s)
2025-12-15 13:21:57,202 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 12.1s: 2384c @197c/s (390ch, ~596t @49t/s)
2025-12-15 13:21:59,223 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 14.1s: 2777c @196c/s (455ch, ~694t @49t/s)
2025-12-15 13:22:01,245 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 16.2s: 3200c @198c/s (520ch, ~800t @49t/s)
2025-12-15 13:22:03,270 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 18.2s: 3634c @200c/s (585ch, ~908t @50t/s)
2025-12-15 13:22:05,297 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 20.2s: 3998c @198c/s (650ch, ~1000t @49t/s)
2025-12-15 13:22:07,325 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 22.2s: 4415c @198c/s (715ch, ~1104t @50t/s)
2025-12-15 13:22:09,354 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 24.3s: 4832c @199c/s (780ch, ~1208t @50t/s)
2025-12-15 13:22:11,384 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 26.3s: 5190c @197c/s (845ch, ~1298t @49t/s)
2025-12-15 13:22:13,387 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 28.3s: 5557c @196c/s (909ch, ~1389t @49t/s)
2025-12-15 13:22:15,515 - src.llm.client - INFO - [app:8d3231] ğŸ“Š 30.4s: 5872c @193c/s (963ch, ~1468t @48t/s)
2025-12-15 13:22:15,516 - src.llm.client - INFO - [app:8d3231] âœ“ Done 40.25s: 5872c (~787w @146c/s)
2025-12-15 13:22:15,518 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:22:15,518 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:22:15,518 - generate_secondary - INFO -     - Length: 5871 chars, 787 words
2025-12-15 13:22:15,518 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:22:15,518 - generate_secondary - INFO -     - Applications: 4
2025-12-15 13:22:15,518 - generate_secondary - INFO -     - Avg words per application: 190
2025-12-15 13:22:15,518 - generate_secondary - WARNING - [WARNING] Application 2 has 203 words (exceeds 200 by 3 words - consider condensing) âš ï¸
2025-12-15 13:22:15,519 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_06/application.md
2025-12-15 13:22:15,519 - generate_secondary - INFO - Generating extension for session 6: Recurrent Predictive Models...
2025-12-15 13:22:15,519 - src.llm.client - INFO - [ext:574806] ğŸš€ ext | m=gemma3:4b | p=27705c | t=120s
2025-12-15 13:22:15,519 - src.llm.client - INFO - [ext:574806] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:22:15,519 - src.llm.client - INFO - [ext:574806] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:22:15,520 - src.llm.client - INFO - [ext:574806] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32739 bytes, prompt=27705 chars
2025-12-15 13:22:15,520 - src.llm.client - INFO - [ext:574806] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:22:50,099 - src.llm.request_handler - INFO - [ext:574806] âœ“ Done 34.58s
2025-12-15 13:22:50,099 - src.llm.client - INFO - [ext:574806] âœ… HTTP 200 in 34.58s
2025-12-15 13:22:50,099 - src.llm.client - INFO - [ext:574806] ğŸ“¡ Stream active (200)
2025-12-15 13:22:50,099 - src.llm.client - INFO - [ext:574806] Starting stream parsing, waiting for first chunk...
2025-12-15 13:22:52,123 - src.llm.client - INFO - [ext:574806] ğŸ“Š 2.0s: 409c @202c/s (65ch, ~102t @51t/s)
2025-12-15 13:22:54,142 - src.llm.client - INFO - [ext:574806] ğŸ“Š 4.0s: 774c @191c/s (130ch, ~194t @48t/s)
2025-12-15 13:22:56,163 - src.llm.client - INFO - [ext:574806] ğŸ“Š 6.1s: 1203c @198c/s (195ch, ~301t @50t/s)
2025-12-15 13:22:58,179 - src.llm.client - INFO - [ext:574806] ğŸ“Š 8.1s: 1624c @201c/s (260ch, ~406t @50t/s)
2025-12-15 13:23:00,197 - src.llm.client - INFO - [ext:574806] ğŸ“Š 10.1s: 2034c @201c/s (325ch, ~508t @50t/s)
2025-12-15 13:23:02,217 - src.llm.client - INFO - [ext:574806] ğŸ“Š 12.1s: 2438c @201c/s (390ch, ~610t @50t/s)
2025-12-15 13:23:04,238 - src.llm.client - INFO - [ext:574806] ğŸ“Š 14.1s: 2833c @200c/s (455ch, ~708t @50t/s)
2025-12-15 13:23:06,260 - src.llm.client - INFO - [ext:574806] ğŸ“Š 16.2s: 3188c @197c/s (520ch, ~797t @49t/s)
2025-12-15 13:23:08,287 - src.llm.client - INFO - [ext:574806] ğŸ“Š 18.2s: 3598c @198c/s (585ch, ~900t @49t/s)
2025-12-15 13:23:10,305 - src.llm.client - INFO - [ext:574806] ğŸ“Š 20.2s: 3957c @196c/s (650ch, ~989t @49t/s)
2025-12-15 13:23:12,328 - src.llm.client - INFO - [ext:574806] ğŸ“Š 22.2s: 4399c @198c/s (715ch, ~1100t @49t/s)
2025-12-15 13:23:13,882 - src.llm.client - INFO - [ext:574806] âœ“ Done 58.36s: 4666c (~619w @80c/s)
2025-12-15 13:23:13,884 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:23:13,884 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:23:13,884 - generate_secondary - INFO -     - Length: 4665 chars, 619 words
2025-12-15 13:23:13,884 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:23:13,884 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:23:13,884 - generate_secondary - INFO -     - Avg words per topic: 198
2025-12-15 13:23:13,884 - generate_secondary - WARNING - [WARNING] Topic 1 has 199 words (exceeds 150 by 49 words - consider condensing) âš ï¸
2025-12-15 13:23:13,884 - generate_secondary - WARNING - [WARNING] Topic 2 has 189 words (exceeds 150 by 39 words - consider condensing) âš ï¸
2025-12-15 13:23:13,884 - generate_secondary - WARNING - [WARNING] Topic 3 has 207 words (exceeds 150 by 57 words - consider condensing) âš ï¸
2025-12-15 13:23:13,884 - generate_secondary - WARNING - [WARNING] Total word count (619) exceeds maximum 600 (exceeds by 19 words - condense content) âš ï¸
2025-12-15 13:23:13,884 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:23:13,884 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:23:13,885 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_06/extension.md
2025-12-15 13:23:13,885 - generate_secondary - INFO - Generating visualization for session 6: Recurrent Predictive Models...
2025-12-15 13:23:13,885 - src.llm.client - INFO - [viz:12a1cd] ğŸš€ viz | m=gemma3:4b | p=26665c | t=120s
2025-12-15 13:23:13,885 - src.llm.client - INFO - [viz:12a1cd] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:23:13,885 - src.llm.client - INFO - [viz:12a1cd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:23:13,886 - src.llm.client - INFO - [viz:12a1cd] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=31021 bytes, prompt=26665 chars
2025-12-15 13:23:13,886 - src.llm.client - INFO - [viz:12a1cd] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:23:24,269 - src.llm.request_handler - INFO - [viz:12a1cd] âœ“ Done 10.38s
2025-12-15 13:23:24,269 - src.llm.client - INFO - [viz:12a1cd] âœ… HTTP 200 in 10.38s
2025-12-15 13:23:24,269 - src.llm.client - INFO - [viz:12a1cd] ğŸ“¡ Stream active (200)
2025-12-15 13:23:24,269 - src.llm.client - INFO - [viz:12a1cd] Starting stream parsing, waiting for first chunk...
2025-12-15 13:23:26,294 - src.llm.client - INFO - [viz:12a1cd] ğŸ“Š 2.0s: 259c @128c/s (65ch, ~65t @32t/s)
2025-12-15 13:23:28,314 - src.llm.client - INFO - [viz:12a1cd] ğŸ“Š 4.0s: 476c @118c/s (130ch, ~119t @29t/s)
2025-12-15 13:23:30,331 - src.llm.client - INFO - [viz:12a1cd] ğŸ“Š 6.1s: 687c @113c/s (195ch, ~172t @28t/s)
2025-12-15 13:23:32,347 - src.llm.client - INFO - [viz:12a1cd] ğŸ“Š 8.1s: 924c @114c/s (260ch, ~231t @29t/s)
2025-12-15 13:23:34,369 - src.llm.client - INFO - [viz:12a1cd] ğŸ“Š 10.1s: 1123c @111c/s (325ch, ~281t @28t/s)
2025-12-15 13:23:36,393 - src.llm.client - INFO - [viz:12a1cd] ğŸ“Š 12.1s: 1416c @117c/s (390ch, ~354t @29t/s)
2025-12-15 13:23:37,204 - src.llm.client - INFO - [viz:12a1cd] âœ“ Done 23.32s: 1525c (~223w @65c/s)
2025-12-15 13:23:37,204 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:23:37,205 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:23:37,205 - generate_secondary - INFO -     - Length: 506 chars (cleaned: 506 chars)
2025-12-15 13:23:37,205 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:23:37,205 - generate_secondary - INFO - [OK] Elements: 36 total (nodes: 13, connections: 23) âœ“
2025-12-15 13:23:37,205 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_06/visualization.mmd
2025-12-15 13:23:37,205 - generate_secondary - INFO - Generating integration for session 6: Recurrent Predictive Models...
2025-12-15 13:23:37,205 - src.llm.client - INFO - [int:35dd30] ğŸš€ int | m=gemma3:4b | p=28014c | t=150s
2025-12-15 13:23:37,205 - src.llm.client - INFO - [int:35dd30] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:23:37,205 - src.llm.client - INFO - [int:35dd30] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:23:37,206 - src.llm.client - INFO - [int:35dd30] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=33387 bytes, prompt=28014 chars
2025-12-15 13:23:37,206 - src.llm.client - INFO - [int:35dd30] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:23:47,021 - src.llm.request_handler - INFO - [int:35dd30] âœ“ Done 9.81s
2025-12-15 13:23:47,021 - src.llm.client - INFO - [int:35dd30] âœ… HTTP 200 in 9.81s
2025-12-15 13:23:47,021 - src.llm.client - INFO - [int:35dd30] ğŸ“¡ Stream active (200)
2025-12-15 13:23:47,021 - src.llm.client - INFO - [int:35dd30] Starting stream parsing, waiting for first chunk...
2025-12-15 13:23:49,050 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 2.0s: 366c @180c/s (65ch, ~92t @45t/s)
2025-12-15 13:23:51,072 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 4.1s: 796c @197c/s (130ch, ~199t @49t/s)
2025-12-15 13:23:53,092 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 6.1s: 1206c @199c/s (195ch, ~302t @50t/s)
2025-12-15 13:23:55,109 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 8.1s: 1600c @198c/s (260ch, ~400t @49t/s)
2025-12-15 13:23:57,126 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 10.1s: 2025c @200c/s (325ch, ~506t @50t/s)
2025-12-15 13:23:59,148 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 12.1s: 2375c @196c/s (390ch, ~594t @49t/s)
2025-12-15 13:24:01,170 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 14.1s: 2806c @198c/s (455ch, ~702t @50t/s)
2025-12-15 13:24:03,184 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 16.2s: 3214c @199c/s (518ch, ~804t @50t/s)
2025-12-15 13:24:05,204 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 18.2s: 3616c @199c/s (583ch, ~904t @50t/s)
2025-12-15 13:24:07,231 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 20.2s: 4006c @198c/s (648ch, ~1002t @50t/s)
2025-12-15 13:24:09,259 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 22.2s: 4416c @199c/s (713ch, ~1104t @50t/s)
2025-12-15 13:24:11,259 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 24.2s: 4715c @195c/s (777ch, ~1179t @49t/s)
2025-12-15 13:24:13,265 - src.llm.client - INFO - [int:35dd30] ğŸ“Š 26.2s: 5006c @191c/s (841ch, ~1252t @48t/s)
2025-12-15 13:24:13,762 - src.llm.client - INFO - [int:35dd30] âœ“ Done 36.56s: 5029c (~652w @138c/s)
2025-12-15 13:24:13,764 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:24:13,764 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:24:13,764 - generate_secondary - INFO -     - Length: 5028 chars, 652 words
2025-12-15 13:24:13,764 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:24:13,764 - generate_secondary - INFO -     - Connections: 20
2025-12-15 13:24:13,764 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:24:13,764 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_06/integration.md
2025-12-15 13:24:13,765 - generate_secondary - INFO - Generating investigation for session 6: Recurrent Predictive Models...
2025-12-15 13:24:13,765 - src.llm.client - INFO - [inv:9d886d] ğŸš€ inv | m=gemma3:4b | p=26927c | t=150s
2025-12-15 13:24:13,765 - src.llm.client - INFO - [inv:9d886d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:24:13,765 - src.llm.client - INFO - [inv:9d886d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:24:13,766 - src.llm.client - INFO - [inv:9d886d] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=31243 bytes, prompt=26927 chars
2025-12-15 13:24:13,766 - src.llm.client - INFO - [inv:9d886d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:24:23,823 - src.llm.request_handler - INFO - [inv:9d886d] âœ“ Done 10.06s
2025-12-15 13:24:23,823 - src.llm.client - INFO - [inv:9d886d] âœ… HTTP 200 in 10.06s
2025-12-15 13:24:23,823 - src.llm.client - INFO - [inv:9d886d] ğŸ“¡ Stream active (200)
2025-12-15 13:24:23,823 - src.llm.client - INFO - [inv:9d886d] Starting stream parsing, waiting for first chunk...
2025-12-15 13:24:25,849 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 2.0s: 377c @186c/s (65ch, ~94t @47t/s)
2025-12-15 13:24:27,871 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 4.0s: 743c @184c/s (130ch, ~186t @46t/s)
2025-12-15 13:24:29,886 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 6.1s: 988c @163c/s (195ch, ~247t @41t/s)
2025-12-15 13:24:31,900 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 8.1s: 1328c @164c/s (260ch, ~332t @41t/s)
2025-12-15 13:24:33,920 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 10.1s: 1738c @172c/s (325ch, ~434t @43t/s)
2025-12-15 13:24:35,929 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 12.1s: 2134c @176c/s (389ch, ~534t @44t/s)
2025-12-15 13:24:37,953 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 14.1s: 2522c @178c/s (454ch, ~630t @45t/s)
2025-12-15 13:24:39,972 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 16.1s: 2854c @177c/s (518ch, ~714t @44t/s)
2025-12-15 13:24:41,995 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 18.2s: 3185c @175c/s (583ch, ~796t @44t/s)
2025-12-15 13:24:44,019 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 20.2s: 3529c @175c/s (648ch, ~882t @44t/s)
2025-12-15 13:24:46,047 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 22.2s: 3917c @176c/s (713ch, ~979t @44t/s)
2025-12-15 13:24:48,072 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 24.2s: 4302c @177c/s (778ch, ~1076t @44t/s)
2025-12-15 13:24:50,097 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 26.3s: 4670c @178c/s (843ch, ~1168t @44t/s)
2025-12-15 13:24:52,125 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 28.3s: 5004c @177c/s (908ch, ~1251t @44t/s)
2025-12-15 13:24:54,127 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 30.3s: 5387c @178c/s (972ch, ~1347t @44t/s)
2025-12-15 13:24:56,131 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 32.3s: 5805c @180c/s (1036ch, ~1451t @45t/s)
2025-12-15 13:24:58,134 - src.llm.client - INFO - [inv:9d886d] ğŸ“Š 34.3s: 6067c @177c/s (1100ch, ~1517t @44t/s)
2025-12-15 13:24:59,603 - src.llm.client - INFO - [inv:9d886d] âœ“ Done 45.84s: 6189c (~907w @135c/s)
2025-12-15 13:24:59,605 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:24:59,605 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:24:59,605 - generate_secondary - INFO -     - Length: 6186 chars, 907 words
2025-12-15 13:24:59,605 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:24:59,605 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:24:59,605 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:24:59,606 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_06/investigation.md
2025-12-15 13:24:59,606 - generate_secondary - INFO - Generating open_questions for session 6: Recurrent Predictive Models...
2025-12-15 13:24:59,606 - src.llm.client - INFO - [opq:3da06f] ğŸš€ opq | m=gemma3:4b | p=27013c | t=150s
2025-12-15 13:24:59,606 - src.llm.client - INFO - [opq:3da06f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:24:59,606 - src.llm.client - INFO - [opq:3da06f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:24:59,607 - src.llm.client - INFO - [opq:3da06f] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=31340 bytes, prompt=27013 chars
2025-12-15 13:24:59,607 - src.llm.client - INFO - [opq:3da06f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:25:09,637 - src.llm.request_handler - INFO - [opq:3da06f] âœ“ Done 10.03s
2025-12-15 13:25:09,637 - src.llm.client - INFO - [opq:3da06f] âœ… HTTP 200 in 10.03s
2025-12-15 13:25:09,637 - src.llm.client - INFO - [opq:3da06f] ğŸ“¡ Stream active (200)
2025-12-15 13:25:09,637 - src.llm.client - INFO - [opq:3da06f] Starting stream parsing, waiting for first chunk...
2025-12-15 13:25:11,663 - src.llm.client - INFO - [opq:3da06f] ğŸ“Š 2.0s: 337c @166c/s (65ch, ~84t @42t/s)
2025-12-15 13:25:13,678 - src.llm.client - INFO - [opq:3da06f] ğŸ“Š 4.0s: 766c @190c/s (130ch, ~192t @47t/s)
2025-12-15 13:25:15,695 - src.llm.client - INFO - [opq:3da06f] ğŸ“Š 6.1s: 1142c @188c/s (195ch, ~286t @47t/s)
2025-12-15 13:25:17,712 - src.llm.client - INFO - [opq:3da06f] ğŸ“Š 8.1s: 1562c @193c/s (260ch, ~390t @48t/s)
2025-12-15 13:25:19,731 - src.llm.client - INFO - [opq:3da06f] ğŸ“Š 10.1s: 1902c @188c/s (325ch, ~476t @47t/s)
2025-12-15 13:25:21,755 - src.llm.client - INFO - [opq:3da06f] ğŸ“Š 12.1s: 2325c @192c/s (390ch, ~581t @48t/s)
2025-12-15 13:25:22,384 - src.llm.client - INFO - [opq:3da06f] âœ“ Done 22.78s: 2383c (~313w @105c/s)
2025-12-15 13:25:22,385 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:25:22,385 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:25:22,385 - generate_secondary - INFO -     - Length: 2382 chars, 313 words
2025-12-15 13:25:22,385 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:25:22,385 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:25:22,385 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:25:22,385 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_06/open_questions.md
2025-12-15 13:25:22,385 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:25:22,385 - generate_secondary - INFO - 
  Session 7/20: Deep Predictive Processing
2025-12-15 13:25:22,386 - generate_secondary - INFO - Generating application for session 7: Deep Predictive Processing...
2025-12-15 13:25:22,386 - src.llm.client - INFO - [app:8f7a8a] ğŸš€ app | m=gemma3:4b | p=33901c | t=150s
2025-12-15 13:25:22,386 - src.llm.client - INFO - [app:8f7a8a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:25:22,386 - src.llm.client - INFO - [app:8f7a8a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:25:22,387 - src.llm.client - INFO - [app:8f7a8a] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35872 bytes, prompt=33901 chars
2025-12-15 13:25:22,387 - src.llm.client - INFO - [app:8f7a8a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:25:32,209 - src.llm.request_handler - INFO - [app:8f7a8a] âœ“ Done 9.82s
2025-12-15 13:25:32,210 - src.llm.client - INFO - [app:8f7a8a] âœ… HTTP 200 in 9.82s
2025-12-15 13:25:32,210 - src.llm.client - INFO - [app:8f7a8a] ğŸ“¡ Stream active (200)
2025-12-15 13:25:32,210 - src.llm.client - INFO - [app:8f7a8a] Starting stream parsing, waiting for first chunk...
2025-12-15 13:25:34,236 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 2.0s: 366c @181c/s (65ch, ~92t @45t/s)
2025-12-15 13:25:36,254 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 4.0s: 725c @179c/s (130ch, ~181t @45t/s)
2025-12-15 13:25:38,278 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 6.1s: 1123c @185c/s (194ch, ~281t @46t/s)
2025-12-15 13:25:40,292 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 8.1s: 1498c @185c/s (259ch, ~374t @46t/s)
2025-12-15 13:25:42,307 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 10.1s: 1855c @184c/s (324ch, ~464t @46t/s)
2025-12-15 13:25:44,326 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 12.1s: 2268c @187c/s (389ch, ~567t @47t/s)
2025-12-15 13:25:46,348 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 14.1s: 2620c @185c/s (454ch, ~655t @46t/s)
2025-12-15 13:25:48,371 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 16.2s: 3028c @187c/s (519ch, ~757t @47t/s)
2025-12-15 13:25:50,394 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 18.2s: 3443c @189c/s (584ch, ~861t @47t/s)
2025-12-15 13:25:52,415 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 20.2s: 3819c @189c/s (649ch, ~955t @47t/s)
2025-12-15 13:25:54,444 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 22.2s: 4192c @189c/s (714ch, ~1048t @47t/s)
2025-12-15 13:25:56,467 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 24.3s: 4603c @190c/s (779ch, ~1151t @47t/s)
2025-12-15 13:25:58,497 - src.llm.client - INFO - [app:8f7a8a] ğŸ“Š 26.3s: 5018c @191c/s (844ch, ~1254t @48t/s)
2025-12-15 13:25:59,800 - src.llm.client - INFO - [app:8f7a8a] âœ“ Done 37.41s: 5189c (~701w @139c/s)
2025-12-15 13:25:59,802 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:25:59,802 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:25:59,802 - generate_secondary - INFO -     - Length: 5177 chars, 699 words
2025-12-15 13:25:59,802 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:25:59,802 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:25:59,803 - generate_secondary - INFO -     - Avg words per application: 135
2025-12-15 13:25:59,803 - generate_secondary - WARNING - [WARNING] Application 2 has 146 words (require 150-200, need 4 more words) âš ï¸
2025-12-15 13:25:59,803 - generate_secondary - WARNING - [WARNING] Application 3 has 126 words (require 150-200, need 24 more words) âš ï¸
2025-12-15 13:25:59,803 - generate_secondary - WARNING - [WARNING] Application 4 has 123 words (require 150-200, need 27 more words) âš ï¸
2025-12-15 13:25:59,803 - generate_secondary - WARNING - [WARNING] Application 5 has 122 words (require 150-200, need 28 more words) âš ï¸
2025-12-15 13:25:59,804 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_07/application.md
2025-12-15 13:25:59,804 - generate_secondary - INFO - Generating extension for session 7: Deep Predictive Processing...
2025-12-15 13:25:59,805 - src.llm.client - INFO - [ext:17dba0] ğŸš€ ext | m=gemma3:4b | p=27787c | t=120s
2025-12-15 13:25:59,805 - src.llm.client - INFO - [ext:17dba0] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:25:59,805 - src.llm.client - INFO - [ext:17dba0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:25:59,806 - src.llm.client - INFO - [ext:17dba0] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32629 bytes, prompt=27787 chars
2025-12-15 13:25:59,806 - src.llm.client - INFO - [ext:17dba0] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:26:09,845 - src.llm.request_handler - INFO - [ext:17dba0] âœ“ Done 10.04s
2025-12-15 13:26:09,845 - src.llm.client - INFO - [ext:17dba0] âœ… HTTP 200 in 10.04s
2025-12-15 13:26:09,845 - src.llm.client - INFO - [ext:17dba0] ğŸ“¡ Stream active (200)
2025-12-15 13:26:09,845 - src.llm.client - INFO - [ext:17dba0] Starting stream parsing, waiting for first chunk...
2025-12-15 13:26:11,869 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 2.0s: 347c @171c/s (65ch, ~87t @43t/s)
2025-12-15 13:26:13,888 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 4.0s: 709c @175c/s (130ch, ~177t @44t/s)
2025-12-15 13:26:15,904 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 6.1s: 1087c @179c/s (195ch, ~272t @45t/s)
2025-12-15 13:26:17,917 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 8.1s: 1439c @178c/s (260ch, ~360t @45t/s)
2025-12-15 13:26:19,936 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 10.1s: 1872c @186c/s (325ch, ~468t @46t/s)
2025-12-15 13:26:21,957 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 12.1s: 2294c @189c/s (390ch, ~574t @47t/s)
2025-12-15 13:26:23,980 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 14.1s: 2671c @189c/s (455ch, ~668t @47t/s)
2025-12-15 13:26:26,003 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 16.2s: 2987c @185c/s (520ch, ~747t @46t/s)
2025-12-15 13:26:28,032 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 18.2s: 3314c @182c/s (585ch, ~828t @46t/s)
2025-12-15 13:26:30,059 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 20.2s: 3694c @183c/s (650ch, ~924t @46t/s)
2025-12-15 13:26:32,067 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 22.2s: 4074c @183c/s (714ch, ~1018t @46t/s)
2025-12-15 13:26:34,092 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 24.2s: 4292c @177c/s (779ch, ~1073t @44t/s)
2025-12-15 13:26:36,207 - src.llm.client - INFO - [ext:17dba0] ğŸ“Š 26.4s: 4493c @170c/s (837ch, ~1123t @43t/s)
2025-12-15 13:26:36,208 - src.llm.client - INFO - [ext:17dba0] âœ“ Done 36.40s: 4493c (~592w @123c/s)
2025-12-15 13:26:36,210 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:26:36,210 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - INFO -     - Length: 4477 chars, 590 words
2025-12-15 13:26:36,210 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:26:36,210 - generate_secondary - INFO -     - Topics: 6
2025-12-15 13:26:36,210 - generate_secondary - INFO -     - Avg words per topic: 95
2025-12-15 13:26:36,210 - generate_secondary - WARNING - [WARNING] Too many topics (6, maximum 4, 2 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - WARNING - [WARNING] Topic 1 has 182 words (exceeds 150 by 32 words - consider condensing) âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - WARNING - [WARNING] Topic 2 has 182 words (exceeds 150 by 32 words - consider condensing) âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - WARNING - [WARNING] Topic 3 has 199 words (exceeds 150 by 49 words - consider condensing) âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - WARNING - [WARNING] Topic 6 has 3 words (require 100-150, need 97 more words) âš ï¸
2025-12-15 13:26:36,210 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:26:36,210 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:26:36,211 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_07/extension.md
2025-12-15 13:26:36,211 - generate_secondary - INFO - Generating visualization for session 7: Deep Predictive Processing...
2025-12-15 13:26:36,211 - src.llm.client - INFO - [viz:d62649] ğŸš€ viz | m=gemma3:4b | p=26747c | t=120s
2025-12-15 13:26:36,211 - src.llm.client - INFO - [viz:d62649] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:26:36,211 - src.llm.client - INFO - [viz:d62649] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:26:36,212 - src.llm.client - INFO - [viz:d62649] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30911 bytes, prompt=26747 chars
2025-12-15 13:26:36,212 - src.llm.client - INFO - [viz:d62649] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:26:46,546 - src.llm.request_handler - INFO - [viz:d62649] âœ“ Done 10.33s
2025-12-15 13:26:46,546 - src.llm.client - INFO - [viz:d62649] âœ… HTTP 200 in 10.33s
2025-12-15 13:26:46,546 - src.llm.client - INFO - [viz:d62649] ğŸ“¡ Stream active (200)
2025-12-15 13:26:46,546 - src.llm.client - INFO - [viz:d62649] Starting stream parsing, waiting for first chunk...
2025-12-15 13:26:48,569 - src.llm.client - INFO - [viz:d62649] ğŸ“Š 2.0s: 272c @134c/s (65ch, ~68t @34t/s)
2025-12-15 13:26:50,591 - src.llm.client - INFO - [viz:d62649] ğŸ“Š 4.0s: 485c @120c/s (130ch, ~121t @30t/s)
2025-12-15 13:26:52,609 - src.llm.client - INFO - [viz:d62649] ğŸ“Š 6.1s: 792c @131c/s (195ch, ~198t @33t/s)
2025-12-15 13:26:54,627 - src.llm.client - INFO - [viz:d62649] ğŸ“Š 8.1s: 1038c @128c/s (260ch, ~260t @32t/s)
2025-12-15 13:26:56,894 - src.llm.client - INFO - [viz:d62649] ğŸ“Š 10.3s: 1362c @132c/s (325ch, ~340t @33t/s)
2025-12-15 13:26:56,894 - src.llm.client - INFO - [viz:d62649] âœ“ Done 20.68s: 1362c (~200w @66c/s)
2025-12-15 13:26:56,895 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-15 13:26:56,895 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:26:56,895 - generate_secondary - INFO -     - Length: 271 chars (cleaned: 271 chars)
2025-12-15 13:26:56,895 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:26:56,895 - generate_secondary - INFO - [CRITICAL] Elements: 20 total (nodes: 9, connections: 11) ğŸ”´
2025-12-15 13:26:56,895 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:26:56,895 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-15 13:26:56,895 - generate_secondary - WARNING - [WARNING] Only 9 nodes found (require at least 10, need 1 more - add more nodes to the diagram) âš ï¸
2025-12-15 13:26:56,895 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:26:56,895 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:26:56,896 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_07/visualization.mmd
2025-12-15 13:26:56,896 - generate_secondary - INFO - Generating integration for session 7: Deep Predictive Processing...
2025-12-15 13:26:56,896 - src.llm.client - INFO - [int:767256] ğŸš€ int | m=gemma3:4b | p=28096c | t=150s
2025-12-15 13:26:56,896 - src.llm.client - INFO - [int:767256] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:26:56,896 - src.llm.client - INFO - [int:767256] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:26:56,897 - src.llm.client - INFO - [int:767256] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=33277 bytes, prompt=28096 chars
2025-12-15 13:26:56,897 - src.llm.client - INFO - [int:767256] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:27:06,713 - src.llm.request_handler - INFO - [int:767256] âœ“ Done 9.82s
2025-12-15 13:27:06,713 - src.llm.client - INFO - [int:767256] âœ… HTTP 200 in 9.82s
2025-12-15 13:27:06,713 - src.llm.client - INFO - [int:767256] ğŸ“¡ Stream active (200)
2025-12-15 13:27:06,713 - src.llm.client - INFO - [int:767256] Starting stream parsing, waiting for first chunk...
2025-12-15 13:27:08,714 - src.llm.client - INFO - [int:767256] ğŸ“Š 2.0s: 361c @180c/s (64ch, ~90t @45t/s)
2025-12-15 13:27:10,736 - src.llm.client - INFO - [int:767256] ğŸ“Š 4.0s: 755c @188c/s (129ch, ~189t @47t/s)
2025-12-15 13:27:12,754 - src.llm.client - INFO - [int:767256] ğŸ“Š 6.0s: 1173c @194c/s (194ch, ~293t @49t/s)
2025-12-15 13:27:14,774 - src.llm.client - INFO - [int:767256] ğŸ“Š 8.1s: 1522c @189c/s (259ch, ~380t @47t/s)
2025-12-15 13:27:16,794 - src.llm.client - INFO - [int:767256] ğŸ“Š 10.1s: 1927c @191c/s (324ch, ~482t @48t/s)
2025-12-15 13:27:18,819 - src.llm.client - INFO - [int:767256] ğŸ“Š 12.1s: 2319c @192c/s (389ch, ~580t @48t/s)
2025-12-15 13:27:20,843 - src.llm.client - INFO - [int:767256] ğŸ“Š 14.1s: 2723c @193c/s (454ch, ~681t @48t/s)
2025-12-15 13:27:22,864 - src.llm.client - INFO - [int:767256] ğŸ“Š 16.2s: 3046c @189c/s (519ch, ~762t @47t/s)
2025-12-15 13:27:24,886 - src.llm.client - INFO - [int:767256] ğŸ“Š 18.2s: 3293c @181c/s (584ch, ~823t @45t/s)
2025-12-15 13:27:26,055 - src.llm.client - INFO - [int:767256] âœ“ Done 29.16s: 3453c (~464w @118c/s)
2025-12-15 13:27:26,056 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:27:26,057 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:27:26,057 - generate_secondary - INFO -     - Length: 3452 chars, 464 words
2025-12-15 13:27:26,057 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:27:26,057 - generate_secondary - INFO -     - Connections: 10
2025-12-15 13:27:26,057 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:27:26,057 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_07/integration.md
2025-12-15 13:27:26,057 - generate_secondary - INFO - Generating investigation for session 7: Deep Predictive Processing...
2025-12-15 13:27:26,057 - src.llm.client - INFO - [inv:2ef8c4] ğŸš€ inv | m=gemma3:4b | p=27009c | t=150s
2025-12-15 13:27:26,057 - src.llm.client - INFO - [inv:2ef8c4] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:27:26,057 - src.llm.client - INFO - [inv:2ef8c4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:27:26,059 - src.llm.client - INFO - [inv:2ef8c4] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=31133 bytes, prompt=27009 chars
2025-12-15 13:27:26,059 - src.llm.client - INFO - [inv:2ef8c4] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:27:36,122 - src.llm.request_handler - INFO - [inv:2ef8c4] âœ“ Done 10.06s
2025-12-15 13:27:36,122 - src.llm.client - INFO - [inv:2ef8c4] âœ… HTTP 200 in 10.06s
2025-12-15 13:27:36,122 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“¡ Stream active (200)
2025-12-15 13:27:36,122 - src.llm.client - INFO - [inv:2ef8c4] Starting stream parsing, waiting for first chunk...
2025-12-15 13:27:38,149 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 2.0s: 331c @163c/s (64ch, ~83t @41t/s)
2025-12-15 13:27:40,159 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 4.0s: 665c @165c/s (128ch, ~166t @41t/s)
2025-12-15 13:27:42,172 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 6.0s: 987c @163c/s (193ch, ~247t @41t/s)
2025-12-15 13:27:44,185 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 8.1s: 1260c @156c/s (258ch, ~315t @39t/s)
2025-12-15 13:27:46,205 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 10.1s: 1606c @159c/s (323ch, ~402t @40t/s)
2025-12-15 13:27:48,230 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 12.1s: 1959c @162c/s (388ch, ~490t @40t/s)
2025-12-15 13:27:50,252 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 14.1s: 2272c @161c/s (453ch, ~568t @40t/s)
2025-12-15 13:27:52,273 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 16.2s: 2496c @155c/s (518ch, ~624t @39t/s)
2025-12-15 13:27:54,295 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 18.2s: 2834c @156c/s (583ch, ~708t @39t/s)
2025-12-15 13:27:56,318 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 20.2s: 3163c @157c/s (648ch, ~791t @39t/s)
2025-12-15 13:27:58,318 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 22.2s: 3489c @157c/s (712ch, ~872t @39t/s)
2025-12-15 13:28:00,345 - src.llm.client - INFO - [inv:2ef8c4] ğŸ“Š 24.2s: 3826c @158c/s (777ch, ~956t @39t/s)
2025-12-15 13:28:01,666 - src.llm.client - INFO - [inv:2ef8c4] âœ“ Done 35.61s: 4013c (~628w @113c/s)
2025-12-15 13:28:01,668 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:28:01,668 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:28:01,668 - generate_secondary - INFO -     - Length: 3995 chars, 626 words
2025-12-15 13:28:01,668 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:28:01,668 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:28:01,668 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:28:01,669 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_07/investigation.md
2025-12-15 13:28:01,669 - generate_secondary - INFO - Generating open_questions for session 7: Deep Predictive Processing...
2025-12-15 13:28:01,669 - src.llm.client - INFO - [opq:028797] ğŸš€ opq | m=gemma3:4b | p=27095c | t=150s
2025-12-15 13:28:01,669 - src.llm.client - INFO - [opq:028797] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:28:01,669 - src.llm.client - INFO - [opq:028797] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:28:01,670 - src.llm.client - INFO - [opq:028797] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=31230 bytes, prompt=27095 chars
2025-12-15 13:28:01,670 - src.llm.client - INFO - [opq:028797] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:28:11,744 - src.llm.request_handler - INFO - [opq:028797] âœ“ Done 10.07s
2025-12-15 13:28:11,745 - src.llm.client - INFO - [opq:028797] âœ… HTTP 200 in 10.07s
2025-12-15 13:28:11,745 - src.llm.client - INFO - [opq:028797] ğŸ“¡ Stream active (200)
2025-12-15 13:28:11,745 - src.llm.client - INFO - [opq:028797] Starting stream parsing, waiting for first chunk...
2025-12-15 13:28:13,768 - src.llm.client - INFO - [opq:028797] ğŸ“Š 2.0s: 344c @170c/s (65ch, ~86t @43t/s)
2025-12-15 13:28:15,784 - src.llm.client - INFO - [opq:028797] ğŸ“Š 4.0s: 737c @182c/s (130ch, ~184t @46t/s)
2025-12-15 13:28:17,804 - src.llm.client - INFO - [opq:028797] ğŸ“Š 6.1s: 1097c @181c/s (195ch, ~274t @45t/s)
2025-12-15 13:28:19,818 - src.llm.client - INFO - [opq:028797] ğŸ“Š 8.1s: 1525c @189c/s (260ch, ~381t @47t/s)
2025-12-15 13:28:21,837 - src.llm.client - INFO - [opq:028797] ğŸ“Š 10.1s: 1835c @182c/s (325ch, ~459t @45t/s)
2025-12-15 13:28:23,856 - src.llm.client - INFO - [opq:028797] ğŸ“Š 12.1s: 2201c @182c/s (390ch, ~550t @45t/s)
2025-12-15 13:28:24,273 - src.llm.client - INFO - [opq:028797] âœ“ Done 22.60s: 2225c (~311w @98c/s)
2025-12-15 13:28:24,274 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:28:24,274 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:28:24,274 - generate_secondary - INFO -     - Length: 2224 chars, 311 words
2025-12-15 13:28:24,274 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:28:24,274 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:28:24,274 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:28:24,274 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_04_hierarchical_generative_models/session_07/open_questions.md
2025-12-15 13:28:24,274 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:28:24,274 - generate_secondary - INFO - 
============================================================
2025-12-15 13:28:24,274 - generate_secondary - INFO - [5/10] Module 5: Precision Weighting & Attention (2 sessions)
2025-12-15 13:28:24,274 - generate_secondary - INFO - ============================================================
2025-12-15 13:28:24,274 - generate_secondary - INFO - 
  Session 8/20: Dynamic Priors
2025-12-15 13:28:24,275 - generate_secondary - INFO - Generating application for session 8: Dynamic Priors...
2025-12-15 13:28:24,275 - src.llm.client - INFO - [app:1f22c1] ğŸš€ app | m=gemma3:4b | p=33017c | t=150s
2025-12-15 13:28:24,275 - src.llm.client - INFO - [app:1f22c1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:28:24,275 - src.llm.client - INFO - [app:1f22c1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:28:24,276 - src.llm.client - INFO - [app:1f22c1] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35282 bytes, prompt=33017 chars
2025-12-15 13:28:24,276 - src.llm.client - INFO - [app:1f22c1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:28:34,097 - src.llm.request_handler - INFO - [app:1f22c1] âœ“ Done 9.82s
2025-12-15 13:28:34,097 - src.llm.client - INFO - [app:1f22c1] âœ… HTTP 200 in 9.82s
2025-12-15 13:28:34,097 - src.llm.client - INFO - [app:1f22c1] ğŸ“¡ Stream active (200)
2025-12-15 13:28:34,097 - src.llm.client - INFO - [app:1f22c1] Starting stream parsing, waiting for first chunk...
2025-12-15 13:28:36,121 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 2.0s: 381c @188c/s (65ch, ~95t @47t/s)
2025-12-15 13:28:38,151 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 4.1s: 773c @191c/s (130ch, ~193t @48t/s)
2025-12-15 13:28:40,173 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 6.1s: 1206c @199c/s (194ch, ~302t @50t/s)
2025-12-15 13:28:42,189 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 8.1s: 1639c @203c/s (259ch, ~410t @51t/s)
2025-12-15 13:28:44,206 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 10.1s: 2018c @200c/s (324ch, ~504t @50t/s)
2025-12-15 13:28:46,228 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 12.1s: 2445c @202c/s (389ch, ~611t @50t/s)
2025-12-15 13:28:48,254 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 14.2s: 2845c @201c/s (454ch, ~711t @50t/s)
2025-12-15 13:28:50,280 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 16.2s: 3229c @200c/s (519ch, ~807t @50t/s)
2025-12-15 13:28:52,300 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 18.2s: 3632c @200c/s (584ch, ~908t @50t/s)
2025-12-15 13:28:54,322 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 20.2s: 4025c @199c/s (649ch, ~1006t @50t/s)
2025-12-15 13:28:56,326 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 22.2s: 4417c @199c/s (711ch, ~1104t @50t/s)
2025-12-15 13:28:58,326 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 24.2s: 4821c @199c/s (777ch, ~1205t @50t/s)
2025-12-15 13:29:00,351 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 26.3s: 5265c @201c/s (844ch, ~1316t @50t/s)
2025-12-15 13:29:02,368 - src.llm.client - INFO - [app:1f22c1] ğŸ“Š 28.3s: 5646c @200c/s (910ch, ~1412t @50t/s)
2025-12-15 13:29:04,007 - src.llm.client - INFO - [app:1f22c1] âœ“ Done 39.73s: 5850c (~791w @147c/s)
2025-12-15 13:29:04,009 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:29:04,010 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:29:04,010 - generate_secondary - INFO -     - Length: 5838 chars, 789 words
2025-12-15 13:29:04,010 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:29:04,010 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:29:04,010 - generate_secondary - INFO -     - Avg words per application: 153
2025-12-15 13:29:04,010 - generate_secondary - WARNING - [WARNING] Application 4 has 144 words (require 150-200, need 6 more words) âš ï¸
2025-12-15 13:29:04,010 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_08/application.md
2025-12-15 13:29:04,010 - generate_secondary - INFO - Generating extension for session 8: Dynamic Priors...
2025-12-15 13:29:04,010 - src.llm.client - INFO - [ext:022061] ğŸš€ ext | m=gemma3:4b | p=26903c | t=120s
2025-12-15 13:29:04,010 - src.llm.client - INFO - [ext:022061] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:29:04,010 - src.llm.client - INFO - [ext:022061] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:29:04,011 - src.llm.client - INFO - [ext:022061] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32039 bytes, prompt=26903 chars
2025-12-15 13:29:04,011 - src.llm.client - INFO - [ext:022061] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:29:15,836 - src.llm.request_handler - INFO - [ext:022061] âœ“ Done 11.82s
2025-12-15 13:29:15,836 - src.llm.client - INFO - [ext:022061] âœ… HTTP 200 in 11.82s
2025-12-15 13:29:15,836 - src.llm.client - INFO - [ext:022061] ğŸ“¡ Stream active (200)
2025-12-15 13:29:15,836 - src.llm.client - INFO - [ext:022061] Starting stream parsing, waiting for first chunk...
2025-12-15 13:29:17,859 - src.llm.client - INFO - [ext:022061] ğŸ“Š 2.0s: 379c @187c/s (67ch, ~95t @47t/s)
2025-12-15 13:29:19,872 - src.llm.client - INFO - [ext:022061] ğŸ“Š 4.0s: 809c @200c/s (134ch, ~202t @50t/s)
2025-12-15 13:29:21,882 - src.llm.client - INFO - [ext:022061] ğŸ“Š 6.0s: 1249c @207c/s (201ch, ~312t @52t/s)
2025-12-15 13:29:23,896 - src.llm.client - INFO - [ext:022061] ğŸ“Š 8.1s: 1679c @208c/s (268ch, ~420t @52t/s)
2025-12-15 13:29:25,913 - src.llm.client - INFO - [ext:022061] ğŸ“Š 10.1s: 2098c @208c/s (335ch, ~524t @52t/s)
2025-12-15 13:29:27,932 - src.llm.client - INFO - [ext:022061] ğŸ“Š 12.1s: 2529c @209c/s (402ch, ~632t @52t/s)
2025-12-15 13:29:29,948 - src.llm.client - INFO - [ext:022061] ğŸ“Š 14.1s: 2923c @207c/s (469ch, ~731t @52t/s)
2025-12-15 13:29:31,968 - src.llm.client - INFO - [ext:022061] ğŸ“Š 16.1s: 3355c @208c/s (536ch, ~839t @52t/s)
2025-12-15 13:29:33,987 - src.llm.client - INFO - [ext:022061] ğŸ“Š 18.2s: 3750c @207c/s (603ch, ~938t @52t/s)
2025-12-15 13:29:36,013 - src.llm.client - INFO - [ext:022061] ğŸ“Š 20.2s: 4015c @199c/s (670ch, ~1004t @50t/s)
2025-12-15 13:29:36,336 - src.llm.client - INFO - [ext:022061] âœ“ Done 32.33s: 4016c (~548w @124c/s)
2025-12-15 13:29:36,337 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:29:36,338 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:29:36,338 - generate_secondary - INFO -     - Length: 3924 chars, 536 words
2025-12-15 13:29:36,338 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:29:36,338 - generate_secondary - INFO -     - Topics: 6
2025-12-15 13:29:36,338 - generate_secondary - INFO -     - Avg words per topic: 87
2025-12-15 13:29:36,338 - generate_secondary - WARNING - [WARNING] Too many topics (6, maximum 4, 2 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-15 13:29:36,338 - generate_secondary - WARNING - [WARNING] Topic 1 has 160 words (exceeds 150 by 10 words - consider condensing) âš ï¸
2025-12-15 13:29:36,338 - generate_secondary - WARNING - [WARNING] Topic 3 has 198 words (exceeds 150 by 48 words - consider condensing) âš ï¸
2025-12-15 13:29:36,338 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:29:36,338 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:29:36,338 - generate_secondary - WARNING - [WARNING] Topic 6 has 16 words (require 100-150, need 84 more words) âš ï¸
2025-12-15 13:29:36,338 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:29:36,338 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:29:36,338 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_08/extension.md
2025-12-15 13:29:36,338 - generate_secondary - INFO - Generating visualization for session 8: Dynamic Priors...
2025-12-15 13:29:36,338 - src.llm.client - INFO - [viz:dbb75e] ğŸš€ viz | m=gemma3:4b | p=25863c | t=120s
2025-12-15 13:29:36,338 - src.llm.client - INFO - [viz:dbb75e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:29:36,338 - src.llm.client - INFO - [viz:dbb75e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:29:36,342 - src.llm.client - INFO - [viz:dbb75e] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30321 bytes, prompt=25863 chars
2025-12-15 13:29:36,342 - src.llm.client - INFO - [viz:dbb75e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:29:46,435 - src.llm.request_handler - INFO - [viz:dbb75e] âœ“ Done 10.09s
2025-12-15 13:29:46,435 - src.llm.client - INFO - [viz:dbb75e] âœ… HTTP 200 in 10.09s
2025-12-15 13:29:46,435 - src.llm.client - INFO - [viz:dbb75e] ğŸ“¡ Stream active (200)
2025-12-15 13:29:46,435 - src.llm.client - INFO - [viz:dbb75e] Starting stream parsing, waiting for first chunk...
2025-12-15 13:29:48,454 - src.llm.client - INFO - [viz:dbb75e] ğŸ“Š 2.0s: 242c @120c/s (67ch, ~60t @30t/s)
2025-12-15 13:29:50,466 - src.llm.client - INFO - [viz:dbb75e] ğŸ“Š 4.0s: 456c @113c/s (134ch, ~114t @28t/s)
2025-12-15 13:29:51,142 - src.llm.client - INFO - [viz:dbb75e] âœ“ Done 14.80s: 488c (~75w @33c/s)
2025-12-15 13:29:51,142 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:29:51,142 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:29:51,142 - generate_secondary - INFO -     - Length: 472 chars (cleaned: 472 chars)
2025-12-15 13:29:51,142 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:29:51,142 - generate_secondary - INFO - [OK] Elements: 29 total (nodes: 10, connections: 19) âœ“
2025-12-15 13:29:51,143 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_08/visualization.mmd
2025-12-15 13:29:51,143 - generate_secondary - INFO - Generating integration for session 8: Dynamic Priors...
2025-12-15 13:29:51,143 - src.llm.client - INFO - [int:25f3f3] ğŸš€ int | m=gemma3:4b | p=27212c | t=150s
2025-12-15 13:29:51,143 - src.llm.client - INFO - [int:25f3f3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:29:51,143 - src.llm.client - INFO - [int:25f3f3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:29:51,144 - src.llm.client - INFO - [int:25f3f3] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=32687 bytes, prompt=27212 chars
2025-12-15 13:29:51,144 - src.llm.client - INFO - [int:25f3f3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:30:00,712 - src.llm.request_handler - INFO - [int:25f3f3] âœ“ Done 9.57s
2025-12-15 13:30:00,712 - src.llm.client - INFO - [int:25f3f3] âœ… HTTP 200 in 9.57s
2025-12-15 13:30:00,712 - src.llm.client - INFO - [int:25f3f3] ğŸ“¡ Stream active (200)
2025-12-15 13:30:00,712 - src.llm.client - INFO - [int:25f3f3] Starting stream parsing, waiting for first chunk...
2025-12-15 13:30:02,736 - src.llm.client - INFO - [int:25f3f3] ğŸ“Š 2.0s: 367c @181c/s (67ch, ~92t @45t/s)
2025-12-15 13:30:04,752 - src.llm.client - INFO - [int:25f3f3] ğŸ“Š 4.0s: 771c @191c/s (134ch, ~193t @48t/s)
2025-12-15 13:30:06,764 - src.llm.client - INFO - [int:25f3f3] ğŸ“Š 6.1s: 1152c @190c/s (201ch, ~288t @48t/s)
2025-12-15 13:30:08,781 - src.llm.client - INFO - [int:25f3f3] ğŸ“Š 8.1s: 1562c @194c/s (268ch, ~390t @48t/s)
2025-12-15 13:30:10,792 - src.llm.client - INFO - [int:25f3f3] ğŸ“Š 10.1s: 2018c @200c/s (335ch, ~504t @50t/s)
2025-12-15 13:30:12,240 - src.llm.client - INFO - [int:25f3f3] âœ“ Done 21.10s: 2267c (~297w @107c/s)
2025-12-15 13:30:12,241 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:30:12,241 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:30:12,241 - generate_secondary - INFO -     - Length: 2264 chars, 297 words
2025-12-15 13:30:12,241 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:30:12,241 - generate_secondary - INFO -     - Connections: 9
2025-12-15 13:30:12,241 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:30:12,241 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_08/integration.md
2025-12-15 13:30:12,241 - generate_secondary - INFO - Generating investigation for session 8: Dynamic Priors...
2025-12-15 13:30:12,242 - src.llm.client - INFO - [inv:cce43b] ğŸš€ inv | m=gemma3:4b | p=26125c | t=150s
2025-12-15 13:30:12,242 - src.llm.client - INFO - [inv:cce43b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:30:12,242 - src.llm.client - INFO - [inv:cce43b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:30:12,243 - src.llm.client - INFO - [inv:cce43b] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30543 bytes, prompt=26125 chars
2025-12-15 13:30:12,243 - src.llm.client - INFO - [inv:cce43b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:30:27,694 - src.llm.request_handler - INFO - [inv:cce43b] âœ“ Done 15.45s
2025-12-15 13:30:27,694 - src.llm.client - INFO - [inv:cce43b] âœ… HTTP 200 in 15.45s
2025-12-15 13:30:27,694 - src.llm.client - INFO - [inv:cce43b] ğŸ“¡ Stream active (200)
2025-12-15 13:30:27,694 - src.llm.client - INFO - [inv:cce43b] Starting stream parsing, waiting for first chunk...
2025-12-15 13:30:29,717 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 2.0s: 361c @178c/s (67ch, ~90t @45t/s)
2025-12-15 13:30:31,728 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 4.0s: 712c @177c/s (134ch, ~178t @44t/s)
2025-12-15 13:30:33,742 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 6.0s: 1028c @170c/s (201ch, ~257t @42t/s)
2025-12-15 13:30:35,754 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 8.1s: 1384c @172c/s (268ch, ~346t @43t/s)
2025-12-15 13:30:37,772 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 10.1s: 1798c @178c/s (335ch, ~450t @45t/s)
2025-12-15 13:30:39,785 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 12.1s: 2166c @179c/s (402ch, ~542t @45t/s)
2025-12-15 13:30:41,799 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 14.1s: 2525c @179c/s (469ch, ~631t @45t/s)
2025-12-15 13:30:43,818 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 16.1s: 2814c @175c/s (536ch, ~704t @44t/s)
2025-12-15 13:30:45,836 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 18.1s: 3114c @172c/s (603ch, ~778t @43t/s)
2025-12-15 13:30:47,856 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 20.2s: 3492c @173c/s (670ch, ~873t @43t/s)
2025-12-15 13:30:49,883 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 22.2s: 3879c @175c/s (737ch, ~970t @44t/s)
2025-12-15 13:30:51,904 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 24.2s: 4258c @176c/s (804ch, ~1064t @44t/s)
2025-12-15 13:30:53,907 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 26.2s: 4557c @174c/s (870ch, ~1139t @43t/s)
2025-12-15 13:30:55,931 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 28.2s: 4925c @174c/s (937ch, ~1231t @44t/s)
2025-12-15 13:30:57,943 - src.llm.client - INFO - [inv:cce43b] ğŸ“Š 30.2s: 5311c @176c/s (1003ch, ~1328t @44t/s)
2025-12-15 13:30:58,848 - src.llm.client - INFO - [inv:cce43b] âœ“ Done 46.61s: 5432c (~789w @117c/s)
2025-12-15 13:30:58,850 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:30:58,850 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:30:58,850 - generate_secondary - INFO -     - Length: 5431 chars, 789 words
2025-12-15 13:30:58,850 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:30:58,850 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:30:58,850 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:30:58,851 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_08/investigation.md
2025-12-15 13:30:58,851 - generate_secondary - INFO - Generating open_questions for session 8: Dynamic Priors...
2025-12-15 13:30:58,851 - src.llm.client - INFO - [opq:486de7] ğŸš€ opq | m=gemma3:4b | p=26211c | t=150s
2025-12-15 13:30:58,851 - src.llm.client - INFO - [opq:486de7] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:30:58,851 - src.llm.client - INFO - [opq:486de7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:30:58,852 - src.llm.client - INFO - [opq:486de7] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=30640 bytes, prompt=26211 chars
2025-12-15 13:30:58,852 - src.llm.client - INFO - [opq:486de7] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:31:08,958 - src.llm.request_handler - INFO - [opq:486de7] âœ“ Done 10.11s
2025-12-15 13:31:08,958 - src.llm.client - INFO - [opq:486de7] âœ… HTTP 200 in 10.11s
2025-12-15 13:31:08,958 - src.llm.client - INFO - [opq:486de7] ğŸ“¡ Stream active (200)
2025-12-15 13:31:08,958 - src.llm.client - INFO - [opq:486de7] Starting stream parsing, waiting for first chunk...
2025-12-15 13:31:10,975 - src.llm.client - INFO - [opq:486de7] ğŸ“Š 2.0s: 361c @179c/s (67ch, ~90t @45t/s)
2025-12-15 13:31:12,988 - src.llm.client - INFO - [opq:486de7] ğŸ“Š 4.0s: 729c @181c/s (134ch, ~182t @45t/s)
2025-12-15 13:31:15,003 - src.llm.client - INFO - [opq:486de7] ğŸ“Š 6.0s: 1074c @178c/s (201ch, ~268t @44t/s)
2025-12-15 13:31:17,016 - src.llm.client - INFO - [opq:486de7] ğŸ“Š 8.1s: 1450c @180c/s (268ch, ~362t @45t/s)
2025-12-15 13:31:19,033 - src.llm.client - INFO - [opq:486de7] ğŸ“Š 10.1s: 1805c @179c/s (335ch, ~451t @45t/s)
2025-12-15 13:31:20,786 - src.llm.client - INFO - [opq:486de7] âœ“ Done 21.93s: 2112c (~292w @96c/s)
2025-12-15 13:31:20,786 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:31:20,787 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:31:20,787 - generate_secondary - INFO -     - Length: 2111 chars, 292 words
2025-12-15 13:31:20,787 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:31:20,787 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:31:20,787 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:31:20,787 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_08/open_questions.md
2025-12-15 13:31:20,787 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:31:20,787 - generate_secondary - INFO - 
  Session 9/20: Attention Mechanisms
2025-12-15 13:31:20,788 - generate_secondary - INFO - Generating application for session 9: Attention Mechanisms...
2025-12-15 13:31:20,789 - src.llm.client - INFO - [app:28c132] ğŸš€ app | m=gemma3:4b | p=34963c | t=150s
2025-12-15 13:31:20,789 - src.llm.client - INFO - [app:28c132] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:31:20,789 - src.llm.client - INFO - [app:28c132] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:31:20,790 - src.llm.client - INFO - [app:28c132] Sending request to Ollama: model=gemma3:4b, operation=application, payload=37140 bytes, prompt=34963 chars
2025-12-15 13:31:20,790 - src.llm.client - INFO - [app:28c132] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:31:30,370 - src.llm.request_handler - INFO - [app:28c132] âœ“ Done 9.58s
2025-12-15 13:31:30,370 - src.llm.client - INFO - [app:28c132] âœ… HTTP 200 in 9.58s
2025-12-15 13:31:30,370 - src.llm.client - INFO - [app:28c132] ğŸ“¡ Stream active (200)
2025-12-15 13:31:30,371 - src.llm.client - INFO - [app:28c132] Starting stream parsing, waiting for first chunk...
2025-12-15 13:31:32,391 - src.llm.client - INFO - [app:28c132] ğŸ“Š 2.0s: 423c @209c/s (67ch, ~106t @52t/s)
2025-12-15 13:31:34,406 - src.llm.client - INFO - [app:28c132] ğŸ“Š 4.0s: 801c @199c/s (134ch, ~200t @50t/s)
2025-12-15 13:31:36,418 - src.llm.client - INFO - [app:28c132] ğŸ“Š 6.0s: 1236c @204c/s (201ch, ~309t @51t/s)
2025-12-15 13:31:38,436 - src.llm.client - INFO - [app:28c132] ğŸ“Š 8.1s: 1649c @204c/s (268ch, ~412t @51t/s)
2025-12-15 13:31:40,455 - src.llm.client - INFO - [app:28c132] ğŸ“Š 10.1s: 2040c @202c/s (335ch, ~510t @51t/s)
2025-12-15 13:31:42,472 - src.llm.client - INFO - [app:28c132] ğŸ“Š 12.1s: 2457c @203c/s (402ch, ~614t @51t/s)
2025-12-15 13:31:44,489 - src.llm.client - INFO - [app:28c132] ğŸ“Š 14.1s: 2869c @203c/s (469ch, ~717t @51t/s)
2025-12-15 13:31:46,504 - src.llm.client - INFO - [app:28c132] ğŸ“Š 16.1s: 3272c @203c/s (536ch, ~818t @51t/s)
2025-12-15 13:31:48,522 - src.llm.client - INFO - [app:28c132] ğŸ“Š 18.2s: 3673c @202c/s (603ch, ~918t @51t/s)
2025-12-15 13:31:50,540 - src.llm.client - INFO - [app:28c132] ğŸ“Š 20.2s: 4076c @202c/s (670ch, ~1019t @51t/s)
2025-12-15 13:31:52,569 - src.llm.client - INFO - [app:28c132] ğŸ“Š 22.2s: 4501c @203c/s (737ch, ~1125t @51t/s)
2025-12-15 13:31:54,597 - src.llm.client - INFO - [app:28c132] ğŸ“Š 24.2s: 4863c @201c/s (804ch, ~1216t @50t/s)
2025-12-15 13:31:56,623 - src.llm.client - INFO - [app:28c132] ğŸ“Š 26.3s: 5272c @201c/s (871ch, ~1318t @50t/s)
2025-12-15 13:31:58,525 - src.llm.client - INFO - [app:28c132] âœ“ Done 37.74s: 5574c (~738w @148c/s)
2025-12-15 13:31:58,528 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:31:58,528 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:31:58,528 - generate_secondary - INFO -     - Length: 5573 chars, 738 words
2025-12-15 13:31:58,528 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:31:58,528 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:31:58,528 - generate_secondary - INFO -     - Avg words per application: 142
2025-12-15 13:31:58,528 - generate_secondary - WARNING - [WARNING] Application 2 has 148 words (require 150-200, need 2 more words) âš ï¸
2025-12-15 13:31:58,528 - generate_secondary - WARNING - [WARNING] Application 3 has 139 words (require 150-200, need 11 more words) âš ï¸
2025-12-15 13:31:58,528 - generate_secondary - WARNING - [WARNING] Application 4 has 140 words (require 150-200, need 10 more words) âš ï¸
2025-12-15 13:31:58,528 - generate_secondary - WARNING - [WARNING] Application 5 has 132 words (require 150-200, need 18 more words) âš ï¸
2025-12-15 13:31:58,528 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_09/application.md
2025-12-15 13:31:58,528 - generate_secondary - INFO - Generating extension for session 9: Attention Mechanisms...
2025-12-15 13:31:58,528 - src.llm.client - INFO - [ext:5bec16] ğŸš€ ext | m=gemma3:4b | p=28849c | t=120s
2025-12-15 13:31:58,528 - src.llm.client - INFO - [ext:5bec16] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:31:58,528 - src.llm.client - INFO - [ext:5bec16] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:31:58,529 - src.llm.client - INFO - [ext:5bec16] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=33897 bytes, prompt=28849 chars
2025-12-15 13:31:58,529 - src.llm.client - INFO - [ext:5bec16] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:32:08,317 - src.llm.request_handler - INFO - [ext:5bec16] âœ“ Done 9.79s
2025-12-15 13:32:08,317 - src.llm.client - INFO - [ext:5bec16] âœ… HTTP 200 in 9.79s
2025-12-15 13:32:08,317 - src.llm.client - INFO - [ext:5bec16] ğŸ“¡ Stream active (200)
2025-12-15 13:32:08,317 - src.llm.client - INFO - [ext:5bec16] Starting stream parsing, waiting for first chunk...
2025-12-15 13:32:10,344 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 2.0s: 415c @205c/s (67ch, ~104t @51t/s)
2025-12-15 13:32:12,359 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 4.0s: 812c @201c/s (134ch, ~203t @50t/s)
2025-12-15 13:32:14,370 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 6.1s: 1251c @207c/s (201ch, ~313t @52t/s)
2025-12-15 13:32:16,383 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 8.1s: 1607c @199c/s (268ch, ~402t @50t/s)
2025-12-15 13:32:18,394 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 10.1s: 2062c @205c/s (335ch, ~516t @51t/s)
2025-12-15 13:32:20,404 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 12.1s: 2431c @201c/s (401ch, ~608t @50t/s)
2025-12-15 13:32:22,431 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 14.1s: 2819c @200c/s (468ch, ~705t @50t/s)
2025-12-15 13:32:24,451 - src.llm.client - INFO - [ext:5bec16] ğŸ“Š 16.1s: 3240c @201c/s (535ch, ~810t @50t/s)
2025-12-15 13:32:25,624 - src.llm.client - INFO - [ext:5bec16] âœ“ Done 27.10s: 3445c (~448w @127c/s)
2025-12-15 13:32:25,625 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:32:25,625 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:32:25,625 - generate_secondary - INFO -     - Length: 3445 chars, 448 words
2025-12-15 13:32:25,625 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:32:25,625 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:32:25,625 - generate_secondary - INFO -     - Avg words per topic: 143
2025-12-15 13:32:25,625 - generate_secondary - WARNING - [WARNING] Topic 1 has 153 words (exceeds 150 by 3 words - consider condensing) âš ï¸
2025-12-15 13:32:25,626 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_09/extension.md
2025-12-15 13:32:25,626 - generate_secondary - INFO - Generating visualization for session 9: Attention Mechanisms...
2025-12-15 13:32:25,626 - src.llm.client - INFO - [viz:81b8b3] ğŸš€ viz | m=gemma3:4b | p=27809c | t=120s
2025-12-15 13:32:25,626 - src.llm.client - INFO - [viz:81b8b3] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:32:25,626 - src.llm.client - INFO - [viz:81b8b3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:32:25,627 - src.llm.client - INFO - [viz:81b8b3] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=32179 bytes, prompt=27809 chars
2025-12-15 13:32:25,627 - src.llm.client - INFO - [viz:81b8b3] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:32:35,442 - src.llm.request_handler - INFO - [viz:81b8b3] âœ“ Done 9.82s
2025-12-15 13:32:35,442 - src.llm.client - INFO - [viz:81b8b3] âœ… HTTP 200 in 9.82s
2025-12-15 13:32:35,443 - src.llm.client - INFO - [viz:81b8b3] ğŸ“¡ Stream active (200)
2025-12-15 13:32:35,443 - src.llm.client - INFO - [viz:81b8b3] Starting stream parsing, waiting for first chunk...
2025-12-15 13:32:37,466 - src.llm.client - INFO - [viz:81b8b3] ğŸ“Š 2.0s: 290c @143c/s (67ch, ~72t @36t/s)
2025-12-15 13:32:39,481 - src.llm.client - INFO - [viz:81b8b3] ğŸ“Š 4.0s: 523c @130c/s (134ch, ~131t @32t/s)
2025-12-15 13:32:41,496 - src.llm.client - INFO - [viz:81b8b3] ğŸ“Š 6.1s: 775c @128c/s (201ch, ~194t @32t/s)
2025-12-15 13:32:42,939 - src.llm.client - INFO - [viz:81b8b3] âœ“ Done 17.31s: 924c (~142w @53c/s)
2025-12-15 13:32:42,939 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:32:42,940 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:32:42,940 - generate_secondary - INFO -     - Length: 325 chars (cleaned: 325 chars)
2025-12-15 13:32:42,940 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:32:42,940 - generate_secondary - INFO - [OK] Elements: 19 total (nodes: 9, connections: 10) âœ“
2025-12-15 13:32:42,940 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_09/visualization.mmd
2025-12-15 13:32:42,940 - generate_secondary - INFO - Generating integration for session 9: Attention Mechanisms...
2025-12-15 13:32:42,940 - src.llm.client - INFO - [int:e3188f] ğŸš€ int | m=gemma3:4b | p=29158c | t=150s
2025-12-15 13:32:42,940 - src.llm.client - INFO - [int:e3188f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:32:42,940 - src.llm.client - INFO - [int:e3188f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:32:42,942 - src.llm.client - INFO - [int:e3188f] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=34545 bytes, prompt=29158 chars
2025-12-15 13:32:42,942 - src.llm.client - INFO - [int:e3188f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:32:52,515 - src.llm.request_handler - INFO - [int:e3188f] âœ“ Done 9.57s
2025-12-15 13:32:52,515 - src.llm.client - INFO - [int:e3188f] âœ… HTTP 200 in 9.57s
2025-12-15 13:32:52,515 - src.llm.client - INFO - [int:e3188f] ğŸ“¡ Stream active (200)
2025-12-15 13:32:52,515 - src.llm.client - INFO - [int:e3188f] Starting stream parsing, waiting for first chunk...
2025-12-15 13:32:54,532 - src.llm.client - INFO - [int:e3188f] ğŸ“Š 2.0s: 343c @170c/s (67ch, ~86t @43t/s)
2025-12-15 13:32:56,544 - src.llm.client - INFO - [int:e3188f] ğŸ“Š 4.0s: 758c @188c/s (134ch, ~190t @47t/s)
2025-12-15 13:32:58,556 - src.llm.client - INFO - [int:e3188f] ğŸ“Š 6.0s: 1158c @192c/s (201ch, ~290t @48t/s)
2025-12-15 13:33:00,572 - src.llm.client - INFO - [int:e3188f] ğŸ“Š 8.1s: 1534c @190c/s (268ch, ~384t @48t/s)
2025-12-15 13:33:02,588 - src.llm.client - INFO - [int:e3188f] ğŸ“Š 10.1s: 1936c @192c/s (335ch, ~484t @48t/s)
2025-12-15 13:33:04,602 - src.llm.client - INFO - [int:e3188f] ğŸ“Š 12.1s: 2301c @190c/s (402ch, ~575t @48t/s)
2025-12-15 13:33:06,615 - src.llm.client - INFO - [int:e3188f] ğŸ“Š 14.1s: 2518c @179c/s (469ch, ~630t @45t/s)
2025-12-15 13:33:07,190 - src.llm.client - INFO - [int:e3188f] âœ“ Done 24.25s: 2548c (~357w @105c/s)
2025-12-15 13:33:07,191 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:33:07,191 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:33:07,191 - generate_secondary - INFO -     - Length: 2534 chars, 355 words
2025-12-15 13:33:07,191 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:33:07,191 - generate_secondary - INFO -     - Connections: 16
2025-12-15 13:33:07,191 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:33:07,192 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_09/integration.md
2025-12-15 13:33:07,192 - generate_secondary - INFO - Generating investigation for session 9: Attention Mechanisms...
2025-12-15 13:33:07,192 - src.llm.client - INFO - [inv:03e013] ğŸš€ inv | m=gemma3:4b | p=28071c | t=150s
2025-12-15 13:33:07,192 - src.llm.client - INFO - [inv:03e013] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:33:07,192 - src.llm.client - INFO - [inv:03e013] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:33:07,193 - src.llm.client - INFO - [inv:03e013] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=32401 bytes, prompt=28071 chars
2025-12-15 13:33:07,194 - src.llm.client - INFO - [inv:03e013] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:33:16,755 - src.llm.request_handler - INFO - [inv:03e013] âœ“ Done 9.56s
2025-12-15 13:33:16,755 - src.llm.client - INFO - [inv:03e013] âœ… HTTP 200 in 9.56s
2025-12-15 13:33:16,755 - src.llm.client - INFO - [inv:03e013] ğŸ“¡ Stream active (200)
2025-12-15 13:33:16,755 - src.llm.client - INFO - [inv:03e013] Starting stream parsing, waiting for first chunk...
2025-12-15 13:33:18,770 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 2.0s: 360c @179c/s (67ch, ~90t @45t/s)
2025-12-15 13:33:20,779 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 4.0s: 758c @188c/s (134ch, ~190t @47t/s)
2025-12-15 13:33:22,787 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 6.0s: 1141c @189c/s (201ch, ~285t @47t/s)
2025-12-15 13:33:24,798 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 8.0s: 1545c @192c/s (268ch, ~386t @48t/s)
2025-12-15 13:33:26,808 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 10.1s: 1962c @195c/s (335ch, ~490t @49t/s)
2025-12-15 13:33:28,825 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 12.1s: 2342c @194c/s (402ch, ~586t @49t/s)
2025-12-15 13:33:30,838 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 14.1s: 2676c @190c/s (469ch, ~669t @48t/s)
2025-12-15 13:33:32,857 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 16.1s: 3016c @187c/s (536ch, ~754t @47t/s)
2025-12-15 13:33:34,865 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 18.1s: 3366c @186c/s (602ch, ~842t @46t/s)
2025-12-15 13:33:36,883 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 20.1s: 3709c @184c/s (669ch, ~927t @46t/s)
2025-12-15 13:33:38,908 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 22.2s: 4104c @185c/s (736ch, ~1026t @46t/s)
2025-12-15 13:33:40,929 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 24.2s: 4529c @187c/s (803ch, ~1132t @47t/s)
2025-12-15 13:33:42,951 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 26.2s: 4874c @186c/s (870ch, ~1218t @47t/s)
2025-12-15 13:33:44,975 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 28.2s: 5239c @186c/s (937ch, ~1310t @46t/s)
2025-12-15 13:33:46,999 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 30.2s: 5653c @187c/s (1004ch, ~1413t @47t/s)
2025-12-15 13:33:49,028 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 32.3s: 6017c @186c/s (1071ch, ~1504t @47t/s)
2025-12-15 13:33:51,055 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 34.3s: 6443c @188c/s (1138ch, ~1611t @47t/s)
2025-12-15 13:33:53,084 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 36.3s: 6730c @185c/s (1205ch, ~1682t @46t/s)
2025-12-15 13:33:55,089 - src.llm.client - INFO - [inv:03e013] ğŸ“Š 38.3s: 7004c @183c/s (1271ch, ~1751t @46t/s)
2025-12-15 13:33:56,748 - src.llm.client - INFO - [inv:03e013] âœ“ Done 49.56s: 7173c (~988w @145c/s)
2025-12-15 13:33:56,751 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:33:56,751 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:33:56,751 - generate_secondary - INFO -     - Length: 7172 chars, 988 words
2025-12-15 13:33:56,751 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:33:56,751 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:33:56,751 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:33:56,751 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_09/investigation.md
2025-12-15 13:33:56,751 - generate_secondary - INFO - Generating open_questions for session 9: Attention Mechanisms...
2025-12-15 13:33:56,751 - src.llm.client - INFO - [opq:1c55ac] ğŸš€ opq | m=gemma3:4b | p=28157c | t=150s
2025-12-15 13:33:56,751 - src.llm.client - INFO - [opq:1c55ac] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:33:56,751 - src.llm.client - INFO - [opq:1c55ac] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:33:56,753 - src.llm.client - INFO - [opq:1c55ac] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=32498 bytes, prompt=28157 chars
2025-12-15 13:33:56,753 - src.llm.client - INFO - [opq:1c55ac] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:34:06,319 - src.llm.request_handler - INFO - [opq:1c55ac] âœ“ Done 9.57s
2025-12-15 13:34:06,319 - src.llm.client - INFO - [opq:1c55ac] âœ… HTTP 200 in 9.57s
2025-12-15 13:34:06,319 - src.llm.client - INFO - [opq:1c55ac] ğŸ“¡ Stream active (200)
2025-12-15 13:34:06,319 - src.llm.client - INFO - [opq:1c55ac] Starting stream parsing, waiting for first chunk...
2025-12-15 13:34:08,341 - src.llm.client - INFO - [opq:1c55ac] ğŸ“Š 2.0s: 378c @187c/s (67ch, ~94t @47t/s)
2025-12-15 13:34:10,353 - src.llm.client - INFO - [opq:1c55ac] ğŸ“Š 4.0s: 773c @192c/s (134ch, ~193t @48t/s)
2025-12-15 13:34:12,364 - src.llm.client - INFO - [opq:1c55ac] ğŸ“Š 6.0s: 1182c @196c/s (201ch, ~296t @49t/s)
2025-12-15 13:34:14,377 - src.llm.client - INFO - [opq:1c55ac] ğŸ“Š 8.1s: 1612c @200c/s (268ch, ~403t @50t/s)
2025-12-15 13:34:16,387 - src.llm.client - INFO - [opq:1c55ac] ğŸ“Š 10.1s: 1995c @198c/s (335ch, ~499t @50t/s)
2025-12-15 13:34:16,784 - src.llm.client - INFO - [opq:1c55ac] âœ“ Done 20.03s: 2019c (~276w @101c/s)
2025-12-15 13:34:16,785 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:34:16,785 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:34:16,785 - generate_secondary - INFO -     - Length: 2005 chars, 274 words
2025-12-15 13:34:16,785 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:34:16,785 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:34:16,785 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:34:16,785 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_05_precision_weighting_attention/session_09/open_questions.md
2025-12-15 13:34:16,785 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:34:16,785 - generate_secondary - INFO - 
============================================================
2025-12-15 13:34:16,785 - generate_secondary - INFO - [6/10] Module 6: Policy Selection & Planning (2 sessions)
2025-12-15 13:34:16,785 - generate_secondary - INFO - ============================================================
2025-12-15 13:34:16,785 - generate_secondary - INFO - 
  Session 10/20: Optimal Control Theory
2025-12-15 13:34:16,787 - generate_secondary - INFO - Generating application for session 10: Optimal Control Theory...
2025-12-15 13:34:16,787 - src.llm.client - INFO - [app:0bd99d] ğŸš€ app | m=gemma3:4b | p=31571c | t=150s
2025-12-15 13:34:16,787 - src.llm.client - INFO - [app:0bd99d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:34:16,787 - src.llm.client - INFO - [app:0bd99d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:34:16,789 - src.llm.client - INFO - [app:0bd99d] Sending request to Ollama: model=gemma3:4b, operation=application, payload=33835 bytes, prompt=31571 chars
2025-12-15 13:34:16,789 - src.llm.client - INFO - [app:0bd99d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:34:26,356 - src.llm.request_handler - INFO - [app:0bd99d] âœ“ Done 9.57s
2025-12-15 13:34:26,357 - src.llm.client - INFO - [app:0bd99d] âœ… HTTP 200 in 9.57s
2025-12-15 13:34:26,357 - src.llm.client - INFO - [app:0bd99d] ğŸ“¡ Stream active (200)
2025-12-15 13:34:26,357 - src.llm.client - INFO - [app:0bd99d] Starting stream parsing, waiting for first chunk...
2025-12-15 13:34:28,379 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 2.0s: 422c @209c/s (67ch, ~106t @52t/s)
2025-12-15 13:34:30,392 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 4.0s: 891c @221c/s (134ch, ~223t @55t/s)
2025-12-15 13:34:32,403 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 6.0s: 1311c @217c/s (201ch, ~328t @54t/s)
2025-12-15 13:34:34,406 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 8.0s: 1733c @215c/s (267ch, ~433t @54t/s)
2025-12-15 13:34:36,417 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 10.1s: 2124c @211c/s (334ch, ~531t @53t/s)
2025-12-15 13:34:38,437 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 12.1s: 2533c @210c/s (401ch, ~633t @52t/s)
2025-12-15 13:34:40,451 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 14.1s: 2933c @208c/s (468ch, ~733t @52t/s)
2025-12-15 13:34:42,470 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 16.1s: 3302c @205c/s (535ch, ~826t @51t/s)
2025-12-15 13:34:44,474 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 18.1s: 3706c @205c/s (601ch, ~926t @51t/s)
2025-12-15 13:34:46,491 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 20.1s: 4061c @202c/s (668ch, ~1015t @50t/s)
2025-12-15 13:34:48,513 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 22.2s: 4437c @200c/s (735ch, ~1109t @50t/s)
2025-12-15 13:34:50,532 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 24.2s: 4879c @202c/s (802ch, ~1220t @50t/s)
2025-12-15 13:34:52,556 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 26.2s: 5267c @201c/s (869ch, ~1317t @50t/s)
2025-12-15 13:34:54,580 - src.llm.client - INFO - [app:0bd99d] ğŸ“Š 28.2s: 5659c @201c/s (936ch, ~1415t @50t/s)
2025-12-15 13:34:55,261 - src.llm.client - INFO - [app:0bd99d] âœ“ Done 38.47s: 5716c (~763w @149c/s)
2025-12-15 13:34:55,263 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:34:55,263 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:34:55,263 - generate_secondary - INFO -     - Length: 5704 chars, 761 words
2025-12-15 13:34:55,263 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:34:55,263 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:34:55,263 - generate_secondary - INFO -     - Avg words per application: 148
2025-12-15 13:34:55,263 - generate_secondary - WARNING - [WARNING] Application 3 has 148 words (require 150-200, need 2 more words) âš ï¸
2025-12-15 13:34:55,263 - generate_secondary - WARNING - [WARNING] Application 4 has 140 words (require 150-200, need 10 more words) âš ï¸
2025-12-15 13:34:55,263 - generate_secondary - WARNING - [WARNING] Application 5 has 141 words (require 150-200, need 9 more words) âš ï¸
2025-12-15 13:34:55,263 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_10/application.md
2025-12-15 13:34:55,263 - generate_secondary - INFO - Generating extension for session 10: Optimal Control Theory...
2025-12-15 13:34:55,264 - src.llm.client - INFO - [ext:438f2c] ğŸš€ ext | m=gemma3:4b | p=25457c | t=120s
2025-12-15 13:34:55,264 - src.llm.client - INFO - [ext:438f2c] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:34:55,264 - src.llm.client - INFO - [ext:438f2c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:34:55,265 - src.llm.client - INFO - [ext:438f2c] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=30592 bytes, prompt=25457 chars
2025-12-15 13:34:55,265 - src.llm.client - INFO - [ext:438f2c] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:35:04,827 - src.llm.request_handler - INFO - [ext:438f2c] âœ“ Done 9.56s
2025-12-15 13:35:04,827 - src.llm.client - INFO - [ext:438f2c] âœ… HTTP 200 in 9.56s
2025-12-15 13:35:04,827 - src.llm.client - INFO - [ext:438f2c] ğŸ“¡ Stream active (200)
2025-12-15 13:35:04,827 - src.llm.client - INFO - [ext:438f2c] Starting stream parsing, waiting for first chunk...
2025-12-15 13:35:06,844 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 2.0s: 336c @167c/s (67ch, ~84t @42t/s)
2025-12-15 13:35:08,868 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 4.0s: 736c @182c/s (134ch, ~184t @46t/s)
2025-12-15 13:35:10,878 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 6.1s: 1180c @195c/s (201ch, ~295t @49t/s)
2025-12-15 13:35:12,888 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 8.1s: 1582c @196c/s (268ch, ~396t @49t/s)
2025-12-15 13:35:14,904 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 10.1s: 2016c @200c/s (335ch, ~504t @50t/s)
2025-12-15 13:35:16,918 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 12.1s: 2452c @203c/s (402ch, ~613t @51t/s)
2025-12-15 13:35:18,937 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 14.1s: 2899c @205c/s (469ch, ~725t @51t/s)
2025-12-15 13:35:20,952 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 16.1s: 3369c @209c/s (536ch, ~842t @52t/s)
2025-12-15 13:35:22,969 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 18.1s: 3810c @210c/s (603ch, ~952t @53t/s)
2025-12-15 13:35:24,987 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 20.2s: 4181c @207c/s (670ch, ~1045t @52t/s)
2025-12-15 13:35:27,010 - src.llm.client - INFO - [ext:438f2c] ğŸ“Š 22.2s: 4441c @200c/s (737ch, ~1110t @50t/s)
2025-12-15 13:35:27,625 - src.llm.client - INFO - [ext:438f2c] âœ“ Done 32.36s: 4480c (~595w @138c/s)
2025-12-15 13:35:27,627 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:35:27,627 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:35:27,627 - generate_secondary - INFO -     - Length: 4479 chars, 595 words
2025-12-15 13:35:27,627 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:35:27,627 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:35:27,628 - generate_secondary - INFO -     - Avg words per topic: 192
2025-12-15 13:35:27,628 - generate_secondary - WARNING - [WARNING] Topic 1 has 177 words (exceeds 150 by 27 words - consider condensing) âš ï¸
2025-12-15 13:35:27,628 - generate_secondary - WARNING - [WARNING] Topic 2 has 175 words (exceeds 150 by 25 words - consider condensing) âš ï¸
2025-12-15 13:35:27,628 - generate_secondary - WARNING - [WARNING] Topic 3 has 224 words (exceeds 150 by 74 words - consider condensing) âš ï¸
2025-12-15 13:35:27,628 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_10/extension.md
2025-12-15 13:35:27,628 - generate_secondary - INFO - Generating visualization for session 10: Optimal Control Theory...
2025-12-15 13:35:27,629 - src.llm.client - INFO - [viz:59ab40] ğŸš€ viz | m=gemma3:4b | p=24417c | t=120s
2025-12-15 13:35:27,629 - src.llm.client - INFO - [viz:59ab40] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:35:27,630 - src.llm.client - INFO - [viz:59ab40] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:35:27,631 - src.llm.client - INFO - [viz:59ab40] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=28874 bytes, prompt=24417 chars
2025-12-15 13:35:27,631 - src.llm.client - INFO - [viz:59ab40] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:35:37,189 - src.llm.request_handler - INFO - [viz:59ab40] âœ“ Done 9.56s
2025-12-15 13:35:37,189 - src.llm.client - INFO - [viz:59ab40] âœ… HTTP 200 in 9.56s
2025-12-15 13:35:37,189 - src.llm.client - INFO - [viz:59ab40] ğŸ“¡ Stream active (200)
2025-12-15 13:35:37,189 - src.llm.client - INFO - [viz:59ab40] Starting stream parsing, waiting for first chunk...
2025-12-15 13:35:39,209 - src.llm.client - INFO - [viz:59ab40] ğŸ“Š 2.0s: 300c @149c/s (67ch, ~75t @37t/s)
2025-12-15 13:35:41,220 - src.llm.client - INFO - [viz:59ab40] ğŸ“Š 4.0s: 552c @137c/s (134ch, ~138t @34t/s)
2025-12-15 13:35:43,230 - src.llm.client - INFO - [viz:59ab40] ğŸ“Š 6.0s: 809c @134c/s (201ch, ~202t @33t/s)
2025-12-15 13:35:45,243 - src.llm.client - INFO - [viz:59ab40] ğŸ“Š 8.1s: 1043c @130c/s (268ch, ~261t @32t/s)
2025-12-15 13:35:47,256 - src.llm.client - INFO - [viz:59ab40] ğŸ“Š 10.1s: 1300c @129c/s (335ch, ~325t @32t/s)
2025-12-15 13:35:48,627 - src.llm.client - INFO - [viz:59ab40] âœ“ Done 21.00s: 1492c (~219w @71c/s)
2025-12-15 13:35:48,627 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-15 13:35:48,628 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:35:48,628 - generate_secondary - INFO -     - Length: 899 chars (cleaned: 899 chars)
2025-12-15 13:35:48,628 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:35:48,628 - generate_secondary - INFO - [OK] Elements: 57 total (nodes: 26, connections: 31) âœ“
2025-12-15 13:35:48,628 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_10/visualization.mmd
2025-12-15 13:35:48,628 - generate_secondary - INFO - Generating integration for session 10: Optimal Control Theory...
2025-12-15 13:35:48,628 - src.llm.client - INFO - [int:07bbbf] ğŸš€ int | m=gemma3:4b | p=25766c | t=150s
2025-12-15 13:35:48,628 - src.llm.client - INFO - [int:07bbbf] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:35:48,628 - src.llm.client - INFO - [int:07bbbf] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:35:48,629 - src.llm.client - INFO - [int:07bbbf] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=31240 bytes, prompt=25766 chars
2025-12-15 13:35:48,629 - src.llm.client - INFO - [int:07bbbf] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:35:58,198 - src.llm.request_handler - INFO - [int:07bbbf] âœ“ Done 9.57s
2025-12-15 13:35:58,198 - src.llm.client - INFO - [int:07bbbf] âœ… HTTP 200 in 9.57s
2025-12-15 13:35:58,198 - src.llm.client - INFO - [int:07bbbf] ğŸ“¡ Stream active (200)
2025-12-15 13:35:58,198 - src.llm.client - INFO - [int:07bbbf] Starting stream parsing, waiting for first chunk...
2025-12-15 13:36:00,223 - src.llm.client - INFO - [int:07bbbf] ğŸ“Š 2.0s: 360c @178c/s (67ch, ~90t @44t/s)
2025-12-15 13:36:02,242 - src.llm.client - INFO - [int:07bbbf] ğŸ“Š 4.0s: 775c @192c/s (134ch, ~194t @48t/s)
2025-12-15 13:36:04,248 - src.llm.client - INFO - [int:07bbbf] ğŸ“Š 6.1s: 1189c @197c/s (200ch, ~297t @49t/s)
2025-12-15 13:36:06,261 - src.llm.client - INFO - [int:07bbbf] ğŸ“Š 8.1s: 1583c @196c/s (267ch, ~396t @49t/s)
2025-12-15 13:36:08,287 - src.llm.client - INFO - [int:07bbbf] ğŸ“Š 10.1s: 1913c @190c/s (334ch, ~478t @47t/s)
2025-12-15 13:36:10,305 - src.llm.client - INFO - [int:07bbbf] ğŸ“Š 12.1s: 2208c @182c/s (401ch, ~552t @46t/s)
2025-12-15 13:36:10,735 - src.llm.client - INFO - [int:07bbbf] âœ“ Done 22.11s: 2216c (~293w @100c/s)
2025-12-15 13:36:10,736 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:36:10,737 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:36:10,737 - generate_secondary - INFO -     - Length: 2215 chars, 293 words
2025-12-15 13:36:10,737 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:36:10,737 - generate_secondary - INFO -     - Connections: 14
2025-12-15 13:36:10,737 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:36:10,737 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_10/integration.md
2025-12-15 13:36:10,737 - generate_secondary - INFO - Generating investigation for session 10: Optimal Control Theory...
2025-12-15 13:36:10,737 - src.llm.client - INFO - [inv:3a618b] ğŸš€ inv | m=gemma3:4b | p=24679c | t=150s
2025-12-15 13:36:10,737 - src.llm.client - INFO - [inv:3a618b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:36:10,737 - src.llm.client - INFO - [inv:3a618b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:36:10,739 - src.llm.client - INFO - [inv:3a618b] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=29096 bytes, prompt=24679 chars
2025-12-15 13:36:10,739 - src.llm.client - INFO - [inv:3a618b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:36:20,306 - src.llm.request_handler - INFO - [inv:3a618b] âœ“ Done 9.57s
2025-12-15 13:36:20,306 - src.llm.client - INFO - [inv:3a618b] âœ… HTTP 200 in 9.57s
2025-12-15 13:36:20,306 - src.llm.client - INFO - [inv:3a618b] ğŸ“¡ Stream active (200)
2025-12-15 13:36:20,306 - src.llm.client - INFO - [inv:3a618b] Starting stream parsing, waiting for first chunk...
2025-12-15 13:36:22,323 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 2.0s: 339c @168c/s (67ch, ~85t @42t/s)
2025-12-15 13:36:24,346 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 4.0s: 734c @182c/s (134ch, ~184t @45t/s)
2025-12-15 13:36:26,358 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 6.1s: 1129c @187c/s (201ch, ~282t @47t/s)
2025-12-15 13:36:28,370 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 8.1s: 1508c @187c/s (268ch, ~377t @47t/s)
2025-12-15 13:36:30,391 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 10.1s: 1903c @189c/s (335ch, ~476t @47t/s)
2025-12-15 13:36:32,405 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 12.1s: 2316c @191c/s (401ch, ~579t @48t/s)
2025-12-15 13:36:34,420 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 14.1s: 2635c @187c/s (468ch, ~659t @47t/s)
2025-12-15 13:36:36,435 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 16.1s: 2985c @185c/s (535ch, ~746t @46t/s)
2025-12-15 13:36:38,456 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 18.1s: 3361c @185c/s (602ch, ~840t @46t/s)
2025-12-15 13:36:40,475 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 20.2s: 3762c @187c/s (669ch, ~940t @47t/s)
2025-12-15 13:36:42,494 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 22.2s: 4173c @188c/s (736ch, ~1043t @47t/s)
2025-12-15 13:36:44,514 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 24.2s: 4570c @189c/s (803ch, ~1142t @47t/s)
2025-12-15 13:36:46,537 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 26.2s: 4912c @187c/s (870ch, ~1228t @47t/s)
2025-12-15 13:36:48,563 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 28.3s: 5322c @188c/s (937ch, ~1330t @47t/s)
2025-12-15 13:36:50,591 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 30.3s: 5730c @189c/s (1004ch, ~1432t @47t/s)
2025-12-15 13:36:52,616 - src.llm.client - INFO - [inv:3a618b] ğŸ“Š 32.3s: 6142c @190c/s (1071ch, ~1536t @48t/s)
2025-12-15 13:36:54,113 - src.llm.client - INFO - [inv:3a618b] âœ“ Done 43.38s: 6364c (~911w @147c/s)
2025-12-15 13:36:54,116 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:36:54,116 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:36:54,116 - generate_secondary - INFO -     - Length: 6350 chars, 909 words
2025-12-15 13:36:54,116 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:36:54,116 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:36:54,116 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:36:54,116 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_10/investigation.md
2025-12-15 13:36:54,116 - generate_secondary - INFO - Generating open_questions for session 10: Optimal Control Theory...
2025-12-15 13:36:54,116 - src.llm.client - INFO - [opq:6b6c89] ğŸš€ opq | m=gemma3:4b | p=24765c | t=150s
2025-12-15 13:36:54,116 - src.llm.client - INFO - [opq:6b6c89] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:36:54,116 - src.llm.client - INFO - [opq:6b6c89] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:36:54,118 - src.llm.client - INFO - [opq:6b6c89] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=29193 bytes, prompt=24765 chars
2025-12-15 13:36:54,118 - src.llm.client - INFO - [opq:6b6c89] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:37:03,679 - src.llm.request_handler - INFO - [opq:6b6c89] âœ“ Done 9.56s
2025-12-15 13:37:03,679 - src.llm.client - INFO - [opq:6b6c89] âœ… HTTP 200 in 9.56s
2025-12-15 13:37:03,679 - src.llm.client - INFO - [opq:6b6c89] ğŸ“¡ Stream active (200)
2025-12-15 13:37:03,679 - src.llm.client - INFO - [opq:6b6c89] Starting stream parsing, waiting for first chunk...
2025-12-15 13:37:05,695 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 2.0s: 368c @183c/s (67ch, ~92t @46t/s)
2025-12-15 13:37:07,711 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 4.0s: 757c @188c/s (134ch, ~189t @47t/s)
2025-12-15 13:37:09,725 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 6.0s: 1152c @191c/s (201ch, ~288t @48t/s)
2025-12-15 13:37:11,738 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 8.1s: 1579c @196c/s (268ch, ~395t @49t/s)
2025-12-15 13:37:13,749 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 10.1s: 1969c @196c/s (335ch, ~492t @49t/s)
2025-12-15 13:37:15,764 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 12.1s: 2372c @196c/s (402ch, ~593t @49t/s)
2025-12-15 13:37:17,776 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 14.1s: 2790c @198c/s (469ch, ~698t @49t/s)
2025-12-15 13:37:19,825 - src.llm.client - INFO - [opq:6b6c89] ğŸ“Š 16.1s: 2982c @185c/s (528ch, ~746t @46t/s)
2025-12-15 13:37:19,826 - src.llm.client - INFO - [opq:6b6c89] âœ“ Done 25.71s: 2982c (~365w @116c/s)
2025-12-15 13:37:19,827 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:37:19,827 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:37:19,827 - generate_secondary - INFO -     - Length: 2979 chars, 365 words
2025-12-15 13:37:19,827 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:37:19,827 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:37:19,827 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:37:19,828 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_10/open_questions.md
2025-12-15 13:37:19,828 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:37:19,828 - generate_secondary - INFO - 
  Session 11/20: Reinforcement Learning Link
2025-12-15 13:37:19,829 - generate_secondary - INFO - Generating application for session 11: Reinforcement Learning Link...
2025-12-15 13:37:19,829 - src.llm.client - INFO - [app:dffdfe] ğŸš€ app | m=gemma3:4b | p=33098c | t=150s
2025-12-15 13:37:19,829 - src.llm.client - INFO - [app:dffdfe] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:37:19,829 - src.llm.client - INFO - [app:dffdfe] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:37:19,831 - src.llm.client - INFO - [app:dffdfe] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35264 bytes, prompt=33098 chars
2025-12-15 13:37:19,831 - src.llm.client - INFO - [app:dffdfe] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:37:29,397 - src.llm.request_handler - INFO - [app:dffdfe] âœ“ Done 9.57s
2025-12-15 13:37:29,397 - src.llm.client - INFO - [app:dffdfe] âœ… HTTP 200 in 9.57s
2025-12-15 13:37:29,397 - src.llm.client - INFO - [app:dffdfe] ğŸ“¡ Stream active (200)
2025-12-15 13:37:29,397 - src.llm.client - INFO - [app:dffdfe] Starting stream parsing, waiting for first chunk...
2025-12-15 13:37:31,410 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 2.0s: 424c @211c/s (67ch, ~106t @53t/s)
2025-12-15 13:37:33,421 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 4.0s: 820c @204c/s (134ch, ~205t @51t/s)
2025-12-15 13:37:35,435 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 6.0s: 1264c @209c/s (201ch, ~316t @52t/s)
2025-12-15 13:37:37,448 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 8.1s: 1671c @208c/s (268ch, ~418t @52t/s)
2025-12-15 13:37:39,462 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 10.1s: 2076c @206c/s (335ch, ~519t @52t/s)
2025-12-15 13:37:41,475 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 12.1s: 2468c @204c/s (402ch, ~617t @51t/s)
2025-12-15 13:37:43,486 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 14.1s: 2881c @204c/s (469ch, ~720t @51t/s)
2025-12-15 13:37:45,501 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 16.1s: 3298c @205c/s (536ch, ~824t @51t/s)
2025-12-15 13:37:47,516 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 18.1s: 3708c @205c/s (603ch, ~927t @51t/s)
2025-12-15 13:37:49,534 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 20.1s: 4089c @203c/s (670ch, ~1022t @51t/s)
2025-12-15 13:37:51,554 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 22.2s: 4505c @203c/s (737ch, ~1126t @51t/s)
2025-12-15 13:37:53,571 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 24.2s: 4869c @201c/s (804ch, ~1217t @50t/s)
2025-12-15 13:37:55,593 - src.llm.client - INFO - [app:dffdfe] ğŸ“Š 26.2s: 5286c @202c/s (871ch, ~1322t @50t/s)
2025-12-15 13:37:57,385 - src.llm.client - INFO - [app:dffdfe] âœ“ Done 37.56s: 5534c (~738w @147c/s)
2025-12-15 13:37:57,387 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:37:57,387 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:37:57,387 - generate_secondary - INFO -     - Length: 5533 chars, 738 words
2025-12-15 13:37:57,387 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:37:57,388 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:37:57,388 - generate_secondary - INFO -     - Avg words per application: 142
2025-12-15 13:37:57,388 - generate_secondary - WARNING - [WARNING] Application 2 has 144 words (require 150-200, need 6 more words) âš ï¸
2025-12-15 13:37:57,388 - generate_secondary - WARNING - [WARNING] Application 3 has 137 words (require 150-200, need 13 more words) âš ï¸
2025-12-15 13:37:57,388 - generate_secondary - WARNING - [WARNING] Application 4 has 143 words (require 150-200, need 7 more words) âš ï¸
2025-12-15 13:37:57,388 - generate_secondary - WARNING - [WARNING] Application 5 has 126 words (require 150-200, need 24 more words) âš ï¸
2025-12-15 13:37:57,388 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_11/application.md
2025-12-15 13:37:57,388 - generate_secondary - INFO - Generating extension for session 11: Reinforcement Learning Link...
2025-12-15 13:37:57,388 - src.llm.client - INFO - [ext:acd0e8] ğŸš€ ext | m=gemma3:4b | p=26984c | t=120s
2025-12-15 13:37:57,388 - src.llm.client - INFO - [ext:acd0e8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:37:57,388 - src.llm.client - INFO - [ext:acd0e8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:37:57,389 - src.llm.client - INFO - [ext:acd0e8] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32021 bytes, prompt=26984 chars
2025-12-15 13:37:57,390 - src.llm.client - INFO - [ext:acd0e8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:38:06,952 - src.llm.request_handler - INFO - [ext:acd0e8] âœ“ Done 9.56s
2025-12-15 13:38:06,952 - src.llm.client - INFO - [ext:acd0e8] âœ… HTTP 200 in 9.56s
2025-12-15 13:38:06,952 - src.llm.client - INFO - [ext:acd0e8] ğŸ“¡ Stream active (200)
2025-12-15 13:38:06,952 - src.llm.client - INFO - [ext:acd0e8] Starting stream parsing, waiting for first chunk...
2025-12-15 13:38:08,975 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 2.0s: 337c @167c/s (67ch, ~84t @42t/s)
2025-12-15 13:38:10,984 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 4.0s: 728c @181c/s (134ch, ~182t @45t/s)
2025-12-15 13:38:12,992 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 6.0s: 1143c @189c/s (201ch, ~286t @47t/s)
2025-12-15 13:38:15,006 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 8.1s: 1507c @187c/s (268ch, ~377t @47t/s)
2025-12-15 13:38:17,018 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 10.1s: 1864c @185c/s (335ch, ~466t @46t/s)
2025-12-15 13:38:19,032 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 12.1s: 2298c @190c/s (402ch, ~574t @48t/s)
2025-12-15 13:38:21,046 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 14.1s: 2633c @187c/s (469ch, ~658t @47t/s)
2025-12-15 13:38:23,061 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 16.1s: 3037c @189c/s (536ch, ~759t @47t/s)
2025-12-15 13:38:25,077 - src.llm.client - INFO - [ext:acd0e8] ğŸ“Š 18.1s: 3484c @192c/s (603ch, ~871t @48t/s)
2025-12-15 13:38:26,312 - src.llm.client - INFO - [ext:acd0e8] âœ“ Done 28.92s: 3672c (~494w @127c/s)
2025-12-15 13:38:26,313 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:38:26,313 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:38:26,313 - generate_secondary - INFO -     - Length: 3659 chars, 492 words
2025-12-15 13:38:26,313 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:38:26,313 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:38:26,313 - generate_secondary - INFO -     - Avg words per topic: 158
2025-12-15 13:38:26,313 - generate_secondary - WARNING - [WARNING] Topic 1 has 152 words (exceeds 150 by 2 words - consider condensing) âš ï¸
2025-12-15 13:38:26,314 - generate_secondary - WARNING - [WARNING] Topic 2 has 153 words (exceeds 150 by 3 words - consider condensing) âš ï¸
2025-12-15 13:38:26,314 - generate_secondary - WARNING - [WARNING] Topic 3 has 169 words (exceeds 150 by 19 words - consider condensing) âš ï¸
2025-12-15 13:38:26,314 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_11/extension.md
2025-12-15 13:38:26,314 - generate_secondary - INFO - Generating visualization for session 11: Reinforcement Learning Link...
2025-12-15 13:38:26,314 - src.llm.client - INFO - [viz:1292a0] ğŸš€ viz | m=gemma3:4b | p=25944c | t=120s
2025-12-15 13:38:26,314 - src.llm.client - INFO - [viz:1292a0] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:38:26,314 - src.llm.client - INFO - [viz:1292a0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:38:26,315 - src.llm.client - INFO - [viz:1292a0] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30303 bytes, prompt=25944 chars
2025-12-15 13:38:26,316 - src.llm.client - INFO - [viz:1292a0] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:38:35,878 - src.llm.request_handler - INFO - [viz:1292a0] âœ“ Done 9.56s
2025-12-15 13:38:35,878 - src.llm.client - INFO - [viz:1292a0] âœ… HTTP 200 in 9.56s
2025-12-15 13:38:35,878 - src.llm.client - INFO - [viz:1292a0] ğŸ“¡ Stream active (200)
2025-12-15 13:38:35,878 - src.llm.client - INFO - [viz:1292a0] Starting stream parsing, waiting for first chunk...
2025-12-15 13:38:37,899 - src.llm.client - INFO - [viz:1292a0] ğŸ“Š 2.0s: 298c @147c/s (67ch, ~74t @37t/s)
2025-12-15 13:38:39,912 - src.llm.client - INFO - [viz:1292a0] ğŸ“Š 4.0s: 495c @123c/s (134ch, ~124t @31t/s)
2025-12-15 13:38:41,923 - src.llm.client - INFO - [viz:1292a0] ğŸ“Š 6.0s: 711c @118c/s (201ch, ~178t @29t/s)
2025-12-15 13:38:43,099 - src.llm.client - INFO - [viz:1292a0] âœ“ Done 16.79s: 854c (~136w @51c/s)
2025-12-15 13:38:43,100 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-15 13:38:43,100 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:38:43,100 - generate_secondary - INFO -     - Length: 342 chars (cleaned: 342 chars)
2025-12-15 13:38:43,100 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:38:43,100 - generate_secondary - INFO - [CRITICAL] Elements: 23 total (nodes: 7, connections: 16) ğŸ”´
2025-12-15 13:38:43,100 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:38:43,100 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-15 13:38:43,100 - generate_secondary - WARNING - [WARNING] Only 7 nodes found (require at least 10, need 3 more - add more nodes to the diagram) âš ï¸
2025-12-15 13:38:43,100 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:38:43,100 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:38:43,100 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_11/visualization.mmd
2025-12-15 13:38:43,100 - generate_secondary - INFO - Generating integration for session 11: Reinforcement Learning Link...
2025-12-15 13:38:43,100 - src.llm.client - INFO - [int:c3c042] ğŸš€ int | m=gemma3:4b | p=27293c | t=150s
2025-12-15 13:38:43,100 - src.llm.client - INFO - [int:c3c042] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:38:43,100 - src.llm.client - INFO - [int:c3c042] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:38:43,102 - src.llm.client - INFO - [int:c3c042] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=32669 bytes, prompt=27293 chars
2025-12-15 13:38:43,102 - src.llm.client - INFO - [int:c3c042] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:38:52,670 - src.llm.request_handler - INFO - [int:c3c042] âœ“ Done 9.57s
2025-12-15 13:38:52,671 - src.llm.client - INFO - [int:c3c042] âœ… HTTP 200 in 9.57s
2025-12-15 13:38:52,671 - src.llm.client - INFO - [int:c3c042] ğŸ“¡ Stream active (200)
2025-12-15 13:38:52,671 - src.llm.client - INFO - [int:c3c042] Starting stream parsing, waiting for first chunk...
2025-12-15 13:38:54,688 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 2.0s: 414c @205c/s (67ch, ~104t @51t/s)
2025-12-15 13:38:56,706 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 4.0s: 828c @205c/s (134ch, ~207t @51t/s)
2025-12-15 13:38:58,718 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 6.0s: 1233c @204c/s (201ch, ~308t @51t/s)
2025-12-15 13:39:00,733 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 8.1s: 1646c @204c/s (268ch, ~412t @51t/s)
2025-12-15 13:39:02,749 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 10.1s: 2053c @204c/s (335ch, ~513t @51t/s)
2025-12-15 13:39:04,777 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 12.1s: 2470c @204c/s (401ch, ~618t @51t/s)
2025-12-15 13:39:06,791 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 14.1s: 2842c @201c/s (468ch, ~710t @50t/s)
2025-12-15 13:39:08,809 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 16.1s: 3271c @203c/s (535ch, ~818t @51t/s)
2025-12-15 13:39:10,822 - src.llm.client - INFO - [int:c3c042] ğŸ“Š 18.2s: 3498c @193c/s (602ch, ~874t @48t/s)
2025-12-15 13:39:12,145 - src.llm.client - INFO - [int:c3c042] âœ“ Done 29.05s: 3661c (~503w @126c/s)
2025-12-15 13:39:12,147 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:39:12,147 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:39:12,147 - generate_secondary - INFO -     - Length: 3647 chars, 501 words
2025-12-15 13:39:12,147 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:39:12,147 - generate_secondary - INFO -     - Connections: 18
2025-12-15 13:39:12,147 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:39:12,148 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_11/integration.md
2025-12-15 13:39:12,148 - generate_secondary - INFO - Generating investigation for session 11: Reinforcement Learning Link...
2025-12-15 13:39:12,148 - src.llm.client - INFO - [inv:f21fdb] ğŸš€ inv | m=gemma3:4b | p=26206c | t=150s
2025-12-15 13:39:12,148 - src.llm.client - INFO - [inv:f21fdb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:39:12,148 - src.llm.client - INFO - [inv:f21fdb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:39:12,150 - src.llm.client - INFO - [inv:f21fdb] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30525 bytes, prompt=26206 chars
2025-12-15 13:39:12,150 - src.llm.client - INFO - [inv:f21fdb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:39:21,713 - src.llm.request_handler - INFO - [inv:f21fdb] âœ“ Done 9.56s
2025-12-15 13:39:21,714 - src.llm.client - INFO - [inv:f21fdb] âœ… HTTP 200 in 9.56s
2025-12-15 13:39:21,714 - src.llm.client - INFO - [inv:f21fdb] ğŸ“¡ Stream active (200)
2025-12-15 13:39:21,714 - src.llm.client - INFO - [inv:f21fdb] Starting stream parsing, waiting for first chunk...
2025-12-15 13:39:23,714 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 2.0s: 346c @173c/s (66ch, ~86t @43t/s)
2025-12-15 13:39:25,725 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 4.0s: 681c @170c/s (133ch, ~170t @42t/s)
2025-12-15 13:39:27,734 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 6.0s: 886c @147c/s (200ch, ~222t @37t/s)
2025-12-15 13:39:29,748 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 8.0s: 1268c @158c/s (267ch, ~317t @39t/s)
2025-12-15 13:39:31,759 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 10.0s: 1635c @163c/s (334ch, ~409t @41t/s)
2025-12-15 13:39:33,778 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 12.1s: 1934c @160c/s (401ch, ~484t @40t/s)
2025-12-15 13:39:35,791 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 14.1s: 2349c @167c/s (468ch, ~587t @42t/s)
2025-12-15 13:39:37,811 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 16.1s: 2739c @170c/s (535ch, ~685t @43t/s)
2025-12-15 13:39:39,827 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 18.1s: 3114c @172c/s (602ch, ~778t @43t/s)
2025-12-15 13:39:41,842 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 20.1s: 3438c @171c/s (669ch, ~860t @43t/s)
2025-12-15 13:39:43,863 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 22.1s: 3807c @172c/s (736ch, ~952t @43t/s)
2025-12-15 13:39:45,883 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 24.2s: 4213c @174c/s (803ch, ~1053t @44t/s)
2025-12-15 13:39:47,906 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 26.2s: 4624c @177c/s (870ch, ~1156t @44t/s)
2025-12-15 13:39:49,927 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 28.2s: 4964c @176c/s (937ch, ~1241t @44t/s)
2025-12-15 13:39:51,950 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 30.2s: 5322c @176c/s (1004ch, ~1330t @44t/s)
2025-12-15 13:39:53,978 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 32.3s: 5622c @174c/s (1071ch, ~1406t @44t/s)
2025-12-15 13:39:56,005 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 34.3s: 5955c @174c/s (1138ch, ~1489t @43t/s)
2025-12-15 13:39:58,006 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 36.3s: 6304c @174c/s (1204ch, ~1576t @43t/s)
2025-12-15 13:40:00,037 - src.llm.client - INFO - [inv:f21fdb] ğŸ“Š 38.3s: 6688c @175c/s (1271ch, ~1672t @44t/s)
2025-12-15 13:40:01,620 - src.llm.client - INFO - [inv:f21fdb] âœ“ Done 49.47s: 6972c (~1050w @141c/s)
2025-12-15 13:40:01,623 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:40:01,623 - generate_secondary - INFO - [NEEDS REVIEW] Investigation generated âš ï¸
2025-12-15 13:40:01,623 - generate_secondary - INFO -     - Length: 6971 chars, 1050 words
2025-12-15 13:40:01,623 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:40:01,623 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:40:01,623 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:40:01,623 - generate_secondary - WARNING - [WARNING] Total word count (1050) exceeds maximum 1000 (exceeds by 50 words - condense content) âš ï¸
2025-12-15 13:40:01,623 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:40:01,623 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:40:01,624 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_11/investigation.md
2025-12-15 13:40:01,624 - generate_secondary - INFO - Generating open_questions for session 11: Reinforcement Learning Link...
2025-12-15 13:40:01,624 - src.llm.client - INFO - [opq:83a13f] ğŸš€ opq | m=gemma3:4b | p=26292c | t=150s
2025-12-15 13:40:01,624 - src.llm.client - INFO - [opq:83a13f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:40:01,624 - src.llm.client - INFO - [opq:83a13f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:40:01,625 - src.llm.client - INFO - [opq:83a13f] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=30622 bytes, prompt=26292 chars
2025-12-15 13:40:01,625 - src.llm.client - INFO - [opq:83a13f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:40:11,195 - src.llm.request_handler - INFO - [opq:83a13f] âœ“ Done 9.57s
2025-12-15 13:40:11,195 - src.llm.client - INFO - [opq:83a13f] âœ… HTTP 200 in 9.57s
2025-12-15 13:40:11,195 - src.llm.client - INFO - [opq:83a13f] ğŸ“¡ Stream active (200)
2025-12-15 13:40:11,195 - src.llm.client - INFO - [opq:83a13f] Starting stream parsing, waiting for first chunk...
2025-12-15 13:40:13,220 - src.llm.client - INFO - [opq:83a13f] ğŸ“Š 2.0s: 380c @188c/s (67ch, ~95t @47t/s)
2025-12-15 13:40:15,234 - src.llm.client - INFO - [opq:83a13f] ğŸ“Š 4.0s: 790c @196c/s (134ch, ~198t @49t/s)
2025-12-15 13:40:17,243 - src.llm.client - INFO - [opq:83a13f] ğŸ“Š 6.0s: 1234c @204c/s (201ch, ~308t @51t/s)
2025-12-15 13:40:19,258 - src.llm.client - INFO - [opq:83a13f] ğŸ“Š 8.1s: 1661c @206c/s (268ch, ~415t @52t/s)
2025-12-15 13:40:20,082 - src.llm.client - INFO - [opq:83a13f] âœ“ Done 18.46s: 1760c (~230w @95c/s)
2025-12-15 13:40:20,083 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:40:20,083 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:40:20,084 - generate_secondary - INFO -     - Length: 1746 chars, 228 words
2025-12-15 13:40:20,084 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:40:20,084 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:40:20,084 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:40:20,084 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_06_policy_selection_planning/session_11/open_questions.md
2025-12-15 13:40:20,084 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:40:20,084 - generate_secondary - INFO - 
============================================================
2025-12-15 13:40:20,084 - generate_secondary - INFO - [7/10] Module 7: Model Learning & Structure Learning (2 sessions)
2025-12-15 13:40:20,084 - generate_secondary - INFO - ============================================================
2025-12-15 13:40:20,084 - generate_secondary - INFO - 
  Session 12/20: Parameter Estimation
2025-12-15 13:40:20,086 - generate_secondary - INFO - Generating application for session 12: Parameter Estimation...
2025-12-15 13:40:20,086 - src.llm.client - INFO - [app:ac86a9] ğŸš€ app | m=gemma3:4b | p=33168c | t=150s
2025-12-15 13:40:20,086 - src.llm.client - INFO - [app:ac86a9] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:40:20,086 - src.llm.client - INFO - [app:ac86a9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:40:20,087 - src.llm.client - INFO - [app:ac86a9] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35219 bytes, prompt=33168 chars
2025-12-15 13:40:20,087 - src.llm.client - INFO - [app:ac86a9] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:40:29,659 - src.llm.request_handler - INFO - [app:ac86a9] âœ“ Done 9.57s
2025-12-15 13:40:29,659 - src.llm.client - INFO - [app:ac86a9] âœ… HTTP 200 in 9.57s
2025-12-15 13:40:29,659 - src.llm.client - INFO - [app:ac86a9] ğŸ“¡ Stream active (200)
2025-12-15 13:40:29,660 - src.llm.client - INFO - [app:ac86a9] Starting stream parsing, waiting for first chunk...
2025-12-15 13:40:31,679 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 2.0s: 416c @206c/s (67ch, ~104t @52t/s)
2025-12-15 13:40:33,702 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 4.0s: 821c @203c/s (134ch, ~205t @51t/s)
2025-12-15 13:40:35,716 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 6.1s: 1223c @202c/s (201ch, ~306t @50t/s)
2025-12-15 13:40:37,732 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 8.1s: 1666c @206c/s (268ch, ~416t @52t/s)
2025-12-15 13:40:39,743 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 10.1s: 2078c @206c/s (335ch, ~520t @52t/s)
2025-12-15 13:40:41,756 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 12.1s: 2490c @206c/s (402ch, ~622t @51t/s)
2025-12-15 13:40:43,773 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 14.1s: 2847c @202c/s (469ch, ~712t @50t/s)
2025-12-15 13:40:45,789 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 16.1s: 3247c @201c/s (536ch, ~812t @50t/s)
2025-12-15 13:40:47,805 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 18.1s: 3659c @202c/s (603ch, ~915t @50t/s)
2025-12-15 13:40:49,822 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 20.2s: 4036c @200c/s (670ch, ~1009t @50t/s)
2025-12-15 13:40:51,826 - src.llm.client - INFO - [app:ac86a9] ğŸ“Š 22.2s: 4426c @200c/s (736ch, ~1106t @50t/s)
2025-12-15 13:40:53,621 - src.llm.client - INFO - [app:ac86a9] âœ“ Done 33.54s: 4721c (~628w @141c/s)
2025-12-15 13:40:53,623 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:40:53,623 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:40:53,624 - generate_secondary - INFO -     - Length: 4709 chars, 626 words
2025-12-15 13:40:53,624 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:40:53,624 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:40:53,624 - generate_secondary - INFO -     - Avg words per application: 120
2025-12-15 13:40:53,624 - generate_secondary - WARNING - [WARNING] Application 1 has 129 words (require 150-200, need 21 more words) âš ï¸
2025-12-15 13:40:53,624 - generate_secondary - WARNING - [WARNING] Application 2 has 132 words (require 150-200, need 18 more words) âš ï¸
2025-12-15 13:40:53,624 - generate_secondary - WARNING - [WARNING] Application 3 has 114 words (require 150-200, need 36 more words) âš ï¸
2025-12-15 13:40:53,624 - generate_secondary - WARNING - [WARNING] Application 4 has 110 words (require 150-200, need 40 more words) âš ï¸
2025-12-15 13:40:53,624 - generate_secondary - WARNING - [WARNING] Application 5 has 116 words (require 150-200, need 34 more words) âš ï¸
2025-12-15 13:40:53,624 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_12/application.md
2025-12-15 13:40:53,624 - generate_secondary - INFO - Generating extension for session 12: Parameter Estimation...
2025-12-15 13:40:53,624 - src.llm.client - INFO - [ext:e07d53] ğŸš€ ext | m=gemma3:4b | p=27054c | t=120s
2025-12-15 13:40:53,624 - src.llm.client - INFO - [ext:e07d53] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:40:53,624 - src.llm.client - INFO - [ext:e07d53] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:40:53,626 - src.llm.client - INFO - [ext:e07d53] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=31976 bytes, prompt=27054 chars
2025-12-15 13:40:53,626 - src.llm.client - INFO - [ext:e07d53] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:41:03,177 - src.llm.request_handler - INFO - [ext:e07d53] âœ“ Done 9.55s
2025-12-15 13:41:03,177 - src.llm.client - INFO - [ext:e07d53] âœ… HTTP 200 in 9.55s
2025-12-15 13:41:03,177 - src.llm.client - INFO - [ext:e07d53] ğŸ“¡ Stream active (200)
2025-12-15 13:41:03,177 - src.llm.client - INFO - [ext:e07d53] Starting stream parsing, waiting for first chunk...
2025-12-15 13:41:05,196 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 2.0s: 384c @190c/s (67ch, ~96t @48t/s)
2025-12-15 13:41:07,212 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 4.0s: 795c @197c/s (134ch, ~199t @49t/s)
2025-12-15 13:41:09,224 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 6.0s: 1238c @205c/s (201ch, ~310t @51t/s)
2025-12-15 13:41:11,237 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 8.1s: 1601c @199c/s (268ch, ~400t @50t/s)
2025-12-15 13:41:13,247 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 10.1s: 2030c @202c/s (335ch, ~508t @50t/s)
2025-12-15 13:41:15,264 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 12.1s: 2493c @206c/s (402ch, ~623t @52t/s)
2025-12-15 13:41:17,287 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 14.1s: 2887c @205c/s (468ch, ~722t @51t/s)
2025-12-15 13:41:19,292 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 16.1s: 3328c @207c/s (534ch, ~832t @52t/s)
2025-12-15 13:41:21,306 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 18.1s: 3778c @208c/s (601ch, ~944t @52t/s)
2025-12-15 13:41:23,324 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 20.1s: 4148c @206c/s (668ch, ~1037t @51t/s)
2025-12-15 13:41:25,343 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 22.2s: 4549c @205c/s (735ch, ~1137t @51t/s)
2025-12-15 13:41:27,361 - src.llm.client - INFO - [ext:e07d53] ğŸ“Š 24.2s: 4947c @205c/s (802ch, ~1237t @51t/s)
2025-12-15 13:41:27,786 - src.llm.client - INFO - [ext:e07d53] âœ“ Done 34.16s: 4973c (~644w @146c/s)
2025-12-15 13:41:27,788 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:41:27,788 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:41:27,788 - generate_secondary - INFO -     - Length: 4972 chars, 644 words
2025-12-15 13:41:27,788 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:41:27,788 - generate_secondary - INFO -     - Topics: 4
2025-12-15 13:41:27,788 - generate_secondary - INFO -     - Avg words per topic: 153
2025-12-15 13:41:27,788 - generate_secondary - WARNING - [WARNING] Topic 1 has 162 words (exceeds 150 by 12 words - consider condensing) âš ï¸
2025-12-15 13:41:27,788 - generate_secondary - WARNING - [WARNING] Topic 2 has 160 words (exceeds 150 by 10 words - consider condensing) âš ï¸
2025-12-15 13:41:27,788 - generate_secondary - WARNING - [WARNING] Total word count (644) exceeds maximum 600 (exceeds by 44 words - condense content) âš ï¸
2025-12-15 13:41:27,788 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:41:27,788 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:41:27,788 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_12/extension.md
2025-12-15 13:41:27,788 - generate_secondary - INFO - Generating visualization for session 12: Parameter Estimation...
2025-12-15 13:41:27,788 - src.llm.client - INFO - [viz:e9ba2d] ğŸš€ viz | m=gemma3:4b | p=26014c | t=120s
2025-12-15 13:41:27,788 - src.llm.client - INFO - [viz:e9ba2d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:41:27,788 - src.llm.client - INFO - [viz:e9ba2d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:41:27,790 - src.llm.client - INFO - [viz:e9ba2d] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30258 bytes, prompt=26014 chars
2025-12-15 13:41:27,790 - src.llm.client - INFO - [viz:e9ba2d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:41:37,356 - src.llm.request_handler - INFO - [viz:e9ba2d] âœ“ Done 9.57s
2025-12-15 13:41:37,356 - src.llm.client - INFO - [viz:e9ba2d] âœ… HTTP 200 in 9.57s
2025-12-15 13:41:37,357 - src.llm.client - INFO - [viz:e9ba2d] ğŸ“¡ Stream active (200)
2025-12-15 13:41:37,357 - src.llm.client - INFO - [viz:e9ba2d] Starting stream parsing, waiting for first chunk...
2025-12-15 13:41:39,374 - src.llm.client - INFO - [viz:e9ba2d] ğŸ“Š 2.0s: 260c @129c/s (67ch, ~65t @32t/s)
2025-12-15 13:41:41,387 - src.llm.client - INFO - [viz:e9ba2d] ğŸ“Š 4.0s: 489c @121c/s (134ch, ~122t @30t/s)
2025-12-15 13:41:43,401 - src.llm.client - INFO - [viz:e9ba2d] ğŸ“Š 6.0s: 771c @128c/s (201ch, ~193t @32t/s)
2025-12-15 13:41:45,422 - src.llm.client - INFO - [viz:e9ba2d] ğŸ“Š 8.1s: 1037c @129c/s (268ch, ~259t @32t/s)
2025-12-15 13:41:47,340 - src.llm.client - INFO - [viz:e9ba2d] âœ“ Done 19.55s: 1315c (~192w @67c/s)
2025-12-15 13:41:47,341 - src.generate.processors.cleanup - INFO - Cleanup complete: 2 issues before, 0 issues after
2025-12-15 13:41:47,341 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:41:47,341 - generate_secondary - INFO -     - Length: 290 chars (cleaned: 290 chars)
2025-12-15 13:41:47,341 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:41:47,341 - generate_secondary - INFO - [OK] Elements: 19 total (nodes: 10, connections: 9) âœ“
2025-12-15 13:41:47,341 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_12/visualization.mmd
2025-12-15 13:41:47,341 - generate_secondary - INFO - Generating integration for session 12: Parameter Estimation...
2025-12-15 13:41:47,341 - src.llm.client - INFO - [int:c9f7fb] ğŸš€ int | m=gemma3:4b | p=27363c | t=150s
2025-12-15 13:41:47,341 - src.llm.client - INFO - [int:c9f7fb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:41:47,341 - src.llm.client - INFO - [int:c9f7fb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:41:47,343 - src.llm.client - INFO - [int:c9f7fb] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=32624 bytes, prompt=27363 chars
2025-12-15 13:41:47,343 - src.llm.client - INFO - [int:c9f7fb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:41:56,910 - src.llm.request_handler - INFO - [int:c9f7fb] âœ“ Done 9.57s
2025-12-15 13:41:56,910 - src.llm.client - INFO - [int:c9f7fb] âœ… HTTP 200 in 9.57s
2025-12-15 13:41:56,910 - src.llm.client - INFO - [int:c9f7fb] ğŸ“¡ Stream active (200)
2025-12-15 13:41:56,910 - src.llm.client - INFO - [int:c9f7fb] Starting stream parsing, waiting for first chunk...
2025-12-15 13:41:58,930 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 2.0s: 391c @194c/s (67ch, ~98t @48t/s)
2025-12-15 13:42:00,949 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 4.0s: 758c @188c/s (134ch, ~190t @47t/s)
2025-12-15 13:42:02,966 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 6.1s: 1180c @195c/s (201ch, ~295t @49t/s)
2025-12-15 13:42:04,979 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 8.1s: 1564c @194c/s (268ch, ~391t @48t/s)
2025-12-15 13:42:06,994 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 10.1s: 1942c @193c/s (335ch, ~486t @48t/s)
2025-12-15 13:42:09,014 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 12.1s: 2412c @199c/s (402ch, ~603t @50t/s)
2025-12-15 13:42:11,027 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 14.1s: 2797c @198c/s (469ch, ~699t @50t/s)
2025-12-15 13:42:13,042 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 16.1s: 3203c @199c/s (536ch, ~801t @50t/s)
2025-12-15 13:42:15,064 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 18.2s: 3646c @201c/s (603ch, ~912t @50t/s)
2025-12-15 13:42:17,088 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 20.2s: 4117c @204c/s (670ch, ~1029t @51t/s)
2025-12-15 13:42:19,101 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 22.2s: 4444c @200c/s (736ch, ~1111t @50t/s)
2025-12-15 13:42:21,122 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 24.2s: 4682c @193c/s (803ch, ~1170t @48t/s)
2025-12-15 13:42:23,146 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 26.2s: 4921c @188c/s (870ch, ~1230t @47t/s)
2025-12-15 13:42:25,168 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 28.3s: 5150c @182c/s (937ch, ~1288t @46t/s)
2025-12-15 13:42:27,190 - src.llm.client - INFO - [int:c9f7fb] ğŸ“Š 30.3s: 5461c @180c/s (1004ch, ~1365t @45t/s)
2025-12-15 13:42:28,536 - src.llm.client - INFO - [int:c9f7fb] âœ“ Done 41.19s: 5621c (~765w @136c/s)
2025-12-15 13:42:28,538 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:42:28,538 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:42:28,538 - generate_secondary - INFO -     - Length: 5605 chars, 763 words
2025-12-15 13:42:28,538 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:42:28,538 - generate_secondary - INFO -     - Connections: 18
2025-12-15 13:42:28,538 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:42:28,539 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_12/integration.md
2025-12-15 13:42:28,539 - generate_secondary - INFO - Generating investigation for session 12: Parameter Estimation...
2025-12-15 13:42:28,539 - src.llm.client - INFO - [inv:b592b4] ğŸš€ inv | m=gemma3:4b | p=26276c | t=150s
2025-12-15 13:42:28,539 - src.llm.client - INFO - [inv:b592b4] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:42:28,539 - src.llm.client - INFO - [inv:b592b4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:42:28,540 - src.llm.client - INFO - [inv:b592b4] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30480 bytes, prompt=26276 chars
2025-12-15 13:42:28,540 - src.llm.client - INFO - [inv:b592b4] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:42:38,103 - src.llm.request_handler - INFO - [inv:b592b4] âœ“ Done 9.56s
2025-12-15 13:42:38,103 - src.llm.client - INFO - [inv:b592b4] âœ… HTTP 200 in 9.56s
2025-12-15 13:42:38,103 - src.llm.client - INFO - [inv:b592b4] ğŸ“¡ Stream active (200)
2025-12-15 13:42:38,103 - src.llm.client - INFO - [inv:b592b4] Starting stream parsing, waiting for first chunk...
2025-12-15 13:42:40,121 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 2.0s: 310c @154c/s (67ch, ~78t @38t/s)
2025-12-15 13:42:42,134 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 4.0s: 648c @161c/s (134ch, ~162t @40t/s)
2025-12-15 13:42:44,145 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 6.0s: 1067c @177c/s (201ch, ~267t @44t/s)
2025-12-15 13:42:46,155 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 8.1s: 1497c @186c/s (268ch, ~374t @46t/s)
2025-12-15 13:42:48,167 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 10.1s: 1830c @182c/s (335ch, ~458t @45t/s)
2025-12-15 13:42:50,183 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 12.1s: 2183c @181c/s (402ch, ~546t @45t/s)
2025-12-15 13:42:52,196 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 14.1s: 2566c @182c/s (469ch, ~642t @46t/s)
2025-12-15 13:42:54,216 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 16.1s: 2955c @183c/s (535ch, ~739t @46t/s)
2025-12-15 13:42:56,230 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 18.1s: 3319c @183c/s (602ch, ~830t @46t/s)
2025-12-15 13:42:58,249 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 20.1s: 3695c @183c/s (669ch, ~924t @46t/s)
2025-12-15 13:43:00,272 - src.llm.client - INFO - [inv:b592b4] ğŸ“Š 22.2s: 4095c @185c/s (736ch, ~1024t @46t/s)
2025-12-15 13:43:01,850 - src.llm.client - INFO - [inv:b592b4] âœ“ Done 33.31s: 4395c (~588w @132c/s)
2025-12-15 13:43:01,852 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:43:01,852 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:43:01,852 - generate_secondary - INFO -     - Length: 4392 chars, 588 words
2025-12-15 13:43:01,852 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:43:01,852 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:43:01,852 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:43:01,852 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_12/investigation.md
2025-12-15 13:43:01,852 - generate_secondary - INFO - Generating open_questions for session 12: Parameter Estimation...
2025-12-15 13:43:01,852 - src.llm.client - INFO - [opq:d48dfe] ğŸš€ opq | m=gemma3:4b | p=26362c | t=150s
2025-12-15 13:43:01,852 - src.llm.client - INFO - [opq:d48dfe] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:43:01,852 - src.llm.client - INFO - [opq:d48dfe] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:43:01,854 - src.llm.client - INFO - [opq:d48dfe] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=30577 bytes, prompt=26362 chars
2025-12-15 13:43:01,854 - src.llm.client - INFO - [opq:d48dfe] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:43:11,422 - src.llm.request_handler - INFO - [opq:d48dfe] âœ“ Done 9.57s
2025-12-15 13:43:11,422 - src.llm.client - INFO - [opq:d48dfe] âœ… HTTP 200 in 9.57s
2025-12-15 13:43:11,422 - src.llm.client - INFO - [opq:d48dfe] ğŸ“¡ Stream active (200)
2025-12-15 13:43:11,422 - src.llm.client - INFO - [opq:d48dfe] Starting stream parsing, waiting for first chunk...
2025-12-15 13:43:13,438 - src.llm.client - INFO - [opq:d48dfe] ğŸ“Š 2.0s: 323c @160c/s (67ch, ~81t @40t/s)
2025-12-15 13:43:15,450 - src.llm.client - INFO - [opq:d48dfe] ğŸ“Š 4.0s: 705c @175c/s (134ch, ~176t @44t/s)
2025-12-15 13:43:17,462 - src.llm.client - INFO - [opq:d48dfe] ğŸ“Š 6.0s: 1086c @180c/s (201ch, ~272t @45t/s)
2025-12-15 13:43:19,478 - src.llm.client - INFO - [opq:d48dfe] ğŸ“Š 8.1s: 1524c @189c/s (268ch, ~381t @47t/s)
2025-12-15 13:43:20,230 - src.llm.client - INFO - [opq:d48dfe] âœ“ Done 18.38s: 1631c (~216w @89c/s)
2025-12-15 13:43:20,231 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:43:20,231 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:43:20,231 - generate_secondary - INFO -     - Length: 1630 chars, 216 words
2025-12-15 13:43:20,231 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:43:20,231 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:43:20,231 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:43:20,232 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_12/open_questions.md
2025-12-15 13:43:20,232 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:43:20,232 - generate_secondary - INFO - 
  Session 13/20: Structure Learning
2025-12-15 13:43:20,233 - generate_secondary - INFO - Generating application for session 13: Structure Learning...
2025-12-15 13:43:20,233 - src.llm.client - INFO - [app:8a3890] ğŸš€ app | m=gemma3:4b | p=33828c | t=150s
2025-12-15 13:43:20,233 - src.llm.client - INFO - [app:8a3890] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:43:20,233 - src.llm.client - INFO - [app:8a3890] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:43:20,235 - src.llm.client - INFO - [app:8a3890] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35716 bytes, prompt=33828 chars
2025-12-15 13:43:20,235 - src.llm.client - INFO - [app:8a3890] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:43:29,812 - src.llm.request_handler - INFO - [app:8a3890] âœ“ Done 9.58s
2025-12-15 13:43:29,813 - src.llm.client - INFO - [app:8a3890] âœ… HTTP 200 in 9.58s
2025-12-15 13:43:29,813 - src.llm.client - INFO - [app:8a3890] ğŸ“¡ Stream active (200)
2025-12-15 13:43:29,813 - src.llm.client - INFO - [app:8a3890] Starting stream parsing, waiting for first chunk...
2025-12-15 13:43:31,830 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 2.0s: 384c @190c/s (67ch, ~96t @48t/s)
2025-12-15 13:43:33,835 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 4.0s: 765c @190c/s (133ch, ~191t @48t/s)
2025-12-15 13:43:35,847 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 6.0s: 1149c @190c/s (200ch, ~287t @48t/s)
2025-12-15 13:43:37,863 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 8.1s: 1537c @191c/s (267ch, ~384t @48t/s)
2025-12-15 13:43:39,877 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 10.1s: 1908c @190c/s (334ch, ~477t @47t/s)
2025-12-15 13:43:41,890 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 12.1s: 2306c @191c/s (401ch, ~576t @48t/s)
2025-12-15 13:43:43,908 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 14.1s: 2693c @191c/s (468ch, ~673t @48t/s)
2025-12-15 13:43:45,924 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 16.1s: 3103c @193c/s (535ch, ~776t @48t/s)
2025-12-15 13:43:47,941 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 18.1s: 3479c @192c/s (602ch, ~870t @48t/s)
2025-12-15 13:43:49,961 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 20.1s: 3855c @191c/s (669ch, ~964t @48t/s)
2025-12-15 13:43:51,979 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 22.2s: 4266c @192c/s (736ch, ~1066t @48t/s)
2025-12-15 13:43:54,001 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 24.2s: 4673c @193c/s (803ch, ~1168t @48t/s)
2025-12-15 13:43:56,030 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 26.2s: 5036c @192c/s (870ch, ~1259t @48t/s)
2025-12-15 13:43:58,052 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 28.2s: 5427c @192c/s (937ch, ~1357t @48t/s)
2025-12-15 13:44:00,080 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 30.3s: 5838c @193c/s (1004ch, ~1460t @48t/s)
2025-12-15 13:44:02,085 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 32.3s: 6175c @191c/s (1070ch, ~1544t @48t/s)
2025-12-15 13:44:04,109 - src.llm.client - INFO - [app:8a3890] ğŸ“Š 34.3s: 6565c @191c/s (1136ch, ~1641t @48t/s)
2025-12-15 13:44:05,242 - src.llm.client - INFO - [app:8a3890] âœ“ Done 45.01s: 6673c (~910w @148c/s)
2025-12-15 13:44:05,244 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:44:05,244 - generate_secondary - INFO - [COMPLIANT] Application generated âœ“
2025-12-15 13:44:05,244 - generate_secondary - INFO -     - Length: 6557 chars, 894 words
2025-12-15 13:44:05,245 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:44:05,245 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:44:05,245 - generate_secondary - INFO -     - Avg words per application: 177
2025-12-15 13:44:05,245 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_13/application.md
2025-12-15 13:44:05,245 - generate_secondary - INFO - Generating extension for session 13: Structure Learning...
2025-12-15 13:44:05,245 - src.llm.client - INFO - [ext:fc6dcd] ğŸš€ ext | m=gemma3:4b | p=27714c | t=120s
2025-12-15 13:44:05,245 - src.llm.client - INFO - [ext:fc6dcd] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:44:05,245 - src.llm.client - INFO - [ext:fc6dcd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:44:05,246 - src.llm.client - INFO - [ext:fc6dcd] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32473 bytes, prompt=27714 chars
2025-12-15 13:44:05,246 - src.llm.client - INFO - [ext:fc6dcd] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:44:14,796 - src.llm.request_handler - INFO - [ext:fc6dcd] âœ“ Done 9.55s
2025-12-15 13:44:14,797 - src.llm.client - INFO - [ext:fc6dcd] âœ… HTTP 200 in 9.55s
2025-12-15 13:44:14,797 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“¡ Stream active (200)
2025-12-15 13:44:14,797 - src.llm.client - INFO - [ext:fc6dcd] Starting stream parsing, waiting for first chunk...
2025-12-15 13:44:16,809 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 2.0s: 374c @186c/s (67ch, ~94t @46t/s)
2025-12-15 13:44:18,818 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 4.0s: 828c @206c/s (134ch, ~207t @51t/s)
2025-12-15 13:44:20,830 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 6.0s: 1275c @211c/s (201ch, ~319t @53t/s)
2025-12-15 13:44:22,843 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 8.0s: 1693c @210c/s (268ch, ~423t @53t/s)
2025-12-15 13:44:24,856 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 10.1s: 2142c @213c/s (335ch, ~536t @53t/s)
2025-12-15 13:44:26,869 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 12.1s: 2564c @212c/s (402ch, ~641t @53t/s)
2025-12-15 13:44:28,884 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 14.1s: 2969c @211c/s (469ch, ~742t @53t/s)
2025-12-15 13:44:30,902 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 16.1s: 3420c @212c/s (536ch, ~855t @53t/s)
2025-12-15 13:44:32,921 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 18.1s: 3800c @210c/s (603ch, ~950t @52t/s)
2025-12-15 13:44:34,939 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 20.1s: 4025c @200c/s (670ch, ~1006t @50t/s)
2025-12-15 13:44:37,018 - src.llm.client - INFO - [ext:fc6dcd] ğŸ“Š 22.2s: 4237c @191c/s (729ch, ~1059t @48t/s)
2025-12-15 13:44:37,019 - src.llm.client - INFO - [ext:fc6dcd] âœ“ Done 31.77s: 4237c (~576w @133c/s)
2025-12-15 13:44:37,021 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:44:37,021 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - INFO -     - Length: 4237 chars, 576 words
2025-12-15 13:44:37,021 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:44:37,021 - generate_secondary - INFO -     - Topics: 6
2025-12-15 13:44:37,021 - generate_secondary - INFO -     - Avg words per topic: 92
2025-12-15 13:44:37,021 - generate_secondary - WARNING - [WARNING] Too many topics (6, maximum 4, 2 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - WARNING - [WARNING] Topic 1 has 166 words (exceeds 150 by 16 words - consider condensing) âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - WARNING - [WARNING] Topic 2 has 151 words (exceeds 150 by 1 words - consider condensing) âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - WARNING - [WARNING] Topic 3 has 195 words (exceeds 150 by 45 words - consider condensing) âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - WARNING - [WARNING] Topic 6 has 36 words (require 100-150, need 64 more words) âš ï¸
2025-12-15 13:44:37,021 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:44:37,021 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:44:37,021 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_13/extension.md
2025-12-15 13:44:37,021 - generate_secondary - INFO - Generating visualization for session 13: Structure Learning...
2025-12-15 13:44:37,021 - src.llm.client - INFO - [viz:9650b9] ğŸš€ viz | m=gemma3:4b | p=26674c | t=120s
2025-12-15 13:44:37,021 - src.llm.client - INFO - [viz:9650b9] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:44:37,021 - src.llm.client - INFO - [viz:9650b9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:44:37,023 - src.llm.client - INFO - [viz:9650b9] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30755 bytes, prompt=26674 chars
2025-12-15 13:44:37,023 - src.llm.client - INFO - [viz:9650b9] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:44:46,587 - src.llm.request_handler - INFO - [viz:9650b9] âœ“ Done 9.56s
2025-12-15 13:44:46,587 - src.llm.client - INFO - [viz:9650b9] âœ… HTTP 200 in 9.56s
2025-12-15 13:44:46,587 - src.llm.client - INFO - [viz:9650b9] ğŸ“¡ Stream active (200)
2025-12-15 13:44:46,587 - src.llm.client - INFO - [viz:9650b9] Starting stream parsing, waiting for first chunk...
2025-12-15 13:44:48,602 - src.llm.client - INFO - [viz:9650b9] ğŸ“Š 2.0s: 261c @130c/s (67ch, ~65t @32t/s)
2025-12-15 13:44:50,617 - src.llm.client - INFO - [viz:9650b9] ğŸ“Š 4.0s: 505c @125c/s (134ch, ~126t @31t/s)
2025-12-15 13:44:52,638 - src.llm.client - INFO - [viz:9650b9] ğŸ“Š 6.1s: 765c @126c/s (201ch, ~191t @32t/s)
2025-12-15 13:44:54,531 - src.llm.client - INFO - [viz:9650b9] âœ“ Done 17.51s: 1021c (~153w @58c/s)
2025-12-15 13:44:54,531 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:44:54,531 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:44:54,531 - generate_secondary - INFO -     - Length: 209 chars (cleaned: 209 chars)
2025-12-15 13:44:54,531 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:44:54,531 - generate_secondary - INFO - [CRITICAL] Elements: 15 total (nodes: 7, connections: 8) ğŸ”´
2025-12-15 13:44:54,531 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:44:54,531 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-15 13:44:54,531 - generate_secondary - WARNING - [WARNING] Only 7 nodes found (require at least 10, need 3 more - add more nodes to the diagram) âš ï¸
2025-12-15 13:44:54,531 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:44:54,531 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:44:54,532 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_13/visualization.mmd
2025-12-15 13:44:54,532 - generate_secondary - INFO - Generating integration for session 13: Structure Learning...
2025-12-15 13:44:54,532 - src.llm.client - INFO - [int:fecba1] ğŸš€ int | m=gemma3:4b | p=28023c | t=150s
2025-12-15 13:44:54,532 - src.llm.client - INFO - [int:fecba1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:44:54,532 - src.llm.client - INFO - [int:fecba1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:44:54,533 - src.llm.client - INFO - [int:fecba1] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=33121 bytes, prompt=28023 chars
2025-12-15 13:44:54,534 - src.llm.client - INFO - [int:fecba1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:45:04,108 - src.llm.request_handler - INFO - [int:fecba1] âœ“ Done 9.57s
2025-12-15 13:45:04,108 - src.llm.client - INFO - [int:fecba1] âœ… HTTP 200 in 9.57s
2025-12-15 13:45:04,108 - src.llm.client - INFO - [int:fecba1] ğŸ“¡ Stream active (200)
2025-12-15 13:45:04,108 - src.llm.client - INFO - [int:fecba1] Starting stream parsing, waiting for first chunk...
2025-12-15 13:45:06,122 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 2.0s: 332c @165c/s (67ch, ~83t @41t/s)
2025-12-15 13:45:08,134 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 4.0s: 703c @175c/s (134ch, ~176t @44t/s)
2025-12-15 13:45:10,148 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 6.0s: 1153c @191c/s (201ch, ~288t @48t/s)
2025-12-15 13:45:12,161 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 8.1s: 1513c @188c/s (268ch, ~378t @47t/s)
2025-12-15 13:45:14,174 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 10.1s: 1890c @188c/s (335ch, ~472t @47t/s)
2025-12-15 13:45:16,189 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 12.1s: 2275c @188c/s (402ch, ~569t @47t/s)
2025-12-15 13:45:18,201 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 14.1s: 2710c @192c/s (469ch, ~678t @48t/s)
2025-12-15 13:45:20,217 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 16.1s: 3031c @188c/s (536ch, ~758t @47t/s)
2025-12-15 13:45:22,234 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 18.1s: 3327c @184c/s (603ch, ~832t @46t/s)
2025-12-15 13:45:24,250 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 20.1s: 3666c @182c/s (670ch, ~916t @46t/s)
2025-12-15 13:45:26,269 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 22.2s: 3899c @176c/s (737ch, ~975t @44t/s)
2025-12-15 13:45:28,286 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 24.2s: 4156c @172c/s (804ch, ~1039t @43t/s)
2025-12-15 13:45:30,312 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 26.2s: 4457c @170c/s (871ch, ~1114t @43t/s)
2025-12-15 13:45:32,336 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 28.2s: 4657c @165c/s (938ch, ~1164t @41t/s)
2025-12-15 13:45:34,383 - src.llm.client - INFO - [int:fecba1] ğŸ“Š 30.3s: 4962c @164c/s (995ch, ~1240t @41t/s)
2025-12-15 13:45:34,384 - src.llm.client - INFO - [int:fecba1] âœ“ Done 39.85s: 4962c (~688w @125c/s)
2025-12-15 13:45:34,386 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:45:34,386 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:45:34,386 - generate_secondary - INFO -     - Length: 4961 chars, 688 words
2025-12-15 13:45:34,386 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:45:34,386 - generate_secondary - INFO -     - Connections: 12
2025-12-15 13:45:34,386 - generate_secondary - INFO -     - Structure: 1 sections
2025-12-15 13:45:34,387 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_13/integration.md
2025-12-15 13:45:34,387 - generate_secondary - INFO - Generating investigation for session 13: Structure Learning...
2025-12-15 13:45:34,387 - src.llm.client - INFO - [inv:278c01] ğŸš€ inv | m=gemma3:4b | p=26936c | t=150s
2025-12-15 13:45:34,387 - src.llm.client - INFO - [inv:278c01] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:45:34,387 - src.llm.client - INFO - [inv:278c01] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:45:34,388 - src.llm.client - INFO - [inv:278c01] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30977 bytes, prompt=26936 chars
2025-12-15 13:45:34,388 - src.llm.client - INFO - [inv:278c01] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:45:43,951 - src.llm.request_handler - INFO - [inv:278c01] âœ“ Done 9.56s
2025-12-15 13:45:43,951 - src.llm.client - INFO - [inv:278c01] âœ… HTTP 200 in 9.56s
2025-12-15 13:45:43,951 - src.llm.client - INFO - [inv:278c01] ğŸ“¡ Stream active (200)
2025-12-15 13:45:43,951 - src.llm.client - INFO - [inv:278c01] Starting stream parsing, waiting for first chunk...
2025-12-15 13:45:45,969 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 2.0s: 363c @180c/s (67ch, ~91t @45t/s)
2025-12-15 13:45:47,980 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 4.0s: 706c @175c/s (134ch, ~176t @44t/s)
2025-12-15 13:45:49,992 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 6.0s: 1015c @168c/s (201ch, ~254t @42t/s)
2025-12-15 13:45:52,004 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 8.1s: 1404c @174c/s (268ch, ~351t @44t/s)
2025-12-15 13:45:54,017 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 10.1s: 1765c @175c/s (335ch, ~441t @44t/s)
2025-12-15 13:45:56,032 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 12.1s: 2174c @180c/s (402ch, ~544t @45t/s)
2025-12-15 13:45:58,045 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 14.1s: 2480c @176c/s (469ch, ~620t @44t/s)
2025-12-15 13:46:00,063 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 16.1s: 2754c @171c/s (536ch, ~688t @43t/s)
2025-12-15 13:46:02,087 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 18.1s: 3100c @171c/s (603ch, ~775t @43t/s)
2025-12-15 13:46:04,110 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 20.2s: 3454c @171c/s (670ch, ~864t @43t/s)
2025-12-15 13:46:06,134 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 22.2s: 3713c @167c/s (737ch, ~928t @42t/s)
2025-12-15 13:46:08,159 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 24.2s: 4051c @167c/s (804ch, ~1013t @42t/s)
2025-12-15 13:46:10,183 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 26.2s: 4442c @169c/s (871ch, ~1110t @42t/s)
2025-12-15 13:46:12,192 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 28.2s: 4761c @169c/s (938ch, ~1190t @42t/s)
2025-12-15 13:46:14,219 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 30.3s: 5121c @169c/s (1006ch, ~1280t @42t/s)
2025-12-15 13:46:16,221 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 32.3s: 5517c @171c/s (1072ch, ~1379t @43t/s)
2025-12-15 13:46:18,249 - src.llm.client - INFO - [inv:278c01] ğŸ“Š 34.3s: 5791c @169c/s (1139ch, ~1448t @42t/s)
2025-12-15 13:46:19,590 - src.llm.client - INFO - [inv:278c01] âœ“ Done 45.20s: 5933c (~892w @131c/s)
2025-12-15 13:46:19,592 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:46:19,592 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:46:19,592 - generate_secondary - INFO -     - Length: 5926 chars, 892 words
2025-12-15 13:46:19,592 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:46:19,592 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:46:19,592 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:46:19,593 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_13/investigation.md
2025-12-15 13:46:19,593 - generate_secondary - INFO - Generating open_questions for session 13: Structure Learning...
2025-12-15 13:46:19,593 - src.llm.client - INFO - [opq:768298] ğŸš€ opq | m=gemma3:4b | p=27022c | t=150s
2025-12-15 13:46:19,593 - src.llm.client - INFO - [opq:768298] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:46:19,593 - src.llm.client - INFO - [opq:768298] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:46:19,594 - src.llm.client - INFO - [opq:768298] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=31074 bytes, prompt=27022 chars
2025-12-15 13:46:19,594 - src.llm.client - INFO - [opq:768298] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:46:29,165 - src.llm.request_handler - INFO - [opq:768298] âœ“ Done 9.57s
2025-12-15 13:46:29,166 - src.llm.client - INFO - [opq:768298] âœ… HTTP 200 in 9.57s
2025-12-15 13:46:29,166 - src.llm.client - INFO - [opq:768298] ğŸ“¡ Stream active (200)
2025-12-15 13:46:29,166 - src.llm.client - INFO - [opq:768298] Starting stream parsing, waiting for first chunk...
2025-12-15 13:46:31,189 - src.llm.client - INFO - [opq:768298] ğŸ“Š 2.0s: 358c @177c/s (67ch, ~90t @44t/s)
2025-12-15 13:46:33,207 - src.llm.client - INFO - [opq:768298] ğŸ“Š 4.0s: 741c @183c/s (134ch, ~185t @46t/s)
2025-12-15 13:46:35,226 - src.llm.client - INFO - [opq:768298] ğŸ“Š 6.1s: 1076c @178c/s (201ch, ~269t @44t/s)
2025-12-15 13:46:37,242 - src.llm.client - INFO - [opq:768298] ğŸ“Š 8.1s: 1438c @178c/s (268ch, ~360t @45t/s)
2025-12-15 13:46:39,254 - src.llm.client - INFO - [opq:768298] ğŸ“Š 10.1s: 1851c @183c/s (335ch, ~463t @46t/s)
2025-12-15 13:46:39,881 - src.llm.client - INFO - [opq:768298] âœ“ Done 20.29s: 1931c (~262w @95c/s)
2025-12-15 13:46:39,882 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:46:39,882 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:46:39,882 - generate_secondary - INFO -     - Length: 1930 chars, 262 words
2025-12-15 13:46:39,882 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:46:39,882 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:46:39,882 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:46:39,882 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_07_model_learning_structure_learning/session_13/open_questions.md
2025-12-15 13:46:39,883 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:46:39,883 - generate_secondary - INFO - 
============================================================
2025-12-15 13:46:39,883 - generate_secondary - INFO - [8/10] Module 8: Neuroscientific Evidence (2 sessions)
2025-12-15 13:46:39,883 - generate_secondary - INFO - ============================================================
2025-12-15 13:46:39,883 - generate_secondary - INFO - 
  Session 14/20: Sensory Coding
2025-12-15 13:46:39,885 - generate_secondary - INFO - Generating application for session 14: Sensory Coding...
2025-12-15 13:46:39,885 - src.llm.client - INFO - [app:4478e3] ğŸš€ app | m=gemma3:4b | p=34455c | t=150s
2025-12-15 13:46:39,885 - src.llm.client - INFO - [app:4478e3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:46:39,885 - src.llm.client - INFO - [app:4478e3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:46:39,886 - src.llm.client - INFO - [app:4478e3] Sending request to Ollama: model=gemma3:4b, operation=application, payload=36560 bytes, prompt=34455 chars
2025-12-15 13:46:39,886 - src.llm.client - INFO - [app:4478e3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:46:49,465 - src.llm.request_handler - INFO - [app:4478e3] âœ“ Done 9.58s
2025-12-15 13:46:49,465 - src.llm.client - INFO - [app:4478e3] âœ… HTTP 200 in 9.58s
2025-12-15 13:46:49,465 - src.llm.client - INFO - [app:4478e3] ğŸ“¡ Stream active (200)
2025-12-15 13:46:49,465 - src.llm.client - INFO - [app:4478e3] Starting stream parsing, waiting for first chunk...
2025-12-15 13:46:51,480 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 2.0s: 391c @194c/s (67ch, ~98t @49t/s)
2025-12-15 13:46:53,490 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 4.0s: 763c @190c/s (134ch, ~191t @47t/s)
2025-12-15 13:46:55,503 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 6.0s: 1182c @196c/s (201ch, ~296t @49t/s)
2025-12-15 13:46:57,514 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 8.0s: 1598c @199c/s (268ch, ~400t @50t/s)
2025-12-15 13:46:59,525 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 10.1s: 2018c @201c/s (335ch, ~504t @50t/s)
2025-12-15 13:47:01,547 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 12.1s: 2344c @194c/s (402ch, ~586t @49t/s)
2025-12-15 13:47:03,560 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 14.1s: 2736c @194c/s (469ch, ~684t @49t/s)
2025-12-15 13:47:05,577 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 16.1s: 3166c @197c/s (536ch, ~792t @49t/s)
2025-12-15 13:47:07,597 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 18.1s: 3562c @196c/s (603ch, ~890t @49t/s)
2025-12-15 13:47:09,616 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 20.2s: 3983c @198c/s (670ch, ~996t @49t/s)
2025-12-15 13:47:11,639 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 22.2s: 4399c @198c/s (737ch, ~1100t @50t/s)
2025-12-15 13:47:13,639 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 24.2s: 4768c @197c/s (803ch, ~1192t @49t/s)
2025-12-15 13:47:15,642 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 26.2s: 5139c @196c/s (869ch, ~1285t @49t/s)
2025-12-15 13:47:17,664 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 28.2s: 5502c @195c/s (936ch, ~1376t @49t/s)
2025-12-15 13:47:19,690 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 30.2s: 5883c @195c/s (1003ch, ~1471t @49t/s)
2025-12-15 13:47:21,719 - src.llm.client - INFO - [app:4478e3] ğŸ“Š 32.3s: 6265c @194c/s (1070ch, ~1566t @49t/s)
2025-12-15 13:47:22,681 - src.llm.client - INFO - [app:4478e3] âœ“ Done 42.80s: 6363c (~879w @149c/s)
2025-12-15 13:47:22,684 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:47:22,684 - generate_secondary - INFO - [COMPLIANT] Application generated âœ“
2025-12-15 13:47:22,684 - generate_secondary - INFO -     - Length: 6362 chars, 879 words
2025-12-15 13:47:22,684 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:47:22,684 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:47:22,684 - generate_secondary - INFO -     - Avg words per application: 171
2025-12-15 13:47:22,684 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_14/application.md
2025-12-15 13:47:22,684 - generate_secondary - INFO - Generating extension for session 14: Sensory Coding...
2025-12-15 13:47:22,684 - src.llm.client - INFO - [ext:75d3f4] ğŸš€ ext | m=gemma3:4b | p=28341c | t=120s
2025-12-15 13:47:22,684 - src.llm.client - INFO - [ext:75d3f4] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:47:22,685 - src.llm.client - INFO - [ext:75d3f4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:47:22,686 - src.llm.client - INFO - [ext:75d3f4] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=33317 bytes, prompt=28341 chars
2025-12-15 13:47:22,686 - src.llm.client - INFO - [ext:75d3f4] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:47:32,246 - src.llm.request_handler - INFO - [ext:75d3f4] âœ“ Done 9.56s
2025-12-15 13:47:32,246 - src.llm.client - INFO - [ext:75d3f4] âœ… HTTP 200 in 9.56s
2025-12-15 13:47:32,246 - src.llm.client - INFO - [ext:75d3f4] ğŸ“¡ Stream active (200)
2025-12-15 13:47:32,246 - src.llm.client - INFO - [ext:75d3f4] Starting stream parsing, waiting for first chunk...
2025-12-15 13:47:34,261 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 2.0s: 388c @193c/s (67ch, ~97t @48t/s)
2025-12-15 13:47:36,273 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 4.0s: 824c @205c/s (134ch, ~206t @51t/s)
2025-12-15 13:47:38,284 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 6.0s: 1277c @212c/s (201ch, ~319t @53t/s)
2025-12-15 13:47:40,298 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 8.1s: 1682c @209c/s (268ch, ~420t @52t/s)
2025-12-15 13:47:42,308 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 10.1s: 2102c @209c/s (335ch, ~526t @52t/s)
2025-12-15 13:47:44,324 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 12.1s: 2557c @212c/s (402ch, ~639t @53t/s)
2025-12-15 13:47:46,341 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 14.1s: 2954c @210c/s (469ch, ~738t @52t/s)
2025-12-15 13:47:48,354 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 16.1s: 3291c @204c/s (536ch, ~823t @51t/s)
2025-12-15 13:47:50,373 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 18.1s: 3695c @204c/s (603ch, ~924t @51t/s)
2025-12-15 13:47:52,390 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 20.1s: 4073c @202c/s (670ch, ~1018t @51t/s)
2025-12-15 13:47:54,410 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 22.2s: 4365c @197c/s (737ch, ~1091t @49t/s)
2025-12-15 13:47:56,431 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 24.2s: 4673c @193c/s (804ch, ~1168t @48t/s)
2025-12-15 13:47:58,452 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 26.2s: 4884c @186c/s (871ch, ~1221t @47t/s)
2025-12-15 13:48:00,482 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 28.2s: 5069c @180c/s (938ch, ~1267t @45t/s)
2025-12-15 13:48:02,510 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 30.3s: 5303c @175c/s (1005ch, ~1326t @44t/s)
2025-12-15 13:48:04,510 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 32.3s: 5527c @171c/s (1071ch, ~1382t @43t/s)
2025-12-15 13:48:06,537 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 34.3s: 5755c @168c/s (1138ch, ~1439t @42t/s)
2025-12-15 13:48:08,541 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 36.3s: 5965c @164c/s (1204ch, ~1491t @41t/s)
2025-12-15 13:48:10,545 - src.llm.client - INFO - [ext:75d3f4] ğŸ“Š 38.3s: 6212c @162c/s (1270ch, ~1553t @41t/s)
2025-12-15 13:48:11,265 - src.llm.client - INFO - [ext:75d3f4] âœ“ Done 48.58s: 6248c (~821w @129c/s)
2025-12-15 13:48:11,267 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:48:11,267 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - INFO -     - Length: 6154 chars, 809 words
2025-12-15 13:48:11,267 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:48:11,267 - generate_secondary - INFO -     - Topics: 14
2025-12-15 13:48:11,267 - generate_secondary - INFO -     - Avg words per topic: 56
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Too many topics (14, maximum 4, 10 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 1 has 184 words (exceeds 150 by 34 words - consider condensing) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 2 has 181 words (exceeds 150 by 31 words - consider condensing) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 3 has 236 words (exceeds 150 by 86 words - consider condensing) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 4 has 8 words (require 100-150, need 92 more words) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 5 has 9 words (require 100-150, need 91 more words) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 6 has 21 words (require 100-150, need 79 more words) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 7 has 47 words (require 100-150, need 53 more words) âš ï¸
2025-12-15 13:48:11,267 - generate_secondary - WARNING - [WARNING] Topic 8 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - WARNING - [WARNING] Topic 9 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - WARNING - [WARNING] Topic 10 has 76 words (require 100-150, need 24 more words) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - WARNING - [WARNING] Topic 11 has 13 words (require 100-150, need 87 more words) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - WARNING - [WARNING] Topic 12 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - WARNING - [WARNING] Topic 13 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - WARNING - [WARNING] Topic 14 has 3 words (require 100-150, need 97 more words) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - WARNING - [WARNING] Total word count (809) exceeds maximum 600 (exceeds by 209 words - condense content) âš ï¸
2025-12-15 13:48:11,268 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:48:11,268 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:48:11,268 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_14/extension.md
2025-12-15 13:48:11,268 - generate_secondary - INFO - Generating visualization for session 14: Sensory Coding...
2025-12-15 13:48:11,268 - src.llm.client - INFO - [viz:f3d7cc] ğŸš€ viz | m=gemma3:4b | p=27301c | t=120s
2025-12-15 13:48:11,268 - src.llm.client - INFO - [viz:f3d7cc] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:48:11,268 - src.llm.client - INFO - [viz:f3d7cc] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:48:11,269 - src.llm.client - INFO - [viz:f3d7cc] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=31599 bytes, prompt=27301 chars
2025-12-15 13:48:11,269 - src.llm.client - INFO - [viz:f3d7cc] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:48:20,824 - src.llm.request_handler - INFO - [viz:f3d7cc] âœ“ Done 9.55s
2025-12-15 13:48:20,824 - src.llm.client - INFO - [viz:f3d7cc] âœ… HTTP 200 in 9.55s
2025-12-15 13:48:20,824 - src.llm.client - INFO - [viz:f3d7cc] ğŸ“¡ Stream active (200)
2025-12-15 13:48:20,824 - src.llm.client - INFO - [viz:f3d7cc] Starting stream parsing, waiting for first chunk...
2025-12-15 13:48:22,839 - src.llm.client - INFO - [viz:f3d7cc] ğŸ“Š 2.0s: 264c @131c/s (67ch, ~66t @33t/s)
2025-12-15 13:48:24,848 - src.llm.client - INFO - [viz:f3d7cc] ğŸ“Š 4.0s: 506c @126c/s (134ch, ~126t @31t/s)
2025-12-15 13:48:26,858 - src.llm.client - INFO - [viz:f3d7cc] ğŸ“Š 6.0s: 803c @133c/s (201ch, ~201t @33t/s)
2025-12-15 13:48:28,869 - src.llm.client - INFO - [viz:f3d7cc] ğŸ“Š 8.0s: 1068c @133c/s (268ch, ~267t @33t/s)
2025-12-15 13:48:30,014 - src.llm.client - INFO - [viz:f3d7cc] âœ“ Done 18.75s: 1201c (~170w @64c/s)
2025-12-15 13:48:30,014 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-15 13:48:30,014 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:48:30,014 - generate_secondary - INFO -     - Length: 380 chars (cleaned: 380 chars)
2025-12-15 13:48:30,014 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:48:30,014 - generate_secondary - INFO - [WARNING] Elements: 21 total (nodes: 14, connections: 7) âš ï¸
2025-12-15 13:48:30,014 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:48:30,014 - generate_secondary - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-15 13:48:30,015 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_14/visualization.mmd
2025-12-15 13:48:30,015 - generate_secondary - INFO - Generating integration for session 14: Sensory Coding...
2025-12-15 13:48:30,015 - src.llm.client - INFO - [int:4a6ac0] ğŸš€ int | m=gemma3:4b | p=28650c | t=150s
2025-12-15 13:48:30,015 - src.llm.client - INFO - [int:4a6ac0] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:48:30,015 - src.llm.client - INFO - [int:4a6ac0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:48:30,017 - src.llm.client - INFO - [int:4a6ac0] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=33965 bytes, prompt=28650 chars
2025-12-15 13:48:30,017 - src.llm.client - INFO - [int:4a6ac0] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:48:39,580 - src.llm.request_handler - INFO - [int:4a6ac0] âœ“ Done 9.56s
2025-12-15 13:48:39,580 - src.llm.client - INFO - [int:4a6ac0] âœ… HTTP 200 in 9.56s
2025-12-15 13:48:39,580 - src.llm.client - INFO - [int:4a6ac0] ğŸ“¡ Stream active (200)
2025-12-15 13:48:39,580 - src.llm.client - INFO - [int:4a6ac0] Starting stream parsing, waiting for first chunk...
2025-12-15 13:48:41,597 - src.llm.client - INFO - [int:4a6ac0] ğŸ“Š 2.0s: 374c @185c/s (67ch, ~94t @46t/s)
2025-12-15 13:48:43,606 - src.llm.client - INFO - [int:4a6ac0] ğŸ“Š 4.0s: 764c @190c/s (134ch, ~191t @47t/s)
2025-12-15 13:48:45,621 - src.llm.client - INFO - [int:4a6ac0] ğŸ“Š 6.0s: 1143c @189c/s (201ch, ~286t @47t/s)
2025-12-15 13:48:47,631 - src.llm.client - INFO - [int:4a6ac0] ğŸ“Š 8.1s: 1519c @189c/s (268ch, ~380t @47t/s)
2025-12-15 13:48:49,642 - src.llm.client - INFO - [int:4a6ac0] ğŸ“Š 10.1s: 1905c @189c/s (335ch, ~476t @47t/s)
2025-12-15 13:48:51,656 - src.llm.client - INFO - [int:4a6ac0] ğŸ“Š 12.1s: 2128c @176c/s (402ch, ~532t @44t/s)
2025-12-15 13:48:53,669 - src.llm.client - INFO - [int:4a6ac0] ğŸ“Š 14.1s: 2427c @172c/s (469ch, ~607t @43t/s)
2025-12-15 13:48:54,582 - src.llm.client - INFO - [int:4a6ac0] âœ“ Done 24.57s: 2541c (~378w @103c/s)
2025-12-15 13:48:54,583 - src.generate.processors.cleanup - INFO - Cleanup complete: 2 issues before, 0 issues after
2025-12-15 13:48:54,583 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:48:54,584 - generate_secondary - INFO -     - Length: 2335 chars, 340 words
2025-12-15 13:48:54,584 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:48:54,584 - generate_secondary - INFO -     - Connections: 21
2025-12-15 13:48:54,584 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:48:54,584 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_14/integration.md
2025-12-15 13:48:54,584 - generate_secondary - INFO - Generating investigation for session 14: Sensory Coding...
2025-12-15 13:48:54,584 - src.llm.client - INFO - [inv:0db7b8] ğŸš€ inv | m=gemma3:4b | p=27563c | t=150s
2025-12-15 13:48:54,584 - src.llm.client - INFO - [inv:0db7b8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:48:54,584 - src.llm.client - INFO - [inv:0db7b8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:48:54,585 - src.llm.client - INFO - [inv:0db7b8] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=31821 bytes, prompt=27563 chars
2025-12-15 13:48:54,585 - src.llm.client - INFO - [inv:0db7b8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:49:04,149 - src.llm.request_handler - INFO - [inv:0db7b8] âœ“ Done 9.56s
2025-12-15 13:49:04,149 - src.llm.client - INFO - [inv:0db7b8] âœ… HTTP 200 in 9.56s
2025-12-15 13:49:04,149 - src.llm.client - INFO - [inv:0db7b8] ğŸ“¡ Stream active (200)
2025-12-15 13:49:04,149 - src.llm.client - INFO - [inv:0db7b8] Starting stream parsing, waiting for first chunk...
2025-12-15 13:49:06,166 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 2.0s: 349c @173c/s (67ch, ~87t @43t/s)
2025-12-15 13:49:08,181 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 4.0s: 665c @165c/s (134ch, ~166t @41t/s)
2025-12-15 13:49:10,195 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 6.0s: 994c @164c/s (201ch, ~248t @41t/s)
2025-12-15 13:49:12,208 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 8.1s: 1333c @165c/s (268ch, ~333t @41t/s)
2025-12-15 13:49:14,222 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 10.1s: 1693c @168c/s (335ch, ~423t @42t/s)
2025-12-15 13:49:16,234 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 12.1s: 2065c @171c/s (402ch, ~516t @43t/s)
2025-12-15 13:49:18,246 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 14.1s: 2400c @170c/s (469ch, ~600t @43t/s)
2025-12-15 13:49:20,263 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 16.1s: 2777c @172c/s (536ch, ~694t @43t/s)
2025-12-15 13:49:22,279 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 18.1s: 3195c @176c/s (603ch, ~799t @44t/s)
2025-12-15 13:49:24,299 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 20.1s: 3623c @180c/s (670ch, ~906t @45t/s)
2025-12-15 13:49:26,318 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 22.2s: 3956c @178c/s (737ch, ~989t @45t/s)
2025-12-15 13:49:28,336 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 24.2s: 4252c @176c/s (804ch, ~1063t @44t/s)
2025-12-15 13:49:30,361 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 26.2s: 4605c @176c/s (871ch, ~1151t @44t/s)
2025-12-15 13:49:32,384 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 28.2s: 4999c @177c/s (938ch, ~1250t @44t/s)
2025-12-15 13:49:34,413 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 30.3s: 5422c @179c/s (1005ch, ~1356t @45t/s)
2025-12-15 13:49:36,438 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 32.3s: 5689c @176c/s (1072ch, ~1422t @44t/s)
2025-12-15 13:49:38,657 - src.llm.client - INFO - [inv:0db7b8] ğŸ“Š 34.5s: 6028c @175c/s (1134ch, ~1507t @44t/s)
2025-12-15 13:49:38,657 - src.llm.client - INFO - [inv:0db7b8] âœ“ Done 44.07s: 6028c (~847w @137c/s)
2025-12-15 13:49:38,660 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:49:38,660 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:49:38,660 - generate_secondary - INFO -     - Length: 6025 chars, 847 words
2025-12-15 13:49:38,660 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:49:38,660 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:49:38,660 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:49:38,660 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_14/investigation.md
2025-12-15 13:49:38,660 - generate_secondary - INFO - Generating open_questions for session 14: Sensory Coding...
2025-12-15 13:49:38,660 - src.llm.client - INFO - [opq:b31832] ğŸš€ opq | m=gemma3:4b | p=27649c | t=150s
2025-12-15 13:49:38,660 - src.llm.client - INFO - [opq:b31832] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:49:38,660 - src.llm.client - INFO - [opq:b31832] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:49:38,662 - src.llm.client - INFO - [opq:b31832] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=31918 bytes, prompt=27649 chars
2025-12-15 13:49:38,662 - src.llm.client - INFO - [opq:b31832] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:49:48,221 - src.llm.request_handler - INFO - [opq:b31832] âœ“ Done 9.56s
2025-12-15 13:49:48,221 - src.llm.client - INFO - [opq:b31832] âœ… HTTP 200 in 9.56s
2025-12-15 13:49:48,222 - src.llm.client - INFO - [opq:b31832] ğŸ“¡ Stream active (200)
2025-12-15 13:49:48,222 - src.llm.client - INFO - [opq:b31832] Starting stream parsing, waiting for first chunk...
2025-12-15 13:49:50,234 - src.llm.client - INFO - [opq:b31832] ğŸ“Š 2.0s: 392c @195c/s (67ch, ~98t @49t/s)
2025-12-15 13:49:52,243 - src.llm.client - INFO - [opq:b31832] ğŸ“Š 4.0s: 803c @200c/s (134ch, ~201t @50t/s)
2025-12-15 13:49:54,253 - src.llm.client - INFO - [opq:b31832] ğŸ“Š 6.0s: 1198c @199c/s (201ch, ~300t @50t/s)
2025-12-15 13:49:56,266 - src.llm.client - INFO - [opq:b31832] ğŸ“Š 8.0s: 1594c @198c/s (268ch, ~398t @50t/s)
2025-12-15 13:49:58,277 - src.llm.client - INFO - [opq:b31832] ğŸ“Š 10.1s: 1940c @193c/s (335ch, ~485t @48t/s)
2025-12-15 13:50:00,294 - src.llm.client - INFO - [opq:b31832] ğŸ“Š 12.1s: 2348c @194c/s (402ch, ~587t @49t/s)
2025-12-15 13:50:01,002 - src.llm.client - INFO - [opq:b31832] âœ“ Done 22.34s: 2445c (~328w @109c/s)
2025-12-15 13:50:01,003 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:50:01,003 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:50:01,003 - generate_secondary - INFO -     - Length: 2444 chars, 328 words
2025-12-15 13:50:01,003 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:50:01,003 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:50:01,003 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:50:01,004 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_14/open_questions.md
2025-12-15 13:50:01,004 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:50:01,004 - generate_secondary - INFO - 
  Session 15/20: Motor Control
2025-12-15 13:50:01,005 - generate_secondary - INFO - Generating application for session 15: Motor Control...
2025-12-15 13:50:01,006 - src.llm.client - INFO - [app:ee1966] ğŸš€ app | m=gemma3:4b | p=32313c | t=150s
2025-12-15 13:50:01,006 - src.llm.client - INFO - [app:ee1966] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:50:01,006 - src.llm.client - INFO - [app:ee1966] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:50:01,007 - src.llm.client - INFO - [app:ee1966] Sending request to Ollama: model=gemma3:4b, operation=application, payload=34365 bytes, prompt=32313 chars
2025-12-15 13:50:01,007 - src.llm.client - INFO - [app:ee1966] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:50:10,578 - src.llm.request_handler - INFO - [app:ee1966] âœ“ Done 9.57s
2025-12-15 13:50:10,578 - src.llm.client - INFO - [app:ee1966] âœ… HTTP 200 in 9.57s
2025-12-15 13:50:10,578 - src.llm.client - INFO - [app:ee1966] ğŸ“¡ Stream active (200)
2025-12-15 13:50:10,578 - src.llm.client - INFO - [app:ee1966] Starting stream parsing, waiting for first chunk...
2025-12-15 13:50:12,594 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 2.0s: 402c @199c/s (67ch, ~100t @50t/s)
2025-12-15 13:50:14,604 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 4.0s: 798c @198c/s (134ch, ~200t @50t/s)
2025-12-15 13:50:16,616 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 6.0s: 1212c @201c/s (201ch, ~303t @50t/s)
2025-12-15 13:50:18,640 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 8.1s: 1631c @202c/s (268ch, ~408t @51t/s)
2025-12-15 13:50:20,658 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 10.1s: 2007c @199c/s (335ch, ~502t @50t/s)
2025-12-15 13:50:22,674 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 12.1s: 2406c @199c/s (402ch, ~602t @50t/s)
2025-12-15 13:50:24,692 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 14.1s: 2770c @196c/s (469ch, ~692t @49t/s)
2025-12-15 13:50:26,708 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 16.1s: 3150c @195c/s (536ch, ~788t @49t/s)
2025-12-15 13:50:28,724 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 18.1s: 3529c @194c/s (603ch, ~882t @49t/s)
2025-12-15 13:50:30,751 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 20.2s: 3905c @194c/s (670ch, ~976t @48t/s)
2025-12-15 13:50:32,773 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 22.2s: 4273c @193c/s (737ch, ~1068t @48t/s)
2025-12-15 13:50:34,796 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 24.2s: 4707c @194c/s (804ch, ~1177t @49t/s)
2025-12-15 13:50:36,819 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 26.2s: 5146c @196c/s (871ch, ~1286t @49t/s)
2025-12-15 13:50:38,845 - src.llm.client - INFO - [app:ee1966] ğŸ“Š 28.3s: 5597c @198c/s (938ch, ~1399t @50t/s)
2025-12-15 13:50:39,755 - src.llm.client - INFO - [app:ee1966] âœ“ Done 38.75s: 5698c (~771w @147c/s)
2025-12-15 13:50:39,757 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:50:39,757 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:50:39,757 - generate_secondary - INFO -     - Length: 5686 chars, 769 words
2025-12-15 13:50:39,757 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:50:39,757 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:50:39,757 - generate_secondary - INFO -     - Avg words per application: 149
2025-12-15 13:50:39,757 - generate_secondary - WARNING - [WARNING] Application 3 has 142 words (require 150-200, need 8 more words) âš ï¸
2025-12-15 13:50:39,757 - generate_secondary - WARNING - [WARNING] Application 4 has 135 words (require 150-200, need 15 more words) âš ï¸
2025-12-15 13:50:39,757 - generate_secondary - WARNING - [WARNING] Application 5 has 135 words (require 150-200, need 15 more words) âš ï¸
2025-12-15 13:50:39,758 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_15/application.md
2025-12-15 13:50:39,758 - generate_secondary - INFO - Generating extension for session 15: Motor Control...
2025-12-15 13:50:39,758 - src.llm.client - INFO - [ext:a2624c] ğŸš€ ext | m=gemma3:4b | p=26199c | t=120s
2025-12-15 13:50:39,758 - src.llm.client - INFO - [ext:a2624c] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:50:39,758 - src.llm.client - INFO - [ext:a2624c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:50:39,759 - src.llm.client - INFO - [ext:a2624c] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=31122 bytes, prompt=26199 chars
2025-12-15 13:50:39,759 - src.llm.client - INFO - [ext:a2624c] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:50:49,314 - src.llm.request_handler - INFO - [ext:a2624c] âœ“ Done 9.55s
2025-12-15 13:50:49,314 - src.llm.client - INFO - [ext:a2624c] âœ… HTTP 200 in 9.55s
2025-12-15 13:50:49,314 - src.llm.client - INFO - [ext:a2624c] ğŸ“¡ Stream active (200)
2025-12-15 13:50:49,314 - src.llm.client - INFO - [ext:a2624c] Starting stream parsing, waiting for first chunk...
2025-12-15 13:50:51,329 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 2.0s: 364c @181c/s (67ch, ~91t @45t/s)
2025-12-15 13:50:53,336 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 4.0s: 774c @192c/s (134ch, ~194t @48t/s)
2025-12-15 13:50:55,350 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 6.0s: 1257c @208c/s (201ch, ~314t @52t/s)
2025-12-15 13:50:57,364 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 8.1s: 1617c @201c/s (268ch, ~404t @50t/s)
2025-12-15 13:50:59,377 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 10.1s: 1999c @199c/s (335ch, ~500t @50t/s)
2025-12-15 13:51:01,402 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 12.1s: 2452c @203c/s (402ch, ~613t @51t/s)
2025-12-15 13:51:03,419 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 14.1s: 2829c @201c/s (469ch, ~707t @50t/s)
2025-12-15 13:51:05,444 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 16.1s: 3253c @202c/s (536ch, ~813t @50t/s)
2025-12-15 13:51:07,462 - src.llm.client - INFO - [ext:a2624c] ğŸ“Š 18.1s: 3681c @203c/s (603ch, ~920t @51t/s)
2025-12-15 13:51:09,016 - src.llm.client - INFO - [ext:a2624c] âœ“ Done 29.26s: 3946c (~537w @135c/s)
2025-12-15 13:51:09,017 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:51:09,018 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:51:09,018 - generate_secondary - INFO -     - Length: 3931 chars, 535 words
2025-12-15 13:51:09,018 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:51:09,018 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:51:09,019 - generate_secondary - INFO -     - Avg words per topic: 173
2025-12-15 13:51:09,019 - generate_secondary - WARNING - [WARNING] Topic 1 has 166 words (exceeds 150 by 16 words - consider condensing) âš ï¸
2025-12-15 13:51:09,019 - generate_secondary - WARNING - [WARNING] Topic 2 has 162 words (exceeds 150 by 12 words - consider condensing) âš ï¸
2025-12-15 13:51:09,019 - generate_secondary - WARNING - [WARNING] Topic 3 has 190 words (exceeds 150 by 40 words - consider condensing) âš ï¸
2025-12-15 13:51:09,019 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_15/extension.md
2025-12-15 13:51:09,019 - generate_secondary - INFO - Generating visualization for session 15: Motor Control...
2025-12-15 13:51:09,019 - src.llm.client - INFO - [viz:025344] ğŸš€ viz | m=gemma3:4b | p=25159c | t=120s
2025-12-15 13:51:09,020 - src.llm.client - INFO - [viz:025344] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:51:09,020 - src.llm.client - INFO - [viz:025344] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:51:09,021 - src.llm.client - INFO - [viz:025344] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=29404 bytes, prompt=25159 chars
2025-12-15 13:51:09,021 - src.llm.client - INFO - [viz:025344] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:51:18,559 - src.llm.request_handler - INFO - [viz:025344] âœ“ Done 9.54s
2025-12-15 13:51:18,560 - src.llm.client - INFO - [viz:025344] âœ… HTTP 200 in 9.54s
2025-12-15 13:51:18,560 - src.llm.client - INFO - [viz:025344] ğŸ“¡ Stream active (200)
2025-12-15 13:51:18,560 - src.llm.client - INFO - [viz:025344] Starting stream parsing, waiting for first chunk...
2025-12-15 13:51:20,576 - src.llm.client - INFO - [viz:025344] ğŸ“Š 2.0s: 205c @102c/s (67ch, ~51t @25t/s)
2025-12-15 13:51:21,916 - src.llm.client - INFO - [viz:025344] âœ“ Done 12.90s: 310c (~43w @24c/s)
2025-12-15 13:51:21,917 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:51:21,917 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:51:21,917 - generate_secondary - INFO -     - Length: 294 chars (cleaned: 294 chars)
2025-12-15 13:51:21,917 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:51:21,917 - generate_secondary - INFO - [OK] Elements: 20 total (nodes: 10, connections: 10) âœ“
2025-12-15 13:51:21,917 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_15/visualization.mmd
2025-12-15 13:51:21,917 - generate_secondary - INFO - Generating integration for session 15: Motor Control...
2025-12-15 13:51:21,917 - src.llm.client - INFO - [int:807ec6] ğŸš€ int | m=gemma3:4b | p=26508c | t=150s
2025-12-15 13:51:21,918 - src.llm.client - INFO - [int:807ec6] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:51:21,918 - src.llm.client - INFO - [int:807ec6] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:51:21,919 - src.llm.client - INFO - [int:807ec6] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=31770 bytes, prompt=26508 chars
2025-12-15 13:51:21,919 - src.llm.client - INFO - [int:807ec6] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:51:31,487 - src.llm.request_handler - INFO - [int:807ec6] âœ“ Done 9.57s
2025-12-15 13:51:31,487 - src.llm.client - INFO - [int:807ec6] âœ… HTTP 200 in 9.57s
2025-12-15 13:51:31,487 - src.llm.client - INFO - [int:807ec6] ğŸ“¡ Stream active (200)
2025-12-15 13:51:31,487 - src.llm.client - INFO - [int:807ec6] Starting stream parsing, waiting for first chunk...
2025-12-15 13:51:33,507 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 2.0s: 361c @179c/s (67ch, ~90t @45t/s)
2025-12-15 13:51:35,520 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 4.0s: 739c @183c/s (134ch, ~185t @46t/s)
2025-12-15 13:51:37,530 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 6.0s: 1136c @188c/s (201ch, ~284t @47t/s)
2025-12-15 13:51:39,545 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 8.1s: 1503c @187c/s (268ch, ~376t @47t/s)
2025-12-15 13:51:41,557 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 10.1s: 1726c @171c/s (335ch, ~432t @43t/s)
2025-12-15 13:51:43,569 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 12.1s: 1961c @162c/s (402ch, ~490t @41t/s)
2025-12-15 13:51:45,584 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 14.1s: 2167c @154c/s (469ch, ~542t @38t/s)
2025-12-15 13:51:47,598 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 16.1s: 2329c @145c/s (536ch, ~582t @36t/s)
2025-12-15 13:51:49,617 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 18.1s: 2512c @139c/s (603ch, ~628t @35t/s)
2025-12-15 13:51:51,635 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 20.1s: 2776c @138c/s (670ch, ~694t @34t/s)
2025-12-15 13:51:53,658 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 22.2s: 3036c @137c/s (737ch, ~759t @34t/s)
2025-12-15 13:51:55,679 - src.llm.client - INFO - [int:807ec6] ğŸ“Š 24.2s: 3301c @136c/s (804ch, ~825t @34t/s)
2025-12-15 13:51:56,282 - src.llm.client - INFO - [int:807ec6] âœ“ Done 34.36s: 3350c (~491w @97c/s)
2025-12-15 13:51:56,283 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:51:56,284 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:51:56,284 - generate_secondary - INFO -     - Length: 3349 chars, 491 words
2025-12-15 13:51:56,284 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:51:56,284 - generate_secondary - INFO -     - Connections: 18
2025-12-15 13:51:56,284 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:51:56,284 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_15/integration.md
2025-12-15 13:51:56,284 - generate_secondary - INFO - Generating investigation for session 15: Motor Control...
2025-12-15 13:51:56,284 - src.llm.client - INFO - [inv:523239] ğŸš€ inv | m=gemma3:4b | p=25421c | t=150s
2025-12-15 13:51:56,284 - src.llm.client - INFO - [inv:523239] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:51:56,284 - src.llm.client - INFO - [inv:523239] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:51:56,286 - src.llm.client - INFO - [inv:523239] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=29626 bytes, prompt=25421 chars
2025-12-15 13:51:56,286 - src.llm.client - INFO - [inv:523239] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:52:05,844 - src.llm.request_handler - INFO - [inv:523239] âœ“ Done 9.56s
2025-12-15 13:52:05,844 - src.llm.client - INFO - [inv:523239] âœ… HTTP 200 in 9.56s
2025-12-15 13:52:05,844 - src.llm.client - INFO - [inv:523239] ğŸ“¡ Stream active (200)
2025-12-15 13:52:05,844 - src.llm.client - INFO - [inv:523239] Starting stream parsing, waiting for first chunk...
2025-12-15 13:52:07,864 - src.llm.client - INFO - [inv:523239] ğŸ“Š 2.0s: 366c @181c/s (67ch, ~92t @45t/s)
2025-12-15 13:52:09,878 - src.llm.client - INFO - [inv:523239] ğŸ“Š 4.0s: 788c @195c/s (134ch, ~197t @49t/s)
2025-12-15 13:52:11,890 - src.llm.client - INFO - [inv:523239] ğŸ“Š 6.0s: 1108c @183c/s (201ch, ~277t @46t/s)
2025-12-15 13:52:13,902 - src.llm.client - INFO - [inv:523239] ğŸ“Š 8.1s: 1500c @186c/s (268ch, ~375t @47t/s)
2025-12-15 13:52:15,930 - src.llm.client - INFO - [inv:523239] ğŸ“Š 10.1s: 1876c @186c/s (335ch, ~469t @47t/s)
2025-12-15 13:52:17,944 - src.llm.client - INFO - [inv:523239] ğŸ“Š 12.1s: 2247c @186c/s (402ch, ~562t @46t/s)
2025-12-15 13:52:19,967 - src.llm.client - INFO - [inv:523239] ğŸ“Š 14.1s: 2623c @186c/s (469ch, ~656t @46t/s)
2025-12-15 13:52:21,982 - src.llm.client - INFO - [inv:523239] ğŸ“Š 16.1s: 3012c @187c/s (536ch, ~753t @47t/s)
2025-12-15 13:52:23,998 - src.llm.client - INFO - [inv:523239] ğŸ“Š 18.2s: 3399c @187c/s (603ch, ~850t @47t/s)
2025-12-15 13:52:26,014 - src.llm.client - INFO - [inv:523239] ğŸ“Š 20.2s: 3763c @187c/s (670ch, ~941t @47t/s)
2025-12-15 13:52:28,035 - src.llm.client - INFO - [inv:523239] ğŸ“Š 22.2s: 4134c @186c/s (737ch, ~1034t @47t/s)
2025-12-15 13:52:30,058 - src.llm.client - INFO - [inv:523239] ğŸ“Š 24.2s: 4539c @187c/s (804ch, ~1135t @47t/s)
2025-12-15 13:52:32,082 - src.llm.client - INFO - [inv:523239] ğŸ“Š 26.2s: 4949c @189c/s (871ch, ~1237t @47t/s)
2025-12-15 13:52:33,154 - src.llm.client - INFO - [inv:523239] âœ“ Done 36.87s: 5089c (~739w @138c/s)
2025-12-15 13:52:33,156 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:52:33,156 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:52:33,156 - generate_secondary - INFO -     - Length: 5088 chars, 739 words
2025-12-15 13:52:33,156 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:52:33,156 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:52:33,156 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:52:33,156 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_15/investigation.md
2025-12-15 13:52:33,156 - generate_secondary - INFO - Generating open_questions for session 15: Motor Control...
2025-12-15 13:52:33,157 - src.llm.client - INFO - [opq:bc802b] ğŸš€ opq | m=gemma3:4b | p=25507c | t=150s
2025-12-15 13:52:33,157 - src.llm.client - INFO - [opq:bc802b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:52:33,157 - src.llm.client - INFO - [opq:bc802b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:52:33,158 - src.llm.client - INFO - [opq:bc802b] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=29723 bytes, prompt=25507 chars
2025-12-15 13:52:33,158 - src.llm.client - INFO - [opq:bc802b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:52:42,717 - src.llm.request_handler - INFO - [opq:bc802b] âœ“ Done 9.56s
2025-12-15 13:52:42,717 - src.llm.client - INFO - [opq:bc802b] âœ… HTTP 200 in 9.56s
2025-12-15 13:52:42,717 - src.llm.client - INFO - [opq:bc802b] ğŸ“¡ Stream active (200)
2025-12-15 13:52:42,717 - src.llm.client - INFO - [opq:bc802b] Starting stream parsing, waiting for first chunk...
2025-12-15 13:52:44,744 - src.llm.client - INFO - [opq:bc802b] ğŸ“Š 2.0s: 344c @170c/s (67ch, ~86t @42t/s)
2025-12-15 13:52:46,754 - src.llm.client - INFO - [opq:bc802b] ğŸ“Š 4.0s: 799c @198c/s (134ch, ~200t @49t/s)
2025-12-15 13:52:48,764 - src.llm.client - INFO - [opq:bc802b] ğŸ“Š 6.0s: 1180c @195c/s (201ch, ~295t @49t/s)
2025-12-15 13:52:50,777 - src.llm.client - INFO - [opq:bc802b] ğŸ“Š 8.1s: 1582c @196c/s (268ch, ~396t @49t/s)
2025-12-15 13:52:52,790 - src.llm.client - INFO - [opq:bc802b] ğŸ“Š 10.1s: 1970c @196c/s (335ch, ~492t @49t/s)
2025-12-15 13:52:53,962 - src.llm.client - INFO - [opq:bc802b] âœ“ Done 20.81s: 2176c (~282w @105c/s)
2025-12-15 13:52:53,963 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:52:53,963 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:52:53,963 - generate_secondary - INFO -     - Length: 2175 chars, 282 words
2025-12-15 13:52:53,963 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:52:53,963 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:52:53,963 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:52:53,964 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_08_neuroscientific_evidence/session_15/open_questions.md
2025-12-15 13:52:53,964 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:52:53,964 - generate_secondary - INFO - 
============================================================
2025-12-15 13:52:53,964 - generate_secondary - INFO - [9/10] Module 9: Applications: AI & Robotics (2 sessions)
2025-12-15 13:52:53,964 - generate_secondary - INFO - ============================================================
2025-12-15 13:52:53,964 - generate_secondary - INFO - 
  Session 16/20: Robot Navigation
2025-12-15 13:52:53,966 - generate_secondary - INFO - Generating application for session 16: Robot Navigation...
2025-12-15 13:52:53,966 - src.llm.client - INFO - [app:d2ec6e] ğŸš€ app | m=gemma3:4b | p=33608c | t=150s
2025-12-15 13:52:53,966 - src.llm.client - INFO - [app:d2ec6e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:52:53,966 - src.llm.client - INFO - [app:d2ec6e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:52:53,967 - src.llm.client - INFO - [app:d2ec6e] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35678 bytes, prompt=33608 chars
2025-12-15 13:52:53,967 - src.llm.client - INFO - [app:d2ec6e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:53:03,539 - src.llm.request_handler - INFO - [app:d2ec6e] âœ“ Done 9.57s
2025-12-15 13:53:03,539 - src.llm.client - INFO - [app:d2ec6e] âœ… HTTP 200 in 9.57s
2025-12-15 13:53:03,539 - src.llm.client - INFO - [app:d2ec6e] ğŸ“¡ Stream active (200)
2025-12-15 13:53:03,539 - src.llm.client - INFO - [app:d2ec6e] Starting stream parsing, waiting for first chunk...
2025-12-15 13:53:05,557 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 2.0s: 420c @208c/s (67ch, ~105t @52t/s)
2025-12-15 13:53:07,568 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 4.0s: 817c @203c/s (134ch, ~204t @51t/s)
2025-12-15 13:53:09,579 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 6.0s: 1231c @204c/s (201ch, ~308t @51t/s)
2025-12-15 13:53:11,590 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 8.1s: 1633c @203c/s (268ch, ~408t @51t/s)
2025-12-15 13:53:13,603 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 10.1s: 2046c @203c/s (335ch, ~512t @51t/s)
2025-12-15 13:53:15,618 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 12.1s: 2492c @206c/s (402ch, ~623t @52t/s)
2025-12-15 13:53:17,632 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 14.1s: 2908c @206c/s (469ch, ~727t @52t/s)
2025-12-15 13:53:19,648 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 16.1s: 3269c @203c/s (536ch, ~817t @51t/s)
2025-12-15 13:53:21,662 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 18.1s: 3672c @203c/s (603ch, ~918t @51t/s)
2025-12-15 13:53:23,680 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 20.1s: 4075c @202c/s (670ch, ~1019t @51t/s)
2025-12-15 13:53:25,700 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 22.2s: 4491c @203c/s (737ch, ~1123t @51t/s)
2025-12-15 13:53:27,718 - src.llm.client - INFO - [app:d2ec6e] ğŸ“Š 24.2s: 4883c @202c/s (804ch, ~1221t @50t/s)
2025-12-15 13:53:28,676 - src.llm.client - INFO - [app:d2ec6e] âœ“ Done 34.71s: 4994c (~683w @144c/s)
2025-12-15 13:53:28,678 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:53:28,678 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:53:28,678 - generate_secondary - INFO -     - Length: 4982 chars, 681 words
2025-12-15 13:53:28,678 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:53:28,678 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:53:28,678 - generate_secondary - INFO -     - Avg words per application: 132
2025-12-15 13:53:28,678 - generate_secondary - WARNING - [WARNING] Application 1 has 134 words (require 150-200, need 16 more words) âš ï¸
2025-12-15 13:53:28,678 - generate_secondary - WARNING - [WARNING] Application 3 has 132 words (require 150-200, need 18 more words) âš ï¸
2025-12-15 13:53:28,678 - generate_secondary - WARNING - [WARNING] Application 4 has 123 words (require 150-200, need 27 more words) âš ï¸
2025-12-15 13:53:28,678 - generate_secondary - WARNING - [WARNING] Application 5 has 119 words (require 150-200, need 31 more words) âš ï¸
2025-12-15 13:53:28,678 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_16/application.md
2025-12-15 13:53:28,678 - generate_secondary - INFO - Generating extension for session 16: Robot Navigation...
2025-12-15 13:53:28,679 - src.llm.client - INFO - [ext:dde541] ğŸš€ ext | m=gemma3:4b | p=27494c | t=120s
2025-12-15 13:53:28,679 - src.llm.client - INFO - [ext:dde541] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:53:28,679 - src.llm.client - INFO - [ext:dde541] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:53:28,680 - src.llm.client - INFO - [ext:dde541] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32435 bytes, prompt=27494 chars
2025-12-15 13:53:28,680 - src.llm.client - INFO - [ext:dde541] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:53:38,238 - src.llm.request_handler - INFO - [ext:dde541] âœ“ Done 9.56s
2025-12-15 13:53:38,238 - src.llm.client - INFO - [ext:dde541] âœ… HTTP 200 in 9.56s
2025-12-15 13:53:38,238 - src.llm.client - INFO - [ext:dde541] ğŸ“¡ Stream active (200)
2025-12-15 13:53:38,238 - src.llm.client - INFO - [ext:dde541] Starting stream parsing, waiting for first chunk...
2025-12-15 13:53:40,254 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 2.0s: 425c @211c/s (67ch, ~106t @53t/s)
2025-12-15 13:53:42,260 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 4.0s: 837c @208c/s (134ch, ~209t @52t/s)
2025-12-15 13:53:44,269 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 6.0s: 1268c @210c/s (201ch, ~317t @53t/s)
2025-12-15 13:53:46,280 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 8.0s: 1711c @213c/s (268ch, ~428t @53t/s)
2025-12-15 13:53:48,292 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 10.1s: 2119c @211c/s (335ch, ~530t @53t/s)
2025-12-15 13:53:50,305 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 12.1s: 2566c @213c/s (402ch, ~642t @53t/s)
2025-12-15 13:53:52,315 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 14.1s: 2936c @209c/s (469ch, ~734t @52t/s)
2025-12-15 13:53:54,333 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 16.1s: 3337c @207c/s (536ch, ~834t @52t/s)
2025-12-15 13:53:56,355 - src.llm.client - INFO - [ext:dde541] ğŸ“Š 18.1s: 3760c @208c/s (603ch, ~940t @52t/s)
2025-12-15 13:53:57,395 - src.llm.client - INFO - [ext:dde541] âœ“ Done 28.72s: 3906c (~509w @136c/s)
2025-12-15 13:53:57,396 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:53:57,397 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:53:57,397 - generate_secondary - INFO -     - Length: 3893 chars, 507 words
2025-12-15 13:53:57,397 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:53:57,397 - generate_secondary - INFO -     - Topics: 3
2025-12-15 13:53:57,397 - generate_secondary - INFO -     - Avg words per topic: 164
2025-12-15 13:53:57,397 - generate_secondary - WARNING - [WARNING] Topic 1 has 167 words (exceeds 150 by 17 words - consider condensing) âš ï¸
2025-12-15 13:53:57,397 - generate_secondary - WARNING - [WARNING] Topic 2 has 156 words (exceeds 150 by 6 words - consider condensing) âš ï¸
2025-12-15 13:53:57,397 - generate_secondary - WARNING - [WARNING] Topic 3 has 168 words (exceeds 150 by 18 words - consider condensing) âš ï¸
2025-12-15 13:53:57,397 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_16/extension.md
2025-12-15 13:53:57,397 - generate_secondary - INFO - Generating visualization for session 16: Robot Navigation...
2025-12-15 13:53:57,397 - src.llm.client - INFO - [viz:20f421] ğŸš€ viz | m=gemma3:4b | p=26454c | t=120s
2025-12-15 13:53:57,397 - src.llm.client - INFO - [viz:20f421] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:53:57,397 - src.llm.client - INFO - [viz:20f421] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:53:57,399 - src.llm.client - INFO - [viz:20f421] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30717 bytes, prompt=26454 chars
2025-12-15 13:53:57,399 - src.llm.client - INFO - [viz:20f421] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:54:06,967 - src.llm.request_handler - INFO - [viz:20f421] âœ“ Done 9.57s
2025-12-15 13:54:06,967 - src.llm.client - INFO - [viz:20f421] âœ… HTTP 200 in 9.57s
2025-12-15 13:54:06,968 - src.llm.client - INFO - [viz:20f421] ğŸ“¡ Stream active (200)
2025-12-15 13:54:06,968 - src.llm.client - INFO - [viz:20f421] Starting stream parsing, waiting for first chunk...
2025-12-15 13:54:08,984 - src.llm.client - INFO - [viz:20f421] ğŸ“Š 2.0s: 265c @131c/s (67ch, ~66t @33t/s)
2025-12-15 13:54:10,994 - src.llm.client - INFO - [viz:20f421] ğŸ“Š 4.0s: 509c @126c/s (134ch, ~127t @32t/s)
2025-12-15 13:54:13,004 - src.llm.client - INFO - [viz:20f421] ğŸ“Š 6.0s: 735c @122c/s (201ch, ~184t @30t/s)
2025-12-15 13:54:15,018 - src.llm.client - INFO - [viz:20f421] ğŸ“Š 8.0s: 1030c @128c/s (268ch, ~258t @32t/s)
2025-12-15 13:54:15,428 - src.llm.client - INFO - [viz:20f421] âœ“ Done 18.03s: 1062c (~159w @59c/s)
2025-12-15 13:54:15,429 - src.generate.processors.cleanup - INFO - Cleanup complete: 2 issues before, 0 issues after
2025-12-15 13:54:15,429 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:54:15,429 - generate_secondary - INFO -     - Length: 252 chars (cleaned: 252 chars)
2025-12-15 13:54:15,429 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:54:15,429 - generate_secondary - INFO - [CRITICAL] Elements: 16 total (nodes: 8, connections: 8) ğŸ”´
2025-12-15 13:54:15,429 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:54:15,429 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-15 13:54:15,429 - generate_secondary - WARNING - [WARNING] Only 8 nodes found (require at least 10, need 2 more - add more nodes to the diagram) âš ï¸
2025-12-15 13:54:15,429 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:54:15,429 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:54:15,429 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_16/visualization.mmd
2025-12-15 13:54:15,429 - generate_secondary - INFO - Generating integration for session 16: Robot Navigation...
2025-12-15 13:54:15,429 - src.llm.client - INFO - [int:6af768] ğŸš€ int | m=gemma3:4b | p=27803c | t=150s
2025-12-15 13:54:15,429 - src.llm.client - INFO - [int:6af768] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:54:15,429 - src.llm.client - INFO - [int:6af768] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:54:15,431 - src.llm.client - INFO - [int:6af768] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=33083 bytes, prompt=27803 chars
2025-12-15 13:54:15,431 - src.llm.client - INFO - [int:6af768] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:54:24,998 - src.llm.request_handler - INFO - [int:6af768] âœ“ Done 9.57s
2025-12-15 13:54:24,999 - src.llm.client - INFO - [int:6af768] âœ… HTTP 200 in 9.57s
2025-12-15 13:54:24,999 - src.llm.client - INFO - [int:6af768] ğŸ“¡ Stream active (200)
2025-12-15 13:54:24,999 - src.llm.client - INFO - [int:6af768] Starting stream parsing, waiting for first chunk...
2025-12-15 13:54:27,018 - src.llm.client - INFO - [int:6af768] ğŸ“Š 2.0s: 377c @187c/s (67ch, ~94t @47t/s)
2025-12-15 13:54:29,030 - src.llm.client - INFO - [int:6af768] ğŸ“Š 4.0s: 805c @200c/s (134ch, ~201t @50t/s)
2025-12-15 13:54:31,046 - src.llm.client - INFO - [int:6af768] ğŸ“Š 6.0s: 1240c @205c/s (201ch, ~310t @51t/s)
2025-12-15 13:54:33,058 - src.llm.client - INFO - [int:6af768] ğŸ“Š 8.1s: 1662c @206c/s (268ch, ~416t @52t/s)
2025-12-15 13:54:35,070 - src.llm.client - INFO - [int:6af768] ğŸ“Š 10.1s: 2085c @207c/s (335ch, ~521t @52t/s)
2025-12-15 13:54:37,089 - src.llm.client - INFO - [int:6af768] ğŸ“Š 12.1s: 2303c @190c/s (402ch, ~576t @48t/s)
2025-12-15 13:54:38,739 - src.llm.client - INFO - [int:6af768] âœ“ Done 23.31s: 2489c (~328w @107c/s)
2025-12-15 13:54:38,741 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:54:38,741 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:54:38,741 - generate_secondary - INFO -     - Length: 2488 chars, 328 words
2025-12-15 13:54:38,741 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:54:38,741 - generate_secondary - INFO -     - Connections: 13
2025-12-15 13:54:38,741 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:54:38,741 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_16/integration.md
2025-12-15 13:54:38,741 - generate_secondary - INFO - Generating investigation for session 16: Robot Navigation...
2025-12-15 13:54:38,741 - src.llm.client - INFO - [inv:b41d59] ğŸš€ inv | m=gemma3:4b | p=26716c | t=150s
2025-12-15 13:54:38,741 - src.llm.client - INFO - [inv:b41d59] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:54:38,742 - src.llm.client - INFO - [inv:b41d59] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:54:38,743 - src.llm.client - INFO - [inv:b41d59] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30939 bytes, prompt=26716 chars
2025-12-15 13:54:38,743 - src.llm.client - INFO - [inv:b41d59] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:54:48,308 - src.llm.request_handler - INFO - [inv:b41d59] âœ“ Done 9.57s
2025-12-15 13:54:48,309 - src.llm.client - INFO - [inv:b41d59] âœ… HTTP 200 in 9.57s
2025-12-15 13:54:48,309 - src.llm.client - INFO - [inv:b41d59] ğŸ“¡ Stream active (200)
2025-12-15 13:54:48,309 - src.llm.client - INFO - [inv:b41d59] Starting stream parsing, waiting for first chunk...
2025-12-15 13:54:50,326 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 2.0s: 399c @198c/s (67ch, ~100t @49t/s)
2025-12-15 13:54:52,339 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 4.0s: 675c @167c/s (134ch, ~169t @42t/s)
2025-12-15 13:54:54,352 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 6.0s: 922c @153c/s (201ch, ~230t @38t/s)
2025-12-15 13:54:56,363 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 8.1s: 1300c @161c/s (268ch, ~325t @40t/s)
2025-12-15 13:54:58,377 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 10.1s: 1703c @169c/s (335ch, ~426t @42t/s)
2025-12-15 13:55:00,395 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 12.1s: 2088c @173c/s (402ch, ~522t @43t/s)
2025-12-15 13:55:02,398 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 14.1s: 2462c @175c/s (468ch, ~616t @44t/s)
2025-12-15 13:55:04,405 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 16.1s: 2770c @172c/s (534ch, ~692t @43t/s)
2025-12-15 13:55:06,422 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 18.1s: 3107c @172c/s (601ch, ~777t @43t/s)
2025-12-15 13:55:08,443 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 20.1s: 3568c @177c/s (668ch, ~892t @44t/s)
2025-12-15 13:55:10,467 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 22.2s: 3991c @180c/s (735ch, ~998t @45t/s)
2025-12-15 13:55:12,487 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 24.2s: 4384c @181c/s (802ch, ~1096t @45t/s)
2025-12-15 13:55:14,513 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 26.2s: 4754c @181c/s (869ch, ~1188t @45t/s)
2025-12-15 13:55:16,536 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 28.2s: 5166c @183c/s (936ch, ~1292t @46t/s)
2025-12-15 13:55:18,562 - src.llm.client - INFO - [inv:b41d59] ğŸ“Š 30.3s: 5589c @185c/s (1003ch, ~1397t @46t/s)
2025-12-15 13:55:20,437 - src.llm.client - INFO - [inv:b41d59] âœ“ Done 41.70s: 5939c (~816w @142c/s)
2025-12-15 13:55:20,440 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:55:20,440 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:55:20,440 - generate_secondary - INFO -     - Length: 5936 chars, 816 words
2025-12-15 13:55:20,440 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:55:20,440 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:55:20,440 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:55:20,440 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_16/investigation.md
2025-12-15 13:55:20,440 - generate_secondary - INFO - Generating open_questions for session 16: Robot Navigation...
2025-12-15 13:55:20,440 - src.llm.client - INFO - [opq:80c8c1] ğŸš€ opq | m=gemma3:4b | p=26802c | t=150s
2025-12-15 13:55:20,441 - src.llm.client - INFO - [opq:80c8c1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:55:20,441 - src.llm.client - INFO - [opq:80c8c1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:55:20,442 - src.llm.client - INFO - [opq:80c8c1] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=31036 bytes, prompt=26802 chars
2025-12-15 13:55:20,442 - src.llm.client - INFO - [opq:80c8c1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:55:30,006 - src.llm.request_handler - INFO - [opq:80c8c1] âœ“ Done 9.56s
2025-12-15 13:55:30,006 - src.llm.client - INFO - [opq:80c8c1] âœ… HTTP 200 in 9.56s
2025-12-15 13:55:30,006 - src.llm.client - INFO - [opq:80c8c1] ğŸ“¡ Stream active (200)
2025-12-15 13:55:30,006 - src.llm.client - INFO - [opq:80c8c1] Starting stream parsing, waiting for first chunk...
2025-12-15 13:55:32,027 - src.llm.client - INFO - [opq:80c8c1] ğŸ“Š 2.0s: 378c @187c/s (67ch, ~94t @47t/s)
2025-12-15 13:55:34,041 - src.llm.client - INFO - [opq:80c8c1] ğŸ“Š 4.0s: 806c @200c/s (134ch, ~202t @50t/s)
2025-12-15 13:55:36,055 - src.llm.client - INFO - [opq:80c8c1] ğŸ“Š 6.0s: 1224c @202c/s (201ch, ~306t @51t/s)
2025-12-15 13:55:38,068 - src.llm.client - INFO - [opq:80c8c1] ğŸ“Š 8.1s: 1612c @200c/s (268ch, ~403t @50t/s)
2025-12-15 13:55:40,082 - src.llm.client - INFO - [opq:80c8c1] ğŸ“Š 10.1s: 2086c @207c/s (335ch, ~522t @52t/s)
2025-12-15 13:55:41,888 - src.llm.client - INFO - [opq:80c8c1] âœ“ Done 21.45s: 2407c (~302w @112c/s)
2025-12-15 13:55:41,889 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:55:41,890 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:55:41,890 - generate_secondary - INFO -     - Length: 2406 chars, 302 words
2025-12-15 13:55:41,890 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:55:41,890 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:55:41,890 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:55:41,890 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_16/open_questions.md
2025-12-15 13:55:41,890 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:55:41,890 - generate_secondary - INFO - 
  Session 17/20: Deep Learning & Active Inference
2025-12-15 13:55:41,892 - generate_secondary - INFO - Generating application for session 17: Deep Learning & Active Inference...
2025-12-15 13:55:41,892 - src.llm.client - INFO - [app:3fe569] ğŸš€ app | m=gemma3:4b | p=33486c | t=150s
2025-12-15 13:55:41,892 - src.llm.client - INFO - [app:3fe569] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:55:41,892 - src.llm.client - INFO - [app:3fe569] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:55:41,894 - src.llm.client - INFO - [app:3fe569] Sending request to Ollama: model=gemma3:4b, operation=application, payload=35475 bytes, prompt=33486 chars
2025-12-15 13:55:41,894 - src.llm.client - INFO - [app:3fe569] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:55:51,462 - src.llm.request_handler - INFO - [app:3fe569] âœ“ Done 9.57s
2025-12-15 13:55:51,463 - src.llm.client - INFO - [app:3fe569] âœ… HTTP 200 in 9.57s
2025-12-15 13:55:51,463 - src.llm.client - INFO - [app:3fe569] ğŸ“¡ Stream active (200)
2025-12-15 13:55:51,463 - src.llm.client - INFO - [app:3fe569] Starting stream parsing, waiting for first chunk...
2025-12-15 13:55:53,475 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 2.0s: 421c @209c/s (67ch, ~105t @52t/s)
2025-12-15 13:55:55,489 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 4.0s: 817c @203c/s (134ch, ~204t @51t/s)
2025-12-15 13:55:57,501 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 6.0s: 1202c @199c/s (201ch, ~300t @50t/s)
2025-12-15 13:55:59,518 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 8.1s: 1647c @204c/s (268ch, ~412t @51t/s)
2025-12-15 13:56:01,537 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 10.1s: 2036c @202c/s (335ch, ~509t @51t/s)
2025-12-15 13:56:03,551 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 12.1s: 2417c @200c/s (402ch, ~604t @50t/s)
2025-12-15 13:56:05,568 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 14.1s: 2811c @199c/s (469ch, ~703t @50t/s)
2025-12-15 13:56:07,586 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 16.1s: 3212c @199c/s (536ch, ~803t @50t/s)
2025-12-15 13:56:09,603 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 18.1s: 3629c @200c/s (603ch, ~907t @50t/s)
2025-12-15 13:56:11,621 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 20.2s: 4006c @199c/s (670ch, ~1002t @50t/s)
2025-12-15 13:56:13,643 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 22.2s: 4381c @198c/s (737ch, ~1095t @49t/s)
2025-12-15 13:56:15,664 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 24.2s: 4776c @197c/s (804ch, ~1194t @49t/s)
2025-12-15 13:56:17,686 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 26.2s: 5143c @196c/s (871ch, ~1286t @49t/s)
2025-12-15 13:56:19,710 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 28.2s: 5546c @196c/s (938ch, ~1386t @49t/s)
2025-12-15 13:56:21,734 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 30.3s: 5962c @197c/s (1005ch, ~1490t @49t/s)
2025-12-15 13:56:23,761 - src.llm.client - INFO - [app:3fe569] ğŸ“Š 32.3s: 6401c @198c/s (1072ch, ~1600t @50t/s)
2025-12-15 13:56:24,870 - src.llm.client - INFO - [app:3fe569] âœ“ Done 42.98s: 6542c (~910w @152c/s)
2025-12-15 13:56:24,873 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:56:24,873 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:56:24,873 - generate_secondary - INFO -     - Length: 6530 chars, 908 words
2025-12-15 13:56:24,873 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:56:24,873 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:56:24,873 - generate_secondary - INFO -     - Avg words per application: 177
2025-12-15 13:56:24,873 - generate_secondary - WARNING - [WARNING] Application 2 has 207 words (exceeds 200 by 7 words - consider condensing) âš ï¸
2025-12-15 13:56:24,873 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_17/application.md
2025-12-15 13:56:24,874 - generate_secondary - INFO - Generating extension for session 17: Deep Learning & Active Inference...
2025-12-15 13:56:24,874 - src.llm.client - INFO - [ext:abedac] ğŸš€ ext | m=gemma3:4b | p=27372c | t=120s
2025-12-15 13:56:24,874 - src.llm.client - INFO - [ext:abedac] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:56:24,874 - src.llm.client - INFO - [ext:abedac] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:56:24,875 - src.llm.client - INFO - [ext:abedac] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=32232 bytes, prompt=27372 chars
2025-12-15 13:56:24,875 - src.llm.client - INFO - [ext:abedac] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:56:34,435 - src.llm.request_handler - INFO - [ext:abedac] âœ“ Done 9.56s
2025-12-15 13:56:34,436 - src.llm.client - INFO - [ext:abedac] âœ… HTTP 200 in 9.56s
2025-12-15 13:56:34,436 - src.llm.client - INFO - [ext:abedac] ğŸ“¡ Stream active (200)
2025-12-15 13:56:34,436 - src.llm.client - INFO - [ext:abedac] Starting stream parsing, waiting for first chunk...
2025-12-15 13:56:36,451 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 2.0s: 399c @198c/s (67ch, ~100t @49t/s)
2025-12-15 13:56:38,465 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 4.0s: 831c @206c/s (134ch, ~208t @52t/s)
2025-12-15 13:56:40,475 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 6.0s: 1258c @208c/s (201ch, ~314t @52t/s)
2025-12-15 13:56:42,486 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 8.1s: 1686c @209c/s (268ch, ~422t @52t/s)
2025-12-15 13:56:44,496 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 10.1s: 2075c @206c/s (335ch, ~519t @52t/s)
2025-12-15 13:56:46,509 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 12.1s: 2483c @206c/s (402ch, ~621t @51t/s)
2025-12-15 13:56:48,520 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 14.1s: 2873c @204c/s (469ch, ~718t @51t/s)
2025-12-15 13:56:50,534 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 16.1s: 3282c @204c/s (536ch, ~820t @51t/s)
2025-12-15 13:56:52,550 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 18.1s: 3714c @205c/s (603ch, ~928t @51t/s)
2025-12-15 13:56:54,567 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 20.1s: 4145c @206c/s (670ch, ~1036t @51t/s)
2025-12-15 13:56:56,587 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 22.2s: 4382c @198c/s (737ch, ~1096t @49t/s)
2025-12-15 13:56:58,823 - src.llm.client - INFO - [ext:abedac] ğŸ“Š 24.4s: 4717c @193c/s (801ch, ~1179t @48t/s)
2025-12-15 13:56:58,824 - src.llm.client - INFO - [ext:abedac] âœ“ Done 33.95s: 4717c (~644w @139c/s)
2025-12-15 13:56:58,826 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:56:58,826 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - INFO -     - Length: 4711 chars, 644 words
2025-12-15 13:56:58,826 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 13:56:58,826 - generate_secondary - INFO -     - Topics: 6
2025-12-15 13:56:58,826 - generate_secondary - INFO -     - Avg words per topic: 103
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Too many topics (6, maximum 4, 2 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Topic 1 has 173 words (exceeds 150 by 23 words - consider condensing) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Topic 2 has 180 words (exceeds 150 by 30 words - consider condensing) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Topic 3 has 210 words (exceeds 150 by 60 words - consider condensing) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Topic 6 has 53 words (require 100-150, need 47 more words) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - WARNING - [WARNING] Total word count (644) exceeds maximum 600 (exceeds by 44 words - condense content) âš ï¸
2025-12-15 13:56:58,826 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:56:58,826 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:56:58,826 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_17/extension.md
2025-12-15 13:56:58,826 - generate_secondary - INFO - Generating visualization for session 17: Deep Learning & Active Inference...
2025-12-15 13:56:58,826 - src.llm.client - INFO - [viz:aa1382] ğŸš€ viz | m=gemma3:4b | p=26332c | t=120s
2025-12-15 13:56:58,826 - src.llm.client - INFO - [viz:aa1382] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:56:58,826 - src.llm.client - INFO - [viz:aa1382] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:56:58,828 - src.llm.client - INFO - [viz:aa1382] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=30514 bytes, prompt=26332 chars
2025-12-15 13:56:58,828 - src.llm.client - INFO - [viz:aa1382] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:57:08,396 - src.llm.request_handler - INFO - [viz:aa1382] âœ“ Done 9.57s
2025-12-15 13:57:08,396 - src.llm.client - INFO - [viz:aa1382] âœ… HTTP 200 in 9.57s
2025-12-15 13:57:08,397 - src.llm.client - INFO - [viz:aa1382] ğŸ“¡ Stream active (200)
2025-12-15 13:57:08,397 - src.llm.client - INFO - [viz:aa1382] Starting stream parsing, waiting for first chunk...
2025-12-15 13:57:10,105 - src.llm.client - INFO - [viz:aa1382] âœ“ Done 11.28s: 162c (~23w @14c/s)
2025-12-15 13:57:10,106 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:57:10,106 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:57:10,106 - generate_secondary - INFO -     - Length: 146 chars (cleaned: 146 chars)
2025-12-15 13:57:10,106 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:57:10,106 - generate_secondary - INFO - [CRITICAL] Elements: 8 total (nodes: 3, connections: 5) ğŸ”´
2025-12-15 13:57:10,106 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:57:10,106 - generate_secondary - WARNING -     - Critical issues: 2 structural problems requiring attention
2025-12-15 13:57:10,106 - generate_secondary - WARNING - [WARNING] Only 3 nodes found (require at least 10, need 7 more - add more nodes to the diagram) âš ï¸
2025-12-15 13:57:10,106 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:57:10,106 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:57:10,106 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_17/visualization.mmd
2025-12-15 13:57:10,106 - generate_secondary - INFO - Generating integration for session 17: Deep Learning & Active Inference...
2025-12-15 13:57:10,107 - src.llm.client - INFO - [int:aa8719] ğŸš€ int | m=gemma3:4b | p=27681c | t=150s
2025-12-15 13:57:10,107 - src.llm.client - INFO - [int:aa8719] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:57:10,107 - src.llm.client - INFO - [int:aa8719] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:57:10,109 - src.llm.client - INFO - [int:aa8719] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=32880 bytes, prompt=27681 chars
2025-12-15 13:57:10,109 - src.llm.client - INFO - [int:aa8719] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:57:19,682 - src.llm.request_handler - INFO - [int:aa8719] âœ“ Done 9.57s
2025-12-15 13:57:19,682 - src.llm.client - INFO - [int:aa8719] âœ… HTTP 200 in 9.57s
2025-12-15 13:57:19,682 - src.llm.client - INFO - [int:aa8719] ğŸ“¡ Stream active (200)
2025-12-15 13:57:19,682 - src.llm.client - INFO - [int:aa8719] Starting stream parsing, waiting for first chunk...
2025-12-15 13:57:21,701 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 2.0s: 395c @196c/s (67ch, ~99t @49t/s)
2025-12-15 13:57:23,710 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 4.0s: 778c @193c/s (134ch, ~194t @48t/s)
2025-12-15 13:57:25,722 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 6.0s: 1142c @189c/s (201ch, ~286t @47t/s)
2025-12-15 13:57:27,731 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 8.0s: 1556c @193c/s (268ch, ~389t @48t/s)
2025-12-15 13:57:29,745 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 10.1s: 1950c @194c/s (335ch, ~488t @48t/s)
2025-12-15 13:57:31,764 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 12.1s: 2371c @196c/s (402ch, ~593t @49t/s)
2025-12-15 13:57:33,781 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 14.1s: 2797c @198c/s (469ch, ~699t @50t/s)
2025-12-15 13:57:35,797 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 16.1s: 3197c @198c/s (536ch, ~799t @50t/s)
2025-12-15 13:57:37,817 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 18.1s: 3619c @200c/s (603ch, ~905t @50t/s)
2025-12-15 13:57:39,839 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 20.2s: 4045c @201c/s (670ch, ~1011t @50t/s)
2025-12-15 13:57:41,858 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 22.2s: 4393c @198c/s (737ch, ~1098t @50t/s)
2025-12-15 13:57:43,877 - src.llm.client - INFO - [int:aa8719] ğŸ“Š 24.2s: 4634c @192c/s (804ch, ~1158t @48t/s)
2025-12-15 13:57:45,699 - src.llm.client - INFO - [int:aa8719] âœ“ Done 35.59s: 4885c (~673w @137c/s)
2025-12-15 13:57:45,701 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:57:45,702 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 13:57:45,702 - generate_secondary - INFO -     - Length: 4884 chars, 673 words
2025-12-15 13:57:45,702 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 13:57:45,702 - generate_secondary - INFO -     - Connections: 27
2025-12-15 13:57:45,702 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 13:57:45,702 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_17/integration.md
2025-12-15 13:57:45,702 - generate_secondary - INFO - Generating investigation for session 17: Deep Learning & Active Inference...
2025-12-15 13:57:45,702 - src.llm.client - INFO - [inv:2e7d5d] ğŸš€ inv | m=gemma3:4b | p=26594c | t=150s
2025-12-15 13:57:45,702 - src.llm.client - INFO - [inv:2e7d5d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:57:45,702 - src.llm.client - INFO - [inv:2e7d5d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:57:45,704 - src.llm.client - INFO - [inv:2e7d5d] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=30736 bytes, prompt=26594 chars
2025-12-15 13:57:45,704 - src.llm.client - INFO - [inv:2e7d5d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:57:55,262 - src.llm.request_handler - INFO - [inv:2e7d5d] âœ“ Done 9.56s
2025-12-15 13:57:55,263 - src.llm.client - INFO - [inv:2e7d5d] âœ… HTTP 200 in 9.56s
2025-12-15 13:57:55,263 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“¡ Stream active (200)
2025-12-15 13:57:55,263 - src.llm.client - INFO - [inv:2e7d5d] Starting stream parsing, waiting for first chunk...
2025-12-15 13:57:57,285 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 2.0s: 366c @181c/s (67ch, ~92t @45t/s)
2025-12-15 13:57:59,297 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 4.0s: 607c @150c/s (134ch, ~152t @38t/s)
2025-12-15 13:58:01,318 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 6.1s: 892c @147c/s (201ch, ~223t @37t/s)
2025-12-15 13:58:03,333 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 8.1s: 1230c @152c/s (268ch, ~308t @38t/s)
2025-12-15 13:58:05,349 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 10.1s: 1654c @164c/s (335ch, ~414t @41t/s)
2025-12-15 13:58:07,371 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 12.1s: 2001c @165c/s (402ch, ~500t @41t/s)
2025-12-15 13:58:09,396 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 14.1s: 2254c @159c/s (469ch, ~564t @40t/s)
2025-12-15 13:58:11,415 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 16.2s: 2592c @160c/s (536ch, ~648t @40t/s)
2025-12-15 13:58:13,430 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 18.2s: 2920c @161c/s (603ch, ~730t @40t/s)
2025-12-15 13:58:15,446 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 20.2s: 3294c @163c/s (670ch, ~824t @41t/s)
2025-12-15 13:58:17,465 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 22.2s: 3569c @161c/s (737ch, ~892t @40t/s)
2025-12-15 13:58:19,485 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 24.2s: 3950c @163c/s (804ch, ~988t @41t/s)
2025-12-15 13:58:21,802 - src.llm.client - INFO - [inv:2e7d5d] ğŸ“Š 26.5s: 4331c @163c/s (871ch, ~1083t @41t/s)
2025-12-15 13:58:21,802 - src.llm.client - INFO - [inv:2e7d5d] âœ“ Done 36.10s: 4331c (~673w @120c/s)
2025-12-15 13:58:21,804 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:58:21,804 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 13:58:21,804 - generate_secondary - INFO -     - Length: 4315 chars, 671 words
2025-12-15 13:58:21,804 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:58:21,804 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 13:58:21,804 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:58:21,805 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_17/investigation.md
2025-12-15 13:58:21,805 - generate_secondary - INFO - Generating open_questions for session 17: Deep Learning & Active Inference...
2025-12-15 13:58:21,805 - src.llm.client - INFO - [opq:0336b4] ğŸš€ opq | m=gemma3:4b | p=26680c | t=150s
2025-12-15 13:58:21,805 - src.llm.client - INFO - [opq:0336b4] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:58:21,805 - src.llm.client - INFO - [opq:0336b4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:58:21,806 - src.llm.client - INFO - [opq:0336b4] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=30833 bytes, prompt=26680 chars
2025-12-15 13:58:21,806 - src.llm.client - INFO - [opq:0336b4] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:58:31,368 - src.llm.request_handler - INFO - [opq:0336b4] âœ“ Done 9.56s
2025-12-15 13:58:31,368 - src.llm.client - INFO - [opq:0336b4] âœ… HTTP 200 in 9.56s
2025-12-15 13:58:31,368 - src.llm.client - INFO - [opq:0336b4] ğŸ“¡ Stream active (200)
2025-12-15 13:58:31,368 - src.llm.client - INFO - [opq:0336b4] Starting stream parsing, waiting for first chunk...
2025-12-15 13:58:33,391 - src.llm.client - INFO - [opq:0336b4] ğŸ“Š 2.0s: 341c @169c/s (66ch, ~85t @42t/s)
2025-12-15 13:58:35,410 - src.llm.client - INFO - [opq:0336b4] ğŸ“Š 4.0s: 751c @186c/s (133ch, ~188t @46t/s)
2025-12-15 13:58:37,426 - src.llm.client - INFO - [opq:0336b4] ğŸ“Š 6.1s: 1147c @189c/s (200ch, ~287t @47t/s)
2025-12-15 13:58:39,436 - src.llm.client - INFO - [opq:0336b4] ğŸ“Š 8.1s: 1550c @192c/s (267ch, ~388t @48t/s)
2025-12-15 13:58:41,446 - src.llm.client - INFO - [opq:0336b4] ğŸ“Š 10.1s: 1892c @188c/s (334ch, ~473t @47t/s)
2025-12-15 13:58:43,459 - src.llm.client - INFO - [opq:0336b4] ğŸ“Š 12.1s: 2267c @188c/s (401ch, ~567t @47t/s)
2025-12-15 13:58:44,981 - src.llm.client - INFO - [opq:0336b4] âœ“ Done 23.18s: 2554c (~336w @110c/s)
2025-12-15 13:58:44,983 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:58:44,983 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 13:58:44,983 - generate_secondary - INFO -     - Length: 2553 chars, 336 words
2025-12-15 13:58:44,983 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 13:58:44,983 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 13:58:44,983 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 13:58:44,983 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_09_applications_ai_robotics/session_17/open_questions.md
2025-12-15 13:58:44,983 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 13:58:44,983 - generate_secondary - INFO - 
============================================================
2025-12-15 13:58:44,983 - generate_secondary - INFO - [10/10] Module 10: Concluding Remarks & Future Directions (3 sessions)
2025-12-15 13:58:44,983 - generate_secondary - INFO - ============================================================
2025-12-15 13:58:44,983 - generate_secondary - INFO - 
  Session 18/20: Philosophical Implications
2025-12-15 13:58:44,985 - generate_secondary - INFO - Generating application for session 18: Philosophical Implications...
2025-12-15 13:58:44,985 - src.llm.client - INFO - [app:df9622] ğŸš€ app | m=gemma3:4b | p=35223c | t=150s
2025-12-15 13:58:44,985 - src.llm.client - INFO - [app:df9622] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:58:44,985 - src.llm.client - INFO - [app:df9622] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:58:44,987 - src.llm.client - INFO - [app:df9622] Sending request to Ollama: model=gemma3:4b, operation=application, payload=37201 bytes, prompt=35223 chars
2025-12-15 13:58:44,987 - src.llm.client - INFO - [app:df9622] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:58:54,567 - src.llm.request_handler - INFO - [app:df9622] âœ“ Done 9.58s
2025-12-15 13:58:54,567 - src.llm.client - INFO - [app:df9622] âœ… HTTP 200 in 9.58s
2025-12-15 13:58:54,567 - src.llm.client - INFO - [app:df9622] ğŸ“¡ Stream active (200)
2025-12-15 13:58:54,567 - src.llm.client - INFO - [app:df9622] Starting stream parsing, waiting for first chunk...
2025-12-15 13:58:56,588 - src.llm.client - INFO - [app:df9622] ğŸ“Š 2.0s: 420c @208c/s (67ch, ~105t @52t/s)
2025-12-15 13:58:58,603 - src.llm.client - INFO - [app:df9622] ğŸ“Š 4.0s: 844c @209c/s (134ch, ~211t @52t/s)
2025-12-15 13:59:00,620 - src.llm.client - INFO - [app:df9622] ğŸ“Š 6.1s: 1244c @206c/s (201ch, ~311t @51t/s)
2025-12-15 13:59:02,634 - src.llm.client - INFO - [app:df9622] ğŸ“Š 8.1s: 1688c @209c/s (268ch, ~422t @52t/s)
2025-12-15 13:59:04,643 - src.llm.client - INFO - [app:df9622] ğŸ“Š 10.1s: 2101c @209c/s (334ch, ~525t @52t/s)
2025-12-15 13:59:06,660 - src.llm.client - INFO - [app:df9622] ğŸ“Š 12.1s: 2495c @206c/s (401ch, ~624t @52t/s)
2025-12-15 13:59:08,676 - src.llm.client - INFO - [app:df9622] ğŸ“Š 14.1s: 2875c @204c/s (468ch, ~719t @51t/s)
2025-12-15 13:59:10,700 - src.llm.client - INFO - [app:df9622] ğŸ“Š 16.1s: 3240c @201c/s (535ch, ~810t @50t/s)
2025-12-15 13:59:12,718 - src.llm.client - INFO - [app:df9622] ğŸ“Š 18.2s: 3639c @200c/s (602ch, ~910t @50t/s)
2025-12-15 13:59:14,735 - src.llm.client - INFO - [app:df9622] ğŸ“Š 20.2s: 4036c @200c/s (669ch, ~1009t @50t/s)
2025-12-15 13:59:16,759 - src.llm.client - INFO - [app:df9622] ğŸ“Š 22.2s: 4462c @201c/s (736ch, ~1116t @50t/s)
2025-12-15 13:59:18,778 - src.llm.client - INFO - [app:df9622] ğŸ“Š 24.2s: 4861c @201c/s (803ch, ~1215t @50t/s)
2025-12-15 13:59:19,353 - src.llm.client - INFO - [app:df9622] âœ“ Done 34.37s: 4881c (~660w @142c/s)
2025-12-15 13:59:19,355 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 13:59:19,355 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 13:59:19,356 - generate_secondary - INFO -     - Length: 4869 chars, 658 words
2025-12-15 13:59:19,356 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 13:59:19,357 - generate_secondary - INFO -     - Applications: 5
2025-12-15 13:59:19,357 - generate_secondary - INFO -     - Avg words per application: 126
2025-12-15 13:59:19,357 - generate_secondary - WARNING - [WARNING] Application 1 has 122 words (require 150-200, need 28 more words) âš ï¸
2025-12-15 13:59:19,357 - generate_secondary - WARNING - [WARNING] Application 2 has 143 words (require 150-200, need 7 more words) âš ï¸
2025-12-15 13:59:19,357 - generate_secondary - WARNING - [WARNING] Application 3 has 128 words (require 150-200, need 22 more words) âš ï¸
2025-12-15 13:59:19,357 - generate_secondary - WARNING - [WARNING] Application 4 has 123 words (require 150-200, need 27 more words) âš ï¸
2025-12-15 13:59:19,357 - generate_secondary - WARNING - [WARNING] Application 5 has 116 words (require 150-200, need 34 more words) âš ï¸
2025-12-15 13:59:19,357 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_18/application.md
2025-12-15 13:59:19,357 - generate_secondary - INFO - Generating extension for session 18: Philosophical Implications...
2025-12-15 13:59:19,357 - src.llm.client - INFO - [ext:7d01d0] ğŸš€ ext | m=gemma3:4b | p=29109c | t=120s
2025-12-15 13:59:19,357 - src.llm.client - INFO - [ext:7d01d0] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:59:19,358 - src.llm.client - INFO - [ext:7d01d0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:59:19,359 - src.llm.client - INFO - [ext:7d01d0] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=33958 bytes, prompt=29109 chars
2025-12-15 13:59:19,359 - src.llm.client - INFO - [ext:7d01d0] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:59:28,913 - src.llm.request_handler - INFO - [ext:7d01d0] âœ“ Done 9.55s
2025-12-15 13:59:28,913 - src.llm.client - INFO - [ext:7d01d0] âœ… HTTP 200 in 9.55s
2025-12-15 13:59:28,913 - src.llm.client - INFO - [ext:7d01d0] ğŸ“¡ Stream active (200)
2025-12-15 13:59:28,913 - src.llm.client - INFO - [ext:7d01d0] Starting stream parsing, waiting for first chunk...
2025-12-15 13:59:30,937 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 2.0s: 381c @188c/s (67ch, ~95t @47t/s)
2025-12-15 13:59:32,953 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 4.0s: 788c @195c/s (134ch, ~197t @49t/s)
2025-12-15 13:59:34,964 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 6.1s: 1198c @198c/s (201ch, ~300t @49t/s)
2025-12-15 13:59:36,978 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 8.1s: 1603c @199c/s (268ch, ~401t @50t/s)
2025-12-15 13:59:38,994 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 10.1s: 2013c @200c/s (335ch, ~503t @50t/s)
2025-12-15 13:59:41,013 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 12.1s: 2417c @200c/s (402ch, ~604t @50t/s)
2025-12-15 13:59:43,026 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 14.1s: 2782c @197c/s (469ch, ~696t @49t/s)
2025-12-15 13:59:45,045 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 16.1s: 3220c @200c/s (536ch, ~805t @50t/s)
2025-12-15 13:59:47,063 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 18.1s: 3618c @199c/s (603ch, ~904t @50t/s)
2025-12-15 13:59:49,077 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 20.2s: 4022c @199c/s (670ch, ~1006t @50t/s)
2025-12-15 13:59:51,099 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 22.2s: 4268c @192c/s (737ch, ~1067t @48t/s)
2025-12-15 13:59:53,121 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 24.2s: 4572c @189c/s (804ch, ~1143t @47t/s)
2025-12-15 13:59:55,142 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 26.2s: 4754c @181c/s (871ch, ~1188t @45t/s)
2025-12-15 13:59:57,165 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 28.3s: 4958c @175c/s (938ch, ~1240t @44t/s)
2025-12-15 13:59:59,189 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 30.3s: 5193c @172c/s (1005ch, ~1298t @43t/s)
2025-12-15 14:00:01,264 - src.llm.client - INFO - [ext:7d01d0] ğŸ“Š 32.4s: 5392c @167c/s (1061ch, ~1348t @42t/s)
2025-12-15 14:00:01,265 - src.llm.client - INFO - [ext:7d01d0] âœ“ Done 41.91s: 5392c (~697w @129c/s)
2025-12-15 14:00:01,267 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:00:01,267 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - INFO -     - Length: 5391 chars, 697 words
2025-12-15 14:00:01,267 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 14:00:01,267 - generate_secondary - INFO -     - Topics: 10
2025-12-15 14:00:01,267 - generate_secondary - INFO -     - Avg words per topic: 66
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Too many topics (10, maximum 4, 6 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 1 has 175 words (exceeds 150 by 25 words - consider condensing) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 2 has 175 words (exceeds 150 by 25 words - consider condensing) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 3 has 223 words (exceeds 150 by 73 words - consider condensing) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 4 has 8 words (require 100-150, need 92 more words) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 5 has 9 words (require 100-150, need 91 more words) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 6 has 21 words (require 100-150, need 79 more words) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 7 has 47 words (require 100-150, need 53 more words) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 8 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 9 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Topic 10 has 3 words (require 100-150, need 97 more words) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - WARNING - [WARNING] Total word count (697) exceeds maximum 600 (exceeds by 97 words - condense content) âš ï¸
2025-12-15 14:00:01,267 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 14:00:01,267 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 14:00:01,268 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_18/extension.md
2025-12-15 14:00:01,268 - generate_secondary - INFO - Generating visualization for session 18: Philosophical Implications...
2025-12-15 14:00:01,268 - src.llm.client - INFO - [viz:90a517] ğŸš€ viz | m=gemma3:4b | p=28069c | t=120s
2025-12-15 14:00:01,268 - src.llm.client - INFO - [viz:90a517] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 14:00:01,268 - src.llm.client - INFO - [viz:90a517] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:00:01,269 - src.llm.client - INFO - [viz:90a517] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=32240 bytes, prompt=28069 chars
2025-12-15 14:00:01,269 - src.llm.client - INFO - [viz:90a517] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 14:00:10,835 - src.llm.request_handler - INFO - [viz:90a517] âœ“ Done 9.57s
2025-12-15 14:00:10,836 - src.llm.client - INFO - [viz:90a517] âœ… HTTP 200 in 9.57s
2025-12-15 14:00:10,836 - src.llm.client - INFO - [viz:90a517] ğŸ“¡ Stream active (200)
2025-12-15 14:00:10,836 - src.llm.client - INFO - [viz:90a517] Starting stream parsing, waiting for first chunk...
2025-12-15 14:00:12,855 - src.llm.client - INFO - [viz:90a517] ğŸ“Š 2.0s: 301c @149c/s (67ch, ~75t @37t/s)
2025-12-15 14:00:14,866 - src.llm.client - INFO - [viz:90a517] ğŸ“Š 4.0s: 526c @130c/s (134ch, ~132t @33t/s)
2025-12-15 14:00:16,878 - src.llm.client - INFO - [viz:90a517] ğŸ“Š 6.0s: 806c @133c/s (201ch, ~202t @33t/s)
2025-12-15 14:00:18,888 - src.llm.client - INFO - [viz:90a517] ğŸ“Š 8.1s: 1032c @128c/s (268ch, ~258t @32t/s)
2025-12-15 14:00:20,900 - src.llm.client - INFO - [viz:90a517] ğŸ“Š 10.1s: 1372c @136c/s (335ch, ~343t @34t/s)
2025-12-15 14:00:23,015 - src.llm.client - INFO - [viz:90a517] ğŸ“Š 12.2s: 1677c @138c/s (396ch, ~419t @34t/s)
2025-12-15 14:00:23,016 - src.llm.client - INFO - [viz:90a517] âœ“ Done 21.75s: 1677c (~239w @77c/s)
2025-12-15 14:00:23,016 - src.generate.processors.cleanup - INFO - Cleanup complete: 3 issues before, 0 issues after
2025-12-15 14:00:23,017 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 14:00:23,017 - generate_secondary - INFO -     - Length: 400 chars (cleaned: 400 chars)
2025-12-15 14:00:23,017 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 14:00:23,017 - generate_secondary - INFO - [OK] Elements: 21 total (nodes: 11, connections: 10) âœ“
2025-12-15 14:00:23,017 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_18/visualization.mmd
2025-12-15 14:00:23,017 - generate_secondary - INFO - Generating integration for session 18: Philosophical Implications...
2025-12-15 14:00:23,017 - src.llm.client - INFO - [int:0ca3ed] ğŸš€ int | m=gemma3:4b | p=29418c | t=150s
2025-12-15 14:00:23,017 - src.llm.client - INFO - [int:0ca3ed] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:00:23,017 - src.llm.client - INFO - [int:0ca3ed] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:00:23,019 - src.llm.client - INFO - [int:0ca3ed] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=34606 bytes, prompt=29418 chars
2025-12-15 14:00:23,019 - src.llm.client - INFO - [int:0ca3ed] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:00:32,587 - src.llm.request_handler - INFO - [int:0ca3ed] âœ“ Done 9.57s
2025-12-15 14:00:32,587 - src.llm.client - INFO - [int:0ca3ed] âœ… HTTP 200 in 9.57s
2025-12-15 14:00:32,587 - src.llm.client - INFO - [int:0ca3ed] ğŸ“¡ Stream active (200)
2025-12-15 14:00:32,587 - src.llm.client - INFO - [int:0ca3ed] Starting stream parsing, waiting for first chunk...
2025-12-15 14:00:34,611 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 2.0s: 400c @198c/s (67ch, ~100t @49t/s)
2025-12-15 14:00:36,626 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 4.0s: 797c @197c/s (134ch, ~199t @49t/s)
2025-12-15 14:00:38,644 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 6.1s: 1164c @192c/s (201ch, ~291t @48t/s)
2025-12-15 14:00:40,657 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 8.1s: 1574c @195c/s (268ch, ~394t @49t/s)
2025-12-15 14:00:42,669 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 10.1s: 1930c @191c/s (335ch, ~482t @48t/s)
2025-12-15 14:00:44,686 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 12.1s: 2359c @195c/s (402ch, ~590t @49t/s)
2025-12-15 14:00:46,701 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 14.1s: 2759c @195c/s (469ch, ~690t @49t/s)
2025-12-15 14:00:48,715 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 16.1s: 3141c @195c/s (536ch, ~785t @49t/s)
2025-12-15 14:00:50,733 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 18.1s: 3525c @194c/s (603ch, ~881t @49t/s)
2025-12-15 14:00:52,749 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 20.2s: 3783c @188c/s (670ch, ~946t @47t/s)
2025-12-15 14:00:54,767 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 22.2s: 4009c @181c/s (737ch, ~1002t @45t/s)
2025-12-15 14:00:56,784 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 24.2s: 4236c @175c/s (804ch, ~1059t @44t/s)
2025-12-15 14:00:58,802 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 26.2s: 4484c @171c/s (871ch, ~1121t @43t/s)
2025-12-15 14:01:00,830 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 28.2s: 4721c @167c/s (938ch, ~1180t @42t/s)
2025-12-15 14:01:02,859 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 30.3s: 4984c @165c/s (1005ch, ~1246t @41t/s)
2025-12-15 14:01:04,886 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 32.3s: 5246c @162c/s (1072ch, ~1312t @41t/s)
2025-12-15 14:01:06,910 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 34.3s: 5504c @160c/s (1139ch, ~1376t @40t/s)
2025-12-15 14:01:08,911 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 36.3s: 5759c @159c/s (1205ch, ~1440t @40t/s)
2025-12-15 14:01:10,941 - src.llm.client - INFO - [int:0ca3ed] ğŸ“Š 38.4s: 6035c @157c/s (1272ch, ~1509t @39t/s)
2025-12-15 14:01:11,508 - src.llm.client - INFO - [int:0ca3ed] âœ“ Done 48.49s: 6072c (~856w @125c/s)
2025-12-15 14:01:11,510 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 14:01:11,511 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 14:01:11,511 - generate_secondary - INFO -     - Length: 6053 chars, 854 words
2025-12-15 14:01:11,511 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 14:01:11,511 - generate_secondary - INFO -     - Connections: 28
2025-12-15 14:01:11,511 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 14:01:11,511 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_18/integration.md
2025-12-15 14:01:11,511 - generate_secondary - INFO - Generating investigation for session 18: Philosophical Implications...
2025-12-15 14:01:11,511 - src.llm.client - INFO - [inv:2e85d5] ğŸš€ inv | m=gemma3:4b | p=28331c | t=150s
2025-12-15 14:01:11,512 - src.llm.client - INFO - [inv:2e85d5] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:01:11,512 - src.llm.client - INFO - [inv:2e85d5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:01:11,514 - src.llm.client - INFO - [inv:2e85d5] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=32462 bytes, prompt=28331 chars
2025-12-15 14:01:11,514 - src.llm.client - INFO - [inv:2e85d5] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:01:21,075 - src.llm.request_handler - INFO - [inv:2e85d5] âœ“ Done 9.56s
2025-12-15 14:01:21,075 - src.llm.client - INFO - [inv:2e85d5] âœ… HTTP 200 in 9.56s
2025-12-15 14:01:21,075 - src.llm.client - INFO - [inv:2e85d5] ğŸ“¡ Stream active (200)
2025-12-15 14:01:21,075 - src.llm.client - INFO - [inv:2e85d5] Starting stream parsing, waiting for first chunk...
2025-12-15 14:01:23,090 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 2.0s: 364c @181c/s (67ch, ~91t @45t/s)
2025-12-15 14:01:25,101 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 4.0s: 656c @163c/s (134ch, ~164t @41t/s)
2025-12-15 14:01:27,110 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 6.0s: 990c @164c/s (201ch, ~248t @41t/s)
2025-12-15 14:01:29,118 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 8.0s: 1365c @170c/s (268ch, ~341t @42t/s)
2025-12-15 14:01:31,134 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 10.1s: 1783c @177c/s (335ch, ~446t @44t/s)
2025-12-15 14:01:33,152 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 12.1s: 2107c @174c/s (402ch, ~527t @44t/s)
2025-12-15 14:01:35,167 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 14.1s: 2493c @177c/s (469ch, ~623t @44t/s)
2025-12-15 14:01:37,186 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 16.1s: 2914c @181c/s (536ch, ~728t @45t/s)
2025-12-15 14:01:39,200 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 18.1s: 3331c @184c/s (603ch, ~833t @46t/s)
2025-12-15 14:01:41,218 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 20.1s: 3712c @184c/s (670ch, ~928t @46t/s)
2025-12-15 14:01:43,236 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 22.2s: 4117c @186c/s (737ch, ~1029t @46t/s)
2025-12-15 14:01:45,237 - src.llm.client - INFO - [inv:2e85d5] ğŸ“Š 24.2s: 4550c @188c/s (803ch, ~1138t @47t/s)
2025-12-15 14:01:46,913 - src.llm.client - INFO - [inv:2e85d5] âœ“ Done 35.40s: 4836c (~665w @137c/s)
2025-12-15 14:01:46,915 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:01:46,915 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 14:01:46,915 - generate_secondary - INFO -     - Length: 4835 chars, 665 words
2025-12-15 14:01:46,916 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 14:01:46,916 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 14:01:46,916 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 14:01:46,916 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_18/investigation.md
2025-12-15 14:01:46,916 - generate_secondary - INFO - Generating open_questions for session 18: Philosophical Implications...
2025-12-15 14:01:46,916 - src.llm.client - INFO - [opq:b342b8] ğŸš€ opq | m=gemma3:4b | p=28417c | t=150s
2025-12-15 14:01:46,916 - src.llm.client - INFO - [opq:b342b8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:01:46,916 - src.llm.client - INFO - [opq:b342b8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:01:46,918 - src.llm.client - INFO - [opq:b342b8] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=32559 bytes, prompt=28417 chars
2025-12-15 14:01:46,918 - src.llm.client - INFO - [opq:b342b8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:01:56,482 - src.llm.request_handler - INFO - [opq:b342b8] âœ“ Done 9.56s
2025-12-15 14:01:56,482 - src.llm.client - INFO - [opq:b342b8] âœ… HTTP 200 in 9.56s
2025-12-15 14:01:56,482 - src.llm.client - INFO - [opq:b342b8] ğŸ“¡ Stream active (200)
2025-12-15 14:01:56,482 - src.llm.client - INFO - [opq:b342b8] Starting stream parsing, waiting for first chunk...
2025-12-15 14:01:58,497 - src.llm.client - INFO - [opq:b342b8] ğŸ“Š 2.0s: 361c @179c/s (67ch, ~90t @45t/s)
2025-12-15 14:02:00,515 - src.llm.client - INFO - [opq:b342b8] ğŸ“Š 4.0s: 799c @198c/s (134ch, ~200t @50t/s)
2025-12-15 14:02:02,535 - src.llm.client - INFO - [opq:b342b8] ğŸ“Š 6.1s: 1166c @193c/s (201ch, ~292t @48t/s)
2025-12-15 14:02:04,548 - src.llm.client - INFO - [opq:b342b8] ğŸ“Š 8.1s: 1578c @196c/s (268ch, ~394t @49t/s)
2025-12-15 14:02:06,563 - src.llm.client - INFO - [opq:b342b8] ğŸ“Š 10.1s: 1966c @195c/s (335ch, ~492t @49t/s)
2025-12-15 14:02:08,584 - src.llm.client - INFO - [opq:b342b8] ğŸ“Š 12.1s: 2363c @195c/s (402ch, ~591t @49t/s)
2025-12-15 14:02:09,563 - src.llm.client - INFO - [opq:b342b8] âœ“ Done 22.65s: 2524c (~335w @111c/s)
2025-12-15 14:02:09,564 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:02:09,565 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 14:02:09,565 - generate_secondary - INFO -     - Length: 2523 chars, 335 words
2025-12-15 14:02:09,565 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 14:02:09,565 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 14:02:09,565 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 14:02:09,565 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_18/open_questions.md
2025-12-15 14:02:09,565 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 14:02:09,565 - generate_secondary - INFO - 
  Session 19/20: Open Questions & Research Frontiers
2025-12-15 14:02:09,566 - generate_secondary - INFO - Generating application for session 19: Open Questions & Research Frontiers...
2025-12-15 14:02:09,566 - src.llm.client - INFO - [app:fade8e] ğŸš€ app | m=gemma3:4b | p=35252c | t=150s
2025-12-15 14:02:09,567 - src.llm.client - INFO - [app:fade8e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:02:09,567 - src.llm.client - INFO - [app:fade8e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:02:09,568 - src.llm.client - INFO - [app:fade8e] Sending request to Ollama: model=gemma3:4b, operation=application, payload=37308 bytes, prompt=35252 chars
2025-12-15 14:02:09,568 - src.llm.client - INFO - [app:fade8e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:02:19,143 - src.llm.request_handler - INFO - [app:fade8e] âœ“ Done 9.58s
2025-12-15 14:02:19,143 - src.llm.client - INFO - [app:fade8e] âœ… HTTP 200 in 9.58s
2025-12-15 14:02:19,143 - src.llm.client - INFO - [app:fade8e] ğŸ“¡ Stream active (200)
2025-12-15 14:02:19,143 - src.llm.client - INFO - [app:fade8e] Starting stream parsing, waiting for first chunk...
2025-12-15 14:02:21,168 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 2.0s: 370c @183c/s (67ch, ~92t @46t/s)
2025-12-15 14:02:23,184 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 4.0s: 800c @198c/s (134ch, ~200t @49t/s)
2025-12-15 14:02:25,198 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 6.1s: 1173c @194c/s (201ch, ~293t @48t/s)
2025-12-15 14:02:27,204 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 8.1s: 1559c @193c/s (268ch, ~390t @48t/s)
2025-12-15 14:02:29,213 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 10.1s: 1930c @192c/s (335ch, ~482t @48t/s)
2025-12-15 14:02:31,228 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 12.1s: 2352c @195c/s (402ch, ~588t @49t/s)
2025-12-15 14:02:33,242 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 14.1s: 2723c @193c/s (469ch, ~681t @48t/s)
2025-12-15 14:02:35,258 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 16.1s: 3132c @194c/s (536ch, ~783t @49t/s)
2025-12-15 14:02:37,276 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 18.1s: 3529c @195c/s (603ch, ~882t @49t/s)
2025-12-15 14:02:39,292 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 20.1s: 3935c @195c/s (670ch, ~984t @49t/s)
2025-12-15 14:02:41,310 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 22.2s: 4363c @197c/s (737ch, ~1091t @49t/s)
2025-12-15 14:02:43,329 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 24.2s: 4753c @197c/s (804ch, ~1188t @49t/s)
2025-12-15 14:02:45,351 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 26.2s: 5190c @198c/s (871ch, ~1298t @50t/s)
2025-12-15 14:02:47,371 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 28.2s: 5592c @198c/s (938ch, ~1398t @50t/s)
2025-12-15 14:02:49,394 - src.llm.client - INFO - [app:fade8e] ğŸ“Š 30.3s: 6016c @199c/s (1005ch, ~1504t @50t/s)
2025-12-15 14:02:50,921 - src.llm.client - INFO - [app:fade8e] âœ“ Done 41.35s: 6239c (~839w @151c/s)
2025-12-15 14:02:50,923 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:02:50,924 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 14:02:50,924 - generate_secondary - INFO -     - Length: 6238 chars, 839 words
2025-12-15 14:02:50,924 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 14:02:50,924 - generate_secondary - INFO -     - Applications: 5
2025-12-15 14:02:50,924 - generate_secondary - INFO -     - Avg words per application: 163
2025-12-15 14:02:50,924 - generate_secondary - WARNING - [WARNING] Application 3 has 145 words (require 150-200, need 5 more words) âš ï¸
2025-12-15 14:02:50,924 - generate_secondary - WARNING - [WARNING] Application 5 has 138 words (require 150-200, need 12 more words) âš ï¸
2025-12-15 14:02:50,924 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_19/application.md
2025-12-15 14:02:50,924 - generate_secondary - INFO - Generating extension for session 19: Open Questions & Research Frontiers...
2025-12-15 14:02:50,924 - src.llm.client - INFO - [ext:3a2f20] ğŸš€ ext | m=gemma3:4b | p=29138c | t=120s
2025-12-15 14:02:50,924 - src.llm.client - INFO - [ext:3a2f20] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 14:02:50,924 - src.llm.client - INFO - [ext:3a2f20] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:02:50,926 - src.llm.client - INFO - [ext:3a2f20] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=34065 bytes, prompt=29138 chars
2025-12-15 14:02:50,926 - src.llm.client - INFO - [ext:3a2f20] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 14:03:00,490 - src.llm.request_handler - INFO - [ext:3a2f20] âœ“ Done 9.56s
2025-12-15 14:03:00,490 - src.llm.client - INFO - [ext:3a2f20] âœ… HTTP 200 in 9.56s
2025-12-15 14:03:00,490 - src.llm.client - INFO - [ext:3a2f20] ğŸ“¡ Stream active (200)
2025-12-15 14:03:00,490 - src.llm.client - INFO - [ext:3a2f20] Starting stream parsing, waiting for first chunk...
2025-12-15 14:03:02,512 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 2.0s: 434c @215c/s (67ch, ~108t @54t/s)
2025-12-15 14:03:04,528 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 4.0s: 863c @214c/s (134ch, ~216t @53t/s)
2025-12-15 14:03:06,541 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 6.1s: 1312c @217c/s (201ch, ~328t @54t/s)
2025-12-15 14:03:08,555 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 8.1s: 1706c @212c/s (268ch, ~426t @53t/s)
2025-12-15 14:03:10,569 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 10.1s: 2158c @214c/s (335ch, ~540t @54t/s)
2025-12-15 14:03:12,584 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 12.1s: 2572c @213c/s (402ch, ~643t @53t/s)
2025-12-15 14:03:14,599 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 14.1s: 2991c @212c/s (469ch, ~748t @53t/s)
2025-12-15 14:03:16,615 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 16.1s: 3413c @212c/s (536ch, ~853t @53t/s)
2025-12-15 14:03:18,631 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 18.1s: 3890c @214c/s (603ch, ~972t @54t/s)
2025-12-15 14:03:20,651 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 20.2s: 4335c @215c/s (670ch, ~1084t @54t/s)
2025-12-15 14:03:22,673 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 22.2s: 4690c @211c/s (737ch, ~1172t @53t/s)
2025-12-15 14:03:24,694 - src.llm.client - INFO - [ext:3a2f20] ğŸ“Š 24.2s: 4928c @204c/s (804ch, ~1232t @51t/s)
2025-12-15 14:03:25,779 - src.llm.client - INFO - [ext:3a2f20] âœ“ Done 34.86s: 5025c (~664w @144c/s)
2025-12-15 14:03:25,781 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 14:03:25,781 - generate_secondary - INFO - [NEEDS REVIEW] Extension generated âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - INFO -     - Length: 5011 chars, 662 words
2025-12-15 14:03:25,781 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 14:03:25,781 - generate_secondary - INFO -     - Topics: 6
2025-12-15 14:03:25,781 - generate_secondary - INFO -     - Avg words per topic: 106
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Too many topics (6, maximum 4, 2 excess - consider consolidating or removing less critical topics) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Topic 1 has 188 words (exceeds 150 by 38 words - consider condensing) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Topic 2 has 187 words (exceeds 150 by 37 words - consider condensing) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Topic 3 has 244 words (exceeds 150 by 94 words - consider condensing) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Topic 4 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Topic 5 has 1 words (require 100-150, need 99 more words) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Topic 6 has 17 words (require 100-150, need 83 more words) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - WARNING - [WARNING] Total word count (662) exceeds maximum 600 (exceeds by 62 words - condense content) âš ï¸
2025-12-15 14:03:25,781 - generate_secondary - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 14:03:25,781 - generate_secondary - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 14:03:25,782 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_19/extension.md
2025-12-15 14:03:25,782 - generate_secondary - INFO - Generating visualization for session 19: Open Questions & Research Frontiers...
2025-12-15 14:03:25,782 - src.llm.client - INFO - [viz:95d862] ğŸš€ viz | m=gemma3:4b | p=28098c | t=120s
2025-12-15 14:03:25,782 - src.llm.client - INFO - [viz:95d862] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 14:03:25,782 - src.llm.client - INFO - [viz:95d862] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:03:25,783 - src.llm.client - INFO - [viz:95d862] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=32347 bytes, prompt=28098 chars
2025-12-15 14:03:25,783 - src.llm.client - INFO - [viz:95d862] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 14:03:35,351 - src.llm.request_handler - INFO - [viz:95d862] âœ“ Done 9.57s
2025-12-15 14:03:35,351 - src.llm.client - INFO - [viz:95d862] âœ… HTTP 200 in 9.57s
2025-12-15 14:03:35,351 - src.llm.client - INFO - [viz:95d862] ğŸ“¡ Stream active (200)
2025-12-15 14:03:35,351 - src.llm.client - INFO - [viz:95d862] Starting stream parsing, waiting for first chunk...
2025-12-15 14:03:37,374 - src.llm.client - INFO - [viz:95d862] ğŸ“Š 2.0s: 280c @138c/s (67ch, ~70t @35t/s)
2025-12-15 14:03:39,385 - src.llm.client - INFO - [viz:95d862] ğŸ“Š 4.0s: 530c @131c/s (134ch, ~132t @33t/s)
2025-12-15 14:03:41,400 - src.llm.client - INFO - [viz:95d862] ğŸ“Š 6.0s: 800c @132c/s (201ch, ~200t @33t/s)
2025-12-15 14:03:43,413 - src.llm.client - INFO - [viz:95d862] ğŸ“Š 8.1s: 1070c @133c/s (268ch, ~268t @33t/s)
2025-12-15 14:03:45,328 - src.llm.client - INFO - [viz:95d862] âœ“ Done 19.55s: 1337c (~184w @68c/s)
2025-12-15 14:03:45,329 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:03:45,329 - generate_secondary - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 14:03:45,329 - generate_secondary - INFO -     - Length: 498 chars (cleaned: 498 chars)
2025-12-15 14:03:45,329 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 14:03:45,329 - generate_secondary - INFO - [OK] Elements: 29 total (nodes: 14, connections: 15) âœ“
2025-12-15 14:03:45,329 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_19/visualization.mmd
2025-12-15 14:03:45,330 - generate_secondary - INFO - Generating integration for session 19: Open Questions & Research Frontiers...
2025-12-15 14:03:45,330 - src.llm.client - INFO - [int:0cb228] ğŸš€ int | m=gemma3:4b | p=29447c | t=150s
2025-12-15 14:03:45,330 - src.llm.client - INFO - [int:0cb228] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:03:45,330 - src.llm.client - INFO - [int:0cb228] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:03:45,331 - src.llm.client - INFO - [int:0cb228] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=34713 bytes, prompt=29447 chars
2025-12-15 14:03:45,331 - src.llm.client - INFO - [int:0cb228] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:03:54,907 - src.llm.request_handler - INFO - [int:0cb228] âœ“ Done 9.58s
2025-12-15 14:03:54,907 - src.llm.client - INFO - [int:0cb228] âœ… HTTP 200 in 9.58s
2025-12-15 14:03:54,907 - src.llm.client - INFO - [int:0cb228] ğŸ“¡ Stream active (200)
2025-12-15 14:03:54,907 - src.llm.client - INFO - [int:0cb228] Starting stream parsing, waiting for first chunk...
2025-12-15 14:03:56,929 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 2.0s: 371c @183c/s (67ch, ~93t @46t/s)
2025-12-15 14:03:58,938 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 4.0s: 780c @194c/s (134ch, ~195t @48t/s)
2025-12-15 14:04:00,955 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 6.0s: 1196c @198c/s (201ch, ~299t @49t/s)
2025-12-15 14:04:02,970 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 8.1s: 1466c @182c/s (268ch, ~366t @45t/s)
2025-12-15 14:04:04,997 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 10.1s: 1719c @170c/s (335ch, ~430t @43t/s)
2025-12-15 14:04:07,026 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 12.1s: 2030c @168c/s (402ch, ~508t @42t/s)
2025-12-15 14:04:09,043 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 14.1s: 2427c @172c/s (469ch, ~607t @43t/s)
2025-12-15 14:04:11,059 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 16.2s: 2828c @175c/s (536ch, ~707t @44t/s)
2025-12-15 14:04:13,077 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 18.2s: 3230c @178c/s (603ch, ~808t @44t/s)
2025-12-15 14:04:15,095 - src.llm.client - INFO - [int:0cb228] ğŸ“Š 20.2s: 3607c @179c/s (670ch, ~902t @45t/s)
2025-12-15 14:04:16,593 - src.llm.client - INFO - [int:0cb228] âœ“ Done 31.26s: 3839c (~501w @123c/s)
2025-12-15 14:04:16,594 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:04:16,595 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 14:04:16,595 - generate_secondary - INFO -     - Length: 3837 chars, 501 words
2025-12-15 14:04:16,595 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 14:04:16,595 - generate_secondary - INFO -     - Connections: 19
2025-12-15 14:04:16,595 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 14:04:16,595 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_19/integration.md
2025-12-15 14:04:16,595 - generate_secondary - INFO - Generating investigation for session 19: Open Questions & Research Frontiers...
2025-12-15 14:04:16,595 - src.llm.client - INFO - [inv:04e8cb] ğŸš€ inv | m=gemma3:4b | p=28360c | t=150s
2025-12-15 14:04:16,595 - src.llm.client - INFO - [inv:04e8cb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:04:16,595 - src.llm.client - INFO - [inv:04e8cb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:04:16,597 - src.llm.client - INFO - [inv:04e8cb] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=32569 bytes, prompt=28360 chars
2025-12-15 14:04:16,597 - src.llm.client - INFO - [inv:04e8cb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:04:26,167 - src.llm.request_handler - INFO - [inv:04e8cb] âœ“ Done 9.57s
2025-12-15 14:04:26,167 - src.llm.client - INFO - [inv:04e8cb] âœ… HTTP 200 in 9.57s
2025-12-15 14:04:26,167 - src.llm.client - INFO - [inv:04e8cb] ğŸ“¡ Stream active (200)
2025-12-15 14:04:26,167 - src.llm.client - INFO - [inv:04e8cb] Starting stream parsing, waiting for first chunk...
2025-12-15 14:04:28,185 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 2.0s: 386c @191c/s (67ch, ~96t @48t/s)
2025-12-15 14:04:30,199 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 4.0s: 761c @189c/s (134ch, ~190t @47t/s)
2025-12-15 14:04:32,214 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 6.0s: 1146c @190c/s (201ch, ~286t @47t/s)
2025-12-15 14:04:34,225 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 8.1s: 1556c @193c/s (268ch, ~389t @48t/s)
2025-12-15 14:04:36,237 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 10.1s: 1944c @193c/s (335ch, ~486t @48t/s)
2025-12-15 14:04:38,254 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 12.1s: 2238c @185c/s (402ch, ~560t @46t/s)
2025-12-15 14:04:40,267 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 14.1s: 2625c @186c/s (469ch, ~656t @47t/s)
2025-12-15 14:04:42,280 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 16.1s: 3003c @186c/s (536ch, ~751t @47t/s)
2025-12-15 14:04:44,293 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 18.1s: 3403c @188c/s (603ch, ~851t @47t/s)
2025-12-15 14:04:46,312 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 20.1s: 3761c @187c/s (670ch, ~940t @47t/s)
2025-12-15 14:04:48,331 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 22.2s: 4111c @185c/s (737ch, ~1028t @46t/s)
2025-12-15 14:04:50,351 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 24.2s: 4507c @186c/s (804ch, ~1127t @47t/s)
2025-12-15 14:04:52,372 - src.llm.client - INFO - [inv:04e8cb] ğŸ“Š 26.2s: 4947c @189c/s (871ch, ~1237t @47t/s)
2025-12-15 14:04:53,590 - src.llm.client - INFO - [inv:04e8cb] âœ“ Done 37.00s: 5145c (~728w @139c/s)
2025-12-15 14:04:53,592 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:04:53,592 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 14:04:53,592 - generate_secondary - INFO -     - Length: 5144 chars, 728 words
2025-12-15 14:04:53,593 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 14:04:53,593 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 14:04:53,593 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 14:04:53,593 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_19/investigation.md
2025-12-15 14:04:53,593 - generate_secondary - INFO - Generating open_questions for session 19: Open Questions & Research Frontiers...
2025-12-15 14:04:53,593 - src.llm.client - INFO - [opq:0dd869] ğŸš€ opq | m=gemma3:4b | p=28446c | t=150s
2025-12-15 14:04:53,593 - src.llm.client - INFO - [opq:0dd869] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:04:53,593 - src.llm.client - INFO - [opq:0dd869] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:04:53,595 - src.llm.client - INFO - [opq:0dd869] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=32666 bytes, prompt=28446 chars
2025-12-15 14:04:53,595 - src.llm.client - INFO - [opq:0dd869] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:05:03,163 - src.llm.request_handler - INFO - [opq:0dd869] âœ“ Done 9.57s
2025-12-15 14:05:03,163 - src.llm.client - INFO - [opq:0dd869] âœ… HTTP 200 in 9.57s
2025-12-15 14:05:03,163 - src.llm.client - INFO - [opq:0dd869] ğŸ“¡ Stream active (200)
2025-12-15 14:05:03,163 - src.llm.client - INFO - [opq:0dd869] Starting stream parsing, waiting for first chunk...
2025-12-15 14:05:05,170 - src.llm.client - INFO - [opq:0dd869] ğŸ“Š 2.0s: 383c @191c/s (66ch, ~96t @48t/s)
2025-12-15 14:05:07,191 - src.llm.client - INFO - [opq:0dd869] ğŸ“Š 4.0s: 760c @189c/s (133ch, ~190t @47t/s)
2025-12-15 14:05:09,205 - src.llm.client - INFO - [opq:0dd869] ğŸ“Š 6.0s: 1151c @191c/s (200ch, ~288t @48t/s)
2025-12-15 14:05:11,219 - src.llm.client - INFO - [opq:0dd869] ğŸ“Š 8.1s: 1566c @194c/s (267ch, ~392t @49t/s)
2025-12-15 14:05:13,232 - src.llm.client - INFO - [opq:0dd869] ğŸ“Š 10.1s: 1905c @189c/s (325ch, ~476t @47t/s)
2025-12-15 14:05:13,232 - src.llm.client - INFO - [opq:0dd869] âœ“ Done 19.64s: 1905c (~245w @97c/s)
2025-12-15 14:05:13,233 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:05:13,233 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 14:05:13,233 - generate_secondary - INFO -     - Length: 1905 chars, 245 words
2025-12-15 14:05:13,233 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 14:05:13,233 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 14:05:13,233 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 14:05:13,234 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_19/open_questions.md
2025-12-15 14:05:13,234 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 14:05:13,234 - generate_secondary - INFO - 
  Session 20/20: Final Q&A
2025-12-15 14:05:13,236 - generate_secondary - INFO - Generating application for session 20: Final Q&A...
2025-12-15 14:05:13,236 - src.llm.client - INFO - [app:0b5542] ğŸš€ app | m=gemma3:4b | p=28991c | t=150s
2025-12-15 14:05:13,236 - src.llm.client - INFO - [app:0b5542] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:05:13,236 - src.llm.client - INFO - [app:0b5542] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:05:13,237 - src.llm.client - INFO - [app:0b5542] Sending request to Ollama: model=gemma3:4b, operation=application, payload=30925 bytes, prompt=28991 chars
2025-12-15 14:05:13,237 - src.llm.client - INFO - [app:0b5542] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:05:22,806 - src.llm.request_handler - INFO - [app:0b5542] âœ“ Done 9.57s
2025-12-15 14:05:22,807 - src.llm.client - INFO - [app:0b5542] âœ… HTTP 200 in 9.57s
2025-12-15 14:05:22,807 - src.llm.client - INFO - [app:0b5542] ğŸ“¡ Stream active (200)
2025-12-15 14:05:22,807 - src.llm.client - INFO - [app:0b5542] Starting stream parsing, waiting for first chunk...
2025-12-15 14:05:24,831 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 2.0s: 389c @192c/s (67ch, ~97t @48t/s)
2025-12-15 14:05:26,847 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 4.0s: 791c @196c/s (134ch, ~198t @49t/s)
2025-12-15 14:05:28,863 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 6.1s: 1207c @199c/s (201ch, ~302t @50t/s)
2025-12-15 14:05:30,877 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 8.1s: 1522c @189c/s (268ch, ~380t @47t/s)
2025-12-15 14:05:32,893 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 10.1s: 1968c @195c/s (335ch, ~492t @49t/s)
2025-12-15 14:05:34,920 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 12.1s: 2375c @196c/s (402ch, ~594t @49t/s)
2025-12-15 14:05:36,938 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 14.1s: 2791c @198c/s (469ch, ~698t @49t/s)
2025-12-15 14:05:38,955 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 16.1s: 3143c @195c/s (536ch, ~786t @49t/s)
2025-12-15 14:05:40,973 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 18.2s: 3508c @193c/s (603ch, ~877t @48t/s)
2025-12-15 14:05:42,988 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 20.2s: 3901c @193c/s (670ch, ~975t @48t/s)
2025-12-15 14:05:45,010 - src.llm.client - INFO - [app:0b5542] ğŸ“Š 22.2s: 4265c @192c/s (737ch, ~1066t @48t/s)
2025-12-15 14:05:46,279 - src.llm.client - INFO - [app:0b5542] âœ“ Done 33.04s: 4443c (~605w @134c/s)
2025-12-15 14:05:46,281 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 14:05:46,281 - generate_secondary - INFO - [NEEDS REVIEW] Application generated âš ï¸
2025-12-15 14:05:46,281 - generate_secondary - INFO -     - Length: 4432 chars, 603 words
2025-12-15 14:05:46,281 - generate_secondary - INFO -     - Requirements: 3-5 applications, 150-200 words each, max 1000 total words
2025-12-15 14:05:46,282 - generate_secondary - INFO -     - Applications: 5
2025-12-15 14:05:46,283 - generate_secondary - INFO -     - Avg words per application: 116
2025-12-15 14:05:46,283 - generate_secondary - WARNING - [WARNING] Application 1 has 141 words (require 150-200, need 9 more words) âš ï¸
2025-12-15 14:05:46,283 - generate_secondary - WARNING - [WARNING] Application 2 has 119 words (require 150-200, need 31 more words) âš ï¸
2025-12-15 14:05:46,283 - generate_secondary - WARNING - [WARNING] Application 3 has 102 words (require 150-200, need 48 more words) âš ï¸
2025-12-15 14:05:46,283 - generate_secondary - WARNING - [WARNING] Application 4 has 104 words (require 150-200, need 46 more words) âš ï¸
2025-12-15 14:05:46,283 - generate_secondary - WARNING - [WARNING] Application 5 has 113 words (require 150-200, need 37 more words) âš ï¸
2025-12-15 14:05:46,283 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_20/application.md
2025-12-15 14:05:46,283 - generate_secondary - INFO - Generating extension for session 20: Final Q&A...
2025-12-15 14:05:46,283 - src.llm.client - INFO - [ext:36cccf] ğŸš€ ext | m=gemma3:4b | p=22877c | t=120s
2025-12-15 14:05:46,283 - src.llm.client - INFO - [ext:36cccf] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 14:05:46,283 - src.llm.client - INFO - [ext:36cccf] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:05:46,285 - src.llm.client - INFO - [ext:36cccf] Sending request to Ollama: model=gemma3:4b, operation=extension, payload=27682 bytes, prompt=22877 chars
2025-12-15 14:05:46,285 - src.llm.client - INFO - [ext:36cccf] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 14:05:55,840 - src.llm.request_handler - INFO - [ext:36cccf] âœ“ Done 9.55s
2025-12-15 14:05:55,840 - src.llm.client - INFO - [ext:36cccf] âœ… HTTP 200 in 9.55s
2025-12-15 14:05:55,840 - src.llm.client - INFO - [ext:36cccf] ğŸ“¡ Stream active (200)
2025-12-15 14:05:55,840 - src.llm.client - INFO - [ext:36cccf] Starting stream parsing, waiting for first chunk...
2025-12-15 14:05:57,861 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 2.0s: 385c @191c/s (67ch, ~96t @48t/s)
2025-12-15 14:05:59,874 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 4.0s: 784c @194c/s (134ch, ~196t @49t/s)
2025-12-15 14:06:01,894 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 6.1s: 1218c @201c/s (201ch, ~304t @50t/s)
2025-12-15 14:06:03,907 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 8.1s: 1600c @198c/s (268ch, ~400t @50t/s)
2025-12-15 14:06:05,925 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 10.1s: 2077c @206c/s (335ch, ~519t @51t/s)
2025-12-15 14:06:07,945 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 12.1s: 2450c @202c/s (402ch, ~612t @51t/s)
2025-12-15 14:06:09,958 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 14.1s: 2929c @207c/s (469ch, ~732t @52t/s)
2025-12-15 14:06:11,972 - src.llm.client - INFO - [ext:36cccf] ğŸ“Š 16.1s: 3358c @208c/s (536ch, ~840t @52t/s)
2025-12-15 14:06:13,579 - src.llm.client - INFO - [ext:36cccf] âœ“ Done 27.30s: 3613c (~458w @132c/s)
2025-12-15 14:06:13,581 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:06:13,581 - generate_secondary - INFO - [COMPLIANT] Extension generated âœ“
2025-12-15 14:06:13,581 - generate_secondary - INFO -     - Length: 3613 chars, 458 words
2025-12-15 14:06:13,581 - generate_secondary - INFO -     - Requirements: 3-4 topics, 100-150 words each, max 600 total words
2025-12-15 14:06:13,581 - generate_secondary - INFO -     - Topics: 3
2025-12-15 14:06:13,581 - generate_secondary - INFO -     - Avg words per topic: 146
2025-12-15 14:06:13,582 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_20/extension.md
2025-12-15 14:06:13,582 - generate_secondary - INFO - Generating visualization for session 20: Final Q&A...
2025-12-15 14:06:13,582 - src.llm.client - INFO - [viz:755d32] ğŸš€ viz | m=gemma3:4b | p=21837c | t=120s
2025-12-15 14:06:13,582 - src.llm.client - INFO - [viz:755d32] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 14:06:13,582 - src.llm.client - INFO - [viz:755d32] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:06:13,583 - src.llm.client - INFO - [viz:755d32] Sending request to Ollama: model=gemma3:4b, operation=visualization, payload=25964 bytes, prompt=21837 chars
2025-12-15 14:06:13,583 - src.llm.client - INFO - [viz:755d32] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 14:06:23,143 - src.llm.request_handler - INFO - [viz:755d32] âœ“ Done 9.56s
2025-12-15 14:06:23,143 - src.llm.client - INFO - [viz:755d32] âœ… HTTP 200 in 9.56s
2025-12-15 14:06:23,143 - src.llm.client - INFO - [viz:755d32] ğŸ“¡ Stream active (200)
2025-12-15 14:06:23,143 - src.llm.client - INFO - [viz:755d32] Starting stream parsing, waiting for first chunk...
2025-12-15 14:06:25,162 - src.llm.client - INFO - [viz:755d32] ğŸ“Š 2.0s: 207c @103c/s (67ch, ~52t @26t/s)
2025-12-15 14:06:27,174 - src.llm.client - INFO - [viz:755d32] ğŸ“Š 4.0s: 414c @103c/s (134ch, ~104t @26t/s)
2025-12-15 14:06:29,181 - src.llm.client - INFO - [viz:755d32] ğŸ“Š 6.0s: 636c @105c/s (201ch, ~159t @26t/s)
2025-12-15 14:06:31,195 - src.llm.client - INFO - [viz:755d32] ğŸ“Š 8.1s: 892c @111c/s (268ch, ~223t @28t/s)
2025-12-15 14:06:32,003 - src.llm.client - INFO - [viz:755d32] âœ“ Done 18.42s: 955c (~139w @52c/s)
2025-12-15 14:06:32,004 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:06:32,004 - generate_secondary - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 14:06:32,004 - generate_secondary - INFO -     - Length: 939 chars (cleaned: 939 chars)
2025-12-15 14:06:32,004 - generate_secondary - INFO -     - Requirements: min 6 diagram elements
2025-12-15 14:06:32,004 - generate_secondary - INFO - [WARNING] Elements: 55 total (nodes: 23, connections: 32) âš ï¸
2025-12-15 14:06:32,004 - generate_secondary - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 14:06:32,004 - generate_secondary - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-15 14:06:32,004 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_20/visualization.mmd
2025-12-15 14:06:32,004 - generate_secondary - INFO - Generating integration for session 20: Final Q&A...
2025-12-15 14:06:32,004 - src.llm.client - INFO - [int:e6b92c] ğŸš€ int | m=gemma3:4b | p=23186c | t=150s
2025-12-15 14:06:32,004 - src.llm.client - INFO - [int:e6b92c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:06:32,004 - src.llm.client - INFO - [int:e6b92c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:06:32,006 - src.llm.client - INFO - [int:e6b92c] Sending request to Ollama: model=gemma3:4b, operation=integration, payload=28330 bytes, prompt=23186 chars
2025-12-15 14:06:32,006 - src.llm.client - INFO - [int:e6b92c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:06:41,570 - src.llm.request_handler - INFO - [int:e6b92c] âœ“ Done 9.56s
2025-12-15 14:06:41,570 - src.llm.client - INFO - [int:e6b92c] âœ… HTTP 200 in 9.56s
2025-12-15 14:06:41,570 - src.llm.client - INFO - [int:e6b92c] ğŸ“¡ Stream active (200)
2025-12-15 14:06:41,570 - src.llm.client - INFO - [int:e6b92c] Starting stream parsing, waiting for first chunk...
2025-12-15 14:06:43,585 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 2.0s: 368c @183c/s (67ch, ~92t @46t/s)
2025-12-15 14:06:45,602 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 4.0s: 770c @191c/s (134ch, ~192t @48t/s)
2025-12-15 14:06:47,614 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 6.0s: 1180c @195c/s (201ch, ~295t @49t/s)
2025-12-15 14:06:49,628 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 8.1s: 1605c @199c/s (268ch, ~401t @50t/s)
2025-12-15 14:06:51,642 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 10.1s: 1997c @198c/s (335ch, ~499t @50t/s)
2025-12-15 14:06:53,660 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 12.1s: 2390c @198c/s (402ch, ~598t @49t/s)
2025-12-15 14:06:55,679 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 14.1s: 2716c @193c/s (469ch, ~679t @48t/s)
2025-12-15 14:06:57,696 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 16.1s: 3058c @190c/s (536ch, ~764t @47t/s)
2025-12-15 14:06:59,711 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 18.1s: 3428c @189c/s (603ch, ~857t @47t/s)
2025-12-15 14:07:01,739 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 20.2s: 3717c @184c/s (670ch, ~929t @46t/s)
2025-12-15 14:07:03,769 - src.llm.client - INFO - [int:e6b92c] ğŸ“Š 22.2s: 4069c @183c/s (737ch, ~1017t @46t/s)
2025-12-15 14:07:04,484 - src.llm.client - INFO - [int:e6b92c] âœ“ Done 32.48s: 4159c (~568w @128c/s)
2025-12-15 14:07:04,486 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:07:04,486 - generate_secondary - INFO - [COMPLIANT] Integration generated âœ“
2025-12-15 14:07:04,486 - generate_secondary - INFO -     - Length: 4159 chars, 568 words
2025-12-15 14:07:04,486 - generate_secondary - INFO -     - Requirements: min 3 connections, max 1000 words
2025-12-15 14:07:04,486 - generate_secondary - INFO -     - Connections: 21
2025-12-15 14:07:04,486 - generate_secondary - INFO -     - Structure: 0 sections
2025-12-15 14:07:04,487 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_20/integration.md
2025-12-15 14:07:04,487 - generate_secondary - INFO - Generating investigation for session 20: Final Q&A...
2025-12-15 14:07:04,487 - src.llm.client - INFO - [inv:1f241c] ğŸš€ inv | m=gemma3:4b | p=22099c | t=150s
2025-12-15 14:07:04,487 - src.llm.client - INFO - [inv:1f241c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:07:04,487 - src.llm.client - INFO - [inv:1f241c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:07:04,488 - src.llm.client - INFO - [inv:1f241c] Sending request to Ollama: model=gemma3:4b, operation=investigation, payload=26186 bytes, prompt=22099 chars
2025-12-15 14:07:04,488 - src.llm.client - INFO - [inv:1f241c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:07:14,050 - src.llm.request_handler - INFO - [inv:1f241c] âœ“ Done 9.56s
2025-12-15 14:07:14,050 - src.llm.client - INFO - [inv:1f241c] âœ… HTTP 200 in 9.56s
2025-12-15 14:07:14,050 - src.llm.client - INFO - [inv:1f241c] ğŸ“¡ Stream active (200)
2025-12-15 14:07:14,050 - src.llm.client - INFO - [inv:1f241c] Starting stream parsing, waiting for first chunk...
2025-12-15 14:07:16,067 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 2.0s: 390c @193c/s (67ch, ~98t @48t/s)
2025-12-15 14:07:18,076 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 4.0s: 748c @186c/s (134ch, ~187t @46t/s)
2025-12-15 14:07:20,088 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 6.0s: 1143c @189c/s (201ch, ~286t @47t/s)
2025-12-15 14:07:22,112 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 8.1s: 1543c @191c/s (268ch, ~386t @48t/s)
2025-12-15 14:07:24,129 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 10.1s: 1929c @191c/s (334ch, ~482t @48t/s)
2025-12-15 14:07:26,146 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 12.1s: 2279c @188c/s (401ch, ~570t @47t/s)
2025-12-15 14:07:28,158 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 14.1s: 2582c @183c/s (468ch, ~646t @46t/s)
2025-12-15 14:07:30,175 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 16.1s: 2924c @181c/s (535ch, ~731t @45t/s)
2025-12-15 14:07:32,194 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 18.1s: 3292c @181c/s (602ch, ~823t @45t/s)
2025-12-15 14:07:34,215 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 20.2s: 3644c @181c/s (669ch, ~911t @45t/s)
2025-12-15 14:07:36,236 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 22.2s: 3999c @180c/s (736ch, ~1000t @45t/s)
2025-12-15 14:07:38,262 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 24.2s: 4323c @179c/s (803ch, ~1081t @45t/s)
2025-12-15 14:07:40,286 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 26.2s: 4767c @182c/s (870ch, ~1192t @45t/s)
2025-12-15 14:07:42,309 - src.llm.client - INFO - [inv:1f241c] ğŸ“Š 28.3s: 5180c @183c/s (937ch, ~1295t @46t/s)
2025-12-15 14:07:42,837 - src.llm.client - INFO - [inv:1f241c] âœ“ Done 38.35s: 5234c (~729w @136c/s)
2025-12-15 14:07:42,839 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 14:07:42,839 - generate_secondary - INFO - [COMPLIANT] Investigation generated âœ“
2025-12-15 14:07:42,839 - generate_secondary - INFO -     - Length: 5234 chars, 729 words
2025-12-15 14:07:42,839 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 14:07:42,839 - generate_secondary - INFO -     - Research questions: 3
2025-12-15 14:07:42,839 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 14:07:42,839 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_20/investigation.md
2025-12-15 14:07:42,840 - generate_secondary - INFO - Generating open_questions for session 20: Final Q&A...
2025-12-15 14:07:42,840 - src.llm.client - INFO - [opq:f12411] ğŸš€ opq | m=gemma3:4b | p=22185c | t=150s
2025-12-15 14:07:42,840 - src.llm.client - INFO - [opq:f12411] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 14:07:42,840 - src.llm.client - INFO - [opq:f12411] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 14:07:42,841 - src.llm.client - INFO - [opq:f12411] Sending request to Ollama: model=gemma3:4b, operation=open_questions, payload=26283 bytes, prompt=22185 chars
2025-12-15 14:07:42,841 - src.llm.client - INFO - [opq:f12411] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 14:07:52,402 - src.llm.request_handler - INFO - [opq:f12411] âœ“ Done 9.56s
2025-12-15 14:07:52,402 - src.llm.client - INFO - [opq:f12411] âœ… HTTP 200 in 9.56s
2025-12-15 14:07:52,402 - src.llm.client - INFO - [opq:f12411] ğŸ“¡ Stream active (200)
2025-12-15 14:07:52,402 - src.llm.client - INFO - [opq:f12411] Starting stream parsing, waiting for first chunk...
2025-12-15 14:07:54,420 - src.llm.client - INFO - [opq:f12411] ğŸ“Š 2.0s: 354c @175c/s (67ch, ~88t @44t/s)
2025-12-15 14:07:56,431 - src.llm.client - INFO - [opq:f12411] ğŸ“Š 4.0s: 781c @194c/s (134ch, ~195t @48t/s)
2025-12-15 14:07:58,443 - src.llm.client - INFO - [opq:f12411] ğŸ“Š 6.0s: 1165c @193c/s (201ch, ~291t @48t/s)
2025-12-15 14:08:00,461 - src.llm.client - INFO - [opq:f12411] ğŸ“Š 8.1s: 1572c @195c/s (268ch, ~393t @49t/s)
2025-12-15 14:08:02,214 - src.llm.client - INFO - [opq:f12411] âœ“ Done 19.37s: 1863c (~241w @96c/s)
2025-12-15 14:08:02,215 - src.generate.processors.cleanup - INFO - Cleanup complete: 1 issues before, 0 issues after
2025-12-15 14:08:02,215 - generate_secondary - INFO - [COMPLIANT] Open questions generated âœ“
2025-12-15 14:08:02,215 - generate_secondary - INFO -     - Length: 1849 chars, 239 words
2025-12-15 14:08:02,215 - generate_secondary - INFO -     - Requirements: min 3 questions, max 1000 words
2025-12-15 14:08:02,215 - generate_secondary - INFO -     - Open questions: 3
2025-12-15 14:08:02,215 - generate_secondary - INFO -     - Structure: 3 sections
2025-12-15 14:08:02,216 - generate_secondary - INFO -   â†’ Saved to: output/active_inference/modules/module_10_concluding_remarks_future_directions/session_20/open_questions.md
2025-12-15 14:08:02,216 - generate_secondary - INFO -   âœ“ Generated 6 secondary materials
2025-12-15 14:08:02,216 - generate_secondary - INFO - 
2025-12-15 14:08:02,216 - generate_secondary - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 14:08:02,216 - generate_secondary - INFO - [ALL COMPLIANT] Secondary Materials Generation - Summary âœ…
2025-12-15 14:08:02,216 - generate_secondary - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 14:08:02,216 - generate_secondary - INFO -   Items Processed: 20
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - [COMPLIANT] Successful: 20
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - [ERROR] Failed: 0
2025-12-15 14:08:02,216 - generate_secondary - INFO - 
2025-12-15 14:08:02,216 - generate_secondary - INFO -   Compliance Breakdown:
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - [COMPLIANT]: 20
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - [NEEDS REVIEW]: 0
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - [CRITICAL]: 0
2025-12-15 14:08:02,216 - generate_secondary - INFO - 
2025-12-15 14:08:02,216 - generate_secondary - INFO -   Issue Statistics:
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - Total Issues: 0
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - Critical Errors: 0
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - Warnings: 0
2025-12-15 14:08:02,216 - generate_secondary - INFO - 
2025-12-15 14:08:02,216 - generate_secondary - INFO -   Recommendations:
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - All content generated successfully
2025-12-15 14:08:02,216 - generate_secondary - INFO -     - No issues detected
2025-12-15 14:08:02,216 - generate_secondary - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 14:08:02,216 - generate_secondary - INFO - 
================================================================================
2025-12-15 14:08:02,216 - generate_secondary - INFO - EXIT CODE: 0 (SUCCESS)
2025-12-15 14:08:02,216 - generate_secondary - INFO - ================================================================================
2025-12-15 14:08:02,216 - generate_secondary - INFO - All sessions processed successfully with no issues
2025-12-15 14:08:02,216 - generate_secondary - INFO - ================================================================================
