2025-12-16 08:33:17,429 - root - INFO - Logging to file: /Users/4d/Documents/GitHub/curriculum/scripts/output/logs/04_generate_primary_20251216_083317.log
2025-12-16 08:33:17,429 - generate_primary - INFO - 
2025-12-16 08:33:17,429 - generate_primary - INFO - ğŸ“š STAGE 04: PRIMARY MATERIALS (Session-Based)
2025-12-16 08:33:17,429 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 08:33:17,429 - generate_primary - INFO - Generating materials PER SESSION (not per module)
2025-12-16 08:33:17,429 - generate_primary - INFO - Output structure: output/modules/module_XX/session_YY/[material].md
2025-12-16 08:33:17,429 - generate_primary - INFO - 
2025-12-16 08:33:17,429 - src.config.loader - INFO - Initialized ConfigLoader with directory: /Users/4d/Documents/GitHub/curriculum/config
2025-12-16 08:33:17,430 - src.config.loader - INFO - Course configuration validated successfully
2025-12-16 08:33:17,443 - src.config.loader - INFO - All configurations validated successfully
2025-12-16 08:33:17,443 - generate_primary - INFO - PRIMARY ARTIFACTS GENERATED PER SESSION:
2025-12-16 08:33:17,443 - generate_primary - INFO -   1. lecture.md - Comprehensive instructional content
2025-12-16 08:33:17,443 - generate_primary - INFO -   2. lab.md - Laboratory exercise with procedures
2025-12-16 08:33:17,443 - generate_primary - INFO -   3. study_notes.md - Concise session summary
2025-12-16 08:33:17,444 - generate_primary - INFO -   4. diagram_1.mmd, diagram_2.mmd, ... (up to 4 diagrams)
2025-12-16 08:33:17,444 - generate_primary - INFO -   5. questions.md - Comprehension assessment questions
2025-12-16 08:33:17,444 - generate_primary - INFO - 
2025-12-16 08:33:17,444 - generate_primary - INFO - 
2025-12-16 08:33:17,444 - generate_primary - INFO - âš™ï¸ CONFIGURATION
2025-12-16 08:33:17,444 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 08:33:17,444 - generate_primary - INFO -   â€¢ Diagrams per Session: 4
2025-12-16 08:33:17,444 - generate_primary - INFO -   â€¢ Log File: output/logs/04_generate_primary_20251216_083317.log
2025-12-16 08:33:17,444 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-16 08:33:17,444 - src.config.loader - INFO - Found most recent outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/active_inference_college/outlines/course_outline_20251216_083317.json
2025-12-16 08:33:17,444 - generate_primary - INFO - Using most recent outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/active_inference_college/outlines/course_outline_20251216_083317.json
2025-12-16 08:33:17,444 - generate_primary - INFO - 
2025-12-16 08:33:17,444 - generate_primary - INFO - Processing ALL modules from outline
2025-12-16 08:33:17,444 - src.generate.orchestration.pipeline - INFO - Initializing Educational Course Generator pipeline...
2025-12-16 08:33:17,445 - src.llm.client - INFO - Initialized OllamaClient: model=gemma3:4b, url=http://localhost:11434/api/generate
2025-12-16 08:33:17,445 - src.generate.stages.stage1_outline - INFO - Initialized OutlineGenerator
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - Pipeline initialized successfully
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - STAGE 2: Generating Primary Content (Session-Based)
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 08:33:17,445 - src.config.loader - INFO - Found most recent outline: /Users/4d/Documents/GitHub/curriculum/scripts/output/active_inference_college/outlines/course_outline_20251216_083317.json
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - Processing 15 modules with session-based generation
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - Using course-specific output directory: output/active_inference_college/
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - Module 1: Introduction to Bayesian Statistics (1 sessions)
2025-12-16 08:33:17,445 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:33:17,446 - src.generate.orchestration.pipeline - INFO - 
[1/15] Session 1: Probability Basics
2025-12-16 08:33:17,446 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:33:17,446 - src.generate.formats.lectures - INFO - Generating lecture for: Introduction to Bayesian Statistics (Session 1/15)
2025-12-16 08:33:17,446 - src.llm.client - INFO - [lec:738335] ğŸš€ lec | m=gemma3:4b | p=3142c | t=180s
2025-12-16 08:33:17,446 - src.llm.client - INFO - [lec:738335] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:33:17,446 - src.llm.client - INFO - [lec:738335] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:33:17,453 - src.llm.client - INFO - [lec:738335] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6776 bytes, prompt=3142 chars
2025-12-16 08:33:17,453 - src.llm.client - INFO - [lec:738335] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:33:18,494 - src.llm.request_handler - INFO - [lec:738335] âœ“ Done 1.04s
2025-12-16 08:33:18,494 - src.llm.client - INFO - [lec:738335] âœ… HTTP 200 in 1.04s
2025-12-16 08:33:18,494 - src.llm.client - INFO - [lec:738335] ğŸ“¡ Stream active (200)
2025-12-16 08:33:18,494 - src.llm.client - INFO - [lec:738335] Starting stream parsing, waiting for first chunk...
2025-12-16 08:33:20,498 - src.llm.client - INFO - [lec:738335] ğŸ“Š 2.0s: 847c @423c/s (145ch, ~212t @106t/s)
2025-12-16 08:33:22,510 - src.llm.client - INFO - [lec:738335] ğŸ“Š 4.0s: 1457c @363c/s (290ch, ~364t @91t/s)
2025-12-16 08:33:24,517 - src.llm.client - INFO - [lec:738335] ğŸ“Š 6.0s: 2205c @366c/s (434ch, ~551t @92t/s)
2025-12-16 08:33:26,531 - src.llm.client - INFO - [lec:738335] ğŸ“Š 8.0s: 2895c @360c/s (578ch, ~724t @90t/s)
2025-12-16 08:33:28,531 - src.llm.client - INFO - [lec:738335] ğŸ“Š 10.0s: 3648c @363c/s (721ch, ~912t @91t/s)
2025-12-16 08:33:30,532 - src.llm.client - INFO - [lec:738335] ğŸ“Š 12.0s: 4307c @358c/s (864ch, ~1077t @89t/s)
2025-12-16 08:33:32,540 - src.llm.client - INFO - [lec:738335] ğŸ“Š 14.0s: 4964c @353c/s (1007ch, ~1241t @88t/s)
2025-12-16 08:33:34,544 - src.llm.client - INFO - [lec:738335] ğŸ“Š 16.0s: 5788c @361c/s (1150ch, ~1447t @90t/s)
2025-12-16 08:33:36,544 - src.llm.client - INFO - [lec:738335] ğŸ“Š 18.1s: 6538c @362c/s (1292ch, ~1634t @91t/s)
2025-12-16 08:33:36,748 - src.llm.client - INFO - [lec:738335] âœ“ Done 19.30s: 6605c (~986w @342c/s)
2025-12-16 08:33:36,750 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:33:36,750 - src.generate.formats.lectures - INFO -     - Length: 6722 chars, 1001 words
2025-12-16 08:33:36,750 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:33:36,750 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:33:36,750 - src.generate.formats.lectures - INFO -     - Content: 9 examples, 6 terms defined
2025-12-16 08:33:36,750 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:33:36,754 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:33:36,755 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:33:36,755 - src.generate.formats.labs - INFO - Generating lab 1 for: Introduction to Bayesian Statistics (Session 1)
2025-12-16 08:33:36,755 - src.llm.client - INFO - [lab:329fa8] ğŸš€ lab | m=gemma3:4b | p=3372c | t=150s
2025-12-16 08:33:36,755 - src.llm.client - INFO - [lab:329fa8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:33:36,755 - src.llm.client - INFO - [lab:329fa8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:33:36,757 - src.llm.client - INFO - [lab:329fa8] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3801 bytes, prompt=3372 chars
2025-12-16 08:33:36,757 - src.llm.client - INFO - [lab:329fa8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:33:37,684 - src.llm.request_handler - INFO - [lab:329fa8] âœ“ Done 0.93s
2025-12-16 08:33:37,685 - src.llm.client - INFO - [lab:329fa8] âœ… HTTP 200 in 0.93s
2025-12-16 08:33:37,685 - src.llm.client - INFO - [lab:329fa8] ğŸ“¡ Stream active (200)
2025-12-16 08:33:37,685 - src.llm.client - INFO - [lab:329fa8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:33:39,697 - src.llm.client - INFO - [lab:329fa8] ğŸ“Š 2.0s: 810c @403c/s (146ch, ~202t @101t/s)
2025-12-16 08:33:41,701 - src.llm.client - INFO - [lab:329fa8] ğŸ“Š 4.0s: 1425c @355c/s (291ch, ~356t @89t/s)
2025-12-16 08:33:43,712 - src.llm.client - INFO - [lab:329fa8] ğŸ“Š 6.0s: 1970c @327c/s (435ch, ~492t @82t/s)
2025-12-16 08:33:45,717 - src.llm.client - INFO - [lab:329fa8] ğŸ“Š 8.0s: 2627c @327c/s (578ch, ~657t @82t/s)
2025-12-16 08:33:47,729 - src.llm.client - INFO - [lab:329fa8] ğŸ“Š 10.0s: 3293c @328c/s (722ch, ~823t @82t/s)
2025-12-16 08:33:49,729 - src.llm.client - INFO - [lab:329fa8] ğŸ“Š 12.0s: 4028c @334c/s (865ch, ~1007t @84t/s)
2025-12-16 08:33:50,399 - src.llm.client - INFO - [lab:329fa8] âœ“ Done 13.64s: 4244c (~635w @311c/s)
2025-12-16 08:33:50,399 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:33:50,399 - src.generate.formats.labs - INFO -     - Length: 4340 chars, 650 words
2025-12-16 08:33:50,399 - src.generate.formats.labs - INFO -     - Procedure: 8 steps
2025-12-16 08:33:50,399 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-16 08:33:50,399 - src.generate.formats.labs - INFO -     - Data tables: 11
2025-12-16 08:33:50,402 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:33:50,402 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:33:50,402 - src.generate.formats.study_notes - INFO - Generating study notes for: Introduction to Bayesian Statistics (Session 1)
2025-12-16 08:33:50,403 - src.llm.client - INFO - [stu:c876b2] ğŸš€ stu | m=gemma3:4b | p=4508c | t=120s
2025-12-16 08:33:50,403 - src.llm.client - INFO - [stu:c876b2] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:33:50,403 - src.llm.client - INFO - [stu:c876b2] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:33:50,404 - src.llm.client - INFO - [stu:c876b2] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8153 bytes, prompt=4508 chars
2025-12-16 08:33:50,404 - src.llm.client - INFO - [stu:c876b2] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:33:51,679 - src.llm.request_handler - INFO - [stu:c876b2] âœ“ Done 1.27s
2025-12-16 08:33:51,679 - src.llm.client - INFO - [stu:c876b2] âœ… HTTP 200 in 1.27s
2025-12-16 08:33:51,679 - src.llm.client - INFO - [stu:c876b2] ğŸ“¡ Stream active (200)
2025-12-16 08:33:51,679 - src.llm.client - INFO - [stu:c876b2] Starting stream parsing, waiting for first chunk...
2025-12-16 08:33:53,686 - src.llm.client - INFO - [stu:c876b2] ğŸ“Š 2.0s: 657c @327c/s (140ch, ~164t @82t/s)
2025-12-16 08:33:55,694 - src.llm.client - INFO - [stu:c876b2] ğŸ“Š 4.0s: 1411c @351c/s (284ch, ~353t @88t/s)
2025-12-16 08:33:57,705 - src.llm.client - INFO - [stu:c876b2] ğŸ“Š 6.0s: 2140c @355c/s (425ch, ~535t @89t/s)
2025-12-16 08:33:59,709 - src.llm.client - INFO - [stu:c876b2] ğŸ“Š 8.0s: 2972c @370c/s (568ch, ~743t @93t/s)
2025-12-16 08:34:01,192 - src.llm.client - INFO - [stu:c876b2] âœ“ Done 10.79s: 3512c (~517w @325c/s)
2025-12-16 08:34:01,193 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:34:01,193 - src.generate.formats.study_notes - INFO -     - Length: 3582 chars, 528 words
2025-12-16 08:34:01,193 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:34:01,193 - src.generate.formats.study_notes - INFO -     - Key concepts: 6
2025-12-16 08:34:01,193 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 08:34:01,193 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:34:01,195 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:34:01,195 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:34:01,195 - src.generate.formats.diagrams - INFO - Generating diagram for: Sample Space (Introduction to Bayesian Statistics)
2025-12-16 08:34:01,195 - src.generate.formats.diagrams - INFO - Generating diagram for: Probability Distributions (Introduction to Bayesian Statistics)
2025-12-16 08:34:01,196 - src.generate.formats.diagrams - INFO - Generating diagram for: Marginalization (Introduction to Bayesian Statistics)
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:9bd4e0] ğŸš€ dia | m=gemma3:4b | p=5744c | t=120s
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:8f0cfe] ğŸš€ dia | m=gemma3:4b | p=5770c | t=120s
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:00dde8] ğŸš€ dia | m=gemma3:4b | p=5750c | t=120s
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:9bd4e0] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:8f0cfe] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:00dde8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:9bd4e0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:8f0cfe] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:34:01,196 - src.llm.client - INFO - [dia:00dde8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:34:01,199 - src.llm.client - INFO - [dia:00dde8] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11059 bytes, prompt=5750 chars
2025-12-16 08:34:01,199 - src.llm.client - INFO - [dia:8f0cfe] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11079 bytes, prompt=5770 chars
2025-12-16 08:34:01,199 - src.llm.client - INFO - [dia:00dde8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:34:01,199 - src.llm.client - INFO - [dia:9bd4e0] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11053 bytes, prompt=5744 chars
2025-12-16 08:34:01,199 - src.llm.client - INFO - [dia:8f0cfe] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:34:01,199 - src.llm.client - INFO - [dia:9bd4e0] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:34:02,912 - src.llm.request_handler - INFO - [dia:00dde8] âœ“ Done 1.71s
2025-12-16 08:34:02,913 - src.llm.client - INFO - [dia:00dde8] âœ… HTTP 200 in 1.71s
2025-12-16 08:34:02,913 - src.llm.client - INFO - [dia:00dde8] ğŸ“¡ Stream active (200)
2025-12-16 08:34:02,913 - src.llm.client - INFO - [dia:00dde8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:34:04,924 - src.llm.client - INFO - [dia:00dde8] ğŸ“Š 2.0s: 508c @253c/s (143ch, ~127t @63t/s)
2025-12-16 08:34:06,567 - src.llm.client - INFO - [dia:00dde8] âœ“ Done 5.37s: 866c (~127w @161c/s)
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Marginalization (Introduction to Bayesian Statistics):
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - INFO -     - Length: 850 chars (cleaned: 850 chars)
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - INFO - [OK] Elements: 52 total (nodes: 17, connections: 35) âœ“
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - INFO -   Cleanup summary: 1 issues fixed (code fences, style commands, etc.)
2025-12-16 08:34:06,568 - src.generate.formats.diagrams - INFO - Generated diagram: 850 characters
2025-12-16 08:34:08,096 - src.llm.request_handler - INFO - [dia:9bd4e0] âœ“ Done 6.90s
2025-12-16 08:34:08,097 - src.llm.client - INFO - [dia:9bd4e0] âœ… HTTP 200 in 6.90s
2025-12-16 08:34:08,097 - src.llm.client - INFO - [dia:9bd4e0] ğŸ“¡ Stream active (200)
2025-12-16 08:34:08,097 - src.llm.client - INFO - [dia:9bd4e0] Starting stream parsing, waiting for first chunk...
2025-12-16 08:34:10,099 - src.llm.client - INFO - [dia:9bd4e0] ğŸ“Š 2.0s: 507c @253c/s (143ch, ~127t @63t/s)
2025-12-16 08:34:12,110 - src.llm.client - INFO - [dia:9bd4e0] ğŸ“Š 4.0s: 882c @220c/s (287ch, ~220t @55t/s)
2025-12-16 08:34:12,812 - src.llm.client - INFO - [dia:9bd4e0] âœ“ Done 11.62s: 990c (~121w @85c/s)
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Sample Space (Introduction to Bayesian Statistics):
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO -     - Length: 634 chars (cleaned: 634 chars)
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO - [OK] Elements: 39 total (nodes: 16, connections: 23) âœ“
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:34:12,812 - src.generate.formats.diagrams - INFO - Generated diagram: 634 characters
2025-12-16 08:34:14,323 - src.llm.request_handler - INFO - [dia:8f0cfe] âœ“ Done 13.12s
2025-12-16 08:34:14,323 - src.llm.client - INFO - [dia:8f0cfe] âœ… HTTP 200 in 13.12s
2025-12-16 08:34:14,323 - src.llm.client - INFO - [dia:8f0cfe] ğŸ“¡ Stream active (200)
2025-12-16 08:34:14,323 - src.llm.client - INFO - [dia:8f0cfe] Starting stream parsing, waiting for first chunk...
2025-12-16 08:34:16,324 - src.llm.client - INFO - [dia:8f0cfe] ğŸ“Š 2.0s: 486c @243c/s (143ch, ~122t @61t/s)
2025-12-16 08:34:18,326 - src.llm.client - INFO - [dia:8f0cfe] ğŸ“Š 4.0s: 860c @215c/s (286ch, ~215t @54t/s)
2025-12-16 08:34:18,654 - src.llm.client - INFO - [dia:8f0cfe] âœ“ Done 17.46s: 903c (~111w @52c/s)
2025-12-16 08:34:18,654 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Probability Distributions (Introduction to Bayesian Statistics):
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO -     - Length: 581 chars (cleaned: 581 chars)
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO - [OK] Elements: 36 total (nodes: 16, connections: 20) âœ“
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:34:18,655 - src.generate.formats.diagrams - INFO - Generated diagram: 581 characters
2025-12-16 08:34:18,655 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:34:18,655 - src.generate.formats.questions - INFO - Generating 10 questions for: Introduction to Bayesian Statistics (Session 1)
2025-12-16 08:34:18,656 - src.llm.client - INFO - [qst:fed4d4] ğŸš€ qst | m=gemma3:4b | p=7379c | t=150s
2025-12-16 08:34:18,656 - src.llm.client - INFO - [qst:fed4d4] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:34:18,656 - src.llm.client - INFO - [qst:fed4d4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:34:18,657 - src.llm.client - INFO - [qst:fed4d4] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11065 bytes, prompt=7379 chars
2025-12-16 08:34:18,657 - src.llm.client - INFO - [qst:fed4d4] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:34:20,620 - src.llm.request_handler - INFO - [qst:fed4d4] âœ“ Done 1.96s
2025-12-16 08:34:20,620 - src.llm.client - INFO - [qst:fed4d4] âœ… HTTP 200 in 1.96s
2025-12-16 08:34:20,620 - src.llm.client - INFO - [qst:fed4d4] ğŸ“¡ Stream active (200)
2025-12-16 08:34:20,620 - src.llm.client - INFO - [qst:fed4d4] Starting stream parsing, waiting for first chunk...
2025-12-16 08:34:22,626 - src.llm.client - INFO - [qst:fed4d4] ğŸ“Š 2.0s: 698c @348c/s (143ch, ~174t @87t/s)
2025-12-16 08:34:24,633 - src.llm.client - INFO - [qst:fed4d4] ğŸ“Š 4.0s: 1177c @293c/s (286ch, ~294t @73t/s)
2025-12-16 08:34:26,646 - src.llm.client - INFO - [qst:fed4d4] ğŸ“Š 6.0s: 1898c @315c/s (428ch, ~474t @79t/s)
2025-12-16 08:34:28,646 - src.llm.client - INFO - [qst:fed4d4] ğŸ“Š 8.0s: 2694c @336c/s (569ch, ~674t @84t/s)
2025-12-16 08:34:30,647 - src.llm.client - INFO - [qst:fed4d4] ğŸ“Š 10.0s: 3466c @346c/s (710ch, ~866t @86t/s)
2025-12-16 08:34:32,648 - src.llm.client - INFO - [qst:fed4d4] ğŸ“Š 12.0s: 4242c @353c/s (851ch, ~1060t @88t/s)
2025-12-16 08:34:34,040 - src.llm.client - INFO - [qst:fed4d4] âœ“ Done 15.38s: 4799c (~700w @312c/s)
2025-12-16 08:34:34,041 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 0, 'total_fixes': 2}
2025-12-16 08:34:34,041 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-16 08:34:34,042 - src.generate.formats.questions - INFO - [COMPLIANT] Questions generated âœ“
2025-12-16 08:34:34,042 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-16 08:34:34,042 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-16 08:34:34,042 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-16 08:34:34,042 - src.generate.formats.questions - INFO - [OK] Question marks: 10 total, 10 questions with '?' âœ“
2025-12-16 08:34:34,042 - src.generate.formats.questions - INFO -     - Question length: avg 12.7 words (range: 7-17)
2025-12-16 08:34:34,042 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-16 08:34:34,044 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:34:34,046 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 1 completed
2025-12-16 08:34:34,046 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:34:34,046 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:34:34,046 - src.generate.orchestration.pipeline - INFO - Module 2: Conditional Probability & Bayesâ€™ Theorem (1 sessions)
2025-12-16 08:34:34,046 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:34:34,046 - src.generate.orchestration.pipeline - INFO - 
[2/15] Session 2: Conditional Probability
2025-12-16 08:34:34,046 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:34:34,046 - src.generate.formats.lectures - INFO - Generating lecture for: Conditional Probability & Bayesâ€™ Theorem (Session 2/15)
2025-12-16 08:34:34,046 - src.llm.client - INFO - [lec:43e62d] ğŸš€ lec | m=gemma3:4b | p=3094c | t=180s
2025-12-16 08:34:34,046 - src.llm.client - INFO - [lec:43e62d] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:34:34,046 - src.llm.client - INFO - [lec:43e62d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:34:34,048 - src.llm.client - INFO - [lec:43e62d] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6735 bytes, prompt=3094 chars
2025-12-16 08:34:34,048 - src.llm.client - INFO - [lec:43e62d] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:34:35,019 - src.llm.request_handler - INFO - [lec:43e62d] âœ“ Done 0.97s
2025-12-16 08:34:35,019 - src.llm.client - INFO - [lec:43e62d] âœ… HTTP 200 in 0.97s
2025-12-16 08:34:35,019 - src.llm.client - INFO - [lec:43e62d] ğŸ“¡ Stream active (200)
2025-12-16 08:34:35,019 - src.llm.client - INFO - [lec:43e62d] Starting stream parsing, waiting for first chunk...
2025-12-16 08:34:37,029 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 2.0s: 836c @416c/s (145ch, ~209t @104t/s)
2025-12-16 08:34:39,033 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 4.0s: 1335c @333c/s (288ch, ~334t @83t/s)
2025-12-16 08:34:41,034 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 6.0s: 1927c @320c/s (430ch, ~482t @80t/s)
2025-12-16 08:34:43,045 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 8.0s: 2479c @309c/s (573ch, ~620t @77t/s)
2025-12-16 08:34:45,047 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 10.0s: 3104c @310c/s (715ch, ~776t @77t/s)
2025-12-16 08:34:47,059 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 12.0s: 3563c @296c/s (858ch, ~891t @74t/s)
2025-12-16 08:34:49,073 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 14.1s: 4216c @300c/s (1001ch, ~1054t @75t/s)
2025-12-16 08:34:51,082 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 16.1s: 4895c @305c/s (1143ch, ~1224t @76t/s)
2025-12-16 08:34:53,085 - src.llm.client - INFO - [lec:43e62d] ğŸ“Š 18.1s: 5633c @312c/s (1282ch, ~1408t @78t/s)
2025-12-16 08:34:53,222 - src.llm.client - INFO - [lec:43e62d] âœ“ Done 19.18s: 5665c (~891w @295c/s)
2025-12-16 08:34:53,223 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 08:34:53,223 - src.generate.formats.lectures - INFO -     - Length: 5777 chars, 905 words
2025-12-16 08:34:53,223 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:34:53,224 - src.generate.formats.lectures - INFO -     - Structure: 6 sections, 0 subsections
2025-12-16 08:34:53,224 - src.generate.formats.lectures - INFO -     - Content: 6 examples, 0 terms defined
2025-12-16 08:34:53,224 - src.generate.formats.lectures - WARNING - [WARNING] Word count (905) below minimum 1000 (need 95 more words - consider regenerating or expanding content) âš ï¸
2025-12-16 08:34:53,224 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 08:34:53,224 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 08:34:53,224 - src.generate.formats.lectures - INFO - Quality score: 85.5/100 (good)
2025-12-16 08:34:53,227 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:34:53,227 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:34:53,227 - src.generate.formats.labs - INFO - Generating lab 2 for: Conditional Probability & Bayesâ€™ Theorem (Session 2)
2025-12-16 08:34:53,227 - src.llm.client - INFO - [lab:af25a4] ğŸš€ lab | m=gemma3:4b | p=3330c | t=150s
2025-12-16 08:34:53,227 - src.llm.client - INFO - [lab:af25a4] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:34:53,227 - src.llm.client - INFO - [lab:af25a4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:34:53,229 - src.llm.client - INFO - [lab:af25a4] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3780 bytes, prompt=3330 chars
2025-12-16 08:34:53,229 - src.llm.client - INFO - [lab:af25a4] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:34:54,175 - src.llm.request_handler - INFO - [lab:af25a4] âœ“ Done 0.95s
2025-12-16 08:34:54,176 - src.llm.client - INFO - [lab:af25a4] âœ… HTTP 200 in 0.95s
2025-12-16 08:34:54,176 - src.llm.client - INFO - [lab:af25a4] ğŸ“¡ Stream active (200)
2025-12-16 08:34:54,176 - src.llm.client - INFO - [lab:af25a4] Starting stream parsing, waiting for first chunk...
2025-12-16 08:34:56,179 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 2.0s: 710c @355c/s (145ch, ~178t @89t/s)
2025-12-16 08:34:58,183 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 4.0s: 1271c @317c/s (286ch, ~318t @79t/s)
2025-12-16 08:35:00,192 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 6.0s: 1816c @302c/s (429ch, ~454t @75t/s)
2025-12-16 08:35:02,201 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 8.0s: 2408c @300c/s (572ch, ~602t @75t/s)
2025-12-16 08:35:04,213 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 10.0s: 2911c @290c/s (715ch, ~728t @73t/s)
2025-12-16 08:35:06,226 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 12.1s: 3309c @275c/s (858ch, ~827t @69t/s)
2025-12-16 08:35:08,229 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 14.1s: 3879c @276c/s (1000ch, ~970t @69t/s)
2025-12-16 08:35:10,230 - src.llm.client - INFO - [lab:af25a4] ğŸ“Š 16.1s: 4430c @276c/s (1141ch, ~1108t @69t/s)
2025-12-16 08:35:11,296 - src.llm.client - INFO - [lab:af25a4] âœ“ Done 18.07s: 4724c (~718w @261c/s)
2025-12-16 08:35:11,297 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:35:11,297 - src.generate.formats.labs - INFO -     - Length: 4823 chars, 734 words
2025-12-16 08:35:11,297 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:35:11,297 - src.generate.formats.labs - INFO -     - Safety: 2 warnings
2025-12-16 08:35:11,297 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 08:35:11,299 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:35:11,300 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:35:11,300 - src.generate.formats.study_notes - INFO - Generating study notes for: Conditional Probability & Bayesâ€™ Theorem (Session 2)
2025-12-16 08:35:11,300 - src.llm.client - INFO - [stu:57e1f8] ğŸš€ stu | m=gemma3:4b | p=4451c | t=120s
2025-12-16 08:35:11,300 - src.llm.client - INFO - [stu:57e1f8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:35:11,300 - src.llm.client - INFO - [stu:57e1f8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:35:11,302 - src.llm.client - INFO - [stu:57e1f8] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8116 bytes, prompt=4451 chars
2025-12-16 08:35:11,302 - src.llm.client - INFO - [stu:57e1f8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:35:12,618 - src.llm.request_handler - INFO - [stu:57e1f8] âœ“ Done 1.32s
2025-12-16 08:35:12,618 - src.llm.client - INFO - [stu:57e1f8] âœ… HTTP 200 in 1.32s
2025-12-16 08:35:12,618 - src.llm.client - INFO - [stu:57e1f8] ğŸ“¡ Stream active (200)
2025-12-16 08:35:12,618 - src.llm.client - INFO - [stu:57e1f8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:35:14,622 - src.llm.client - INFO - [stu:57e1f8] ğŸ“Š 2.0s: 780c @389c/s (143ch, ~195t @97t/s)
2025-12-16 08:35:16,635 - src.llm.client - INFO - [stu:57e1f8] ğŸ“Š 4.0s: 1277c @318c/s (286ch, ~319t @79t/s)
2025-12-16 08:35:18,646 - src.llm.client - INFO - [stu:57e1f8] ğŸ“Š 6.0s: 1809c @300c/s (429ch, ~452t @75t/s)
2025-12-16 08:35:20,652 - src.llm.client - INFO - [stu:57e1f8] ğŸ“Š 8.0s: 2497c @311c/s (570ch, ~624t @78t/s)
2025-12-16 08:35:22,069 - src.llm.client - INFO - [stu:57e1f8] âœ“ Done 10.77s: 2933c (~447w @272c/s)
2025-12-16 08:35:22,070 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:35:22,070 - src.generate.formats.study_notes - INFO -     - Length: 3008 chars, 459 words
2025-12-16 08:35:22,070 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:35:22,070 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-16 08:35:22,070 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 6 bullets
2025-12-16 08:35:22,070 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:35:22,071 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:35:22,072 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:35:22,072 - src.generate.formats.diagrams - INFO - Generating diagram for: Definition (Conditional Probability & Bayesâ€™ Theorem)
2025-12-16 08:35:22,072 - src.llm.client - INFO - [dia:fbd7db] ğŸš€ dia | m=gemma3:4b | p=5750c | t=120s
2025-12-16 08:35:22,072 - src.generate.formats.diagrams - INFO - Generating diagram for: Chain Rule (Conditional Probability & Bayesâ€™ Theorem)
2025-12-16 08:35:22,072 - src.llm.client - INFO - [dia:fbd7db] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:35:22,072 - src.llm.client - INFO - [dia:63ee68] ğŸš€ dia | m=gemma3:4b | p=5750c | t=120s
2025-12-16 08:35:22,072 - src.llm.client - INFO - [dia:fbd7db] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:35:22,072 - src.llm.client - INFO - [dia:63ee68] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:35:22,072 - src.llm.client - INFO - [dia:63ee68] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:35:22,074 - src.llm.client - INFO - [dia:fbd7db] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11064 bytes, prompt=5750 chars
2025-12-16 08:35:22,074 - src.llm.client - INFO - [dia:63ee68] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11064 bytes, prompt=5750 chars
2025-12-16 08:35:22,074 - src.llm.client - INFO - [dia:fbd7db] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:35:22,074 - src.llm.client - INFO - [dia:63ee68] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:35:23,777 - src.llm.request_handler - INFO - [dia:63ee68] âœ“ Done 1.70s
2025-12-16 08:35:23,777 - src.llm.client - INFO - [dia:63ee68] âœ… HTTP 200 in 1.70s
2025-12-16 08:35:23,778 - src.llm.client - INFO - [dia:63ee68] ğŸ“¡ Stream active (200)
2025-12-16 08:35:23,778 - src.llm.client - INFO - [dia:63ee68] Starting stream parsing, waiting for first chunk...
2025-12-16 08:35:25,780 - src.llm.client - INFO - [dia:63ee68] ğŸ“Š 2.0s: 376c @188c/s (142ch, ~94t @47t/s)
2025-12-16 08:35:27,388 - src.llm.client - INFO - [dia:63ee68] âœ“ Done 5.32s: 680c (~113w @128c/s)
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Chain Rule (Conditional Probability & Bayesâ€™ Theorem):
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - INFO -     - Length: 665 chars (cleaned: 665 chars)
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - INFO - [OK] Elements: 43 total (nodes: 17, connections: 26) âœ“
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - INFO -   Cleanup summary: 1 issues fixed (code fences, style commands, etc.)
2025-12-16 08:35:27,389 - src.generate.formats.diagrams - INFO - Generated diagram: 665 characters
2025-12-16 08:35:28,908 - src.llm.request_handler - INFO - [dia:fbd7db] âœ“ Done 6.83s
2025-12-16 08:35:28,908 - src.llm.client - INFO - [dia:fbd7db] âœ… HTTP 200 in 6.83s
2025-12-16 08:35:28,908 - src.llm.client - INFO - [dia:fbd7db] ğŸ“¡ Stream active (200)
2025-12-16 08:35:28,908 - src.llm.client - INFO - [dia:fbd7db] Starting stream parsing, waiting for first chunk...
2025-12-16 08:35:30,909 - src.llm.client - INFO - [dia:fbd7db] ğŸ“Š 2.0s: 464c @232c/s (142ch, ~116t @58t/s)
2025-12-16 08:35:32,914 - src.llm.client - INFO - [dia:fbd7db] ğŸ“Š 4.0s: 859c @214c/s (284ch, ~215t @54t/s)
2025-12-16 08:35:34,919 - src.llm.client - INFO - [dia:fbd7db] ğŸ“Š 6.0s: 1201c @200c/s (426ch, ~300t @50t/s)
2025-12-16 08:35:35,193 - src.llm.client - INFO - [dia:fbd7db] âœ“ Done 13.12s: 1239c (~159w @94c/s)
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Definition (Conditional Probability & Bayesâ€™ Theorem):
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO -     - Length: 714 chars (cleaned: 714 chars)
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO - [OK] Elements: 48 total (nodes: 16, connections: 32) âœ“
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:35:35,194 - src.generate.formats.diagrams - INFO - Generated diagram: 714 characters
2025-12-16 08:35:35,195 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:35:35,195 - src.generate.formats.questions - INFO - Generating 10 questions for: Conditional Probability & Bayesâ€™ Theorem (Session 2)
2025-12-16 08:35:35,195 - src.llm.client - INFO - [qst:1b9a18] ğŸš€ qst | m=gemma3:4b | p=7339c | t=150s
2025-12-16 08:35:35,195 - src.llm.client - INFO - [qst:1b9a18] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:35:35,195 - src.llm.client - INFO - [qst:1b9a18] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:35:35,196 - src.llm.client - INFO - [qst:1b9a18] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11063 bytes, prompt=7339 chars
2025-12-16 08:35:35,196 - src.llm.client - INFO - [qst:1b9a18] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:35:37,218 - src.llm.request_handler - INFO - [qst:1b9a18] âœ“ Done 2.02s
2025-12-16 08:35:37,219 - src.llm.client - INFO - [qst:1b9a18] âœ… HTTP 200 in 2.02s
2025-12-16 08:35:37,219 - src.llm.client - INFO - [qst:1b9a18] ğŸ“¡ Stream active (200)
2025-12-16 08:35:37,219 - src.llm.client - INFO - [qst:1b9a18] Starting stream parsing, waiting for first chunk...
2025-12-16 08:35:39,224 - src.llm.client - INFO - [qst:1b9a18] ğŸ“Š 2.0s: 672c @335c/s (142ch, ~168t @84t/s)
2025-12-16 08:35:41,233 - src.llm.client - INFO - [qst:1b9a18] ğŸ“Š 4.0s: 1361c @339c/s (284ch, ~340t @85t/s)
2025-12-16 08:35:43,235 - src.llm.client - INFO - [qst:1b9a18] ğŸ“Š 6.0s: 2027c @337c/s (425ch, ~507t @84t/s)
2025-12-16 08:35:45,238 - src.llm.client - INFO - [qst:1b9a18] ğŸ“Š 8.0s: 2721c @339c/s (566ch, ~680t @85t/s)
2025-12-16 08:35:47,251 - src.llm.client - INFO - [qst:1b9a18] ğŸ“Š 10.0s: 3515c @350c/s (708ch, ~879t @88t/s)
2025-12-16 08:35:49,262 - src.llm.client - INFO - [qst:1b9a18] ğŸ“Š 12.0s: 4201c @349c/s (849ch, ~1050t @87t/s)
2025-12-16 08:35:51,262 - src.llm.client - INFO - [qst:1b9a18] ğŸ“Š 14.0s: 4986c @355c/s (989ch, ~1246t @89t/s)
2025-12-16 08:35:51,763 - src.llm.client - INFO - [qst:1b9a18] âœ“ Done 16.57s: 5130c (~763w @310c/s)
2025-12-16 08:35:51,763 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 2, 'total_fixes': 3}
2025-12-16 08:35:51,763 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 2 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -     Context: Module 2 Session 2
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 3 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -     Context: Module 2 Session 2
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Conditional Probability & Bayesâ€™ Theorem (Session 2)
2025-12-16 08:35:51,764 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:35:51,766 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:35:51,768 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 2 completed
2025-12-16 08:35:51,768 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:35:51,768 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:35:51,768 - src.generate.orchestration.pipeline - INFO - Module 3: Bayesâ€™ Theorem â€“ Derivation & Intuition (1 sessions)
2025-12-16 08:35:51,768 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:35:51,768 - src.generate.orchestration.pipeline - INFO - 
[3/15] Session 3: Bayes' Theorem Derivation
2025-12-16 08:35:51,768 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:35:51,768 - src.generate.formats.lectures - INFO - Generating lecture for: Bayesâ€™ Theorem â€“ Derivation & Intuition (Session 3/15)
2025-12-16 08:35:51,768 - src.llm.client - INFO - [lec:28783b] ğŸš€ lec | m=gemma3:4b | p=3132c | t=180s
2025-12-16 08:35:51,768 - src.llm.client - INFO - [lec:28783b] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:35:51,768 - src.llm.client - INFO - [lec:28783b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:35:51,770 - src.llm.client - INFO - [lec:28783b] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6788 bytes, prompt=3132 chars
2025-12-16 08:35:51,770 - src.llm.client - INFO - [lec:28783b] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:35:52,801 - src.llm.request_handler - INFO - [lec:28783b] âœ“ Done 1.03s
2025-12-16 08:35:52,802 - src.llm.client - INFO - [lec:28783b] âœ… HTTP 200 in 1.03s
2025-12-16 08:35:52,802 - src.llm.client - INFO - [lec:28783b] ğŸ“¡ Stream active (200)
2025-12-16 08:35:52,802 - src.llm.client - INFO - [lec:28783b] Starting stream parsing, waiting for first chunk...
2025-12-16 08:35:54,812 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 2.0s: 791c @393c/s (144ch, ~198t @98t/s)
2025-12-16 08:35:56,824 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 4.0s: 1523c @379c/s (287ch, ~381t @95t/s)
2025-12-16 08:35:58,831 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 6.0s: 2100c @348c/s (428ch, ~525t @87t/s)
2025-12-16 08:36:00,843 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 8.0s: 2785c @346c/s (571ch, ~696t @87t/s)
2025-12-16 08:36:02,849 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 10.0s: 3314c @330c/s (713ch, ~828t @82t/s)
2025-12-16 08:36:04,857 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 12.1s: 3824c @317c/s (855ch, ~956t @79t/s)
2025-12-16 08:36:06,860 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 14.1s: 4265c @303c/s (997ch, ~1066t @76t/s)
2025-12-16 08:36:08,873 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 16.1s: 4560c @284c/s (1139ch, ~1140t @71t/s)
2025-12-16 08:36:10,886 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 18.1s: 4737c @262c/s (1280ch, ~1184t @65t/s)
2025-12-16 08:36:12,887 - src.llm.client - INFO - [lec:28783b] ğŸ“Š 20.1s: 5438c @271c/s (1421ch, ~1360t @68t/s)
2025-12-16 08:36:14,624 - src.llm.client - INFO - [lec:28783b] âœ“ Done 22.86s: 6134c (~976w @268c/s)
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO -     - Length: 6235 chars, 991 words
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO -     - Structure: 6 sections, 0 subsections
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO -     - Content: 5 examples, 1 terms defined
2025-12-16 08:36:14,625 - src.generate.formats.lectures - WARNING - [WARNING] Word count (991) below minimum 1000 (need 9 more words - consider regenerating or expanding content) âš ï¸
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 08:36:14,625 - src.generate.formats.lectures - INFO - Quality score: 94.1/100 (excellent)
2025-12-16 08:36:14,628 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:36:14,628 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:36:14,628 - src.generate.formats.labs - INFO - Generating lab 3 for: Bayesâ€™ Theorem â€“ Derivation & Intuition (Session 3)
2025-12-16 08:36:14,628 - src.llm.client - INFO - [lab:f8a652] ğŸš€ lab | m=gemma3:4b | p=3371c | t=150s
2025-12-16 08:36:14,628 - src.llm.client - INFO - [lab:f8a652] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:36:14,628 - src.llm.client - INFO - [lab:f8a652] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:36:14,630 - src.llm.client - INFO - [lab:f8a652] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3847 bytes, prompt=3371 chars
2025-12-16 08:36:14,630 - src.llm.client - INFO - [lab:f8a652] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:36:15,579 - src.llm.request_handler - INFO - [lab:f8a652] âœ“ Done 0.95s
2025-12-16 08:36:15,579 - src.llm.client - INFO - [lab:f8a652] âœ… HTTP 200 in 0.95s
2025-12-16 08:36:15,579 - src.llm.client - INFO - [lab:f8a652] ğŸ“¡ Stream active (200)
2025-12-16 08:36:15,579 - src.llm.client - INFO - [lab:f8a652] Starting stream parsing, waiting for first chunk...
2025-12-16 08:36:17,586 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 2.0s: 750c @374c/s (145ch, ~188t @93t/s)
2025-12-16 08:36:19,588 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 4.0s: 1397c @348c/s (288ch, ~349t @87t/s)
2025-12-16 08:36:21,595 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 6.0s: 1910c @317c/s (429ch, ~478t @79t/s)
2025-12-16 08:36:23,608 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 8.0s: 2517c @314c/s (571ch, ~629t @78t/s)
2025-12-16 08:36:25,609 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 10.0s: 3027c @302c/s (713ch, ~757t @75t/s)
2025-12-16 08:36:27,610 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 12.0s: 3321c @276c/s (855ch, ~830t @69t/s)
2025-12-16 08:36:29,614 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 14.0s: 3569c @254c/s (997ch, ~892t @64t/s)
2025-12-16 08:36:31,622 - src.llm.client - INFO - [lab:f8a652] ğŸ“Š 16.0s: 4029c @251c/s (1139ch, ~1007t @63t/s)
2025-12-16 08:36:32,928 - src.llm.client - INFO - [lab:f8a652] âœ“ Done 18.30s: 4398c (~744w @240c/s)
2025-12-16 08:36:32,928 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:36:32,928 - src.generate.formats.labs - INFO -     - Length: 4519 chars, 762 words
2025-12-16 08:36:32,928 - src.generate.formats.labs - INFO -     - Procedure: 10 steps
2025-12-16 08:36:32,928 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-16 08:36:32,928 - src.generate.formats.labs - INFO -     - Data tables: 13
2025-12-16 08:36:32,931 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:36:32,931 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:36:32,931 - src.generate.formats.study_notes - INFO - Generating study notes for: Bayesâ€™ Theorem â€“ Derivation & Intuition (Session 3)
2025-12-16 08:36:32,931 - src.llm.client - INFO - [stu:5d6941] ğŸš€ stu | m=gemma3:4b | p=4480c | t=120s
2025-12-16 08:36:32,931 - src.llm.client - INFO - [stu:5d6941] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:36:32,931 - src.llm.client - INFO - [stu:5d6941] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:36:32,933 - src.llm.client - INFO - [stu:5d6941] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8171 bytes, prompt=4480 chars
2025-12-16 08:36:32,933 - src.llm.client - INFO - [stu:5d6941] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:36:34,217 - src.llm.request_handler - INFO - [stu:5d6941] âœ“ Done 1.28s
2025-12-16 08:36:34,217 - src.llm.client - INFO - [stu:5d6941] âœ… HTTP 200 in 1.28s
2025-12-16 08:36:34,217 - src.llm.client - INFO - [stu:5d6941] ğŸ“¡ Stream active (200)
2025-12-16 08:36:34,217 - src.llm.client - INFO - [stu:5d6941] Starting stream parsing, waiting for first chunk...
2025-12-16 08:36:36,224 - src.llm.client - INFO - [stu:5d6941] ğŸ“Š 2.0s: 789c @393c/s (144ch, ~197t @98t/s)
2025-12-16 08:36:38,234 - src.llm.client - INFO - [stu:5d6941] ğŸ“Š 4.0s: 1482c @369c/s (287ch, ~370t @92t/s)
2025-12-16 08:36:40,247 - src.llm.client - INFO - [stu:5d6941] ğŸ“Š 6.0s: 2045c @339c/s (430ch, ~511t @85t/s)
2025-12-16 08:36:42,250 - src.llm.client - INFO - [stu:5d6941] ğŸ“Š 8.0s: 2672c @333c/s (572ch, ~668t @83t/s)
2025-12-16 08:36:44,253 - src.llm.client - INFO - [stu:5d6941] ğŸ“Š 10.0s: 3347c @334c/s (714ch, ~837t @83t/s)
2025-12-16 08:36:46,264 - src.llm.client - INFO - [stu:5d6941] ğŸ“Š 12.0s: 3943c @327c/s (856ch, ~986t @82t/s)
2025-12-16 08:36:46,370 - src.llm.client - INFO - [stu:5d6941] âœ“ Done 13.44s: 3965c (~607w @295c/s)
2025-12-16 08:36:46,370 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:36:46,370 - src.generate.formats.study_notes - INFO -     - Length: 4039 chars, 620 words
2025-12-16 08:36:46,370 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:36:46,370 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-16 08:36:46,370 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 6 bullets
2025-12-16 08:36:46,370 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:36:46,372 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:36:46,372 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:36:46,373 - src.generate.formats.diagrams - INFO - Generating diagram for: Conditional Probability revisited (Bayesâ€™ Theorem â€“ Derivation & Intuition)
2025-12-16 08:36:46,373 - src.generate.formats.diagrams - INFO - Generating diagram for: Derivation steps (Bayesâ€™ Theorem â€“ Derivation & Intuition)
2025-12-16 08:36:46,373 - src.llm.client - INFO - [dia:4c1ac1] ğŸš€ dia | m=gemma3:4b | p=5797c | t=120s
2025-12-16 08:36:46,373 - src.llm.client - INFO - [dia:9f7379] ğŸš€ dia | m=gemma3:4b | p=5763c | t=120s
2025-12-16 08:36:46,373 - src.llm.client - INFO - [dia:4c1ac1] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:36:46,373 - src.llm.client - INFO - [dia:9f7379] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:36:46,373 - src.llm.client - INFO - [dia:4c1ac1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:36:46,373 - src.llm.client - INFO - [dia:9f7379] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:36:46,375 - src.llm.client - INFO - [dia:9f7379] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11082 bytes, prompt=5763 chars
2025-12-16 08:36:46,375 - src.llm.client - INFO - [dia:4c1ac1] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11116 bytes, prompt=5797 chars
2025-12-16 08:36:46,375 - src.llm.client - INFO - [dia:9f7379] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:36:46,375 - src.llm.client - INFO - [dia:4c1ac1] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:36:48,104 - src.llm.request_handler - INFO - [dia:9f7379] âœ“ Done 1.73s
2025-12-16 08:36:48,105 - src.llm.client - INFO - [dia:9f7379] âœ… HTTP 200 in 1.73s
2025-12-16 08:36:48,105 - src.llm.client - INFO - [dia:9f7379] ğŸ“¡ Stream active (200)
2025-12-16 08:36:48,105 - src.llm.client - INFO - [dia:9f7379] Starting stream parsing, waiting for first chunk...
2025-12-16 08:36:50,118 - src.llm.client - INFO - [dia:9f7379] ğŸ“Š 2.0s: 389c @193c/s (143ch, ~97t @48t/s)
2025-12-16 08:36:52,118 - src.llm.client - INFO - [dia:9f7379] ğŸ“Š 4.0s: 709c @177c/s (285ch, ~177t @44t/s)
2025-12-16 08:36:52,189 - src.llm.client - INFO - [dia:9f7379] âœ“ Done 5.82s: 715c (~101w @123c/s)
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Derivation steps (Bayesâ€™ Theorem â€“ Derivation & Intuition):
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO -     - Length: 582 chars (cleaned: 582 chars)
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 38 total (nodes: 16, connections: 22) âš ï¸
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 2 long nodes) âš ï¸
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO -   Cleanup summary: 5 issues fixed (code fences, style commands, etc.)
2025-12-16 08:36:52,190 - src.generate.formats.diagrams - INFO - Generated diagram: 582 characters
2025-12-16 08:36:53,752 - src.llm.request_handler - INFO - [dia:4c1ac1] âœ“ Done 7.38s
2025-12-16 08:36:53,752 - src.llm.client - INFO - [dia:4c1ac1] âœ… HTTP 200 in 7.38s
2025-12-16 08:36:53,752 - src.llm.client - INFO - [dia:4c1ac1] ğŸ“¡ Stream active (200)
2025-12-16 08:36:53,752 - src.llm.client - INFO - [dia:4c1ac1] Starting stream parsing, waiting for first chunk...
2025-12-16 08:36:55,766 - src.llm.client - INFO - [dia:4c1ac1] ğŸ“Š 2.0s: 353c @175c/s (143ch, ~88t @44t/s)
2025-12-16 08:36:57,769 - src.llm.client - INFO - [dia:4c1ac1] ğŸ“Š 4.0s: 743c @185c/s (284ch, ~186t @46t/s)
2025-12-16 08:36:59,772 - src.llm.client - INFO - [dia:4c1ac1] ğŸ“Š 6.0s: 1066c @177c/s (426ch, ~266t @44t/s)
2025-12-16 08:37:00,792 - src.llm.client - INFO - [dia:4c1ac1] âœ“ Done 14.42s: 1218c (~158w @84c/s)
2025-12-16 08:37:00,792 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Conditional Probability revisited (Bayesâ€™ Theorem â€“ Derivation & Intuition):
2025-12-16 08:37:00,792 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:37:00,792 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - INFO -     - Length: 693 chars (cleaned: 693 chars)
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 45 total (nodes: 19, connections: 26) âš ï¸
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 08:37:00,793 - src.generate.formats.diagrams - INFO - Generated diagram: 693 characters
2025-12-16 08:37:00,793 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:37:00,793 - src.generate.formats.questions - INFO - Generating 10 questions for: Bayesâ€™ Theorem â€“ Derivation & Intuition (Session 3)
2025-12-16 08:37:00,794 - src.llm.client - INFO - [qst:fe9019] ğŸš€ qst | m=gemma3:4b | p=7357c | t=150s
2025-12-16 08:37:00,794 - src.llm.client - INFO - [qst:fe9019] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:37:00,794 - src.llm.client - INFO - [qst:fe9019] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:37:00,795 - src.llm.client - INFO - [qst:fe9019] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11131 bytes, prompt=7357 chars
2025-12-16 08:37:00,795 - src.llm.client - INFO - [qst:fe9019] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:37:02,800 - src.llm.request_handler - INFO - [qst:fe9019] âœ“ Done 2.00s
2025-12-16 08:37:02,800 - src.llm.client - INFO - [qst:fe9019] âœ… HTTP 200 in 2.00s
2025-12-16 08:37:02,800 - src.llm.client - INFO - [qst:fe9019] ğŸ“¡ Stream active (200)
2025-12-16 08:37:02,800 - src.llm.client - INFO - [qst:fe9019] Starting stream parsing, waiting for first chunk...
2025-12-16 08:37:04,804 - src.llm.client - INFO - [qst:fe9019] ğŸ“Š 2.0s: 696c @347c/s (142ch, ~174t @87t/s)
2025-12-16 08:37:06,816 - src.llm.client - INFO - [qst:fe9019] ğŸ“Š 4.0s: 1223c @305c/s (284ch, ~306t @76t/s)
2025-12-16 08:37:08,827 - src.llm.client - INFO - [qst:fe9019] ğŸ“Š 6.0s: 1653c @274c/s (426ch, ~413t @69t/s)
2025-12-16 08:37:10,838 - src.llm.client - INFO - [qst:fe9019] ğŸ“Š 8.0s: 2256c @281c/s (568ch, ~564t @70t/s)
2025-12-16 08:37:12,843 - src.llm.client - INFO - [qst:fe9019] ğŸ“Š 10.0s: 3034c @302c/s (709ch, ~758t @76t/s)
2025-12-16 08:37:14,288 - src.llm.client - INFO - [qst:fe9019] âœ“ Done 13.49s: 3401c (~517w @252c/s)
2025-12-16 08:37:14,289 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 0, 'mc_options_fixed': 1, 'total_fixes': 1}
2025-12-16 08:37:14,289 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING - [CRITICAL] Format Issue: Only 8/9 questions end with '?' (1 missing question marks - ensure questions are properly formatted) ğŸ”´
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Context: Module 3 Session 3
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Impact: Questions may not be properly formatted for parsing
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Recommendation: Ensure all questions end with '?' and use **Question N:** format
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Context: Module 3 Session 3
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 3 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Context: Module 3 Session 3
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING - Only 9 questions detected (expected 10) for Bayesâ€™ Theorem â€“ Derivation & Intuition (Session 3)
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 3 issues
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Bayesâ€™ Theorem â€“ Derivation & Intuition (Session 3)
2025-12-16 08:37:14,289 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:37:14,291 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:37:14,293 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 3 completed
2025-12-16 08:37:14,293 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:37:14,293 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:37:14,293 - src.generate.orchestration.pipeline - INFO - Module 4: Bayesian Inference â€“ Model Specification (1 sessions)
2025-12-16 08:37:14,294 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:37:14,294 - src.generate.orchestration.pipeline - INFO - 
[4/15] Session 4: Model Selection Criteria
2025-12-16 08:37:14,294 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:37:14,294 - src.generate.formats.lectures - INFO - Generating lecture for: Bayesian Inference â€“ Model Specification (Session 4/15)
2025-12-16 08:37:14,294 - src.llm.client - INFO - [lec:b2b6e4] ğŸš€ lec | m=gemma3:4b | p=3103c | t=180s
2025-12-16 08:37:14,294 - src.llm.client - INFO - [lec:b2b6e4] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:37:14,294 - src.llm.client - INFO - [lec:b2b6e4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:37:14,296 - src.llm.client - INFO - [lec:b2b6e4] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6745 bytes, prompt=3103 chars
2025-12-16 08:37:14,296 - src.llm.client - INFO - [lec:b2b6e4] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:37:15,271 - src.llm.request_handler - INFO - [lec:b2b6e4] âœ“ Done 0.97s
2025-12-16 08:37:15,271 - src.llm.client - INFO - [lec:b2b6e4] âœ… HTTP 200 in 0.98s
2025-12-16 08:37:15,271 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“¡ Stream active (200)
2025-12-16 08:37:15,271 - src.llm.client - INFO - [lec:b2b6e4] Starting stream parsing, waiting for first chunk...
2025-12-16 08:37:17,281 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 2.0s: 823c @410c/s (145ch, ~206t @102t/s)
2025-12-16 08:37:19,285 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 4.0s: 1591c @396c/s (288ch, ~398t @99t/s)
2025-12-16 08:37:21,296 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 6.0s: 2305c @383c/s (431ch, ~576t @96t/s)
2025-12-16 08:37:23,309 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 8.0s: 2941c @366c/s (574ch, ~735t @91t/s)
2025-12-16 08:37:25,320 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 10.0s: 3606c @359c/s (717ch, ~902t @90t/s)
2025-12-16 08:37:27,322 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 12.1s: 4180c @347c/s (859ch, ~1045t @87t/s)
2025-12-16 08:37:29,324 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 14.1s: 4784c @340c/s (1001ch, ~1196t @85t/s)
2025-12-16 08:37:31,332 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 16.1s: 5337c @332c/s (1143ch, ~1334t @83t/s)
2025-12-16 08:37:33,338 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 18.1s: 6099c @338c/s (1285ch, ~1525t @84t/s)
2025-12-16 08:37:35,339 - src.llm.client - INFO - [lec:b2b6e4] ğŸ“Š 20.1s: 6898c @344c/s (1426ch, ~1724t @86t/s)
2025-12-16 08:37:35,825 - src.llm.client - INFO - [lec:b2b6e4] âœ“ Done 21.53s: 7092c (~1112w @329c/s)
2025-12-16 08:37:35,826 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:37:35,826 - src.generate.formats.lectures - INFO -     - Length: 7196 chars, 1126 words
2025-12-16 08:37:35,826 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:37:35,826 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:37:35,826 - src.generate.formats.lectures - INFO -     - Content: 12 examples, 2 terms defined
2025-12-16 08:37:35,826 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:37:35,829 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:37:35,830 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:37:35,830 - src.generate.formats.labs - INFO - Generating lab 4 for: Bayesian Inference â€“ Model Specification (Session 4)
2025-12-16 08:37:35,830 - src.llm.client - INFO - [lab:98b7f8] ğŸš€ lab | m=gemma3:4b | p=3343c | t=150s
2025-12-16 08:37:35,830 - src.llm.client - INFO - [lab:98b7f8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:37:35,830 - src.llm.client - INFO - [lab:98b7f8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:37:35,832 - src.llm.client - INFO - [lab:98b7f8] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3762 bytes, prompt=3343 chars
2025-12-16 08:37:35,832 - src.llm.client - INFO - [lab:98b7f8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:37:36,714 - src.llm.request_handler - INFO - [lab:98b7f8] âœ“ Done 0.88s
2025-12-16 08:37:36,715 - src.llm.client - INFO - [lab:98b7f8] âœ… HTTP 200 in 0.88s
2025-12-16 08:37:36,715 - src.llm.client - INFO - [lab:98b7f8] ğŸ“¡ Stream active (200)
2025-12-16 08:37:36,715 - src.llm.client - INFO - [lab:98b7f8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:37:38,724 - src.llm.client - INFO - [lab:98b7f8] ğŸ“Š 2.0s: 755c @376c/s (145ch, ~189t @94t/s)
2025-12-16 08:37:40,725 - src.llm.client - INFO - [lab:98b7f8] ğŸ“Š 4.0s: 1423c @355c/s (289ch, ~356t @89t/s)
2025-12-16 08:37:42,732 - src.llm.client - INFO - [lab:98b7f8] ğŸ“Š 6.0s: 2101c @349c/s (432ch, ~525t @87t/s)
2025-12-16 08:37:44,741 - src.llm.client - INFO - [lab:98b7f8] ğŸ“Š 8.0s: 2736c @341c/s (575ch, ~684t @85t/s)
2025-12-16 08:37:46,750 - src.llm.client - INFO - [lab:98b7f8] ğŸ“Š 10.0s: 3358c @335c/s (718ch, ~840t @84t/s)
2025-12-16 08:37:48,750 - src.llm.client - INFO - [lab:98b7f8] ğŸ“Š 12.0s: 3843c @319c/s (860ch, ~961t @80t/s)
2025-12-16 08:37:50,764 - src.llm.client - INFO - [lab:98b7f8] ğŸ“Š 14.0s: 4499c @320c/s (1003ch, ~1125t @80t/s)
2025-12-16 08:37:52,006 - src.llm.client - INFO - [lab:98b7f8] âœ“ Done 16.18s: 4925c (~734w @304c/s)
2025-12-16 08:37:52,006 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:37:52,006 - src.generate.formats.labs - INFO -     - Length: 5035 chars, 751 words
2025-12-16 08:37:52,006 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:37:52,006 - src.generate.formats.labs - INFO -     - Safety: 4 warnings
2025-12-16 08:37:52,006 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 08:37:52,009 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:37:52,009 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:37:52,009 - src.generate.formats.study_notes - INFO - Generating study notes for: Bayesian Inference â€“ Model Specification (Session 4)
2025-12-16 08:37:52,009 - src.llm.client - INFO - [stu:659dce] ğŸš€ stu | m=gemma3:4b | p=4450c | t=120s
2025-12-16 08:37:52,009 - src.llm.client - INFO - [stu:659dce] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:37:52,009 - src.llm.client - INFO - [stu:659dce] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:37:52,011 - src.llm.client - INFO - [stu:659dce] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8084 bytes, prompt=4450 chars
2025-12-16 08:37:52,011 - src.llm.client - INFO - [stu:659dce] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:37:53,213 - src.llm.request_handler - INFO - [stu:659dce] âœ“ Done 1.20s
2025-12-16 08:37:53,214 - src.llm.client - INFO - [stu:659dce] âœ… HTTP 200 in 1.20s
2025-12-16 08:37:53,214 - src.llm.client - INFO - [stu:659dce] ğŸ“¡ Stream active (200)
2025-12-16 08:37:53,214 - src.llm.client - INFO - [stu:659dce] Starting stream parsing, waiting for first chunk...
2025-12-16 08:37:55,220 - src.llm.client - INFO - [stu:659dce] ğŸ“Š 2.0s: 736c @367c/s (143ch, ~184t @92t/s)
2025-12-16 08:37:57,234 - src.llm.client - INFO - [stu:659dce] ğŸ“Š 4.0s: 1527c @380c/s (287ch, ~382t @95t/s)
2025-12-16 08:37:59,236 - src.llm.client - INFO - [stu:659dce] ğŸ“Š 6.0s: 2230c @370c/s (428ch, ~558t @93t/s)
2025-12-16 08:38:00,784 - src.llm.client - INFO - [stu:659dce] âœ“ Done 8.78s: 2832c (~412w @323c/s)
2025-12-16 08:38:00,785 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:38:00,785 - src.generate.formats.study_notes - INFO -     - Length: 2907 chars, 424 words
2025-12-16 08:38:00,785 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:38:00,785 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-16 08:38:00,785 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 08:38:00,785 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:38:00,786 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:38:00,787 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:38:00,787 - src.generate.formats.diagrams - INFO - Generating diagram for: Likelihood ratio test (Bayesian Inference â€“ Model Specification)
2025-12-16 08:38:00,787 - src.generate.formats.diagrams - INFO - Generating diagram for: AIC (Bayesian Inference â€“ Model Specification)
2025-12-16 08:38:00,787 - src.generate.formats.diagrams - INFO - Generating diagram for: BIC (Bayesian Inference â€“ Model Specification)
2025-12-16 08:38:00,787 - src.llm.client - INFO - [dia:517c03] ğŸš€ dia | m=gemma3:4b | p=5773c | t=120s
2025-12-16 08:38:00,787 - src.llm.client - INFO - [dia:8c6b21] ğŸš€ dia | m=gemma3:4b | p=5737c | t=120s
2025-12-16 08:38:00,787 - src.llm.client - INFO - [dia:f74c72] ğŸš€ dia | m=gemma3:4b | p=5737c | t=120s
2025-12-16 08:38:00,788 - src.llm.client - INFO - [dia:517c03] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:38:00,788 - src.llm.client - INFO - [dia:8c6b21] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:38:00,788 - src.llm.client - INFO - [dia:f74c72] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:38:00,788 - src.llm.client - INFO - [dia:517c03] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:38:00,788 - src.llm.client - INFO - [dia:8c6b21] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:38:00,788 - src.llm.client - INFO - [dia:f74c72] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:38:00,790 - src.llm.client - INFO - [dia:517c03] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11087 bytes, prompt=5773 chars
2025-12-16 08:38:00,790 - src.llm.client - INFO - [dia:517c03] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:38:00,790 - src.llm.client - INFO - [dia:8c6b21] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11051 bytes, prompt=5737 chars
2025-12-16 08:38:00,790 - src.llm.client - INFO - [dia:f74c72] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11051 bytes, prompt=5737 chars
2025-12-16 08:38:00,790 - src.llm.client - INFO - [dia:8c6b21] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:38:00,790 - src.llm.client - INFO - [dia:f74c72] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:38:02,480 - src.llm.request_handler - INFO - [dia:f74c72] âœ“ Done 1.69s
2025-12-16 08:38:02,480 - src.llm.client - INFO - [dia:f74c72] âœ… HTTP 200 in 1.69s
2025-12-16 08:38:02,480 - src.llm.client - INFO - [dia:f74c72] ğŸ“¡ Stream active (200)
2025-12-16 08:38:02,480 - src.llm.client - INFO - [dia:f74c72] Starting stream parsing, waiting for first chunk...
2025-12-16 08:38:04,494 - src.llm.client - INFO - [dia:f74c72] ğŸ“Š 2.0s: 466c @231c/s (143ch, ~116t @58t/s)
2025-12-16 08:38:06,054 - src.llm.client - INFO - [dia:f74c72] âœ“ Done 5.27s: 767c (~79w @146c/s)
2025-12-16 08:38:06,054 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for BIC (Bayesian Inference â€“ Model Specification):
2025-12-16 08:38:06,054 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:38:06,054 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:38:06,055 - src.generate.formats.diagrams - INFO -     - Length: 752 chars (cleaned: 752 chars)
2025-12-16 08:38:06,055 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:38:06,055 - src.generate.formats.diagrams - INFO - [OK] Elements: 34 total (nodes: 25, connections: 9) âœ“
2025-12-16 08:38:06,055 - src.generate.formats.diagrams - INFO -   Cleanup summary: 1 issues fixed (code fences, style commands, etc.)
2025-12-16 08:38:06,055 - src.generate.formats.diagrams - INFO - Generated diagram: 752 characters
2025-12-16 08:38:07,564 - src.llm.request_handler - INFO - [dia:517c03] âœ“ Done 6.77s
2025-12-16 08:38:07,564 - src.llm.client - INFO - [dia:517c03] âœ… HTTP 200 in 6.77s
2025-12-16 08:38:07,564 - src.llm.client - INFO - [dia:517c03] ğŸ“¡ Stream active (200)
2025-12-16 08:38:07,564 - src.llm.client - INFO - [dia:517c03] Starting stream parsing, waiting for first chunk...
2025-12-16 08:38:09,577 - src.llm.client - INFO - [dia:517c03] ğŸ“Š 2.0s: 517c @257c/s (143ch, ~129t @64t/s)
2025-12-16 08:38:09,967 - src.llm.client - INFO - [dia:517c03] âœ“ Done 9.18s: 571c (~81w @62c/s)
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Likelihood ratio test (Bayesian Inference â€“ Model Specification):
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO -     - Length: 522 chars (cleaned: 522 chars)
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO - [OK] Elements: 31 total (nodes: 12, connections: 19) âœ“
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:38:09,968 - src.generate.formats.diagrams - INFO - Generated diagram: 522 characters
2025-12-16 08:38:11,483 - src.llm.request_handler - INFO - [dia:8c6b21] âœ“ Done 10.69s
2025-12-16 08:38:11,483 - src.llm.client - INFO - [dia:8c6b21] âœ… HTTP 200 in 10.69s
2025-12-16 08:38:11,483 - src.llm.client - INFO - [dia:8c6b21] ğŸ“¡ Stream active (200)
2025-12-16 08:38:11,483 - src.llm.client - INFO - [dia:8c6b21] Starting stream parsing, waiting for first chunk...
2025-12-16 08:38:13,497 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 2.0s: 628c @312c/s (143ch, ~157t @78t/s)
2025-12-16 08:38:15,499 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 4.0s: 1228c @306c/s (285ch, ~307t @76t/s)
2025-12-16 08:38:17,499 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 6.0s: 1569c @261c/s (427ch, ~392t @65t/s)
2025-12-16 08:38:19,512 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 8.0s: 1912c @238c/s (570ch, ~478t @60t/s)
2025-12-16 08:38:21,517 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 10.0s: 2253c @225c/s (712ch, ~563t @56t/s)
2025-12-16 08:38:23,528 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 12.0s: 2592c @215c/s (854ch, ~648t @54t/s)
2025-12-16 08:38:25,539 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 14.1s: 2933c @209c/s (996ch, ~733t @52t/s)
2025-12-16 08:38:27,553 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 16.1s: 3275c @204c/s (1138ch, ~819t @51t/s)
2025-12-16 08:38:29,566 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 18.1s: 3616c @200c/s (1280ch, ~904t @50t/s)
2025-12-16 08:38:31,572 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 20.1s: 3953c @197c/s (1421ch, ~988t @49t/s)
2025-12-16 08:38:33,577 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 22.1s: 4293c @194c/s (1562ch, ~1073t @49t/s)
2025-12-16 08:38:35,590 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 24.1s: 4632c @192c/s (1704ch, ~1158t @48t/s)
2025-12-16 08:38:37,595 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 26.1s: 4972c @190c/s (1845ch, ~1243t @48t/s)
2025-12-16 08:38:39,607 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 28.1s: 5309c @189c/s (1986ch, ~1327t @47t/s)
2025-12-16 08:38:41,622 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 30.1s: 5649c @187c/s (2127ch, ~1412t @47t/s)
2025-12-16 08:38:43,637 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 32.2s: 5987c @186c/s (2268ch, ~1497t @47t/s)
2025-12-16 08:38:45,638 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 34.2s: 6323c @185c/s (2408ch, ~1581t @46t/s)
2025-12-16 08:38:47,652 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 36.2s: 6660c @184c/s (2549ch, ~1665t @46t/s)
2025-12-16 08:38:49,654 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 38.2s: 6988c @183c/s (2685ch, ~1747t @46t/s)
2025-12-16 08:38:51,665 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 40.2s: 7329c @182c/s (2827ch, ~1832t @46t/s)
2025-12-16 08:38:53,666 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 42.2s: 7661c @182c/s (2966ch, ~1915t @45t/s)
2025-12-16 08:38:55,669 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 44.2s: 8001c @181c/s (3107ch, ~2000t @45t/s)
2025-12-16 08:38:57,676 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 46.2s: 8337c @180c/s (3247ch, ~2084t @45t/s)
2025-12-16 08:38:59,683 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 48.2s: 8675c @180c/s (3388ch, ~2169t @45t/s)
2025-12-16 08:39:01,687 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 50.2s: 9012c @180c/s (3529ch, ~2253t @45t/s)
2025-12-16 08:39:03,699 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 52.2s: 9352c @179c/s (3670ch, ~2338t @45t/s)
2025-12-16 08:39:05,706 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 54.2s: 9689c @179c/s (3811ch, ~2422t @45t/s)
2025-12-16 08:39:07,720 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 56.2s: 10029c @178c/s (3952ch, ~2507t @45t/s)
2025-12-16 08:39:09,724 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 58.2s: 10367c @178c/s (4093ch, ~2592t @45t/s)
2025-12-16 08:39:11,731 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 60.2s: 10704c @178c/s (4234ch, ~2676t @44t/s)
2025-12-16 08:39:13,741 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 62.3s: 11039c @177c/s (4373ch, ~2760t @44t/s)
2025-12-16 08:39:15,755 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 64.3s: 11376c @177c/s (4514ch, ~2844t @44t/s)
2025-12-16 08:39:17,759 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 66.3s: 11716c @177c/s (4655ch, ~2929t @44t/s)
2025-12-16 08:39:19,770 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 68.3s: 12059c @177c/s (4798ch, ~3015t @44t/s)
2025-12-16 08:39:21,771 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 70.3s: 12401c @176c/s (4941ch, ~3100t @44t/s)
2025-12-16 08:39:23,773 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 72.3s: 12743c @176c/s (5083ch, ~3186t @44t/s)
2025-12-16 08:39:25,774 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 74.3s: 13084c @176c/s (5225ch, ~3271t @44t/s)
2025-12-16 08:39:27,777 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 76.3s: 13425c @176c/s (5367ch, ~3356t @44t/s)
2025-12-16 08:39:29,785 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 78.3s: 13768c @176c/s (5510ch, ~3442t @44t/s)
2025-12-16 08:39:31,798 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 80.3s: 14111c @176c/s (5653ch, ~3528t @44t/s)
2025-12-16 08:39:33,811 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 82.3s: 14453c @176c/s (5796ch, ~3613t @44t/s)
2025-12-16 08:39:35,819 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 84.3s: 14796c @175c/s (5939ch, ~3699t @44t/s)
2025-12-16 08:39:37,830 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 86.3s: 15136c @175c/s (6080ch, ~3784t @44t/s)
2025-12-16 08:39:39,837 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 88.4s: 15472c @175c/s (6220ch, ~3868t @44t/s)
2025-12-16 08:39:41,840 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 90.4s: 15808c @175c/s (6360ch, ~3952t @44t/s)
2025-12-16 08:39:43,843 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 92.4s: 16144c @175c/s (6500ch, ~4036t @44t/s)
2025-12-16 08:39:45,854 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 94.4s: 16481c @175c/s (6641ch, ~4120t @44t/s)
2025-12-16 08:39:47,865 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 96.4s: 16821c @175c/s (6782ch, ~4205t @44t/s)
2025-12-16 08:39:49,869 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 98.4s: 17160c @174c/s (6924ch, ~4290t @44t/s)
2025-12-16 08:39:51,876 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 100.4s: 17501c @174c/s (7066ch, ~4375t @44t/s)
2025-12-16 08:39:53,887 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 102.4s: 17843c @174c/s (7208ch, ~4461t @44t/s)
2025-12-16 08:39:55,899 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 104.4s: 18180c @174c/s (7349ch, ~4545t @44t/s)
2025-12-16 08:39:57,912 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 106.4s: 18520c @174c/s (7490ch, ~4630t @44t/s)
2025-12-16 08:39:59,921 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 108.4s: 18852c @174c/s (7629ch, ~4713t @43t/s)
2025-12-16 08:40:01,927 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 110.4s: 19192c @174c/s (7770ch, ~4798t @43t/s)
2025-12-16 08:40:03,928 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 112.4s: 19529c @174c/s (7911ch, ~4882t @43t/s)
2025-12-16 08:40:05,938 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 114.5s: 19871c @174c/s (8053ch, ~4968t @43t/s)
2025-12-16 08:40:07,939 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 116.5s: 20208c @174c/s (8194ch, ~5052t @43t/s)
2025-12-16 08:40:09,941 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 118.5s: 20548c @173c/s (8335ch, ~5137t @43t/s)
2025-12-16 08:40:11,948 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 120.5s: 20885c @173c/s (8476ch, ~5221t @43t/s)
2025-12-16 08:40:13,949 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 122.5s: 21221c @173c/s (8616ch, ~5305t @43t/s)
2025-12-16 08:40:15,949 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 124.5s: 21557c @173c/s (8756ch, ~5389t @43t/s)
2025-12-16 08:40:17,960 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 126.5s: 21899c @173c/s (8898ch, ~5475t @43t/s)
2025-12-16 08:40:19,969 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 128.5s: 22241c @173c/s (9041ch, ~5560t @43t/s)
2025-12-16 08:40:21,979 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 130.5s: 22583c @173c/s (9183ch, ~5646t @43t/s)
2025-12-16 08:40:23,988 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 132.5s: 22924c @173c/s (9325ch, ~5731t @43t/s)
2025-12-16 08:40:25,999 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 134.5s: 23265c @173c/s (9467ch, ~5816t @43t/s)
2025-12-16 08:40:28,001 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 136.5s: 23604c @173c/s (9609ch, ~5901t @43t/s)
2025-12-16 08:40:30,003 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 138.5s: 23945c @173c/s (9751ch, ~5986t @43t/s)
2025-12-16 08:40:32,009 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 140.5s: 24287c @173c/s (9893ch, ~6072t @43t/s)
2025-12-16 08:40:34,015 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 142.5s: 24628c @173c/s (10035ch, ~6157t @43t/s)
2025-12-16 08:40:35,486 - src.llm.client - INFO - [dia:8c6b21] Stream making progress - extending timeout by 60.0s (new limit: 240.0s, max: 240.0s)
2025-12-16 08:40:36,026 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 144.5s: 24964c @173c/s (10175ch, ~6241t @43t/s)
2025-12-16 08:40:38,033 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 146.5s: 25296c @173c/s (10314ch, ~6324t @43t/s)
2025-12-16 08:40:40,037 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 148.6s: 25625c @172c/s (10451ch, ~6406t @43t/s)
2025-12-16 08:40:42,038 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 150.6s: 25961c @172c/s (10591ch, ~6490t @43t/s)
2025-12-16 08:40:44,039 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 152.6s: 26297c @172c/s (10731ch, ~6574t @43t/s)
2025-12-16 08:40:46,052 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 154.6s: 26632c @172c/s (10870ch, ~6658t @43t/s)
2025-12-16 08:40:48,055 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 156.6s: 26969c @172c/s (11011ch, ~6742t @43t/s)
2025-12-16 08:40:50,057 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 158.6s: 27309c @172c/s (11152ch, ~6827t @43t/s)
2025-12-16 08:40:52,062 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 160.6s: 27648c @172c/s (11294ch, ~6912t @43t/s)
2025-12-16 08:40:54,074 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 162.6s: 27984c @172c/s (11434ch, ~6996t @43t/s)
2025-12-16 08:40:56,086 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 164.6s: 28325c @172c/s (11576ch, ~7081t @43t/s)
2025-12-16 08:40:58,093 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 166.6s: 28667c @172c/s (11718ch, ~7167t @43t/s)
2025-12-16 08:41:00,105 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 168.6s: 29003c @172c/s (11858ch, ~7251t @43t/s)
2025-12-16 08:41:02,111 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 170.6s: 29340c @172c/s (11999ch, ~7335t @43t/s)
2025-12-16 08:41:04,115 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 172.6s: 29681c @172c/s (12141ch, ~7420t @43t/s)
2025-12-16 08:41:06,121 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 174.6s: 30033c @172c/s (12287ch, ~7508t @43t/s)
2025-12-16 08:41:08,126 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 176.6s: 30376c @172c/s (12430ch, ~7594t @43t/s)
2025-12-16 08:41:10,131 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 178.6s: 30719c @172c/s (12573ch, ~7680t @43t/s)
2025-12-16 08:41:12,135 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 180.7s: 31056c @172c/s (12714ch, ~7764t @43t/s)
2025-12-16 08:41:14,140 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 182.7s: 31396c @172c/s (12855ch, ~7849t @43t/s)
2025-12-16 08:41:16,152 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 184.7s: 31737c @172c/s (12997ch, ~7934t @43t/s)
2025-12-16 08:41:18,158 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 186.7s: 32080c @172c/s (13140ch, ~8020t @43t/s)
2025-12-16 08:41:20,163 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 188.7s: 32417c @172c/s (13281ch, ~8104t @43t/s)
2025-12-16 08:41:22,169 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 190.7s: 32760c @172c/s (13424ch, ~8190t @43t/s)
2025-12-16 08:41:24,182 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 192.7s: 33105c @172c/s (13567ch, ~8276t @43t/s)
2025-12-16 08:41:26,190 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 194.7s: 33448c @172c/s (13710ch, ~8362t @43t/s)
2025-12-16 08:41:28,193 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 196.7s: 33791c @172c/s (13853ch, ~8448t @43t/s)
2025-12-16 08:41:30,205 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 198.7s: 34139c @172c/s (13998ch, ~8535t @43t/s)
2025-12-16 08:41:32,210 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 200.7s: 34481c @172c/s (14141ch, ~8620t @43t/s)
2025-12-16 08:41:34,221 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 202.7s: 34821c @172c/s (14282ch, ~8705t @43t/s)
2025-12-16 08:41:36,235 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 204.8s: 35159c @172c/s (14423ch, ~8790t @43t/s)
2025-12-16 08:41:38,244 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 206.8s: 35496c @172c/s (14564ch, ~8874t @43t/s)
2025-12-16 08:41:40,248 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 208.8s: 35832c @172c/s (14704ch, ~8958t @43t/s)
2025-12-16 08:41:42,249 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 210.8s: 36172c @172c/s (14845ch, ~9043t @43t/s)
2025-12-16 08:41:44,258 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 212.8s: 36513c @172c/s (14987ch, ~9128t @43t/s)
2025-12-16 08:41:46,271 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 214.8s: 36856c @172c/s (15130ch, ~9214t @43t/s)
2025-12-16 08:41:48,274 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 216.8s: 37199c @172c/s (15273ch, ~9300t @43t/s)
2025-12-16 08:41:50,275 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 218.8s: 37540c @172c/s (15415ch, ~9385t @43t/s)
2025-12-16 08:41:52,285 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 220.8s: 37883c @172c/s (15558ch, ~9471t @43t/s)
2025-12-16 08:41:54,297 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 222.8s: 38224c @172c/s (15700ch, ~9556t @43t/s)
2025-12-16 08:41:56,312 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 224.8s: 38567c @172c/s (15843ch, ~9642t @43t/s)
2025-12-16 08:41:58,313 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 226.8s: 38908c @172c/s (15985ch, ~9727t @43t/s)
2025-12-16 08:42:00,321 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 228.8s: 39245c @171c/s (16126ch, ~9811t @43t/s)
2025-12-16 08:42:02,329 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 230.8s: 39587c @171c/s (16268ch, ~9897t @43t/s)
2025-12-16 08:42:04,336 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 232.9s: 39928c @171c/s (16410ch, ~9982t @43t/s)
2025-12-16 08:42:06,349 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 234.9s: 40269c @171c/s (16552ch, ~10067t @43t/s)
2025-12-16 08:42:08,351 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 236.9s: 40607c @171c/s (16693ch, ~10152t @43t/s)
2025-12-16 08:42:10,356 - src.llm.client - INFO - [dia:8c6b21] ğŸ“Š 238.9s: 40948c @171c/s (16835ch, ~10237t @43t/s)
2025-12-16 08:42:11,495 - src.llm.client - ERROR - [dia:8c6b21] Stream timeout: 240.01s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 16918 chunks, 1598828 bytes, 41147 chars (~10287 tokens) before timeout. Performance: 171.4 chars/s, ~42.9 tok/s. Generation was slow (171.4 chars/s, ~42.9 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.
2025-12-16 08:42:11,496 - src.generate.orchestration.pipeline - WARNING -   Transient error in diagram 2 generation (attempt 1/3): [dia:8c6b21] Stream timeout: 240.01s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 16918 chunks, 1598828 bytes, 41147 chars (~10287 tokens) before timeout. Performance: 171.4 chars/s, ~42.9 tok/s. Generation was slow (171.4 chars/s, ~42.9 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.. Retrying in 2.0s...
2025-12-16 08:42:13,501 - src.generate.formats.diagrams - INFO - Generating diagram for: AIC (Bayesian Inference â€“ Model Specification)
2025-12-16 08:42:13,502 - src.llm.client - INFO - [dia:add0dd] ğŸš€ dia | m=gemma3:4b | p=5737c | t=120s
2025-12-16 08:42:13,503 - src.llm.client - INFO - [dia:add0dd] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:42:13,503 - src.llm.client - INFO - [dia:add0dd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:42:13,508 - src.llm.client - INFO - [dia:add0dd] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11051 bytes, prompt=5737 chars
2025-12-16 08:42:13,509 - src.llm.client - INFO - [dia:add0dd] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:42:15,211 - src.llm.request_handler - INFO - [dia:add0dd] âœ“ Done 1.70s
2025-12-16 08:42:15,212 - src.llm.client - INFO - [dia:add0dd] âœ… HTTP 200 in 1.70s
2025-12-16 08:42:15,212 - src.llm.client - INFO - [dia:add0dd] ğŸ“¡ Stream active (200)
2025-12-16 08:42:15,212 - src.llm.client - INFO - [dia:add0dd] Starting stream parsing, waiting for first chunk...
2025-12-16 08:42:17,216 - src.llm.client - INFO - [dia:add0dd] ğŸ“Š 2.0s: 504c @251c/s (144ch, ~126t @63t/s)
2025-12-16 08:42:19,224 - src.llm.client - INFO - [dia:add0dd] ğŸ“Š 4.0s: 876c @218c/s (288ch, ~219t @55t/s)
2025-12-16 08:42:19,311 - src.llm.client - INFO - [dia:add0dd] âœ“ Done 5.81s: 882c (~120w @152c/s)
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for AIC (Bayesian Inference â€“ Model Specification):
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO -     - Length: 759 chars (cleaned: 759 chars)
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO - [OK] Elements: 43 total (nodes: 19, connections: 24) âœ“
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:42:19,312 - src.generate.formats.diagrams - INFO - Generated diagram: 759 characters
2025-12-16 08:42:19,312 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:42:19,313 - src.generate.formats.questions - INFO - Generating 10 questions for: Bayesian Inference â€“ Model Specification (Session 4)
2025-12-16 08:42:19,313 - src.llm.client - INFO - [qst:ebfa2c] ğŸš€ qst | m=gemma3:4b | p=7341c | t=150s
2025-12-16 08:42:19,313 - src.llm.client - INFO - [qst:ebfa2c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:42:19,313 - src.llm.client - INFO - [qst:ebfa2c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:42:19,314 - src.llm.client - INFO - [qst:ebfa2c] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11016 bytes, prompt=7341 chars
2025-12-16 08:42:19,314 - src.llm.client - INFO - [qst:ebfa2c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:42:21,194 - src.llm.request_handler - INFO - [qst:ebfa2c] âœ“ Done 1.88s
2025-12-16 08:42:21,194 - src.llm.client - INFO - [qst:ebfa2c] âœ… HTTP 200 in 1.88s
2025-12-16 08:42:21,194 - src.llm.client - INFO - [qst:ebfa2c] ğŸ“¡ Stream active (200)
2025-12-16 08:42:21,194 - src.llm.client - INFO - [qst:ebfa2c] Starting stream parsing, waiting for first chunk...
2025-12-16 08:42:23,200 - src.llm.client - INFO - [qst:ebfa2c] ğŸ“Š 2.0s: 688c @343c/s (144ch, ~172t @86t/s)
2025-12-16 08:42:25,202 - src.llm.client - INFO - [qst:ebfa2c] ğŸ“Š 4.0s: 1373c @343c/s (289ch, ~343t @86t/s)
2025-12-16 08:42:27,210 - src.llm.client - INFO - [qst:ebfa2c] ğŸ“Š 6.0s: 2107c @350c/s (436ch, ~527t @88t/s)
2025-12-16 08:42:29,214 - src.llm.client - INFO - [qst:ebfa2c] ğŸ“Š 8.0s: 2872c @358c/s (581ch, ~718t @90t/s)
2025-12-16 08:42:31,227 - src.llm.client - INFO - [qst:ebfa2c] ğŸ“Š 10.0s: 3564c @355c/s (723ch, ~891t @89t/s)
2025-12-16 08:42:32,108 - src.llm.client - INFO - [qst:ebfa2c] âœ“ Done 12.80s: 3860c (~563w @302c/s)
2025-12-16 08:42:32,109 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 1, 'total_fixes': 2}
2025-12-16 08:42:32,109 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 2 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -     Context: Module 4 Session 4
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 4 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -     Context: Module 4 Session 4
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING - Only 9 questions detected (expected 10) for Bayesian Inference â€“ Model Specification (Session 4)
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Bayesian Inference â€“ Model Specification (Session 4)
2025-12-16 08:42:32,109 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:42:32,111 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:42:32,113 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 4 completed
2025-12-16 08:42:32,113 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:42:32,113 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:42:32,113 - src.generate.orchestration.pipeline - INFO - Module 5: Variational Inference â€“ Introduction (1 sessions)
2025-12-16 08:42:32,113 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:42:32,113 - src.generate.orchestration.pipeline - INFO - 
[5/15] Session 5: The Challenge of Exact Inference
2025-12-16 08:42:32,113 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:42:32,113 - src.generate.formats.lectures - INFO - Generating lecture for: Variational Inference â€“ Introduction (Session 5/15)
2025-12-16 08:42:32,113 - src.llm.client - INFO - [lec:9e539f] ğŸš€ lec | m=gemma3:4b | p=3128c | t=180s
2025-12-16 08:42:32,113 - src.llm.client - INFO - [lec:9e539f] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:42:32,113 - src.llm.client - INFO - [lec:9e539f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:42:32,115 - src.llm.client - INFO - [lec:9e539f] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6769 bytes, prompt=3128 chars
2025-12-16 08:42:32,115 - src.llm.client - INFO - [lec:9e539f] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:42:33,106 - src.llm.request_handler - INFO - [lec:9e539f] âœ“ Done 0.99s
2025-12-16 08:42:33,106 - src.llm.client - INFO - [lec:9e539f] âœ… HTTP 200 in 0.99s
2025-12-16 08:42:33,106 - src.llm.client - INFO - [lec:9e539f] ğŸ“¡ Stream active (200)
2025-12-16 08:42:33,106 - src.llm.client - INFO - [lec:9e539f] Starting stream parsing, waiting for first chunk...
2025-12-16 08:42:35,113 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 2.0s: 923c @460c/s (148ch, ~231t @115t/s)
2025-12-16 08:42:37,114 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 4.0s: 1649c @412c/s (294ch, ~412t @103t/s)
2025-12-16 08:42:39,123 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 6.0s: 2406c @400c/s (436ch, ~602t @100t/s)
2025-12-16 08:42:41,126 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 8.0s: 3126c @390c/s (584ch, ~782t @97t/s)
2025-12-16 08:42:43,129 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 10.0s: 3886c @388c/s (728ch, ~972t @97t/s)
2025-12-16 08:42:45,139 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 12.0s: 4504c @374c/s (871ch, ~1126t @94t/s)
2025-12-16 08:42:47,149 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 14.0s: 5008c @357c/s (1019ch, ~1252t @89t/s)
2025-12-16 08:42:49,151 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 16.0s: 5671c @353c/s (1163ch, ~1418t @88t/s)
2025-12-16 08:42:51,159 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 18.1s: 6441c @357c/s (1308ch, ~1610t @89t/s)
2025-12-16 08:42:53,173 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 20.1s: 7150c @356c/s (1455ch, ~1788t @89t/s)
2025-12-16 08:42:55,181 - src.llm.client - INFO - [lec:9e539f] ğŸ“Š 22.1s: 7931c @359c/s (1596ch, ~1983t @90t/s)
2025-12-16 08:42:55,688 - src.llm.client - INFO - [lec:9e539f] âœ“ Done 23.57s: 8123c (~1201w @345c/s)
2025-12-16 08:42:55,689 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:42:55,689 - src.generate.formats.lectures - INFO -     - Length: 8230 chars, 1215 words
2025-12-16 08:42:55,689 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:42:55,689 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:42:55,689 - src.generate.formats.lectures - INFO -     - Content: 13 examples, 0 terms defined
2025-12-16 08:42:55,689 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:42:55,692 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:42:55,693 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:42:55,693 - src.generate.formats.labs - INFO - Generating lab 5 for: Variational Inference â€“ Introduction (Session 5)
2025-12-16 08:42:55,693 - src.llm.client - INFO - [lab:f39e7e] ğŸš€ lab | m=gemma3:4b | p=3352c | t=150s
2025-12-16 08:42:55,693 - src.llm.client - INFO - [lab:f39e7e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:42:55,693 - src.llm.client - INFO - [lab:f39e7e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:42:55,694 - src.llm.client - INFO - [lab:f39e7e] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3817 bytes, prompt=3352 chars
2025-12-16 08:42:55,694 - src.llm.client - INFO - [lab:f39e7e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:42:56,549 - src.llm.request_handler - INFO - [lab:f39e7e] âœ“ Done 0.85s
2025-12-16 08:42:56,549 - src.llm.client - INFO - [lab:f39e7e] âœ… HTTP 200 in 0.85s
2025-12-16 08:42:56,549 - src.llm.client - INFO - [lab:f39e7e] ğŸ“¡ Stream active (200)
2025-12-16 08:42:56,549 - src.llm.client - INFO - [lab:f39e7e] Starting stream parsing, waiting for first chunk...
2025-12-16 08:42:58,564 - src.llm.client - INFO - [lab:f39e7e] ğŸ“Š 2.0s: 778c @386c/s (148ch, ~194t @97t/s)
2025-12-16 08:43:00,567 - src.llm.client - INFO - [lab:f39e7e] ğŸ“Š 4.0s: 1434c @357c/s (292ch, ~358t @89t/s)
2025-12-16 08:43:02,576 - src.llm.client - INFO - [lab:f39e7e] ğŸ“Š 6.0s: 2091c @347c/s (439ch, ~523t @87t/s)
2025-12-16 08:43:04,579 - src.llm.client - INFO - [lab:f39e7e] ğŸ“Š 8.0s: 2736c @341c/s (585ch, ~684t @85t/s)
2025-12-16 08:43:06,593 - src.llm.client - INFO - [lab:f39e7e] ğŸ“Š 10.0s: 3520c @350c/s (728ch, ~880t @88t/s)
2025-12-16 08:43:08,601 - src.llm.client - INFO - [lab:f39e7e] ğŸ“Š 12.1s: 4318c @358c/s (871ch, ~1080t @90t/s)
2025-12-16 08:43:10,081 - src.llm.client - INFO - [lab:f39e7e] âœ“ Done 14.39s: 4953c (~647w @344c/s)
2025-12-16 08:43:10,082 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:43:10,082 - src.generate.formats.labs - INFO -     - Length: 5056 chars, 662 words
2025-12-16 08:43:10,082 - src.generate.formats.labs - INFO -     - Procedure: 10 steps
2025-12-16 08:43:10,082 - src.generate.formats.labs - INFO -     - Safety: 2 warnings
2025-12-16 08:43:10,082 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 08:43:10,084 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:43:10,084 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:43:10,084 - src.generate.formats.study_notes - INFO - Generating study notes for: Variational Inference â€“ Introduction (Session 5)
2025-12-16 08:43:10,085 - src.llm.client - INFO - [stu:6e25db] ğŸš€ stu | m=gemma3:4b | p=4471c | t=120s
2025-12-16 08:43:10,085 - src.llm.client - INFO - [stu:6e25db] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:43:10,085 - src.llm.client - INFO - [stu:6e25db] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:43:10,086 - src.llm.client - INFO - [stu:6e25db] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8151 bytes, prompt=4471 chars
2025-12-16 08:43:10,086 - src.llm.client - INFO - [stu:6e25db] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:43:11,264 - src.llm.request_handler - INFO - [stu:6e25db] âœ“ Done 1.18s
2025-12-16 08:43:11,264 - src.llm.client - INFO - [stu:6e25db] âœ… HTTP 200 in 1.18s
2025-12-16 08:43:11,264 - src.llm.client - INFO - [stu:6e25db] ğŸ“¡ Stream active (200)
2025-12-16 08:43:11,264 - src.llm.client - INFO - [stu:6e25db] Starting stream parsing, waiting for first chunk...
2025-12-16 08:43:13,268 - src.llm.client - INFO - [stu:6e25db] ğŸ“Š 2.0s: 891c @445c/s (146ch, ~223t @111t/s)
2025-12-16 08:43:15,278 - src.llm.client - INFO - [stu:6e25db] ğŸ“Š 4.0s: 1520c @379c/s (290ch, ~380t @95t/s)
2025-12-16 08:43:17,289 - src.llm.client - INFO - [stu:6e25db] ğŸ“Š 6.0s: 2313c @384c/s (434ch, ~578t @96t/s)
2025-12-16 08:43:18,776 - src.llm.client - INFO - [stu:6e25db] âœ“ Done 8.69s: 2816c (~382w @324c/s)
2025-12-16 08:43:18,776 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:43:18,776 - src.generate.formats.study_notes - INFO -     - Length: 2887 chars, 393 words
2025-12-16 08:43:18,776 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:43:18,776 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-16 08:43:18,776 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 08:43:18,776 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:43:18,777 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:43:18,778 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:43:18,778 - src.generate.formats.diagrams - INFO - Generating diagram for: Computational Cost (Variational Inference â€“ Introduction)
2025-12-16 08:43:18,778 - src.generate.formats.diagrams - INFO - Generating diagram for: Approximation Methods (Variational Inference â€“ Introduction)
2025-12-16 08:43:18,778 - src.llm.client - INFO - [dia:491206] ğŸš€ dia | m=gemma3:4b | p=5771c | t=120s
2025-12-16 08:43:18,778 - src.llm.client - INFO - [dia:3f6517] ğŸš€ dia | m=gemma3:4b | p=5777c | t=120s
2025-12-16 08:43:18,778 - src.llm.client - INFO - [dia:491206] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:43:18,778 - src.llm.client - INFO - [dia:3f6517] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:43:18,778 - src.llm.client - INFO - [dia:491206] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:43:18,778 - src.llm.client - INFO - [dia:3f6517] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:43:18,780 - src.llm.client - INFO - [dia:3f6517] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11091 bytes, prompt=5777 chars
2025-12-16 08:43:18,780 - src.llm.client - INFO - [dia:491206] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11085 bytes, prompt=5771 chars
2025-12-16 08:43:18,780 - src.llm.client - INFO - [dia:3f6517] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:43:18,780 - src.llm.client - INFO - [dia:491206] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:43:20,445 - src.llm.request_handler - INFO - [dia:3f6517] âœ“ Done 1.66s
2025-12-16 08:43:20,445 - src.llm.client - INFO - [dia:3f6517] âœ… HTTP 200 in 1.66s
2025-12-16 08:43:20,445 - src.llm.client - INFO - [dia:3f6517] ğŸ“¡ Stream active (200)
2025-12-16 08:43:20,445 - src.llm.client - INFO - [dia:3f6517] Starting stream parsing, waiting for first chunk...
2025-12-16 08:43:22,454 - src.llm.client - INFO - [dia:3f6517] ğŸ“Š 2.0s: 462c @230c/s (143ch, ~116t @58t/s)
2025-12-16 08:43:23,848 - src.llm.client - INFO - [dia:3f6517] âœ“ Done 5.07s: 761c (~105w @150c/s)
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Approximation Methods (Variational Inference â€“ Introduction):
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO -     - Length: 673 chars (cleaned: 673 chars)
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO - [OK] Elements: 40 total (nodes: 19, connections: 21) âœ“
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:43:23,849 - src.generate.formats.diagrams - INFO - Generated diagram: 673 characters
2025-12-16 08:43:25,368 - src.llm.request_handler - INFO - [dia:491206] âœ“ Done 6.59s
2025-12-16 08:43:25,368 - src.llm.client - INFO - [dia:491206] âœ… HTTP 200 in 6.59s
2025-12-16 08:43:25,368 - src.llm.client - INFO - [dia:491206] ğŸ“¡ Stream active (200)
2025-12-16 08:43:25,368 - src.llm.client - INFO - [dia:491206] Starting stream parsing, waiting for first chunk...
2025-12-16 08:43:27,368 - src.llm.client - INFO - [dia:491206] ğŸ“Š 2.0s: 504c @252c/s (142ch, ~126t @63t/s)
2025-12-16 08:43:29,380 - src.llm.client - INFO - [dia:491206] ğŸ“Š 4.0s: 1021c @255c/s (285ch, ~255t @64t/s)
2025-12-16 08:43:29,545 - src.llm.client - INFO - [dia:491206] âœ“ Done 10.77s: 1041c (~146w @97c/s)
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Computational Cost (Variational Inference â€“ Introduction):
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - INFO -     - Length: 1026 chars (cleaned: 1026 chars)
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - INFO - [OK] Elements: 61 total (nodes: 23, connections: 38) âœ“
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - INFO -   Cleanup summary: 1 issues fixed (code fences, style commands, etc.)
2025-12-16 08:43:29,546 - src.generate.formats.diagrams - INFO - Generated diagram: 1026 characters
2025-12-16 08:43:29,547 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:43:29,547 - src.generate.formats.questions - INFO - Generating 10 questions for: Variational Inference â€“ Introduction (Session 5)
2025-12-16 08:43:29,547 - src.llm.client - INFO - [qst:dc0185] ğŸš€ qst | m=gemma3:4b | p=7353c | t=150s
2025-12-16 08:43:29,547 - src.llm.client - INFO - [qst:dc0185] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:43:29,547 - src.llm.client - INFO - [qst:dc0185] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:43:29,548 - src.llm.client - INFO - [qst:dc0185] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11087 bytes, prompt=7353 chars
2025-12-16 08:43:29,548 - src.llm.client - INFO - [qst:dc0185] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:43:31,445 - src.llm.request_handler - INFO - [qst:dc0185] âœ“ Done 1.90s
2025-12-16 08:43:31,445 - src.llm.client - INFO - [qst:dc0185] âœ… HTTP 200 in 1.90s
2025-12-16 08:43:31,445 - src.llm.client - INFO - [qst:dc0185] ğŸ“¡ Stream active (200)
2025-12-16 08:43:31,446 - src.llm.client - INFO - [qst:dc0185] Starting stream parsing, waiting for first chunk...
2025-12-16 08:43:33,455 - src.llm.client - INFO - [qst:dc0185] ğŸ“Š 2.0s: 754c @375c/s (147ch, ~188t @94t/s)
2025-12-16 08:43:35,465 - src.llm.client - INFO - [qst:dc0185] ğŸ“Š 4.0s: 1471c @366c/s (291ch, ~368t @91t/s)
2025-12-16 08:43:37,475 - src.llm.client - INFO - [qst:dc0185] ğŸ“Š 6.0s: 2224c @369c/s (434ch, ~556t @92t/s)
2025-12-16 08:43:39,485 - src.llm.client - INFO - [qst:dc0185] ğŸ“Š 8.0s: 2964c @369c/s (576ch, ~741t @92t/s)
2025-12-16 08:43:41,492 - src.llm.client - INFO - [qst:dc0185] ğŸ“Š 10.0s: 3838c @382c/s (718ch, ~960t @96t/s)
2025-12-16 08:43:43,505 - src.llm.client - INFO - [qst:dc0185] ğŸ“Š 12.1s: 4666c @387c/s (864ch, ~1166t @97t/s)
2025-12-16 08:43:43,594 - src.llm.client - INFO - [qst:dc0185] âœ“ Done 14.05s: 4685c (~639w @334c/s)
2025-12-16 08:43:43,594 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 1, 'total_fixes': 3}
2025-12-16 08:43:43,594 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING -     Context: Module 5 Session 5
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 4 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING -     Context: Module 5 Session 5
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:43:43,594 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:43:43,595 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 08:43:43,595 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Variational Inference â€“ Introduction (Session 5)
2025-12-16 08:43:43,595 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:43:43,596 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:43:43,598 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 5 completed
2025-12-16 08:43:43,598 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:43:43,598 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:43:43,599 - src.generate.orchestration.pipeline - INFO - Module 6: Variational Free Energy â€“ Definition & Interpretation (1 sessions)
2025-12-16 08:43:43,599 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:43:43,599 - src.generate.orchestration.pipeline - INFO - 
[6/15] Session 6: Defining VFE
2025-12-16 08:43:43,599 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:43:43,599 - src.generate.formats.lectures - INFO - Generating lecture for: Variational Free Energy â€“ Definition & Interpretation (Session 6/15)
2025-12-16 08:43:43,599 - src.llm.client - INFO - [lec:74e66b] ğŸš€ lec | m=gemma3:4b | p=3104c | t=180s
2025-12-16 08:43:43,599 - src.llm.client - INFO - [lec:74e66b] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:43:43,599 - src.llm.client - INFO - [lec:74e66b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:43:43,600 - src.llm.client - INFO - [lec:74e66b] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6745 bytes, prompt=3104 chars
2025-12-16 08:43:43,600 - src.llm.client - INFO - [lec:74e66b] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:43:44,581 - src.llm.request_handler - INFO - [lec:74e66b] âœ“ Done 0.98s
2025-12-16 08:43:44,581 - src.llm.client - INFO - [lec:74e66b] âœ… HTTP 200 in 0.98s
2025-12-16 08:43:44,581 - src.llm.client - INFO - [lec:74e66b] ğŸ“¡ Stream active (200)
2025-12-16 08:43:44,581 - src.llm.client - INFO - [lec:74e66b] Starting stream parsing, waiting for first chunk...
2025-12-16 08:43:46,589 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 2.0s: 809c @403c/s (145ch, ~202t @101t/s)
2025-12-16 08:43:48,602 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 4.0s: 1572c @391c/s (289ch, ~393t @98t/s)
2025-12-16 08:43:50,615 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 6.0s: 2267c @376c/s (432ch, ~567t @94t/s)
2025-12-16 08:43:52,625 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 8.0s: 2874c @357c/s (575ch, ~718t @89t/s)
2025-12-16 08:43:54,638 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 10.1s: 3432c @341c/s (719ch, ~858t @85t/s)
2025-12-16 08:43:56,642 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 12.1s: 3966c @329c/s (863ch, ~992t @82t/s)
2025-12-16 08:43:58,643 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 14.1s: 4664c @332c/s (999ch, ~1166t @83t/s)
2025-12-16 08:44:00,649 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 16.1s: 5229c @325c/s (1138ch, ~1307t @81t/s)
2025-12-16 08:44:02,660 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 18.1s: 5971c @330c/s (1280ch, ~1493t @83t/s)
2025-12-16 08:44:04,668 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 20.1s: 6650c @331c/s (1422ch, ~1662t @83t/s)
2025-12-16 08:44:06,671 - src.llm.client - INFO - [lec:74e66b] ğŸ“Š 22.1s: 7282c @330c/s (1563ch, ~1820t @82t/s)
2025-12-16 08:44:07,671 - src.llm.client - INFO - [lec:74e66b] âœ“ Done 24.07s: 7683c (~1165w @319c/s)
2025-12-16 08:44:07,672 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:44:07,672 - src.generate.formats.lectures - INFO -     - Length: 7786 chars, 1180 words
2025-12-16 08:44:07,672 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:44:07,672 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:44:07,672 - src.generate.formats.lectures - INFO -     - Content: 7 examples, 0 terms defined
2025-12-16 08:44:07,672 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:44:07,675 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:44:07,675 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:44:07,675 - src.generate.formats.labs - INFO - Generating lab 6 for: Variational Free Energy â€“ Definition & Interpretation (Session 6)
2025-12-16 08:44:07,676 - src.llm.client - INFO - [lab:cb1826] ğŸš€ lab | m=gemma3:4b | p=3363c | t=150s
2025-12-16 08:44:07,676 - src.llm.client - INFO - [lab:cb1826] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:44:07,676 - src.llm.client - INFO - [lab:cb1826] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:44:07,677 - src.llm.client - INFO - [lab:cb1826] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3807 bytes, prompt=3363 chars
2025-12-16 08:44:07,677 - src.llm.client - INFO - [lab:cb1826] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:44:08,590 - src.llm.request_handler - INFO - [lab:cb1826] âœ“ Done 0.91s
2025-12-16 08:44:08,590 - src.llm.client - INFO - [lab:cb1826] âœ… HTTP 200 in 0.91s
2025-12-16 08:44:08,590 - src.llm.client - INFO - [lab:cb1826] ğŸ“¡ Stream active (200)
2025-12-16 08:44:08,590 - src.llm.client - INFO - [lab:cb1826] Starting stream parsing, waiting for first chunk...
2025-12-16 08:44:10,602 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 2.0s: 815c @405c/s (145ch, ~204t @101t/s)
2025-12-16 08:44:12,607 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 4.0s: 1498c @373c/s (282ch, ~374t @93t/s)
2025-12-16 08:44:14,609 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 6.0s: 2172c @361c/s (424ch, ~543t @90t/s)
2025-12-16 08:44:16,611 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 8.0s: 2717c @339c/s (568ch, ~679t @85t/s)
2025-12-16 08:44:18,612 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 10.0s: 3360c @335c/s (710ch, ~840t @84t/s)
2025-12-16 08:44:20,621 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 12.0s: 4043c @336c/s (851ch, ~1011t @84t/s)
2025-12-16 08:44:22,630 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 14.0s: 4728c @337c/s (993ch, ~1182t @84t/s)
2025-12-16 08:44:24,639 - src.llm.client - INFO - [lab:cb1826] ğŸ“Š 16.0s: 5344c @333c/s (1136ch, ~1336t @83t/s)
2025-12-16 08:44:25,720 - src.llm.client - INFO - [lab:cb1826] âœ“ Done 18.04s: 5720c (~786w @317c/s)
2025-12-16 08:44:25,721 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:44:25,721 - src.generate.formats.labs - INFO -     - Length: 5846 chars, 805 words
2025-12-16 08:44:25,721 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:44:25,721 - src.generate.formats.labs - INFO -     - Safety: 2 warnings
2025-12-16 08:44:25,721 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 08:44:25,723 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:44:25,723 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:44:25,724 - src.generate.formats.study_notes - INFO - Generating study notes for: Variational Free Energy â€“ Definition & Interpretation (Session 6)
2025-12-16 08:44:25,724 - src.llm.client - INFO - [stu:33e834] ğŸš€ stu | m=gemma3:4b | p=4464c | t=120s
2025-12-16 08:44:25,724 - src.llm.client - INFO - [stu:33e834] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:44:25,724 - src.llm.client - INFO - [stu:33e834] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:44:25,725 - src.llm.client - INFO - [stu:33e834] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8123 bytes, prompt=4464 chars
2025-12-16 08:44:25,725 - src.llm.client - INFO - [stu:33e834] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:44:26,973 - src.llm.request_handler - INFO - [stu:33e834] âœ“ Done 1.25s
2025-12-16 08:44:26,973 - src.llm.client - INFO - [stu:33e834] âœ… HTTP 200 in 1.25s
2025-12-16 08:44:26,973 - src.llm.client - INFO - [stu:33e834] ğŸ“¡ Stream active (200)
2025-12-16 08:44:26,973 - src.llm.client - INFO - [stu:33e834] Starting stream parsing, waiting for first chunk...
2025-12-16 08:44:28,986 - src.llm.client - INFO - [stu:33e834] ğŸ“Š 2.0s: 821c @408c/s (141ch, ~205t @102t/s)
2025-12-16 08:44:30,991 - src.llm.client - INFO - [stu:33e834] ğŸ“Š 4.0s: 1504c @374c/s (283ch, ~376t @94t/s)
2025-12-16 08:44:32,943 - src.llm.client - INFO - [stu:33e834] âœ“ Done 7.22s: 2178c (~305w @302c/s)
2025-12-16 08:44:32,943 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:44:32,943 - src.generate.formats.study_notes - INFO -     - Length: 2266 chars, 319 words
2025-12-16 08:44:32,943 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:44:32,943 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-16 08:44:32,943 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 08:44:32,943 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:44:32,944 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:44:32,944 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:44:32,945 - src.generate.formats.diagrams - INFO - Generating diagram for: Mathematical Formulation (Variational Free Energy â€“ Definition & Interpretation)
2025-12-16 08:44:32,945 - src.generate.formats.diagrams - INFO - Generating diagram for: Relationship to Surprise (Variational Free Energy â€“ Definition & Interpretation)
2025-12-16 08:44:32,945 - src.llm.client - INFO - [dia:5ad9f8] ğŸš€ dia | m=gemma3:4b | p=5780c | t=120s
2025-12-16 08:44:32,945 - src.llm.client - INFO - [dia:44dfd8] ğŸš€ dia | m=gemma3:4b | p=5780c | t=120s
2025-12-16 08:44:32,945 - src.llm.client - INFO - [dia:5ad9f8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:44:32,945 - src.llm.client - INFO - [dia:44dfd8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:44:32,945 - src.llm.client - INFO - [dia:44dfd8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:44:32,945 - src.llm.client - INFO - [dia:5ad9f8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:44:32,947 - src.llm.client - INFO - [dia:5ad9f8] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11094 bytes, prompt=5780 chars
2025-12-16 08:44:32,947 - src.llm.client - INFO - [dia:44dfd8] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11094 bytes, prompt=5780 chars
2025-12-16 08:44:32,947 - src.llm.client - INFO - [dia:5ad9f8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:44:32,947 - src.llm.client - INFO - [dia:44dfd8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:44:34,620 - src.llm.request_handler - INFO - [dia:5ad9f8] âœ“ Done 1.67s
2025-12-16 08:44:34,620 - src.llm.client - INFO - [dia:5ad9f8] âœ… HTTP 200 in 1.67s
2025-12-16 08:44:34,620 - src.llm.client - INFO - [dia:5ad9f8] ğŸ“¡ Stream active (200)
2025-12-16 08:44:34,620 - src.llm.client - INFO - [dia:5ad9f8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:44:36,629 - src.llm.client - INFO - [dia:5ad9f8] ğŸ“Š 2.0s: 460c @229c/s (144ch, ~115t @57t/s)
2025-12-16 08:44:38,636 - src.llm.client - INFO - [dia:5ad9f8] ğŸ“Š 4.0s: 790c @197c/s (287ch, ~198t @49t/s)
2025-12-16 08:44:39,182 - src.llm.client - INFO - [dia:5ad9f8] âœ“ Done 6.24s: 844c (~108w @135c/s)
2025-12-16 08:44:39,182 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Relationship to Surprise (Variational Free Energy â€“ Definition & Interpretation):
2025-12-16 08:44:39,182 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:44:39,182 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO -     - Length: 672 chars (cleaned: 672 chars)
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO - [OK] Elements: 29 total (nodes: 15, connections: 14) âœ“
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 08:44:39,183 - src.generate.formats.diagrams - INFO - Generated diagram: 672 characters
2025-12-16 08:44:40,716 - src.llm.request_handler - INFO - [dia:44dfd8] âœ“ Done 7.77s
2025-12-16 08:44:40,716 - src.llm.client - INFO - [dia:44dfd8] âœ… HTTP 200 in 7.77s
2025-12-16 08:44:40,716 - src.llm.client - INFO - [dia:44dfd8] ğŸ“¡ Stream active (200)
2025-12-16 08:44:40,716 - src.llm.client - INFO - [dia:44dfd8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:44:42,726 - src.llm.client - INFO - [dia:44dfd8] ğŸ“Š 2.0s: 482c @240c/s (144ch, ~120t @60t/s)
2025-12-16 08:44:43,813 - src.llm.client - INFO - [dia:44dfd8] âœ“ Done 10.87s: 650c (~98w @60c/s)
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Mathematical Formulation (Variational Free Energy â€“ Definition & Interpretation):
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO -     - Length: 480 chars (cleaned: 480 chars)
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO - [OK] Elements: 33 total (nodes: 12, connections: 21) âœ“
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:44:43,814 - src.generate.formats.diagrams - INFO - Generated diagram: 480 characters
2025-12-16 08:44:43,815 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:44:43,815 - src.generate.formats.questions - INFO - Generating 10 questions for: Variational Free Energy â€“ Definition & Interpretation (Session 6)
2025-12-16 08:44:43,815 - src.llm.client - INFO - [qst:f817a3] ğŸš€ qst | m=gemma3:4b | p=7358c | t=150s
2025-12-16 08:44:43,815 - src.llm.client - INFO - [qst:f817a3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:44:43,815 - src.llm.client - INFO - [qst:f817a3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:44:43,816 - src.llm.client - INFO - [qst:f817a3] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11074 bytes, prompt=7358 chars
2025-12-16 08:44:43,816 - src.llm.client - INFO - [qst:f817a3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:44:45,702 - src.llm.request_handler - INFO - [qst:f817a3] âœ“ Done 1.89s
2025-12-16 08:44:45,703 - src.llm.client - INFO - [qst:f817a3] âœ… HTTP 200 in 1.89s
2025-12-16 08:44:45,703 - src.llm.client - INFO - [qst:f817a3] ğŸ“¡ Stream active (200)
2025-12-16 08:44:45,703 - src.llm.client - INFO - [qst:f817a3] Starting stream parsing, waiting for first chunk...
2025-12-16 08:44:47,704 - src.llm.client - INFO - [qst:f817a3] ğŸ“Š 2.0s: 727c @363c/s (143ch, ~182t @91t/s)
2025-12-16 08:44:49,712 - src.llm.client - INFO - [qst:f817a3] ğŸ“Š 4.0s: 1392c @347c/s (286ch, ~348t @87t/s)
2025-12-16 08:44:51,721 - src.llm.client - INFO - [qst:f817a3] ğŸ“Š 6.0s: 2053c @341c/s (428ch, ~513t @85t/s)
2025-12-16 08:44:53,723 - src.llm.client - INFO - [qst:f817a3] ğŸ“Š 8.0s: 2805c @350c/s (570ch, ~701t @87t/s)
2025-12-16 08:44:55,726 - src.llm.client - INFO - [qst:f817a3] ğŸ“Š 10.0s: 3572c @356c/s (710ch, ~893t @89t/s)
2025-12-16 08:44:57,731 - src.llm.client - INFO - [qst:f817a3] ğŸ“Š 12.0s: 4347c @361c/s (854ch, ~1087t @90t/s)
2025-12-16 08:44:59,066 - src.llm.client - INFO - [qst:f817a3] âœ“ Done 15.25s: 4826c (~684w @316c/s)
2025-12-16 08:44:59,066 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 5 question format issues: {'format_standardized': 0, 'question_marks_added': 5, 'mc_options_fixed': 0, 'total_fixes': 5}
2025-12-16 08:44:59,067 - src.generate.formats.questions - INFO - Applied 5 auto-fixes to questions
2025-12-16 08:44:59,067 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:44:59,067 - src.generate.formats.questions - WARNING -     Context: Module 6 Session 6
2025-12-16 08:44:59,067 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:44:59,067 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:44:59,067 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 4 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:44:59,067 - src.generate.formats.questions - WARNING -     Context: Module 6 Session 6
2025-12-16 08:44:59,067 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:44:59,068 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:44:59,068 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 08:44:59,069 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Variational Free Energy â€“ Definition & Interpretation (Session 6)
2025-12-16 08:44:59,069 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:44:59,072 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:44:59,075 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 6 completed
2025-12-16 08:44:59,075 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:44:59,075 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:44:59,075 - src.generate.orchestration.pipeline - INFO - Module 7: Markov Models â€“ Introduction & State Spaces (1 sessions)
2025-12-16 08:44:59,075 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:44:59,075 - src.generate.orchestration.pipeline - INFO - 
[7/15] Session 7: Markov Property
2025-12-16 08:44:59,075 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:44:59,075 - src.generate.formats.lectures - INFO - Generating lecture for: Markov Models â€“ Introduction & State Spaces (Session 7/15)
2025-12-16 08:44:59,075 - src.llm.client - INFO - [lec:0b1cdc] ğŸš€ lec | m=gemma3:4b | p=3082c | t=180s
2025-12-16 08:44:59,075 - src.llm.client - INFO - [lec:0b1cdc] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:44:59,075 - src.llm.client - INFO - [lec:0b1cdc] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:44:59,077 - src.llm.client - INFO - [lec:0b1cdc] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6723 bytes, prompt=3082 chars
2025-12-16 08:44:59,081 - src.llm.client - INFO - [lec:0b1cdc] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:45:00,063 - src.llm.request_handler - INFO - [lec:0b1cdc] âœ“ Done 0.98s
2025-12-16 08:45:00,063 - src.llm.client - INFO - [lec:0b1cdc] âœ… HTTP 200 in 0.98s
2025-12-16 08:45:00,063 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“¡ Stream active (200)
2025-12-16 08:45:00,063 - src.llm.client - INFO - [lec:0b1cdc] Starting stream parsing, waiting for first chunk...
2025-12-16 08:45:02,073 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 2.0s: 815c @406c/s (148ch, ~204t @101t/s)
2025-12-16 08:45:04,084 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 4.0s: 1556c @387c/s (297ch, ~389t @97t/s)
2025-12-16 08:45:06,090 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 6.0s: 2235c @371c/s (440ch, ~559t @93t/s)
2025-12-16 08:45:08,094 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 8.0s: 2900c @361c/s (583ch, ~725t @90t/s)
2025-12-16 08:45:10,098 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 10.0s: 3614c @360c/s (729ch, ~904t @90t/s)
2025-12-16 08:45:12,100 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 12.0s: 4229c @351c/s (874ch, ~1057t @88t/s)
2025-12-16 08:45:14,112 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 14.0s: 4642c @330c/s (1021ch, ~1160t @83t/s)
2025-12-16 08:45:16,120 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 16.1s: 5380c @335c/s (1169ch, ~1345t @84t/s)
2025-12-16 08:45:18,127 - src.llm.client - INFO - [lec:0b1cdc] ğŸ“Š 18.1s: 6101c @338c/s (1311ch, ~1525t @84t/s)
2025-12-16 08:45:19,519 - src.llm.client - INFO - [lec:0b1cdc] âœ“ Done 20.44s: 6684c (~1035w @327c/s)
2025-12-16 08:45:19,520 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:45:19,520 - src.generate.formats.lectures - INFO -     - Length: 6793 chars, 1051 words
2025-12-16 08:45:19,520 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:45:19,520 - src.generate.formats.lectures - INFO -     - Structure: 7 sections, 0 subsections
2025-12-16 08:45:19,520 - src.generate.formats.lectures - INFO -     - Content: 14 examples, 0 terms defined
2025-12-16 08:45:19,520 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:45:19,522 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:45:19,523 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:45:19,523 - src.generate.formats.labs - INFO - Generating lab 7 for: Markov Models â€“ Introduction & State Spaces (Session 7)
2025-12-16 08:45:19,523 - src.llm.client - INFO - [lab:1d0b00] ğŸš€ lab | m=gemma3:4b | p=3355c | t=150s
2025-12-16 08:45:19,523 - src.llm.client - INFO - [lab:1d0b00] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:45:19,523 - src.llm.client - INFO - [lab:1d0b00] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:45:19,524 - src.llm.client - INFO - [lab:1d0b00] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3804 bytes, prompt=3355 chars
2025-12-16 08:45:19,524 - src.llm.client - INFO - [lab:1d0b00] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:45:20,421 - src.llm.request_handler - INFO - [lab:1d0b00] âœ“ Done 0.90s
2025-12-16 08:45:20,422 - src.llm.client - INFO - [lab:1d0b00] âœ… HTTP 200 in 0.90s
2025-12-16 08:45:20,422 - src.llm.client - INFO - [lab:1d0b00] ğŸ“¡ Stream active (200)
2025-12-16 08:45:20,422 - src.llm.client - INFO - [lab:1d0b00] Starting stream parsing, waiting for first chunk...
2025-12-16 08:45:22,428 - src.llm.client - INFO - [lab:1d0b00] ğŸ“Š 2.0s: 754c @376c/s (148ch, ~188t @94t/s)
2025-12-16 08:45:24,440 - src.llm.client - INFO - [lab:1d0b00] ğŸ“Š 4.0s: 1348c @336c/s (298ch, ~337t @84t/s)
2025-12-16 08:45:26,446 - src.llm.client - INFO - [lab:1d0b00] ğŸ“Š 6.0s: 1950c @324c/s (442ch, ~488t @81t/s)
2025-12-16 08:45:28,456 - src.llm.client - INFO - [lab:1d0b00] ğŸ“Š 8.0s: 2530c @315c/s (586ch, ~632t @79t/s)
2025-12-16 08:45:30,467 - src.llm.client - INFO - [lab:1d0b00] ğŸ“Š 10.0s: 3086c @307c/s (731ch, ~772t @77t/s)
2025-12-16 08:45:32,475 - src.llm.client - INFO - [lab:1d0b00] ğŸ“Š 12.1s: 3456c @287c/s (874ch, ~864t @72t/s)
2025-12-16 08:45:34,476 - src.llm.client - INFO - [lab:1d0b00] ğŸ“Š 14.1s: 4086c @291c/s (1016ch, ~1022t @73t/s)
2025-12-16 08:45:35,364 - src.llm.client - INFO - [lab:1d0b00] âœ“ Done 15.84s: 4470c (~712w @282c/s)
2025-12-16 08:45:35,364 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:45:35,364 - src.generate.formats.labs - INFO -     - Length: 4579 chars, 730 words
2025-12-16 08:45:35,364 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:45:35,364 - src.generate.formats.labs - INFO -     - Safety: 9 warnings
2025-12-16 08:45:35,364 - src.generate.formats.labs - INFO -     - Data tables: 9
2025-12-16 08:45:35,366 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:45:35,366 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:45:35,366 - src.generate.formats.study_notes - INFO - Generating study notes for: Markov Models â€“ Introduction & State Spaces (Session 7)
2025-12-16 08:45:35,366 - src.llm.client - INFO - [stu:1aa660] ğŸš€ stu | m=gemma3:4b | p=4463c | t=120s
2025-12-16 08:45:35,366 - src.llm.client - INFO - [stu:1aa660] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:45:35,366 - src.llm.client - INFO - [stu:1aa660] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:45:35,368 - src.llm.client - INFO - [stu:1aa660] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8127 bytes, prompt=4463 chars
2025-12-16 08:45:35,368 - src.llm.client - INFO - [stu:1aa660] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:45:36,606 - src.llm.request_handler - INFO - [stu:1aa660] âœ“ Done 1.24s
2025-12-16 08:45:36,606 - src.llm.client - INFO - [stu:1aa660] âœ… HTTP 200 in 1.24s
2025-12-16 08:45:36,606 - src.llm.client - INFO - [stu:1aa660] ğŸ“¡ Stream active (200)
2025-12-16 08:45:36,606 - src.llm.client - INFO - [stu:1aa660] Starting stream parsing, waiting for first chunk...
2025-12-16 08:45:38,620 - src.llm.client - INFO - [stu:1aa660] ğŸ“Š 2.0s: 821c @408c/s (147ch, ~205t @102t/s)
2025-12-16 08:45:40,620 - src.llm.client - INFO - [stu:1aa660] ğŸ“Š 4.0s: 1505c @375c/s (289ch, ~376t @94t/s)
2025-12-16 08:45:42,622 - src.llm.client - INFO - [stu:1aa660] ğŸ“Š 6.0s: 2254c @375c/s (433ch, ~564t @94t/s)
2025-12-16 08:45:44,625 - src.llm.client - INFO - [stu:1aa660] ğŸ“Š 8.0s: 3015c @376c/s (580ch, ~754t @94t/s)
2025-12-16 08:45:46,634 - src.llm.client - INFO - [stu:1aa660] ğŸ“Š 10.0s: 3711c @370c/s (724ch, ~928t @93t/s)
2025-12-16 08:45:47,980 - src.llm.client - INFO - [stu:1aa660] âœ“ Done 12.61s: 4128c (~621w @327c/s)
2025-12-16 08:45:47,980 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:45:47,980 - src.generate.formats.study_notes - INFO -     - Length: 4206 chars, 635 words
2025-12-16 08:45:47,980 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:45:47,981 - src.generate.formats.study_notes - INFO -     - Key concepts: 7
2025-12-16 08:45:47,981 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 6 bullets
2025-12-16 08:45:47,981 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:45:47,983 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:45:47,984 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:45:47,984 - src.generate.formats.diagrams - INFO - Generating diagram for: State Transitions (Markov Models â€“ Introduction & State Spaces)
2025-12-16 08:45:47,984 - src.generate.formats.diagrams - INFO - Generating diagram for: Transition Probabilities (Markov Models â€“ Introduction & State Spaces)
2025-12-16 08:45:47,984 - src.llm.client - INFO - [dia:cf9f56] ğŸš€ dia | m=gemma3:4b | p=5759c | t=120s
2025-12-16 08:45:47,984 - src.llm.client - INFO - [dia:80fa2e] ğŸš€ dia | m=gemma3:4b | p=5773c | t=120s
2025-12-16 08:45:47,985 - src.llm.client - INFO - [dia:cf9f56] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:45:47,985 - src.llm.client - INFO - [dia:80fa2e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:45:47,985 - src.llm.client - INFO - [dia:cf9f56] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:45:47,985 - src.llm.client - INFO - [dia:80fa2e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:45:47,987 - src.llm.client - INFO - [dia:cf9f56] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11073 bytes, prompt=5759 chars
2025-12-16 08:45:47,987 - src.llm.client - INFO - [dia:cf9f56] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:45:47,987 - src.llm.client - INFO - [dia:80fa2e] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11087 bytes, prompt=5773 chars
2025-12-16 08:45:47,987 - src.llm.client - INFO - [dia:80fa2e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:45:49,682 - src.llm.request_handler - INFO - [dia:cf9f56] âœ“ Done 1.70s
2025-12-16 08:45:49,682 - src.llm.client - INFO - [dia:cf9f56] âœ… HTTP 200 in 1.70s
2025-12-16 08:45:49,682 - src.llm.client - INFO - [dia:cf9f56] ğŸ“¡ Stream active (200)
2025-12-16 08:45:49,683 - src.llm.client - INFO - [dia:cf9f56] Starting stream parsing, waiting for first chunk...
2025-12-16 08:45:51,694 - src.llm.client - INFO - [dia:cf9f56] ğŸ“Š 2.0s: 416c @207c/s (148ch, ~104t @52t/s)
2025-12-16 08:45:52,495 - src.llm.client - INFO - [dia:cf9f56] âœ“ Done 4.51s: 574c (~101w @127c/s)
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for State Transitions (Markov Models â€“ Introduction & State Spaces):
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO -     - Length: 490 chars (cleaned: 490 chars)
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO - [OK] Elements: 37 total (nodes: 9, connections: 28) âœ“
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:45:52,495 - src.generate.formats.diagrams - INFO - Generated diagram: 490 characters
2025-12-16 08:45:54,021 - src.llm.request_handler - INFO - [dia:80fa2e] âœ“ Done 6.03s
2025-12-16 08:45:54,022 - src.llm.client - INFO - [dia:80fa2e] âœ… HTTP 200 in 6.04s
2025-12-16 08:45:54,022 - src.llm.client - INFO - [dia:80fa2e] ğŸ“¡ Stream active (200)
2025-12-16 08:45:54,022 - src.llm.client - INFO - [dia:80fa2e] Starting stream parsing, waiting for first chunk...
2025-12-16 08:45:56,030 - src.llm.client - INFO - [dia:80fa2e] ğŸ“Š 2.0s: 636c @317c/s (143ch, ~159t @79t/s)
2025-12-16 08:45:58,038 - src.llm.client - INFO - [dia:80fa2e] ğŸ“Š 4.0s: 1152c @287c/s (284ch, ~288t @72t/s)
2025-12-16 08:45:58,039 - src.llm.client - INFO - [dia:80fa2e] âœ“ Done 10.05s: 1152c (~102w @115c/s)
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Transition Probabilities (Markov Models â€“ Introduction & State Spaces):
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO -     - Length: 879 chars (cleaned: 879 chars)
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO - [OK] Elements: 29 total (nodes: 9, connections: 20) âœ“
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 08:45:58,039 - src.generate.formats.diagrams - INFO - Generated diagram: 879 characters
2025-12-16 08:45:58,040 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:45:58,040 - src.generate.formats.questions - INFO - Generating 10 questions for: Markov Models â€“ Introduction & State Spaces (Session 7)
2025-12-16 08:45:58,040 - src.llm.client - INFO - [qst:c7b11e] ğŸš€ qst | m=gemma3:4b | p=7357c | t=150s
2025-12-16 08:45:58,040 - src.llm.client - INFO - [qst:c7b11e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:45:58,040 - src.llm.client - INFO - [qst:c7b11e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:45:58,041 - src.llm.client - INFO - [qst:c7b11e] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11117 bytes, prompt=7357 chars
2025-12-16 08:45:58,041 - src.llm.client - INFO - [qst:c7b11e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:45:59,985 - src.llm.request_handler - INFO - [qst:c7b11e] âœ“ Done 1.94s
2025-12-16 08:45:59,985 - src.llm.client - INFO - [qst:c7b11e] âœ… HTTP 200 in 1.94s
2025-12-16 08:45:59,985 - src.llm.client - INFO - [qst:c7b11e] ğŸ“¡ Stream active (200)
2025-12-16 08:45:59,985 - src.llm.client - INFO - [qst:c7b11e] Starting stream parsing, waiting for first chunk...
2025-12-16 08:46:01,985 - src.llm.client - INFO - [qst:c7b11e] ğŸ“Š 2.0s: 698c @349c/s (146ch, ~174t @87t/s)
2025-12-16 08:46:03,986 - src.llm.client - INFO - [qst:c7b11e] ğŸ“Š 4.0s: 1337c @334c/s (290ch, ~334t @84t/s)
2025-12-16 08:46:05,997 - src.llm.client - INFO - [qst:c7b11e] ğŸ“Š 6.0s: 2074c @345c/s (437ch, ~518t @86t/s)
2025-12-16 08:46:07,998 - src.llm.client - INFO - [qst:c7b11e] ğŸ“Š 8.0s: 2823c @352c/s (578ch, ~706t @88t/s)
2025-12-16 08:46:10,008 - src.llm.client - INFO - [qst:c7b11e] ğŸ“Š 10.0s: 3603c @359c/s (722ch, ~901t @90t/s)
2025-12-16 08:46:12,014 - src.llm.client - INFO - [qst:c7b11e] ğŸ“Š 12.0s: 4367c @363c/s (868ch, ~1092t @91t/s)
2025-12-16 08:46:14,020 - src.llm.client - INFO - [qst:c7b11e] ğŸ“Š 14.0s: 5108c @364c/s (1010ch, ~1277t @91t/s)
2025-12-16 08:46:14,637 - src.llm.client - INFO - [qst:c7b11e] âœ“ Done 16.60s: 5354c (~783w @323c/s)
2025-12-16 08:46:14,637 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 1, 'total_fixes': 3}
2025-12-16 08:46:14,637 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 08:46:14,637 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:46:14,637 - src.generate.formats.questions - WARNING -     Context: Module 7 Session 7
2025-12-16 08:46:14,637 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:46:14,637 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:46:14,637 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:46:14,638 - src.generate.formats.questions - WARNING -     Context: Module 7 Session 7
2025-12-16 08:46:14,638 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:46:14,638 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:46:14,638 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 08:46:14,638 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Markov Models â€“ Introduction & State Spaces (Session 7)
2025-12-16 08:46:14,638 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:46:14,640 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:46:14,641 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 7 completed
2025-12-16 08:46:14,642 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:46:14,642 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:46:14,642 - src.generate.orchestration.pipeline - INFO - Module 8: Probabilistic State-Space Models â€“ Formulation (1 sessions)
2025-12-16 08:46:14,642 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:46:14,642 - src.generate.orchestration.pipeline - INFO - 
[8/15] Session 8: Model Equations
2025-12-16 08:46:14,642 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:46:14,642 - src.generate.formats.lectures - INFO - Generating lecture for: Probabilistic State-Space Models â€“ Formulation (Session 8/15)
2025-12-16 08:46:14,642 - src.llm.client - INFO - [lec:2666e1] ğŸš€ lec | m=gemma3:4b | p=3121c | t=180s
2025-12-16 08:46:14,642 - src.llm.client - INFO - [lec:2666e1] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:46:14,642 - src.llm.client - INFO - [lec:2666e1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:46:14,644 - src.llm.client - INFO - [lec:2666e1] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6763 bytes, prompt=3121 chars
2025-12-16 08:46:14,644 - src.llm.client - INFO - [lec:2666e1] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:46:15,630 - src.llm.request_handler - INFO - [lec:2666e1] âœ“ Done 0.99s
2025-12-16 08:46:15,631 - src.llm.client - INFO - [lec:2666e1] âœ… HTTP 200 in 0.99s
2025-12-16 08:46:15,631 - src.llm.client - INFO - [lec:2666e1] ğŸ“¡ Stream active (200)
2025-12-16 08:46:15,631 - src.llm.client - INFO - [lec:2666e1] Starting stream parsing, waiting for first chunk...
2025-12-16 08:46:17,634 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 2.0s: 801c @400c/s (148ch, ~200t @100t/s)
2025-12-16 08:46:19,643 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 4.0s: 1508c @376c/s (295ch, ~377t @94t/s)
2025-12-16 08:46:21,646 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 6.0s: 2106c @350c/s (439ch, ~526t @88t/s)
2025-12-16 08:46:23,652 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 8.0s: 2658c @331c/s (586ch, ~664t @83t/s)
2025-12-16 08:46:25,663 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 10.0s: 3290c @328c/s (733ch, ~822t @82t/s)
2025-12-16 08:46:27,664 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 12.0s: 3858c @321c/s (875ch, ~964t @80t/s)
2025-12-16 08:46:29,671 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 14.0s: 4486c @320c/s (1017ch, ~1122t @80t/s)
2025-12-16 08:46:31,676 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 16.0s: 5111c @319c/s (1159ch, ~1278t @80t/s)
2025-12-16 08:46:33,686 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 18.1s: 5740c @318c/s (1301ch, ~1435t @79t/s)
2025-12-16 08:46:35,699 - src.llm.client - INFO - [lec:2666e1] ğŸ“Š 20.1s: 6485c @323c/s (1443ch, ~1621t @81t/s)
2025-12-16 08:46:37,537 - src.llm.client - INFO - [lec:2666e1] âœ“ Done 22.89s: 7219c (~1052w @315c/s)
2025-12-16 08:46:37,538 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:46:37,538 - src.generate.formats.lectures - INFO -     - Length: 7328 chars, 1066 words
2025-12-16 08:46:37,538 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:46:37,538 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:46:37,538 - src.generate.formats.lectures - INFO -     - Content: 6 examples, 0 terms defined
2025-12-16 08:46:37,538 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:46:37,542 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:46:37,542 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:46:37,542 - src.generate.formats.labs - INFO - Generating lab 8 for: Probabilistic State-Space Models â€“ Formulation (Session 8)
2025-12-16 08:46:37,542 - src.llm.client - INFO - [lab:921fa5] ğŸš€ lab | m=gemma3:4b | p=3347c | t=150s
2025-12-16 08:46:37,542 - src.llm.client - INFO - [lab:921fa5] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:46:37,542 - src.llm.client - INFO - [lab:921fa5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:46:37,544 - src.llm.client - INFO - [lab:921fa5] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3792 bytes, prompt=3347 chars
2025-12-16 08:46:37,544 - src.llm.client - INFO - [lab:921fa5] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:46:38,493 - src.llm.request_handler - INFO - [lab:921fa5] âœ“ Done 0.95s
2025-12-16 08:46:38,493 - src.llm.client - INFO - [lab:921fa5] âœ… HTTP 200 in 0.95s
2025-12-16 08:46:38,493 - src.llm.client - INFO - [lab:921fa5] ğŸ“¡ Stream active (200)
2025-12-16 08:46:38,493 - src.llm.client - INFO - [lab:921fa5] Starting stream parsing, waiting for first chunk...
2025-12-16 08:46:40,494 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 2.0s: 553c @276c/s (143ch, ~138t @69t/s)
2025-12-16 08:46:42,506 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 4.0s: 1213c @302c/s (287ch, ~303t @76t/s)
2025-12-16 08:46:44,511 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 6.0s: 1702c @283c/s (430ch, ~426t @71t/s)
2025-12-16 08:46:46,514 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 8.0s: 2208c @275c/s (576ch, ~552t @69t/s)
2025-12-16 08:46:48,525 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 10.0s: 2917c @291c/s (723ch, ~729t @73t/s)
2025-12-16 08:46:50,534 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 12.0s: 3544c @294c/s (868ch, ~886t @74t/s)
2025-12-16 08:46:52,543 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 14.1s: 4143c @295c/s (1016ch, ~1036t @74t/s)
2025-12-16 08:46:54,552 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 16.1s: 4654c @290c/s (1156ch, ~1164t @72t/s)
2025-12-16 08:46:56,559 - src.llm.client - INFO - [lab:921fa5] ğŸ“Š 18.1s: 5299c @293c/s (1303ch, ~1325t @73t/s)
2025-12-16 08:46:57,123 - src.llm.client - INFO - [lab:921fa5] âœ“ Done 19.58s: 5468c (~794w @279c/s)
2025-12-16 08:46:57,123 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:46:57,123 - src.generate.formats.labs - INFO -     - Length: 5576 chars, 810 words
2025-12-16 08:46:57,123 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:46:57,123 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-16 08:46:57,123 - src.generate.formats.labs - INFO -     - Data tables: 8
2025-12-16 08:46:57,125 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:46:57,126 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:46:57,126 - src.generate.formats.study_notes - INFO - Generating study notes for: Probabilistic State-Space Models â€“ Formulation (Session 8)
2025-12-16 08:46:57,126 - src.llm.client - INFO - [stu:5155a8] ğŸš€ stu | m=gemma3:4b | p=4487c | t=120s
2025-12-16 08:46:57,126 - src.llm.client - INFO - [stu:5155a8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:46:57,126 - src.llm.client - INFO - [stu:5155a8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:46:57,127 - src.llm.client - INFO - [stu:5155a8] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8148 bytes, prompt=4487 chars
2025-12-16 08:46:57,127 - src.llm.client - INFO - [stu:5155a8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:46:58,429 - src.llm.request_handler - INFO - [stu:5155a8] âœ“ Done 1.30s
2025-12-16 08:46:58,429 - src.llm.client - INFO - [stu:5155a8] âœ… HTTP 200 in 1.30s
2025-12-16 08:46:58,429 - src.llm.client - INFO - [stu:5155a8] ğŸ“¡ Stream active (200)
2025-12-16 08:46:58,429 - src.llm.client - INFO - [stu:5155a8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:47:00,434 - src.llm.client - INFO - [stu:5155a8] ğŸ“Š 2.0s: 773c @386c/s (142ch, ~193t @96t/s)
2025-12-16 08:47:02,439 - src.llm.client - INFO - [stu:5155a8] ğŸ“Š 4.0s: 1473c @367c/s (284ch, ~368t @92t/s)
2025-12-16 08:47:04,449 - src.llm.client - INFO - [stu:5155a8] ğŸ“Š 6.0s: 2008c @334c/s (420ch, ~502t @83t/s)
2025-12-16 08:47:06,450 - src.llm.client - INFO - [stu:5155a8] ğŸ“Š 8.0s: 2477c @309c/s (559ch, ~619t @77t/s)
2025-12-16 08:47:08,457 - src.llm.client - INFO - [stu:5155a8] ğŸ“Š 10.0s: 3102c @309c/s (699ch, ~776t @77t/s)
2025-12-16 08:47:10,460 - src.llm.client - INFO - [stu:5155a8] ğŸ“Š 12.0s: 3804c @316c/s (837ch, ~951t @79t/s)
2025-12-16 08:47:12,407 - src.llm.client - INFO - [stu:5155a8] âœ“ Done 15.28s: 4593c (~639w @301c/s)
2025-12-16 08:47:12,407 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:47:12,407 - src.generate.formats.study_notes - INFO -     - Length: 4674 chars, 651 words
2025-12-16 08:47:12,407 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:47:12,407 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-16 08:47:12,407 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 0 bullets
2025-12-16 08:47:12,407 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:47:12,409 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:47:12,409 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:47:12,409 - src.generate.formats.diagrams - INFO - Generating diagram for: Continuous-Time Dynamics (Probabilistic State-Space Models â€“ Formulation)
2025-12-16 08:47:12,409 - src.generate.formats.diagrams - INFO - Generating diagram for: Sensory Input (Probabilistic State-Space Models â€“ Formulation)
2025-12-16 08:47:12,410 - src.llm.client - INFO - [dia:d1733e] ğŸš€ dia | m=gemma3:4b | p=5776c | t=120s
2025-12-16 08:47:12,410 - src.llm.client - INFO - [dia:b532c1] ğŸš€ dia | m=gemma3:4b | p=5754c | t=120s
2025-12-16 08:47:12,410 - src.llm.client - INFO - [dia:d1733e] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:47:12,410 - src.llm.client - INFO - [dia:b532c1] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:47:12,410 - src.llm.client - INFO - [dia:d1733e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:47:12,410 - src.llm.client - INFO - [dia:b532c1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:47:12,412 - src.llm.client - INFO - [dia:b532c1] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11068 bytes, prompt=5754 chars
2025-12-16 08:47:12,412 - src.llm.client - INFO - [dia:d1733e] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11090 bytes, prompt=5776 chars
2025-12-16 08:47:12,412 - src.llm.client - INFO - [dia:b532c1] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:47:12,412 - src.llm.client - INFO - [dia:d1733e] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:47:14,180 - src.llm.request_handler - INFO - [dia:d1733e] âœ“ Done 1.77s
2025-12-16 08:47:14,180 - src.llm.client - INFO - [dia:d1733e] âœ… HTTP 200 in 1.77s
2025-12-16 08:47:14,180 - src.llm.client - INFO - [dia:d1733e] ğŸ“¡ Stream active (200)
2025-12-16 08:47:14,180 - src.llm.client - INFO - [dia:d1733e] Starting stream parsing, waiting for first chunk...
2025-12-16 08:47:16,188 - src.llm.client - INFO - [dia:d1733e] ğŸ“Š 2.0s: 398c @198c/s (143ch, ~100t @50t/s)
2025-12-16 08:47:17,846 - src.llm.client - INFO - [dia:d1733e] âœ“ Done 5.44s: 659c (~107w @121c/s)
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Continuous-Time Dynamics (Probabilistic State-Space Models â€“ Formulation):
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO -     - Length: 508 chars (cleaned: 508 chars)
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO - [OK] Elements: 34 total (nodes: 12, connections: 22) âœ“
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 08:47:17,846 - src.generate.formats.diagrams - INFO - Generated diagram: 508 characters
2025-12-16 08:47:19,399 - src.llm.request_handler - INFO - [dia:b532c1] âœ“ Done 6.99s
2025-12-16 08:47:19,399 - src.llm.client - INFO - [dia:b532c1] âœ… HTTP 200 in 6.99s
2025-12-16 08:47:19,399 - src.llm.client - INFO - [dia:b532c1] ğŸ“¡ Stream active (200)
2025-12-16 08:47:19,399 - src.llm.client - INFO - [dia:b532c1] Starting stream parsing, waiting for first chunk...
2025-12-16 08:47:21,402 - src.llm.client - INFO - [dia:b532c1] ğŸ“Š 2.0s: 472c @236c/s (140ch, ~118t @59t/s)
2025-12-16 08:47:23,410 - src.llm.client - INFO - [dia:b532c1] ğŸ“Š 4.0s: 867c @216c/s (283ch, ~217t @54t/s)
2025-12-16 08:47:25,179 - src.llm.client - INFO - [dia:b532c1] âœ“ Done 12.77s: 1155c (~138w @90c/s)
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Sensory Input (Probabilistic State-Space Models â€“ Formulation):
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO -     - Length: 630 chars (cleaned: 630 chars)
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO - [OK] Elements: 38 total (nodes: 16, connections: 22) âœ“
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:47:25,179 - src.generate.formats.diagrams - INFO - Generated diagram: 630 characters
2025-12-16 08:47:25,180 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:47:25,180 - src.generate.formats.questions - INFO - Generating 10 questions for: Probabilistic State-Space Models â€“ Formulation (Session 8)
2025-12-16 08:47:25,180 - src.llm.client - INFO - [qst:652361] ğŸš€ qst | m=gemma3:4b | p=7353c | t=150s
2025-12-16 08:47:25,180 - src.llm.client - INFO - [qst:652361] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:47:25,180 - src.llm.client - INFO - [qst:652361] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:47:25,181 - src.llm.client - INFO - [qst:652361] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11081 bytes, prompt=7353 chars
2025-12-16 08:47:25,181 - src.llm.client - INFO - [qst:652361] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:47:27,287 - src.llm.request_handler - INFO - [qst:652361] âœ“ Done 2.11s
2025-12-16 08:47:27,287 - src.llm.client - INFO - [qst:652361] âœ… HTTP 200 in 2.11s
2025-12-16 08:47:27,287 - src.llm.client - INFO - [qst:652361] ğŸ“¡ Stream active (200)
2025-12-16 08:47:27,287 - src.llm.client - INFO - [qst:652361] Starting stream parsing, waiting for first chunk...
2025-12-16 08:47:29,301 - src.llm.client - INFO - [qst:652361] ğŸ“Š 2.0s: 675c @335c/s (143ch, ~169t @84t/s)
2025-12-16 08:47:31,305 - src.llm.client - INFO - [qst:652361] ğŸ“Š 4.0s: 1272c @317c/s (284ch, ~318t @79t/s)
2025-12-16 08:47:33,318 - src.llm.client - INFO - [qst:652361] ğŸ“Š 6.0s: 1776c @295c/s (417ch, ~444t @74t/s)
2025-12-16 08:47:35,320 - src.llm.client - INFO - [qst:652361] ğŸ“Š 8.0s: 2379c @296c/s (558ch, ~595t @74t/s)
2025-12-16 08:47:37,332 - src.llm.client - INFO - [qst:652361] ğŸ“Š 10.0s: 3006c @299c/s (701ch, ~752t @75t/s)
2025-12-16 08:47:39,338 - src.llm.client - INFO - [qst:652361] ğŸ“Š 12.1s: 3688c @306c/s (841ch, ~922t @77t/s)
2025-12-16 08:47:41,346 - src.llm.client - INFO - [qst:652361] ğŸ“Š 14.1s: 4419c @314c/s (982ch, ~1105t @79t/s)
2025-12-16 08:47:43,360 - src.llm.client - INFO - [qst:652361] ğŸ“Š 16.1s: 5158c @321c/s (1118ch, ~1290t @80t/s)
2025-12-16 08:47:43,361 - src.llm.client - INFO - [qst:652361] âœ“ Done 18.18s: 5158c (~742w @284c/s)
2025-12-16 08:47:43,361 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 4 question format issues: {'format_standardized': 0, 'question_marks_added': 4, 'mc_options_fixed': 0, 'total_fixes': 4}
2025-12-16 08:47:43,361 - src.generate.formats.questions - INFO - Applied 4 auto-fixes to questions
2025-12-16 08:47:43,361 - src.generate.formats.questions - WARNING - [CRITICAL] Format Issue: Only 9/10 questions end with '?' (1 missing question marks - ensure questions are properly formatted) ğŸ”´
2025-12-16 08:47:43,361 - src.generate.formats.questions - WARNING -     Context: Module 8 Session 8
2025-12-16 08:47:43,361 - src.generate.formats.questions - WARNING -     Impact: Questions may not be properly formatted for parsing
2025-12-16 08:47:43,361 - src.generate.formats.questions - WARNING -     Recommendation: Ensure all questions end with '?' and use **Question N:** format
2025-12-16 08:47:43,361 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -     Context: Module 8 Session 8
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 3 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -     Context: Module 8 Session 8
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 3 issues
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Probabilistic State-Space Models â€“ Formulation (Session 8)
2025-12-16 08:47:43,362 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:47:43,364 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:47:43,366 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 8 completed
2025-12-16 08:47:43,366 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:47:43,366 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:47:43,366 - src.generate.orchestration.pipeline - INFO - Module 9: Precision Weighting & Attention (1 sessions)
2025-12-16 08:47:43,366 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:47:43,366 - src.generate.orchestration.pipeline - INFO - 
[9/15] Session 9: Weighting States
2025-12-16 08:47:43,366 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:47:43,366 - src.generate.formats.lectures - INFO - Generating lecture for: Precision Weighting & Attention (Session 9/15)
2025-12-16 08:47:43,367 - src.llm.client - INFO - [lec:396485] ğŸš€ lec | m=gemma3:4b | p=3023c | t=180s
2025-12-16 08:47:43,367 - src.llm.client - INFO - [lec:396485] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:47:43,367 - src.llm.client - INFO - [lec:396485] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:47:43,368 - src.llm.client - INFO - [lec:396485] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6653 bytes, prompt=3023 chars
2025-12-16 08:47:43,368 - src.llm.client - INFO - [lec:396485] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:47:44,347 - src.llm.request_handler - INFO - [lec:396485] âœ“ Done 0.98s
2025-12-16 08:47:44,348 - src.llm.client - INFO - [lec:396485] âœ… HTTP 200 in 0.98s
2025-12-16 08:47:44,348 - src.llm.client - INFO - [lec:396485] ğŸ“¡ Stream active (200)
2025-12-16 08:47:44,348 - src.llm.client - INFO - [lec:396485] Starting stream parsing, waiting for first chunk...
2025-12-16 08:47:46,358 - src.llm.client - INFO - [lec:396485] ğŸ“Š 2.0s: 762c @379c/s (146ch, ~190t @95t/s)
2025-12-16 08:47:48,363 - src.llm.client - INFO - [lec:396485] ğŸ“Š 4.0s: 1571c @391c/s (290ch, ~393t @98t/s)
2025-12-16 08:47:50,375 - src.llm.client - INFO - [lec:396485] ğŸ“Š 6.0s: 2382c @395c/s (434ch, ~596t @99t/s)
2025-12-16 08:47:52,385 - src.llm.client - INFO - [lec:396485] ğŸ“Š 8.0s: 3110c @387c/s (577ch, ~778t @97t/s)
2025-12-16 08:47:54,388 - src.llm.client - INFO - [lec:396485] ğŸ“Š 10.0s: 3893c @388c/s (719ch, ~973t @97t/s)
2025-12-16 08:47:56,391 - src.llm.client - INFO - [lec:396485] ğŸ“Š 12.0s: 4561c @379c/s (860ch, ~1140t @95t/s)
2025-12-16 08:47:58,393 - src.llm.client - INFO - [lec:396485] ğŸ“Š 14.0s: 5306c @378c/s (1002ch, ~1326t @94t/s)
2025-12-16 08:48:00,397 - src.llm.client - INFO - [lec:396485] ğŸ“Š 16.0s: 6078c @379c/s (1141ch, ~1520t @95t/s)
2025-12-16 08:48:02,400 - src.llm.client - INFO - [lec:396485] ğŸ“Š 18.1s: 6804c @377c/s (1283ch, ~1701t @94t/s)
2025-12-16 08:48:04,403 - src.llm.client - INFO - [lec:396485] ğŸ“Š 20.1s: 7596c @379c/s (1425ch, ~1899t @95t/s)
2025-12-16 08:48:06,252 - src.llm.client - INFO - [lec:396485] âœ“ Done 22.89s: 8336c (~1206w @364c/s)
2025-12-16 08:48:06,254 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:48:06,254 - src.generate.formats.lectures - INFO -     - Length: 8424 chars, 1218 words
2025-12-16 08:48:06,254 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:48:06,254 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:48:06,254 - src.generate.formats.lectures - INFO -     - Content: 14 examples, 2 terms defined
2025-12-16 08:48:06,254 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:48:06,257 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:48:06,258 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:48:06,258 - src.generate.formats.labs - INFO - Generating lab 9 for: Precision Weighting & Attention (Session 9)
2025-12-16 08:48:06,258 - src.llm.client - INFO - [lab:c3c773] ğŸš€ lab | m=gemma3:4b | p=3311c | t=150s
2025-12-16 08:48:06,258 - src.llm.client - INFO - [lab:c3c773] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:48:06,258 - src.llm.client - INFO - [lab:c3c773] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:48:06,259 - src.llm.client - INFO - [lab:c3c773] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3746 bytes, prompt=3311 chars
2025-12-16 08:48:06,259 - src.llm.client - INFO - [lab:c3c773] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:48:07,147 - src.llm.request_handler - INFO - [lab:c3c773] âœ“ Done 0.89s
2025-12-16 08:48:07,147 - src.llm.client - INFO - [lab:c3c773] âœ… HTTP 200 in 0.89s
2025-12-16 08:48:07,147 - src.llm.client - INFO - [lab:c3c773] ğŸ“¡ Stream active (200)
2025-12-16 08:48:07,147 - src.llm.client - INFO - [lab:c3c773] Starting stream parsing, waiting for first chunk...
2025-12-16 08:48:09,158 - src.llm.client - INFO - [lab:c3c773] ğŸ“Š 2.0s: 730c @363c/s (145ch, ~182t @91t/s)
2025-12-16 08:48:11,163 - src.llm.client - INFO - [lab:c3c773] ğŸ“Š 4.0s: 1431c @356c/s (290ch, ~358t @89t/s)
2025-12-16 08:48:13,171 - src.llm.client - INFO - [lab:c3c773] ğŸ“Š 6.0s: 2073c @344c/s (434ch, ~518t @86t/s)
2025-12-16 08:48:15,179 - src.llm.client - INFO - [lab:c3c773] ğŸ“Š 8.0s: 2696c @336c/s (577ch, ~674t @84t/s)
2025-12-16 08:48:17,185 - src.llm.client - INFO - [lab:c3c773] ğŸ“Š 10.0s: 3382c @337c/s (720ch, ~846t @84t/s)
2025-12-16 08:48:19,188 - src.llm.client - INFO - [lab:c3c773] ğŸ“Š 12.0s: 3950c @328c/s (862ch, ~988t @82t/s)
2025-12-16 08:48:21,201 - src.llm.client - INFO - [lab:c3c773] ğŸ“Š 14.1s: 4673c @333c/s (1005ch, ~1168t @83t/s)
2025-12-16 08:48:22,255 - src.llm.client - INFO - [lab:c3c773] âœ“ Done 16.00s: 5113c (~717w @320c/s)
2025-12-16 08:48:22,255 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:48:22,255 - src.generate.formats.labs - INFO -     - Length: 5212 chars, 733 words
2025-12-16 08:48:22,256 - src.generate.formats.labs - INFO -     - Procedure: 10 steps
2025-12-16 08:48:22,256 - src.generate.formats.labs - INFO -     - Safety: 7 warnings
2025-12-16 08:48:22,256 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 08:48:22,258 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:48:22,258 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:48:22,258 - src.generate.formats.study_notes - INFO - Generating study notes for: Precision Weighting & Attention (Session 9)
2025-12-16 08:48:22,258 - src.llm.client - INFO - [stu:3c42a4] ğŸš€ stu | m=gemma3:4b | p=4415c | t=120s
2025-12-16 08:48:22,259 - src.llm.client - INFO - [stu:3c42a4] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:48:22,259 - src.llm.client - INFO - [stu:3c42a4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:48:22,260 - src.llm.client - INFO - [stu:3c42a4] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8065 bytes, prompt=4415 chars
2025-12-16 08:48:22,260 - src.llm.client - INFO - [stu:3c42a4] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:48:23,476 - src.llm.request_handler - INFO - [stu:3c42a4] âœ“ Done 1.22s
2025-12-16 08:48:23,477 - src.llm.client - INFO - [stu:3c42a4] âœ… HTTP 200 in 1.22s
2025-12-16 08:48:23,477 - src.llm.client - INFO - [stu:3c42a4] ğŸ“¡ Stream active (200)
2025-12-16 08:48:23,477 - src.llm.client - INFO - [stu:3c42a4] Starting stream parsing, waiting for first chunk...
2025-12-16 08:48:25,478 - src.llm.client - INFO - [stu:3c42a4] ğŸ“Š 2.0s: 774c @387c/s (143ch, ~194t @97t/s)
2025-12-16 08:48:27,481 - src.llm.client - INFO - [stu:3c42a4] ğŸ“Š 4.0s: 1568c @392c/s (286ch, ~392t @98t/s)
2025-12-16 08:48:29,489 - src.llm.client - INFO - [stu:3c42a4] ğŸ“Š 6.0s: 2419c @402c/s (429ch, ~605t @101t/s)
2025-12-16 08:48:31,492 - src.llm.client - INFO - [stu:3c42a4] ğŸ“Š 8.0s: 3281c @409c/s (572ch, ~820t @102t/s)
2025-12-16 08:48:32,512 - src.llm.client - INFO - [stu:3c42a4] âœ“ Done 10.25s: 3652c (~480w @356c/s)
2025-12-16 08:48:32,513 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:48:32,513 - src.generate.formats.study_notes - INFO -     - Length: 3718 chars, 491 words
2025-12-16 08:48:32,513 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:48:32,513 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-16 08:48:32,513 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 08:48:32,513 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:48:32,514 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:48:32,514 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:48:32,514 - src.generate.formats.diagrams - INFO - Generating diagram for: Precision of States (Precision Weighting & Attention)
2025-12-16 08:48:32,514 - src.llm.client - INFO - [dia:4d7031] ğŸš€ dia | m=gemma3:4b | p=5752c | t=120s
2025-12-16 08:48:32,514 - src.llm.client - INFO - [dia:4d7031] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:48:32,515 - src.llm.client - INFO - [dia:4d7031] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:48:32,516 - src.llm.client - INFO - [dia:4d7031] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11061 bytes, prompt=5752 chars
2025-12-16 08:48:32,516 - src.llm.client - INFO - [dia:4d7031] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:48:34,185 - src.llm.request_handler - INFO - [dia:4d7031] âœ“ Done 1.67s
2025-12-16 08:48:34,185 - src.llm.client - INFO - [dia:4d7031] âœ… HTTP 200 in 1.67s
2025-12-16 08:48:34,185 - src.llm.client - INFO - [dia:4d7031] ğŸ“¡ Stream active (200)
2025-12-16 08:48:34,185 - src.llm.client - INFO - [dia:4d7031] Starting stream parsing, waiting for first chunk...
2025-12-16 08:48:36,198 - src.llm.client - INFO - [dia:4d7031] ğŸ“Š 2.0s: 500c @248c/s (145ch, ~125t @62t/s)
2025-12-16 08:48:38,200 - src.llm.client - INFO - [dia:4d7031] ğŸ“Š 4.0s: 844c @210c/s (288ch, ~211t @53t/s)
2025-12-16 08:48:38,607 - src.llm.client - INFO - [dia:4d7031] âœ“ Done 6.09s: 899c (~117w @148c/s)
2025-12-16 08:48:38,607 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Precision of States (Precision Weighting & Attention):
2025-12-16 08:48:38,607 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:48:38,607 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:48:38,607 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:48:38,608 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:48:38,608 - src.generate.formats.diagrams - INFO -     - Length: 557 chars (cleaned: 557 chars)
2025-12-16 08:48:38,608 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:48:38,608 - src.generate.formats.diagrams - INFO - [OK] Elements: 35 total (nodes: 12, connections: 23) âœ“
2025-12-16 08:48:38,608 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:48:38,608 - src.generate.formats.diagrams - INFO - Generated diagram: 557 characters
2025-12-16 08:48:38,608 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:48:38,608 - src.generate.formats.questions - INFO - Generating 10 questions for: Precision Weighting & Attention (Session 9)
2025-12-16 08:48:38,608 - src.llm.client - INFO - [qst:f0db21] ğŸš€ qst | m=gemma3:4b | p=7311c | t=150s
2025-12-16 08:48:38,608 - src.llm.client - INFO - [qst:f0db21] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:48:38,608 - src.llm.client - INFO - [qst:f0db21] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:48:38,610 - src.llm.client - INFO - [qst:f0db21] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11084 bytes, prompt=7311 chars
2025-12-16 08:48:38,610 - src.llm.client - INFO - [qst:f0db21] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:48:40,536 - src.llm.request_handler - INFO - [qst:f0db21] âœ“ Done 1.93s
2025-12-16 08:48:40,536 - src.llm.client - INFO - [qst:f0db21] âœ… HTTP 200 in 1.93s
2025-12-16 08:48:40,536 - src.llm.client - INFO - [qst:f0db21] ğŸ“¡ Stream active (200)
2025-12-16 08:48:40,536 - src.llm.client - INFO - [qst:f0db21] Starting stream parsing, waiting for first chunk...
2025-12-16 08:48:42,550 - src.llm.client - INFO - [qst:f0db21] ğŸ“Š 2.0s: 716c @356c/s (144ch, ~179t @89t/s)
2025-12-16 08:48:44,553 - src.llm.client - INFO - [qst:f0db21] ğŸ“Š 4.0s: 1468c @365c/s (287ch, ~367t @91t/s)
2025-12-16 08:48:46,560 - src.llm.client - INFO - [qst:f0db21] ğŸ“Š 6.0s: 2132c @354c/s (430ch, ~533t @88t/s)
2025-12-16 08:48:48,573 - src.llm.client - INFO - [qst:f0db21] ğŸ“Š 8.0s: 2915c @363c/s (573ch, ~729t @91t/s)
2025-12-16 08:48:50,575 - src.llm.client - INFO - [qst:f0db21] ğŸ“Š 10.0s: 3686c @367c/s (715ch, ~922t @92t/s)
2025-12-16 08:48:52,579 - src.llm.client - INFO - [qst:f0db21] ğŸ“Š 12.0s: 4460c @370c/s (857ch, ~1115t @93t/s)
2025-12-16 08:48:54,585 - src.llm.client - INFO - [qst:f0db21] ğŸ“Š 14.0s: 5294c @377c/s (999ch, ~1324t @94t/s)
2025-12-16 08:48:55,468 - src.llm.client - INFO - [qst:f0db21] âœ“ Done 16.86s: 5648c (~768w @335c/s)
2025-12-16 08:48:55,469 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 5 question format issues: {'format_standardized': 0, 'question_marks_added': 5, 'mc_options_fixed': 0, 'total_fixes': 5}
2025-12-16 08:48:55,469 - src.generate.formats.questions - INFO - Applied 5 auto-fixes to questions
2025-12-16 08:48:55,469 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:48:55,469 - src.generate.formats.questions - WARNING -     Context: Module 9 Session 9
2025-12-16 08:48:55,469 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:48:55,469 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:48:55,469 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 08:48:55,469 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Precision Weighting & Attention (Session 9)
2025-12-16 08:48:55,469 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:48:55,472 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:48:55,473 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 9 completed
2025-12-16 08:48:55,473 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:48:55,473 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:48:55,473 - src.generate.orchestration.pipeline - INFO - Module 10: Predictive Coding â€“ Neural Basis (1 sessions)
2025-12-16 08:48:55,473 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:48:55,474 - src.generate.orchestration.pipeline - INFO - 
[10/15] Session 10: Encoder-Decoder Model
2025-12-16 08:48:55,474 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:48:55,474 - src.generate.formats.lectures - INFO - Generating lecture for: Predictive Coding â€“ Neural Basis (Session 10/15)
2025-12-16 08:48:55,474 - src.llm.client - INFO - [lec:1b2189] ğŸš€ lec | m=gemma3:4b | p=3074c | t=180s
2025-12-16 08:48:55,474 - src.llm.client - INFO - [lec:1b2189] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:48:55,474 - src.llm.client - INFO - [lec:1b2189] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:48:55,476 - src.llm.client - INFO - [lec:1b2189] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6714 bytes, prompt=3074 chars
2025-12-16 08:48:55,476 - src.llm.client - INFO - [lec:1b2189] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:48:56,444 - src.llm.request_handler - INFO - [lec:1b2189] âœ“ Done 0.97s
2025-12-16 08:48:56,444 - src.llm.client - INFO - [lec:1b2189] âœ… HTTP 200 in 0.97s
2025-12-16 08:48:56,444 - src.llm.client - INFO - [lec:1b2189] ğŸ“¡ Stream active (200)
2025-12-16 08:48:56,444 - src.llm.client - INFO - [lec:1b2189] Starting stream parsing, waiting for first chunk...
2025-12-16 08:48:58,447 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 2.0s: 831c @415c/s (143ch, ~208t @104t/s)
2025-12-16 08:49:00,454 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 4.0s: 1584c @395c/s (286ch, ~396t @99t/s)
2025-12-16 08:49:02,457 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 6.0s: 2319c @386c/s (429ch, ~580t @96t/s)
2025-12-16 08:49:04,465 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 8.0s: 3162c @394c/s (572ch, ~790t @99t/s)
2025-12-16 08:49:06,474 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 10.0s: 3876c @386c/s (715ch, ~969t @97t/s)
2025-12-16 08:49:08,480 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 12.0s: 4626c @384c/s (858ch, ~1156t @96t/s)
2025-12-16 08:49:10,491 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 14.0s: 5372c @382c/s (1001ch, ~1343t @96t/s)
2025-12-16 08:49:12,503 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 16.1s: 6135c @382c/s (1144ch, ~1534t @96t/s)
2025-12-16 08:49:14,510 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 18.1s: 6959c @385c/s (1286ch, ~1740t @96t/s)
2025-12-16 08:49:16,520 - src.llm.client - INFO - [lec:1b2189] ğŸ“Š 20.1s: 7785c @388c/s (1428ch, ~1946t @97t/s)
2025-12-16 08:49:18,227 - src.llm.client - INFO - [lec:1b2189] âœ“ Done 22.75s: 8510c (~1248w @374c/s)
2025-12-16 08:49:18,228 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:49:18,229 - src.generate.formats.lectures - INFO -     - Length: 8616 chars, 1264 words
2025-12-16 08:49:18,229 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:49:18,229 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:49:18,229 - src.generate.formats.lectures - INFO -     - Content: 10 examples, 0 terms defined
2025-12-16 08:49:18,229 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:49:18,232 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:49:18,233 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:49:18,233 - src.generate.formats.labs - INFO - Generating lab 10 for: Predictive Coding â€“ Neural Basis (Session 10)
2025-12-16 08:49:18,233 - src.llm.client - INFO - [lab:198b5f] ğŸš€ lab | m=gemma3:4b | p=3340c | t=150s
2025-12-16 08:49:18,233 - src.llm.client - INFO - [lab:198b5f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:49:18,233 - src.llm.client - INFO - [lab:198b5f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:49:18,234 - src.llm.client - INFO - [lab:198b5f] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3789 bytes, prompt=3340 chars
2025-12-16 08:49:18,234 - src.llm.client - INFO - [lab:198b5f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:49:19,115 - src.llm.request_handler - INFO - [lab:198b5f] âœ“ Done 0.88s
2025-12-16 08:49:19,115 - src.llm.client - INFO - [lab:198b5f] âœ… HTTP 200 in 0.88s
2025-12-16 08:49:19,115 - src.llm.client - INFO - [lab:198b5f] ğŸ“¡ Stream active (200)
2025-12-16 08:49:19,116 - src.llm.client - INFO - [lab:198b5f] Starting stream parsing, waiting for first chunk...
2025-12-16 08:49:21,117 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 2.0s: 785c @392c/s (145ch, ~196t @98t/s)
2025-12-16 08:49:23,130 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 4.0s: 1473c @367c/s (290ch, ~368t @92t/s)
2025-12-16 08:49:25,136 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 6.0s: 1983c @329c/s (433ch, ~496t @82t/s)
2025-12-16 08:49:27,142 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 8.0s: 2613c @326c/s (576ch, ~653t @81t/s)
2025-12-16 08:49:29,157 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 10.0s: 3137c @312c/s (719ch, ~784t @78t/s)
2025-12-16 08:49:31,162 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 12.0s: 3885c @323c/s (862ch, ~971t @81t/s)
2025-12-16 08:49:33,176 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 14.1s: 4679c @333c/s (1005ch, ~1170t @83t/s)
2025-12-16 08:49:35,177 - src.llm.client - INFO - [lab:198b5f] ğŸ“Š 16.1s: 5437c @339c/s (1147ch, ~1359t @85t/s)
2025-12-16 08:49:35,988 - src.llm.client - INFO - [lab:198b5f] âœ“ Done 17.75s: 5741c (~778w @323c/s)
2025-12-16 08:49:35,988 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:49:35,988 - src.generate.formats.labs - INFO -     - Length: 5847 chars, 794 words
2025-12-16 08:49:35,988 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:49:35,988 - src.generate.formats.labs - INFO -     - Safety: 6 warnings
2025-12-16 08:49:35,988 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 08:49:35,991 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:49:35,992 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:49:35,992 - src.generate.formats.study_notes - INFO - Generating study notes for: Predictive Coding â€“ Neural Basis (Session 10)
2025-12-16 08:49:35,992 - src.llm.client - INFO - [stu:c9d804] ğŸš€ stu | m=gemma3:4b | p=4442c | t=120s
2025-12-16 08:49:35,992 - src.llm.client - INFO - [stu:c9d804] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:49:35,992 - src.llm.client - INFO - [stu:c9d804] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:49:35,993 - src.llm.client - INFO - [stu:c9d804] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8106 bytes, prompt=4442 chars
2025-12-16 08:49:35,993 - src.llm.client - INFO - [stu:c9d804] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:49:37,197 - src.llm.request_handler - INFO - [stu:c9d804] âœ“ Done 1.20s
2025-12-16 08:49:37,198 - src.llm.client - INFO - [stu:c9d804] âœ… HTTP 200 in 1.20s
2025-12-16 08:49:37,198 - src.llm.client - INFO - [stu:c9d804] ğŸ“¡ Stream active (200)
2025-12-16 08:49:37,198 - src.llm.client - INFO - [stu:c9d804] Starting stream parsing, waiting for first chunk...
2025-12-16 08:49:39,202 - src.llm.client - INFO - [stu:c9d804] ğŸ“Š 2.0s: 814c @406c/s (142ch, ~204t @102t/s)
2025-12-16 08:49:41,216 - src.llm.client - INFO - [stu:c9d804] ğŸ“Š 4.0s: 1698c @423c/s (285ch, ~424t @106t/s)
2025-12-16 08:49:43,224 - src.llm.client - INFO - [stu:c9d804] ğŸ“Š 6.0s: 2554c @424c/s (428ch, ~638t @106t/s)
2025-12-16 08:49:45,227 - src.llm.client - INFO - [stu:c9d804] ğŸ“Š 8.0s: 3373c @420c/s (570ch, ~843t @105t/s)
2025-12-16 08:49:47,231 - src.llm.client - INFO - [stu:c9d804] ğŸ“Š 10.0s: 4183c @417c/s (712ch, ~1046t @104t/s)
2025-12-16 08:49:47,492 - src.llm.client - INFO - [stu:c9d804] âœ“ Done 11.50s: 4260c (~591w @370c/s)
2025-12-16 08:49:47,492 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:49:47,492 - src.generate.formats.study_notes - INFO -     - Length: 4327 chars, 603 words
2025-12-16 08:49:47,492 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:49:47,492 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-16 08:49:47,492 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-16 08:49:47,492 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:49:47,494 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:49:47,495 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:49:47,495 - src.generate.formats.diagrams - INFO - Generating diagram for: Hierarchical Predictions (Predictive Coding â€“ Neural Basis)
2025-12-16 08:49:47,495 - src.llm.client - INFO - [dia:da8e78] ğŸš€ dia | m=gemma3:4b | p=5768c | t=120s
2025-12-16 08:49:47,495 - src.llm.client - INFO - [dia:da8e78] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:49:47,495 - src.llm.client - INFO - [dia:da8e78] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:49:47,496 - src.llm.client - INFO - [dia:da8e78] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11082 bytes, prompt=5768 chars
2025-12-16 08:49:47,496 - src.llm.client - INFO - [dia:da8e78] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:49:49,181 - src.llm.request_handler - INFO - [dia:da8e78] âœ“ Done 1.68s
2025-12-16 08:49:49,181 - src.llm.client - INFO - [dia:da8e78] âœ… HTTP 200 in 1.69s
2025-12-16 08:49:49,181 - src.llm.client - INFO - [dia:da8e78] ğŸ“¡ Stream active (200)
2025-12-16 08:49:49,182 - src.llm.client - INFO - [dia:da8e78] Starting stream parsing, waiting for first chunk...
2025-12-16 08:49:51,189 - src.llm.client - INFO - [dia:da8e78] ğŸ“Š 2.0s: 497c @248c/s (143ch, ~124t @62t/s)
2025-12-16 08:49:53,089 - src.llm.client - INFO - [dia:da8e78] âœ“ Done 5.59s: 780c (~97w @139c/s)
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Hierarchical Predictions (Predictive Coding â€“ Neural Basis):
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO -     - Length: 469 chars (cleaned: 469 chars)
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO - [OK] Elements: 29 total (nodes: 10, connections: 19) âœ“
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:49:53,089 - src.generate.formats.diagrams - INFO - Generated diagram: 469 characters
2025-12-16 08:49:53,090 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:49:53,090 - src.generate.formats.questions - INFO - Generating 10 questions for: Predictive Coding â€“ Neural Basis (Session 10)
2025-12-16 08:49:53,090 - src.llm.client - INFO - [qst:6409d0] ğŸš€ qst | m=gemma3:4b | p=7334c | t=150s
2025-12-16 08:49:53,090 - src.llm.client - INFO - [qst:6409d0] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:49:53,090 - src.llm.client - INFO - [qst:6409d0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:49:53,091 - src.llm.client - INFO - [qst:6409d0] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11072 bytes, prompt=7334 chars
2025-12-16 08:49:53,091 - src.llm.client - INFO - [qst:6409d0] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:49:55,026 - src.llm.request_handler - INFO - [qst:6409d0] âœ“ Done 1.94s
2025-12-16 08:49:55,027 - src.llm.client - INFO - [qst:6409d0] âœ… HTTP 200 in 1.94s
2025-12-16 08:49:55,027 - src.llm.client - INFO - [qst:6409d0] ğŸ“¡ Stream active (200)
2025-12-16 08:49:55,027 - src.llm.client - INFO - [qst:6409d0] Starting stream parsing, waiting for first chunk...
2025-12-16 08:49:57,033 - src.llm.client - INFO - [qst:6409d0] ğŸ“Š 2.0s: 704c @351c/s (143ch, ~176t @88t/s)
2025-12-16 08:49:59,045 - src.llm.client - INFO - [qst:6409d0] ğŸ“Š 4.0s: 1386c @345c/s (285ch, ~346t @86t/s)
2025-12-16 08:50:01,050 - src.llm.client - INFO - [qst:6409d0] ğŸ“Š 6.0s: 2052c @341c/s (427ch, ~513t @85t/s)
2025-12-16 08:50:03,061 - src.llm.client - INFO - [qst:6409d0] ğŸ“Š 8.0s: 2744c @342c/s (569ch, ~686t @85t/s)
2025-12-16 08:50:05,070 - src.llm.client - INFO - [qst:6409d0] ğŸ“Š 10.0s: 3524c @351c/s (711ch, ~881t @88t/s)
2025-12-16 08:50:07,080 - src.llm.client - INFO - [qst:6409d0] ğŸ“Š 12.1s: 4375c @363c/s (853ch, ~1094t @91t/s)
2025-12-16 08:50:09,099 - src.llm.client - INFO - [qst:6409d0] ğŸ“Š 14.1s: 5120c @364c/s (989ch, ~1280t @91t/s)
2025-12-16 08:50:09,099 - src.llm.client - INFO - [qst:6409d0] âœ“ Done 16.01s: 5120c (~703w @320c/s)
2025-12-16 08:50:09,100 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 1, 'total_fixes': 3}
2025-12-16 08:50:09,100 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -     Context: Module 10 Session 10
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 4 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -     Context: Module 10 Session 10
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Predictive Coding â€“ Neural Basis (Session 10)
2025-12-16 08:50:09,100 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:50:09,102 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:50:09,104 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 10 completed
2025-12-16 08:50:09,104 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:50:09,105 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:50:09,105 - src.generate.orchestration.pipeline - INFO - Module 11: Model Learning & Adaptation (1 sessions)
2025-12-16 08:50:09,105 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:50:09,105 - src.generate.orchestration.pipeline - INFO - 
[11/15] Session 11: Parameter Estimation
2025-12-16 08:50:09,105 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:50:09,105 - src.generate.formats.lectures - INFO - Generating lecture for: Model Learning & Adaptation (Session 11/15)
2025-12-16 08:50:09,105 - src.llm.client - INFO - [lec:aa0591] ğŸš€ lec | m=gemma3:4b | p=3047c | t=180s
2025-12-16 08:50:09,105 - src.llm.client - INFO - [lec:aa0591] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:50:09,105 - src.llm.client - INFO - [lec:aa0591] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:50:09,106 - src.llm.client - INFO - [lec:aa0591] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6677 bytes, prompt=3047 chars
2025-12-16 08:50:09,107 - src.llm.client - INFO - [lec:aa0591] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:50:10,083 - src.llm.request_handler - INFO - [lec:aa0591] âœ“ Done 0.98s
2025-12-16 08:50:10,084 - src.llm.client - INFO - [lec:aa0591] âœ… HTTP 200 in 0.98s
2025-12-16 08:50:10,084 - src.llm.client - INFO - [lec:aa0591] ğŸ“¡ Stream active (200)
2025-12-16 08:50:10,084 - src.llm.client - INFO - [lec:aa0591] Starting stream parsing, waiting for first chunk...
2025-12-16 08:50:12,097 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 2.0s: 801c @398c/s (145ch, ~200t @99t/s)
2025-12-16 08:50:14,110 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 4.0s: 1511c @375c/s (289ch, ~378t @94t/s)
2025-12-16 08:50:16,119 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 6.0s: 2234c @370c/s (432ch, ~558t @93t/s)
2025-12-16 08:50:18,132 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 8.0s: 2957c @367c/s (575ch, ~739t @92t/s)
2025-12-16 08:50:20,142 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 10.1s: 3553c @353c/s (718ch, ~888t @88t/s)
2025-12-16 08:50:22,146 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 12.1s: 4306c @357c/s (860ch, ~1076t @89t/s)
2025-12-16 08:50:24,147 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 14.1s: 5128c @365c/s (1002ch, ~1282t @91t/s)
2025-12-16 08:50:26,147 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 16.1s: 5790c @360c/s (1144ch, ~1448t @90t/s)
2025-12-16 08:50:28,155 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 18.1s: 6473c @358c/s (1286ch, ~1618t @90t/s)
2025-12-16 08:50:30,161 - src.llm.client - INFO - [lec:aa0591] ğŸ“Š 20.1s: 7264c @362c/s (1428ch, ~1816t @90t/s)
2025-12-16 08:50:30,511 - src.llm.client - INFO - [lec:aa0591] âœ“ Done 21.41s: 7380c (~1127w @345c/s)
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO -     - Length: 7467 chars, 1140 words
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO -     - Content: 7 examples, 9 terms defined
2025-12-16 08:50:30,512 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 08:50:30,512 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 08:50:30,515 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:50:30,516 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:50:30,516 - src.generate.formats.labs - INFO - Generating lab 11 for: Model Learning & Adaptation (Session 11)
2025-12-16 08:50:30,516 - src.llm.client - INFO - [lab:60fca9] ğŸš€ lab | m=gemma3:4b | p=3331c | t=150s
2025-12-16 08:50:30,516 - src.llm.client - INFO - [lab:60fca9] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:50:30,516 - src.llm.client - INFO - [lab:60fca9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:50:30,517 - src.llm.client - INFO - [lab:60fca9] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3778 bytes, prompt=3331 chars
2025-12-16 08:50:30,518 - src.llm.client - INFO - [lab:60fca9] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:50:31,437 - src.llm.request_handler - INFO - [lab:60fca9] âœ“ Done 0.92s
2025-12-16 08:50:31,437 - src.llm.client - INFO - [lab:60fca9] âœ… HTTP 200 in 0.92s
2025-12-16 08:50:31,437 - src.llm.client - INFO - [lab:60fca9] ğŸ“¡ Stream active (200)
2025-12-16 08:50:31,437 - src.llm.client - INFO - [lab:60fca9] Starting stream parsing, waiting for first chunk...
2025-12-16 08:50:33,451 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 2.0s: 743c @369c/s (146ch, ~186t @92t/s)
2025-12-16 08:50:35,461 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 4.0s: 1369c @340c/s (291ch, ~342t @85t/s)
2025-12-16 08:50:37,465 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 6.0s: 2055c @341c/s (434ch, ~514t @85t/s)
2025-12-16 08:50:39,477 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 8.0s: 2629c @327c/s (576ch, ~657t @82t/s)
2025-12-16 08:50:41,485 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 10.0s: 3136c @312c/s (719ch, ~784t @78t/s)
2025-12-16 08:50:43,497 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 12.1s: 3615c @300c/s (862ch, ~904t @75t/s)
2025-12-16 08:50:45,499 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 14.1s: 4187c @298c/s (1004ch, ~1047t @74t/s)
2025-12-16 08:50:47,507 - src.llm.client - INFO - [lab:60fca9] ğŸ“Š 16.1s: 4842c @301c/s (1146ch, ~1210t @75t/s)
2025-12-16 08:50:47,784 - src.llm.client - INFO - [lab:60fca9] âœ“ Done 17.27s: 4917c (~743w @285c/s)
2025-12-16 08:50:47,784 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:50:47,784 - src.generate.formats.labs - INFO -     - Length: 5023 chars, 759 words
2025-12-16 08:50:47,784 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-16 08:50:47,784 - src.generate.formats.labs - INFO -     - Safety: 5 warnings
2025-12-16 08:50:47,784 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 08:50:47,787 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:50:47,788 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:50:47,788 - src.generate.formats.study_notes - INFO - Generating study notes for: Model Learning & Adaptation (Session 11)
2025-12-16 08:50:47,788 - src.llm.client - INFO - [stu:18332d] ğŸš€ stu | m=gemma3:4b | p=4428c | t=120s
2025-12-16 08:50:47,788 - src.llm.client - INFO - [stu:18332d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:50:47,788 - src.llm.client - INFO - [stu:18332d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:50:47,789 - src.llm.client - INFO - [stu:18332d] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8090 bytes, prompt=4428 chars
2025-12-16 08:50:47,789 - src.llm.client - INFO - [stu:18332d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:50:49,054 - src.llm.request_handler - INFO - [stu:18332d] âœ“ Done 1.26s
2025-12-16 08:50:49,054 - src.llm.client - INFO - [stu:18332d] âœ… HTTP 200 in 1.26s
2025-12-16 08:50:49,054 - src.llm.client - INFO - [stu:18332d] ğŸ“¡ Stream active (200)
2025-12-16 08:50:49,054 - src.llm.client - INFO - [stu:18332d] Starting stream parsing, waiting for first chunk...
2025-12-16 08:50:51,055 - src.llm.client - INFO - [stu:18332d] ğŸ“Š 2.0s: 797c @398c/s (142ch, ~199t @100t/s)
2025-12-16 08:50:53,061 - src.llm.client - INFO - [stu:18332d] ğŸ“Š 4.0s: 1509c @377c/s (285ch, ~377t @94t/s)
2025-12-16 08:50:55,070 - src.llm.client - INFO - [stu:18332d] ğŸ“Š 6.0s: 2235c @372c/s (428ch, ~559t @93t/s)
2025-12-16 08:50:57,071 - src.llm.client - INFO - [stu:18332d] ğŸ“Š 8.0s: 3038c @379c/s (570ch, ~760t @95t/s)
2025-12-16 08:50:59,084 - src.llm.client - INFO - [stu:18332d] ğŸ“Š 10.0s: 3793c @378c/s (712ch, ~948t @95t/s)
2025-12-16 08:50:59,405 - src.llm.client - INFO - [stu:18332d] âœ“ Done 11.62s: 3873c (~580w @333c/s)
2025-12-16 08:50:59,406 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:50:59,406 - src.generate.formats.study_notes - INFO -     - Length: 3935 chars, 591 words
2025-12-16 08:50:59,406 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:50:59,406 - src.generate.formats.study_notes - INFO -     - Key concepts: 7
2025-12-16 08:50:59,406 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 4 bullets
2025-12-16 08:50:59,406 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:50:59,408 - src.generate.processors.cleanup - INFO - Cleanup complete: 2 issues before, 0 issues after
2025-12-16 08:50:59,409 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:50:59,409 - src.generate.formats.diagrams - INFO - Generating diagram for: Maximum Likelihood Estimation (Model Learning & Adaptation)
2025-12-16 08:50:59,409 - src.llm.client - INFO - [dia:2d35c7] ğŸš€ dia | m=gemma3:4b | p=5772c | t=120s
2025-12-16 08:50:59,409 - src.llm.client - INFO - [dia:2d35c7] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:50:59,409 - src.llm.client - INFO - [dia:2d35c7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:50:59,410 - src.llm.client - INFO - [dia:2d35c7] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11081 bytes, prompt=5772 chars
2025-12-16 08:50:59,410 - src.llm.client - INFO - [dia:2d35c7] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:51:01,092 - src.llm.request_handler - INFO - [dia:2d35c7] âœ“ Done 1.68s
2025-12-16 08:51:01,093 - src.llm.client - INFO - [dia:2d35c7] âœ… HTTP 200 in 1.68s
2025-12-16 08:51:01,093 - src.llm.client - INFO - [dia:2d35c7] ğŸ“¡ Stream active (200)
2025-12-16 08:51:01,093 - src.llm.client - INFO - [dia:2d35c7] Starting stream parsing, waiting for first chunk...
2025-12-16 08:51:03,104 - src.llm.client - INFO - [dia:2d35c7] ğŸ“Š 2.0s: 493c @245c/s (144ch, ~123t @61t/s)
2025-12-16 08:51:05,128 - src.llm.client - INFO - [dia:2d35c7] ğŸ“Š 4.0s: 953c @236c/s (285ch, ~238t @59t/s)
2025-12-16 08:51:05,128 - src.llm.client - INFO - [dia:2d35c7] âœ“ Done 5.72s: 953c (~130w @167c/s)
2025-12-16 08:51:05,128 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Maximum Likelihood Estimation (Model Learning & Adaptation):
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO -     - Length: 869 chars (cleaned: 869 chars)
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO - [OK] Elements: 46 total (nodes: 19, connections: 27) âœ“
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:51:05,129 - src.generate.formats.diagrams - INFO - Generated diagram: 869 characters
2025-12-16 08:51:05,129 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:51:05,130 - src.generate.formats.questions - INFO - Generating 10 questions for: Model Learning & Adaptation (Session 11)
2025-12-16 08:51:05,130 - src.llm.client - INFO - [qst:b9ea9b] ğŸš€ qst | m=gemma3:4b | p=7320c | t=150s
2025-12-16 08:51:05,130 - src.llm.client - INFO - [qst:b9ea9b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:51:05,130 - src.llm.client - INFO - [qst:b9ea9b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:51:05,131 - src.llm.client - INFO - [qst:b9ea9b] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11031 bytes, prompt=7320 chars
2025-12-16 08:51:05,131 - src.llm.client - INFO - [qst:b9ea9b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:51:07,061 - src.llm.request_handler - INFO - [qst:b9ea9b] âœ“ Done 1.93s
2025-12-16 08:51:07,061 - src.llm.client - INFO - [qst:b9ea9b] âœ… HTTP 200 in 1.93s
2025-12-16 08:51:07,061 - src.llm.client - INFO - [qst:b9ea9b] ğŸ“¡ Stream active (200)
2025-12-16 08:51:07,062 - src.llm.client - INFO - [qst:b9ea9b] Starting stream parsing, waiting for first chunk...
2025-12-16 08:51:09,072 - src.llm.client - INFO - [qst:b9ea9b] ğŸ“Š 2.0s: 712c @354c/s (143ch, ~178t @89t/s)
2025-12-16 08:51:11,076 - src.llm.client - INFO - [qst:b9ea9b] ğŸ“Š 4.0s: 1419c @353c/s (286ch, ~355t @88t/s)
2025-12-16 08:51:13,079 - src.llm.client - INFO - [qst:b9ea9b] ğŸ“Š 6.0s: 2159c @359c/s (428ch, ~540t @90t/s)
2025-12-16 08:51:15,082 - src.llm.client - INFO - [qst:b9ea9b] ğŸ“Š 8.0s: 2901c @362c/s (570ch, ~725t @90t/s)
2025-12-16 08:51:17,091 - src.llm.client - INFO - [qst:b9ea9b] ğŸ“Š 10.0s: 3705c @369c/s (712ch, ~926t @92t/s)
2025-12-16 08:51:18,314 - src.llm.client - INFO - [qst:b9ea9b] âœ“ Done 13.18s: 4177c (~586w @317c/s)
2025-12-16 08:51:18,314 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 0, 'total_fixes': 3}
2025-12-16 08:51:18,314 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 08:51:18,315 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:51:18,315 - src.generate.formats.questions - WARNING -     Context: Module 11 Session 11
2025-12-16 08:51:18,315 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:51:18,315 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:51:18,315 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 08:51:18,315 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Model Learning & Adaptation (Session 11)
2025-12-16 08:51:18,315 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:51:18,317 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:51:18,319 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 11 completed
2025-12-16 08:51:18,319 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:51:18,319 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:51:18,319 - src.generate.orchestration.pipeline - INFO - Module 12: Reinforcement Learning â€“ Policy Optimization (1 sessions)
2025-12-16 08:51:18,319 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:51:18,319 - src.generate.orchestration.pipeline - INFO - 
[12/15] Session 12: Markov Decision Processes
2025-12-16 08:51:18,319 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:51:18,319 - src.generate.formats.lectures - INFO - Generating lecture for: Reinforcement Learning â€“ Policy Optimization (Session 12/15)
2025-12-16 08:51:18,320 - src.llm.client - INFO - [lec:1d0c0d] ğŸš€ lec | m=gemma3:4b | p=3091c | t=180s
2025-12-16 08:51:18,320 - src.llm.client - INFO - [lec:1d0c0d] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:51:18,320 - src.llm.client - INFO - [lec:1d0c0d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:51:18,321 - src.llm.client - INFO - [lec:1d0c0d] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6732 bytes, prompt=3091 chars
2025-12-16 08:51:18,321 - src.llm.client - INFO - [lec:1d0c0d] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:51:19,293 - src.llm.request_handler - INFO - [lec:1d0c0d] âœ“ Done 0.97s
2025-12-16 08:51:19,293 - src.llm.client - INFO - [lec:1d0c0d] âœ… HTTP 200 in 0.97s
2025-12-16 08:51:19,293 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“¡ Stream active (200)
2025-12-16 08:51:19,293 - src.llm.client - INFO - [lec:1d0c0d] Starting stream parsing, waiting for first chunk...
2025-12-16 08:51:21,301 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 2.0s: 773c @385c/s (144ch, ~193t @96t/s)
2025-12-16 08:51:23,307 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 4.0s: 1420c @354c/s (287ch, ~355t @88t/s)
2025-12-16 08:51:25,310 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 6.0s: 2068c @344c/s (428ch, ~517t @86t/s)
2025-12-16 08:51:27,327 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 8.0s: 2694c @335c/s (568ch, ~674t @84t/s)
2025-12-16 08:51:29,339 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 10.0s: 3324c @331c/s (698ch, ~831t @83t/s)
2025-12-16 08:51:31,353 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 12.1s: 3873c @321c/s (831ch, ~968t @80t/s)
2025-12-16 08:51:33,368 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 14.1s: 4561c @324c/s (968ch, ~1140t @81t/s)
2025-12-16 08:51:35,380 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 16.1s: 5238c @326c/s (1104ch, ~1310t @81t/s)
2025-12-16 08:51:37,388 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 18.1s: 5869c @324c/s (1246ch, ~1467t @81t/s)
2025-12-16 08:51:39,401 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 20.1s: 6418c @319c/s (1387ch, ~1604t @80t/s)
2025-12-16 08:51:41,408 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 22.1s: 7060c @319c/s (1526ch, ~1765t @80t/s)
2025-12-16 08:51:43,414 - src.llm.client - INFO - [lec:1d0c0d] ğŸ“Š 24.1s: 7825c @324c/s (1668ch, ~1956t @81t/s)
2025-12-16 08:51:44,252 - src.llm.client - INFO - [lec:1d0c0d] âœ“ Done 25.93s: 8130c (~1239w @314c/s)
2025-12-16 08:51:44,254 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:51:44,254 - src.generate.formats.lectures - INFO -     - Length: 8226 chars, 1252 words
2025-12-16 08:51:44,254 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:51:44,254 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:51:44,254 - src.generate.formats.lectures - INFO -     - Content: 8 examples, 8 terms defined
2025-12-16 08:51:44,254 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:51:44,257 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:51:44,258 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:51:44,258 - src.generate.formats.labs - INFO - Generating lab 12 for: Reinforcement Learning â€“ Policy Optimization (Session 12)
2025-12-16 08:51:44,258 - src.llm.client - INFO - [lab:6bc277] ğŸš€ lab | m=gemma3:4b | p=3348c | t=150s
2025-12-16 08:51:44,258 - src.llm.client - INFO - [lab:6bc277] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:51:44,258 - src.llm.client - INFO - [lab:6bc277] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:51:44,259 - src.llm.client - INFO - [lab:6bc277] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3793 bytes, prompt=3348 chars
2025-12-16 08:51:44,260 - src.llm.client - INFO - [lab:6bc277] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:51:45,219 - src.llm.request_handler - INFO - [lab:6bc277] âœ“ Done 0.96s
2025-12-16 08:51:45,219 - src.llm.client - INFO - [lab:6bc277] âœ… HTTP 200 in 0.96s
2025-12-16 08:51:45,219 - src.llm.client - INFO - [lab:6bc277] ğŸ“¡ Stream active (200)
2025-12-16 08:51:45,219 - src.llm.client - INFO - [lab:6bc277] Starting stream parsing, waiting for first chunk...
2025-12-16 08:51:47,225 - src.llm.client - INFO - [lab:6bc277] ğŸ“Š 2.0s: 661c @330c/s (146ch, ~165t @82t/s)
2025-12-16 08:51:49,238 - src.llm.client - INFO - [lab:6bc277] ğŸ“Š 4.0s: 1351c @336c/s (291ch, ~338t @84t/s)
2025-12-16 08:51:51,239 - src.llm.client - INFO - [lab:6bc277] ğŸ“Š 6.0s: 2025c @336c/s (434ch, ~506t @84t/s)
2025-12-16 08:51:53,251 - src.llm.client - INFO - [lab:6bc277] ğŸ“Š 8.0s: 2724c @339c/s (578ch, ~681t @85t/s)
2025-12-16 08:51:55,258 - src.llm.client - INFO - [lab:6bc277] ğŸ“Š 10.0s: 3288c @328c/s (721ch, ~822t @82t/s)
2025-12-16 08:51:57,266 - src.llm.client - INFO - [lab:6bc277] ğŸ“Š 12.0s: 4016c @333c/s (864ch, ~1004t @83t/s)
2025-12-16 08:51:59,267 - src.llm.client - INFO - [lab:6bc277] ğŸ“Š 14.0s: 4643c @331c/s (1003ch, ~1161t @83t/s)
2025-12-16 08:52:00,824 - src.llm.client - INFO - [lab:6bc277] âœ“ Done 16.57s: 5221c (~691w @315c/s)
2025-12-16 08:52:00,824 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:52:00,824 - src.generate.formats.labs - INFO -     - Length: 5339 chars, 707 words
2025-12-16 08:52:00,825 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:52:00,825 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-16 08:52:00,825 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 08:52:00,827 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:52:00,827 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:52:00,827 - src.generate.formats.study_notes - INFO - Generating study notes for: Reinforcement Learning â€“ Policy Optimization (Session 12)
2025-12-16 08:52:00,828 - src.llm.client - INFO - [stu:34b613] ğŸš€ stu | m=gemma3:4b | p=4443c | t=120s
2025-12-16 08:52:00,828 - src.llm.client - INFO - [stu:34b613] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:52:00,828 - src.llm.client - INFO - [stu:34b613] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:52:00,829 - src.llm.client - INFO - [stu:34b613] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8103 bytes, prompt=4443 chars
2025-12-16 08:52:00,829 - src.llm.client - INFO - [stu:34b613] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:52:02,115 - src.llm.request_handler - INFO - [stu:34b613] âœ“ Done 1.29s
2025-12-16 08:52:02,115 - src.llm.client - INFO - [stu:34b613] âœ… HTTP 200 in 1.29s
2025-12-16 08:52:02,115 - src.llm.client - INFO - [stu:34b613] ğŸ“¡ Stream active (200)
2025-12-16 08:52:02,115 - src.llm.client - INFO - [stu:34b613] Starting stream parsing, waiting for first chunk...
2025-12-16 08:52:04,120 - src.llm.client - INFO - [stu:34b613] ğŸ“Š 2.0s: 744c @371c/s (144ch, ~186t @93t/s)
2025-12-16 08:52:06,129 - src.llm.client - INFO - [stu:34b613] ğŸ“Š 4.0s: 1421c @354c/s (287ch, ~355t @89t/s)
2025-12-16 08:52:08,134 - src.llm.client - INFO - [stu:34b613] ğŸ“Š 6.0s: 2129c @354c/s (430ch, ~532t @88t/s)
2025-12-16 08:52:10,137 - src.llm.client - INFO - [stu:34b613] ğŸ“Š 8.0s: 2754c @343c/s (573ch, ~688t @86t/s)
2025-12-16 08:52:11,926 - src.llm.client - INFO - [stu:34b613] âœ“ Done 11.10s: 3265c (~491w @294c/s)
2025-12-16 08:52:11,926 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:52:11,926 - src.generate.formats.study_notes - INFO -     - Length: 3344 chars, 503 words
2025-12-16 08:52:11,926 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:52:11,926 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-16 08:52:11,926 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 2 bullets
2025-12-16 08:52:11,926 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:52:11,928 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:52:11,928 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:52:11,928 - src.generate.formats.diagrams - INFO - Generating diagram for: Reward Function (Reinforcement Learning â€“ Policy Optimization)
2025-12-16 08:52:11,928 - src.llm.client - INFO - [dia:e1ba8a] ğŸš€ dia | m=gemma3:4b | p=5766c | t=120s
2025-12-16 08:52:11,928 - src.generate.formats.diagrams - INFO - Generating diagram for: Transition Probabilities (Reinforcement Learning â€“ Policy Optimization)
2025-12-16 08:52:11,929 - src.llm.client - INFO - [dia:e1ba8a] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:52:11,929 - src.llm.client - INFO - [dia:d6f5b3] ğŸš€ dia | m=gemma3:4b | p=5784c | t=120s
2025-12-16 08:52:11,929 - src.llm.client - INFO - [dia:e1ba8a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:52:11,929 - src.llm.client - INFO - [dia:d6f5b3] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:52:11,929 - src.llm.client - INFO - [dia:d6f5b3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:52:11,931 - src.llm.client - INFO - [dia:d6f5b3] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11098 bytes, prompt=5784 chars
2025-12-16 08:52:11,931 - src.llm.client - INFO - [dia:e1ba8a] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11080 bytes, prompt=5766 chars
2025-12-16 08:52:11,931 - src.llm.client - INFO - [dia:d6f5b3] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:52:11,931 - src.llm.client - INFO - [dia:e1ba8a] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:52:13,662 - src.llm.request_handler - INFO - [dia:e1ba8a] âœ“ Done 1.73s
2025-12-16 08:52:13,663 - src.llm.client - INFO - [dia:e1ba8a] âœ… HTTP 200 in 1.73s
2025-12-16 08:52:13,664 - src.llm.client - INFO - [dia:e1ba8a] ğŸ“¡ Stream active (200)
2025-12-16 08:52:13,664 - src.llm.client - INFO - [dia:e1ba8a] Starting stream parsing, waiting for first chunk...
2025-12-16 08:52:15,667 - src.llm.client - INFO - [dia:e1ba8a] ğŸ“Š 2.0s: 512c @256c/s (143ch, ~128t @64t/s)
2025-12-16 08:52:17,677 - src.llm.client - INFO - [dia:e1ba8a] ğŸ“Š 4.0s: 875c @218c/s (286ch, ~219t @55t/s)
2025-12-16 08:52:17,957 - src.llm.client - INFO - [dia:e1ba8a] âœ“ Done 6.03s: 915c (~124w @152c/s)
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Reward Function (Reinforcement Learning â€“ Policy Optimization):
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO -     - Length: 594 chars (cleaned: 594 chars)
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO - [OK] Elements: 38 total (nodes: 11, connections: 27) âœ“
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:52:17,957 - src.generate.formats.diagrams - INFO - Generated diagram: 594 characters
2025-12-16 08:52:19,484 - src.llm.request_handler - INFO - [dia:d6f5b3] âœ“ Done 7.55s
2025-12-16 08:52:19,484 - src.llm.client - INFO - [dia:d6f5b3] âœ… HTTP 200 in 7.55s
2025-12-16 08:52:19,484 - src.llm.client - INFO - [dia:d6f5b3] ğŸ“¡ Stream active (200)
2025-12-16 08:52:19,484 - src.llm.client - INFO - [dia:d6f5b3] Starting stream parsing, waiting for first chunk...
2025-12-16 08:52:21,497 - src.llm.client - INFO - [dia:d6f5b3] ğŸ“Š 2.0s: 519c @258c/s (143ch, ~130t @64t/s)
2025-12-16 08:52:23,355 - src.llm.client - INFO - [dia:d6f5b3] âœ“ Done 11.43s: 913c (~136w @80c/s)
2025-12-16 08:52:23,355 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Transition Probabilities (Reinforcement Learning â€“ Policy Optimization):
2025-12-16 08:52:23,355 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:52:23,355 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:52:23,355 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:52:23,355 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:52:23,355 - src.generate.formats.diagrams - INFO -     - Length: 790 chars (cleaned: 790 chars)
2025-12-16 08:52:23,356 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:52:23,356 - src.generate.formats.diagrams - INFO - [OK] Elements: 49 total (nodes: 13, connections: 36) âœ“
2025-12-16 08:52:23,356 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-16 08:52:23,356 - src.generate.formats.diagrams - INFO - Generated diagram: 790 characters
2025-12-16 08:52:23,356 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:52:23,356 - src.generate.formats.questions - INFO - Generating 10 questions for: Reinforcement Learning â€“ Policy Optimization (Session 12)
2025-12-16 08:52:23,356 - src.llm.client - INFO - [qst:c20fd3] ğŸš€ qst | m=gemma3:4b | p=7342c | t=150s
2025-12-16 08:52:23,356 - src.llm.client - INFO - [qst:c20fd3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:52:23,356 - src.llm.client - INFO - [qst:c20fd3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:52:23,358 - src.llm.client - INFO - [qst:c20fd3] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11085 bytes, prompt=7342 chars
2025-12-16 08:52:23,358 - src.llm.client - INFO - [qst:c20fd3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:52:25,318 - src.llm.request_handler - INFO - [qst:c20fd3] âœ“ Done 1.96s
2025-12-16 08:52:25,319 - src.llm.client - INFO - [qst:c20fd3] âœ… HTTP 200 in 1.96s
2025-12-16 08:52:25,319 - src.llm.client - INFO - [qst:c20fd3] ğŸ“¡ Stream active (200)
2025-12-16 08:52:25,319 - src.llm.client - INFO - [qst:c20fd3] Starting stream parsing, waiting for first chunk...
2025-12-16 08:52:27,331 - src.llm.client - INFO - [qst:c20fd3] ğŸ“Š 2.0s: 691c @343c/s (144ch, ~173t @86t/s)
2025-12-16 08:52:29,332 - src.llm.client - INFO - [qst:c20fd3] ğŸ“Š 4.0s: 1365c @340c/s (287ch, ~341t @85t/s)
2025-12-16 08:52:31,339 - src.llm.client - INFO - [qst:c20fd3] ğŸ“Š 6.0s: 2062c @343c/s (430ch, ~516t @86t/s)
2025-12-16 08:52:33,351 - src.llm.client - INFO - [qst:c20fd3] ğŸ“Š 8.0s: 2723c @339c/s (573ch, ~681t @85t/s)
2025-12-16 08:52:35,362 - src.llm.client - INFO - [qst:c20fd3] ğŸ“Š 10.0s: 3493c @348c/s (716ch, ~873t @87t/s)
2025-12-16 08:52:37,364 - src.llm.client - INFO - [qst:c20fd3] ğŸ“Š 12.0s: 4207c @349c/s (858ch, ~1052t @87t/s)
2025-12-16 08:52:39,198 - src.llm.client - INFO - [qst:c20fd3] âœ“ Done 15.84s: 4853c (~708w @306c/s)
2025-12-16 08:52:39,199 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 5 question format issues: {'format_standardized': 0, 'question_marks_added': 5, 'mc_options_fixed': 0, 'total_fixes': 5}
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO - Applied 5 auto-fixes to questions
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO - [COMPLIANT] Questions generated âœ“
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO - [OK] Question marks: 10 total, 10 questions with '?' âœ“
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO -     - Question length: avg 11.8 words (range: 7-14)
2025-12-16 08:52:39,199 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-16 08:52:39,201 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:52:39,203 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 12 completed
2025-12-16 08:52:39,203 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:52:39,203 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:52:39,203 - src.generate.orchestration.pipeline - INFO - Module 13: Generative Models â€“ Hierarchical Structures (1 sessions)
2025-12-16 08:52:39,203 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:52:39,203 - src.generate.orchestration.pipeline - INFO - 
[13/15] Session 13: Multi-Level Models
2025-12-16 08:52:39,203 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:52:39,203 - src.generate.formats.lectures - INFO - Generating lecture for: Generative Models â€“ Hierarchical Structures (Session 13/15)
2025-12-16 08:52:39,204 - src.llm.client - INFO - [lec:8fd0bc] ğŸš€ lec | m=gemma3:4b | p=3084c | t=180s
2025-12-16 08:52:39,204 - src.llm.client - INFO - [lec:8fd0bc] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:52:39,204 - src.llm.client - INFO - [lec:8fd0bc] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:52:39,205 - src.llm.client - INFO - [lec:8fd0bc] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6724 bytes, prompt=3084 chars
2025-12-16 08:52:39,205 - src.llm.client - INFO - [lec:8fd0bc] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:52:40,207 - src.llm.request_handler - INFO - [lec:8fd0bc] âœ“ Done 1.00s
2025-12-16 08:52:40,208 - src.llm.client - INFO - [lec:8fd0bc] âœ… HTTP 200 in 1.00s
2025-12-16 08:52:40,208 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“¡ Stream active (200)
2025-12-16 08:52:40,208 - src.llm.client - INFO - [lec:8fd0bc] Starting stream parsing, waiting for first chunk...
2025-12-16 08:52:42,211 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 2.0s: 778c @388c/s (145ch, ~194t @97t/s)
2025-12-16 08:52:44,219 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 4.0s: 1549c @386c/s (288ch, ~387t @97t/s)
2025-12-16 08:52:46,223 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 6.0s: 2316c @385c/s (427ch, ~579t @96t/s)
2025-12-16 08:52:48,230 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 8.0s: 3047c @380c/s (571ch, ~762t @95t/s)
2025-12-16 08:52:50,235 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 10.0s: 3753c @374c/s (714ch, ~938t @94t/s)
2025-12-16 08:52:52,239 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 12.0s: 4464c @371c/s (856ch, ~1116t @93t/s)
2025-12-16 08:52:54,241 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 14.0s: 5149c @367c/s (997ch, ~1287t @92t/s)
2025-12-16 08:52:56,251 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 16.0s: 5909c @368c/s (1138ch, ~1477t @92t/s)
2025-12-16 08:52:58,258 - src.llm.client - INFO - [lec:8fd0bc] ğŸ“Š 18.0s: 6722c @372c/s (1280ch, ~1680t @93t/s)
2025-12-16 08:52:58,953 - src.llm.client - INFO - [lec:8fd0bc] âœ“ Done 19.75s: 7007c (~1001w @355c/s)
2025-12-16 08:52:58,955 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-16 08:52:58,955 - src.generate.formats.lectures - INFO -     - Length: 7112 chars, 1015 words
2025-12-16 08:52:58,955 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:52:58,955 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:52:58,955 - src.generate.formats.lectures - INFO -     - Content: 11 examples, 3 terms defined
2025-12-16 08:52:58,955 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:52:58,958 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:52:58,967 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:52:58,968 - src.generate.formats.labs - INFO - Generating lab 13 for: Generative Models â€“ Hierarchical Structures (Session 13)
2025-12-16 08:52:58,970 - src.llm.client - INFO - [lab:9e164b] ğŸš€ lab | m=gemma3:4b | p=3327c | t=150s
2025-12-16 08:52:58,970 - src.llm.client - INFO - [lab:9e164b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:52:58,971 - src.llm.client - INFO - [lab:9e164b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:52:58,977 - src.llm.client - INFO - [lab:9e164b] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3757 bytes, prompt=3327 chars
2025-12-16 08:52:58,978 - src.llm.client - INFO - [lab:9e164b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:52:59,882 - src.llm.request_handler - INFO - [lab:9e164b] âœ“ Done 0.90s
2025-12-16 08:52:59,882 - src.llm.client - INFO - [lab:9e164b] âœ… HTTP 200 in 0.90s
2025-12-16 08:52:59,882 - src.llm.client - INFO - [lab:9e164b] ğŸ“¡ Stream active (200)
2025-12-16 08:52:59,882 - src.llm.client - INFO - [lab:9e164b] Starting stream parsing, waiting for first chunk...
2025-12-16 08:53:01,887 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 2.0s: 732c @365c/s (145ch, ~183t @91t/s)
2025-12-16 08:53:03,895 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 4.0s: 1351c @337c/s (289ch, ~338t @84t/s)
2025-12-16 08:53:05,907 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 6.0s: 1938c @322c/s (433ch, ~484t @80t/s)
2025-12-16 08:53:07,911 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 8.0s: 2634c @328c/s (575ch, ~658t @82t/s)
2025-12-16 08:53:09,921 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 10.0s: 3289c @328c/s (719ch, ~822t @82t/s)
2025-12-16 08:53:11,926 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 12.0s: 4095c @340c/s (862ch, ~1024t @85t/s)
2025-12-16 08:53:13,938 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 14.1s: 4773c @340c/s (1005ch, ~1193t @85t/s)
2025-12-16 08:53:15,985 - src.llm.client - INFO - [lab:9e164b] ğŸ“Š 16.1s: 5554c @345c/s (1145ch, ~1388t @86t/s)
2025-12-16 08:53:15,985 - src.llm.client - INFO - [lab:9e164b] âœ“ Done 17.02s: 5554c (~746w @326c/s)
2025-12-16 08:53:15,986 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:53:15,986 - src.generate.formats.labs - INFO -     - Length: 5665 chars, 762 words
2025-12-16 08:53:15,986 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:53:15,986 - src.generate.formats.labs - INFO -     - Safety: 2 warnings
2025-12-16 08:53:15,986 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 08:53:15,988 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:53:15,989 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:53:15,989 - src.generate.formats.study_notes - INFO - Generating study notes for: Generative Models â€“ Hierarchical Structures (Session 13)
2025-12-16 08:53:15,989 - src.llm.client - INFO - [stu:611f24] ğŸš€ stu | m=gemma3:4b | p=4438c | t=120s
2025-12-16 08:53:15,989 - src.llm.client - INFO - [stu:611f24] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:53:15,989 - src.llm.client - INFO - [stu:611f24] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:53:15,991 - src.llm.client - INFO - [stu:611f24] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8083 bytes, prompt=4438 chars
2025-12-16 08:53:15,991 - src.llm.client - INFO - [stu:611f24] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:53:17,237 - src.llm.request_handler - INFO - [stu:611f24] âœ“ Done 1.25s
2025-12-16 08:53:17,237 - src.llm.client - INFO - [stu:611f24] âœ… HTTP 200 in 1.25s
2025-12-16 08:53:17,238 - src.llm.client - INFO - [stu:611f24] ğŸ“¡ Stream active (200)
2025-12-16 08:53:17,238 - src.llm.client - INFO - [stu:611f24] Starting stream parsing, waiting for first chunk...
2025-12-16 08:53:19,239 - src.llm.client - INFO - [stu:611f24] ğŸ“Š 2.0s: 757c @378c/s (142ch, ~189t @95t/s)
2025-12-16 08:53:21,246 - src.llm.client - INFO - [stu:611f24] ğŸ“Š 4.0s: 1579c @394c/s (284ch, ~395t @98t/s)
2025-12-16 08:53:23,258 - src.llm.client - INFO - [stu:611f24] ğŸ“Š 6.0s: 2403c @399c/s (426ch, ~601t @100t/s)
2025-12-16 08:53:25,268 - src.llm.client - INFO - [stu:611f24] ğŸ“Š 8.0s: 3186c @397c/s (569ch, ~796t @99t/s)
2025-12-16 08:53:27,278 - src.llm.client - INFO - [stu:611f24] ğŸ“Š 10.0s: 3895c @388c/s (713ch, ~974t @97t/s)
2025-12-16 08:53:29,279 - src.llm.client - INFO - [stu:611f24] ğŸ“Š 12.0s: 4628c @384c/s (856ch, ~1157t @96t/s)
2025-12-16 08:53:29,580 - src.llm.client - INFO - [stu:611f24] âœ“ Done 13.59s: 4685c (~649w @345c/s)
2025-12-16 08:53:29,580 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:53:29,580 - src.generate.formats.study_notes - INFO -     - Length: 4763 chars, 661 words
2025-12-16 08:53:29,580 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:53:29,580 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-16 08:53:29,580 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 3 bullets
2025-12-16 08:53:29,580 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:53:29,583 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:53:29,583 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:53:29,583 - src.generate.formats.diagrams - INFO - Generating diagram for: Recurrent Networks (Generative Models â€“ Hierarchical Structures)
2025-12-16 08:53:29,583 - src.llm.client - INFO - [dia:a39eb1] ğŸš€ dia | m=gemma3:4b | p=5764c | t=120s
2025-12-16 08:53:29,583 - src.llm.client - INFO - [dia:a39eb1] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:53:29,583 - src.llm.client - INFO - [dia:a39eb1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:53:29,585 - src.llm.client - INFO - [dia:a39eb1] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11078 bytes, prompt=5764 chars
2025-12-16 08:53:29,585 - src.llm.client - INFO - [dia:a39eb1] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:53:31,271 - src.llm.request_handler - INFO - [dia:a39eb1] âœ“ Done 1.69s
2025-12-16 08:53:31,271 - src.llm.client - INFO - [dia:a39eb1] âœ… HTTP 200 in 1.69s
2025-12-16 08:53:31,272 - src.llm.client - INFO - [dia:a39eb1] ğŸ“¡ Stream active (200)
2025-12-16 08:53:31,272 - src.llm.client - INFO - [dia:a39eb1] Starting stream parsing, waiting for first chunk...
2025-12-16 08:53:33,286 - src.llm.client - INFO - [dia:a39eb1] ğŸ“Š 2.0s: 579c @288c/s (144ch, ~145t @72t/s)
2025-12-16 08:53:34,843 - src.llm.client - INFO - [dia:a39eb1] âœ“ Done 5.26s: 833c (~108w @158c/s)
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Recurrent Networks (Generative Models â€“ Hierarchical Structures):
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 4 long nodes) âš ï¸
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO -     - Length: 743 chars (cleaned: 743 chars)
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 27 total (nodes: 12, connections: 15) âš ï¸
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 4 long nodes) âš ï¸
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 08:53:34,844 - src.generate.formats.diagrams - INFO - Generated diagram: 743 characters
2025-12-16 08:53:34,844 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:53:34,844 - src.generate.formats.questions - INFO - Generating 10 questions for: Generative Models â€“ Hierarchical Structures (Session 13)
2025-12-16 08:53:34,845 - src.llm.client - INFO - [qst:4ad4ba] ğŸš€ qst | m=gemma3:4b | p=7327c | t=150s
2025-12-16 08:53:34,845 - src.llm.client - INFO - [qst:4ad4ba] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:53:34,845 - src.llm.client - INFO - [qst:4ad4ba] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:53:34,846 - src.llm.client - INFO - [qst:4ad4ba] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11045 bytes, prompt=7327 chars
2025-12-16 08:53:34,846 - src.llm.client - INFO - [qst:4ad4ba] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:53:36,786 - src.llm.request_handler - INFO - [qst:4ad4ba] âœ“ Done 1.94s
2025-12-16 08:53:36,786 - src.llm.client - INFO - [qst:4ad4ba] âœ… HTTP 200 in 1.94s
2025-12-16 08:53:36,786 - src.llm.client - INFO - [qst:4ad4ba] ğŸ“¡ Stream active (200)
2025-12-16 08:53:36,786 - src.llm.client - INFO - [qst:4ad4ba] Starting stream parsing, waiting for first chunk...
2025-12-16 08:53:38,788 - src.llm.client - INFO - [qst:4ad4ba] ğŸ“Š 2.0s: 701c @350c/s (143ch, ~175t @88t/s)
2025-12-16 08:53:40,799 - src.llm.client - INFO - [qst:4ad4ba] ğŸ“Š 4.0s: 1429c @356c/s (283ch, ~357t @89t/s)
2025-12-16 08:53:42,807 - src.llm.client - INFO - [qst:4ad4ba] ğŸ“Š 6.0s: 2095c @348c/s (426ch, ~524t @87t/s)
2025-12-16 08:53:44,814 - src.llm.client - INFO - [qst:4ad4ba] ğŸ“Š 8.0s: 2756c @343c/s (569ch, ~689t @86t/s)
2025-12-16 08:53:46,820 - src.llm.client - INFO - [qst:4ad4ba] ğŸ“Š 10.0s: 3481c @347c/s (712ch, ~870t @87t/s)
2025-12-16 08:53:48,834 - src.llm.client - INFO - [qst:4ad4ba] ğŸ“Š 12.0s: 4291c @356c/s (854ch, ~1073t @89t/s)
2025-12-16 08:53:48,919 - src.llm.client - INFO - [qst:4ad4ba] âœ“ Done 14.07s: 4292c (~608w @305c/s)
2025-12-16 08:53:48,920 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 3 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:53:48,920 - src.generate.formats.questions - WARNING -     Context: Module 13 Session 13
2025-12-16 08:53:48,920 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:53:48,920 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:53:48,920 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 08:53:48,920 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Generative Models â€“ Hierarchical Structures (Session 13)
2025-12-16 08:53:48,920 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:53:48,922 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:53:48,924 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 13 completed
2025-12-16 08:53:48,924 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:53:48,924 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:53:48,924 - src.generate.orchestration.pipeline - INFO - Module 14: Model Selection & Validation (1 sessions)
2025-12-16 08:53:48,924 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:53:48,924 - src.generate.orchestration.pipeline - INFO - 
[14/15] Session 14: Cross-Validation
2025-12-16 08:53:48,924 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:53:48,924 - src.generate.formats.lectures - INFO - Generating lecture for: Model Selection & Validation (Session 14/15)
2025-12-16 08:53:48,924 - src.llm.client - INFO - [lec:47354b] ğŸš€ lec | m=gemma3:4b | p=3033c | t=180s
2025-12-16 08:53:48,924 - src.llm.client - INFO - [lec:47354b] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:53:48,924 - src.llm.client - INFO - [lec:47354b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:53:48,926 - src.llm.client - INFO - [lec:47354b] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6663 bytes, prompt=3033 chars
2025-12-16 08:53:48,926 - src.llm.client - INFO - [lec:47354b] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:53:49,910 - src.llm.request_handler - INFO - [lec:47354b] âœ“ Done 0.98s
2025-12-16 08:53:49,910 - src.llm.client - INFO - [lec:47354b] âœ… HTTP 200 in 0.98s
2025-12-16 08:53:49,910 - src.llm.client - INFO - [lec:47354b] ğŸ“¡ Stream active (200)
2025-12-16 08:53:49,910 - src.llm.client - INFO - [lec:47354b] Starting stream parsing, waiting for first chunk...
2025-12-16 08:53:51,919 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 2.0s: 750c @373c/s (146ch, ~188t @93t/s)
2025-12-16 08:53:53,932 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 4.0s: 1413c @351c/s (289ch, ~353t @88t/s)
2025-12-16 08:53:55,944 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 6.0s: 2143c @355c/s (433ch, ~536t @89t/s)
2025-12-16 08:53:57,956 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 8.0s: 2758c @343c/s (573ch, ~690t @86t/s)
2025-12-16 08:53:59,969 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 10.1s: 3274c @325c/s (715ch, ~818t @81t/s)
2025-12-16 08:54:01,978 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 12.1s: 3850c @319c/s (857ch, ~962t @80t/s)
2025-12-16 08:54:03,979 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 14.1s: 4448c @316c/s (998ch, ~1112t @79t/s)
2025-12-16 08:54:05,983 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 16.1s: 5145c @320c/s (1140ch, ~1286t @80t/s)
2025-12-16 08:54:07,983 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 18.1s: 5821c @322c/s (1281ch, ~1455t @81t/s)
2025-12-16 08:54:09,985 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 20.1s: 6486c @323c/s (1421ch, ~1622t @81t/s)
2025-12-16 08:54:11,999 - src.llm.client - INFO - [lec:47354b] ğŸ“Š 22.1s: 7160c @324c/s (1562ch, ~1790t @81t/s)
2025-12-16 08:54:13,134 - src.llm.client - INFO - [lec:47354b] âœ“ Done 24.21s: 7563c (~1141w @312c/s)
2025-12-16 08:54:13,135 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 08:54:13,135 - src.generate.formats.lectures - INFO -     - Length: 7652 chars, 1154 words
2025-12-16 08:54:13,135 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:54:13,135 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-16 08:54:13,136 - src.generate.formats.lectures - INFO -     - Content: 7 examples, 0 terms defined
2025-12-16 08:54:13,136 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-16 08:54:13,136 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 08:54:13,136 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 08:54:13,136 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 08:54:13,139 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:54:13,139 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:54:13,139 - src.generate.formats.labs - INFO - Generating lab 14 for: Model Selection & Validation (Session 14)
2025-12-16 08:54:13,139 - src.llm.client - INFO - [lab:b7b9db] ğŸš€ lab | m=gemma3:4b | p=3301c | t=150s
2025-12-16 08:54:13,139 - src.llm.client - INFO - [lab:b7b9db] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:54:13,139 - src.llm.client - INFO - [lab:b7b9db] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:54:13,141 - src.llm.client - INFO - [lab:b7b9db] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3750 bytes, prompt=3301 chars
2025-12-16 08:54:13,141 - src.llm.client - INFO - [lab:b7b9db] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:54:14,094 - src.llm.request_handler - INFO - [lab:b7b9db] âœ“ Done 0.95s
2025-12-16 08:54:14,094 - src.llm.client - INFO - [lab:b7b9db] âœ… HTTP 200 in 0.95s
2025-12-16 08:54:14,094 - src.llm.client - INFO - [lab:b7b9db] ğŸ“¡ Stream active (200)
2025-12-16 08:54:14,094 - src.llm.client - INFO - [lab:b7b9db] Starting stream parsing, waiting for first chunk...
2025-12-16 08:54:16,096 - src.llm.client - INFO - [lab:b7b9db] ğŸ“Š 2.0s: 722c @361c/s (143ch, ~180t @90t/s)
2025-12-16 08:54:18,109 - src.llm.client - INFO - [lab:b7b9db] ğŸ“Š 4.0s: 1354c @337c/s (286ch, ~338t @84t/s)
2025-12-16 08:54:20,118 - src.llm.client - INFO - [lab:b7b9db] ğŸ“Š 6.0s: 2030c @337c/s (428ch, ~508t @84t/s)
2025-12-16 08:54:22,119 - src.llm.client - INFO - [lab:b7b9db] ğŸ“Š 8.0s: 2551c @318c/s (568ch, ~638t @79t/s)
2025-12-16 08:54:24,131 - src.llm.client - INFO - [lab:b7b9db] ğŸ“Š 10.0s: 3213c @320c/s (710ch, ~803t @80t/s)
2025-12-16 08:54:26,142 - src.llm.client - INFO - [lab:b7b9db] ğŸ“Š 12.0s: 3628c @301c/s (852ch, ~907t @75t/s)
2025-12-16 08:54:28,143 - src.llm.client - INFO - [lab:b7b9db] ğŸ“Š 14.0s: 4227c @301c/s (992ch, ~1057t @75t/s)
2025-12-16 08:54:29,533 - src.llm.client - INFO - [lab:b7b9db] âœ“ Done 16.39s: 4715c (~717w @288c/s)
2025-12-16 08:54:29,533 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:54:29,533 - src.generate.formats.labs - INFO -     - Length: 4806 chars, 732 words
2025-12-16 08:54:29,533 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-16 08:54:29,533 - src.generate.formats.labs - INFO -     - Safety: 7 warnings
2025-12-16 08:54:29,533 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-16 08:54:29,536 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:54:29,536 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:54:29,536 - src.generate.formats.study_notes - INFO - Generating study notes for: Model Selection & Validation (Session 14)
2025-12-16 08:54:29,536 - src.llm.client - INFO - [stu:9077a5] ğŸš€ stu | m=gemma3:4b | p=4420c | t=120s
2025-12-16 08:54:29,536 - src.llm.client - INFO - [stu:9077a5] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:54:29,536 - src.llm.client - INFO - [stu:9077a5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:54:29,537 - src.llm.client - INFO - [stu:9077a5] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8084 bytes, prompt=4420 chars
2025-12-16 08:54:29,537 - src.llm.client - INFO - [stu:9077a5] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:54:30,824 - src.llm.request_handler - INFO - [stu:9077a5] âœ“ Done 1.29s
2025-12-16 08:54:30,824 - src.llm.client - INFO - [stu:9077a5] âœ… HTTP 200 in 1.29s
2025-12-16 08:54:30,824 - src.llm.client - INFO - [stu:9077a5] ğŸ“¡ Stream active (200)
2025-12-16 08:54:30,824 - src.llm.client - INFO - [stu:9077a5] Starting stream parsing, waiting for first chunk...
2025-12-16 08:54:32,835 - src.llm.client - INFO - [stu:9077a5] ğŸ“Š 2.0s: 730c @363c/s (142ch, ~182t @91t/s)
2025-12-16 08:54:34,848 - src.llm.client - INFO - [stu:9077a5] ğŸ“Š 4.0s: 1449c @360c/s (283ch, ~362t @90t/s)
2025-12-16 08:54:36,861 - src.llm.client - INFO - [stu:9077a5] ğŸ“Š 6.0s: 2201c @365c/s (425ch, ~550t @91t/s)
2025-12-16 08:54:38,863 - src.llm.client - INFO - [stu:9077a5] ğŸ“Š 8.0s: 2819c @351c/s (566ch, ~705t @88t/s)
2025-12-16 08:54:40,865 - src.llm.client - INFO - [stu:9077a5] ğŸ“Š 10.0s: 3363c @335c/s (706ch, ~841t @84t/s)
2025-12-16 08:54:42,868 - src.llm.client - INFO - [stu:9077a5] ğŸ“Š 12.0s: 4025c @334c/s (847ch, ~1006t @84t/s)
2025-12-16 08:54:43,116 - src.llm.client - INFO - [stu:9077a5] âœ“ Done 13.58s: 4103c (~624w @302c/s)
2025-12-16 08:54:43,116 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:54:43,116 - src.generate.formats.study_notes - INFO -     - Length: 4166 chars, 635 words
2025-12-16 08:54:43,116 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:54:43,116 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-16 08:54:43,116 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 3 bullets
2025-12-16 08:54:43,116 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:54:43,118 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:54:43,122 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:54:43,125 - src.generate.formats.diagrams - INFO - Generating diagram for: Hold-out Sets (Model Selection & Validation)
2025-12-16 08:54:43,125 - src.llm.client - INFO - [dia:ec0a8c] ğŸš€ dia | m=gemma3:4b | p=5737c | t=120s
2025-12-16 08:54:43,125 - src.llm.client - INFO - [dia:ec0a8c] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:54:43,125 - src.llm.client - INFO - [dia:ec0a8c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:54:43,127 - src.llm.client - INFO - [dia:ec0a8c] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11046 bytes, prompt=5737 chars
2025-12-16 08:54:43,130 - src.llm.client - INFO - [dia:ec0a8c] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:54:44,840 - src.llm.request_handler - INFO - [dia:ec0a8c] âœ“ Done 1.71s
2025-12-16 08:54:44,840 - src.llm.client - INFO - [dia:ec0a8c] âœ… HTTP 200 in 1.71s
2025-12-16 08:54:44,840 - src.llm.client - INFO - [dia:ec0a8c] ğŸ“¡ Stream active (200)
2025-12-16 08:54:44,840 - src.llm.client - INFO - [dia:ec0a8c] Starting stream parsing, waiting for first chunk...
2025-12-16 08:54:46,842 - src.llm.client - INFO - [dia:ec0a8c] ğŸ“Š 2.0s: 465c @232c/s (141ch, ~116t @58t/s)
2025-12-16 08:54:48,854 - src.llm.client - INFO - [dia:ec0a8c] ğŸ“Š 4.0s: 884c @220c/s (283ch, ~221t @55t/s)
2025-12-16 08:54:49,090 - src.llm.client - INFO - [dia:ec0a8c] âœ“ Done 5.96s: 938c (~128w @157c/s)
2025-12-16 08:54:49,090 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Hold-out Sets (Model Selection & Validation):
2025-12-16 08:54:49,090 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:54:49,090 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO - [FIXED] Removed classDef command (not supported in all renderers) âœ“
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO -     - Length: 739 chars (cleaned: 739 chars)
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO - [OK] Elements: 46 total (nodes: 19, connections: 27) âœ“
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 08:54:49,091 - src.generate.formats.diagrams - INFO - Generated diagram: 739 characters
2025-12-16 08:54:49,091 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:54:49,091 - src.generate.formats.questions - INFO - Generating 10 questions for: Model Selection & Validation (Session 14)
2025-12-16 08:54:49,091 - src.llm.client - INFO - [qst:1c455a] ğŸš€ qst | m=gemma3:4b | p=7306c | t=150s
2025-12-16 08:54:49,092 - src.llm.client - INFO - [qst:1c455a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:54:49,092 - src.llm.client - INFO - [qst:1c455a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:54:49,094 - src.llm.client - INFO - [qst:1c455a] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11033 bytes, prompt=7306 chars
2025-12-16 08:54:49,094 - src.llm.client - INFO - [qst:1c455a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:54:51,050 - src.llm.request_handler - INFO - [qst:1c455a] âœ“ Done 1.96s
2025-12-16 08:54:51,050 - src.llm.client - INFO - [qst:1c455a] âœ… HTTP 200 in 1.96s
2025-12-16 08:54:51,051 - src.llm.client - INFO - [qst:1c455a] ğŸ“¡ Stream active (200)
2025-12-16 08:54:51,051 - src.llm.client - INFO - [qst:1c455a] Starting stream parsing, waiting for first chunk...
2025-12-16 08:54:53,053 - src.llm.client - INFO - [qst:1c455a] ğŸ“Š 2.0s: 660c @330c/s (142ch, ~165t @82t/s)
2025-12-16 08:54:55,058 - src.llm.client - INFO - [qst:1c455a] ğŸ“Š 4.0s: 1330c @332c/s (282ch, ~332t @83t/s)
2025-12-16 08:54:57,067 - src.llm.client - INFO - [qst:1c455a] ğŸ“Š 6.0s: 2043c @340c/s (424ch, ~511t @85t/s)
2025-12-16 08:54:59,074 - src.llm.client - INFO - [qst:1c455a] ğŸ“Š 8.0s: 2648c @330c/s (562ch, ~662t @83t/s)
2025-12-16 08:55:01,082 - src.llm.client - INFO - [qst:1c455a] ğŸ“Š 10.0s: 3304c @329c/s (694ch, ~826t @82t/s)
2025-12-16 08:55:03,088 - src.llm.client - INFO - [qst:1c455a] ğŸ“Š 12.0s: 4030c @335c/s (832ch, ~1008t @84t/s)
2025-12-16 08:55:05,126 - src.llm.client - INFO - [qst:1c455a] ğŸ“Š 14.1s: 4716c @335c/s (969ch, ~1179t @84t/s)
2025-12-16 08:55:05,128 - src.llm.client - INFO - [qst:1c455a] âœ“ Done 16.04s: 4716c (~693w @294c/s)
2025-12-16 08:55:05,129 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 0, 'total_fixes': 3}
2025-12-16 08:55:05,129 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 08:55:05,129 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 5 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-16 08:55:05,130 - src.generate.formats.questions - WARNING -     Context: Module 14 Session 14
2025-12-16 08:55:05,130 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-16 08:55:05,130 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-16 08:55:05,130 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-16 08:55:05,130 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Model Selection & Validation (Session 14)
2025-12-16 08:55:05,130 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:55:05,132 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:55:05,134 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 14 completed
2025-12-16 08:55:05,134 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:55:05,134 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:55:05,134 - src.generate.orchestration.pipeline - INFO - Module 15: Applications & Future Directions (1 sessions)
2025-12-16 08:55:05,134 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:55:05,134 - src.generate.orchestration.pipeline - INFO - 
[15/15] Session 15: Concluding Remarks
2025-12-16 08:55:05,134 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-16 08:55:05,134 - src.generate.formats.lectures - INFO - Generating lecture for: Applications & Future Directions (Session 15/15)
2025-12-16 08:55:05,135 - src.llm.client - INFO - [lec:8d1994] ğŸš€ lec | m=gemma3:4b | p=3071c | t=180s
2025-12-16 08:55:05,135 - src.llm.client - INFO - [lec:8d1994] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-16 08:55:05,135 - src.llm.client - INFO - [lec:8d1994] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:55:05,137 - src.llm.client - INFO - [lec:8d1994] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6702 bytes, prompt=3071 chars
2025-12-16 08:55:05,137 - src.llm.client - INFO - [lec:8d1994] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-16 08:55:06,137 - src.llm.request_handler - INFO - [lec:8d1994] âœ“ Done 1.00s
2025-12-16 08:55:06,138 - src.llm.client - INFO - [lec:8d1994] âœ… HTTP 200 in 1.00s
2025-12-16 08:55:06,138 - src.llm.client - INFO - [lec:8d1994] ğŸ“¡ Stream active (200)
2025-12-16 08:55:06,138 - src.llm.client - INFO - [lec:8d1994] Starting stream parsing, waiting for first chunk...
2025-12-16 08:55:08,143 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 2.0s: 863c @430c/s (145ch, ~216t @108t/s)
2025-12-16 08:55:10,156 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 4.0s: 1653c @411c/s (287ch, ~413t @103t/s)
2025-12-16 08:55:12,168 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 6.0s: 2486c @412c/s (424ch, ~622t @103t/s)
2025-12-16 08:55:14,174 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 8.0s: 3291c @410c/s (559ch, ~823t @102t/s)
2025-12-16 08:55:16,180 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 10.0s: 4120c @410c/s (699ch, ~1030t @103t/s)
2025-12-16 08:55:18,184 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 12.0s: 4913c @408c/s (840ch, ~1228t @102t/s)
2025-12-16 08:55:20,195 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 14.1s: 5616c @400c/s (971ch, ~1404t @100t/s)
2025-12-16 08:55:22,209 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 16.1s: 6345c @395c/s (1110ch, ~1586t @99t/s)
2025-12-16 08:55:24,222 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 18.1s: 7108c @393c/s (1252ch, ~1777t @98t/s)
2025-12-16 08:55:26,230 - src.llm.client - INFO - [lec:8d1994] ğŸ“Š 20.1s: 7927c @395c/s (1390ch, ~1982t @99t/s)
2025-12-16 08:55:27,902 - src.llm.client - INFO - [lec:8d1994] âœ“ Done 22.77s: 8554c (~1207w @376c/s)
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO -     - Length: 8640 chars, 1219 words
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO -     - Content: 19 examples, 2 terms defined
2025-12-16 08:55:27,904 - src.generate.formats.lectures - WARNING - [WARNING] Too many examples (19, maximum 15, 4 excess - consider consolidating or removing less critical examples) âš ï¸
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-16 08:55:27,904 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-16 08:55:27,908 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:55:27,908 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-16 08:55:27,908 - src.generate.formats.labs - INFO - Generating lab 15 for: Applications & Future Directions (Session 15)
2025-12-16 08:55:27,909 - src.llm.client - INFO - [lab:e27b3b] ğŸš€ lab | m=gemma3:4b | p=3332c | t=150s
2025-12-16 08:55:27,909 - src.llm.client - INFO - [lab:e27b3b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:55:27,909 - src.llm.client - INFO - [lab:e27b3b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:55:27,910 - src.llm.client - INFO - [lab:e27b3b] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3758 bytes, prompt=3332 chars
2025-12-16 08:55:27,910 - src.llm.client - INFO - [lab:e27b3b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:55:28,814 - src.llm.request_handler - INFO - [lab:e27b3b] âœ“ Done 0.90s
2025-12-16 08:55:28,814 - src.llm.client - INFO - [lab:e27b3b] âœ… HTTP 200 in 0.90s
2025-12-16 08:55:28,814 - src.llm.client - INFO - [lab:e27b3b] ğŸ“¡ Stream active (200)
2025-12-16 08:55:28,814 - src.llm.client - INFO - [lab:e27b3b] Starting stream parsing, waiting for first chunk...
2025-12-16 08:55:30,819 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 2.0s: 785c @392c/s (141ch, ~196t @98t/s)
2025-12-16 08:55:32,824 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 4.0s: 1427c @356c/s (281ch, ~357t @89t/s)
2025-12-16 08:55:34,825 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 6.0s: 1867c @311c/s (423ch, ~467t @78t/s)
2025-12-16 08:55:36,834 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 8.0s: 2571c @321c/s (565ch, ~643t @80t/s)
2025-12-16 08:55:38,846 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 10.0s: 3072c @306c/s (704ch, ~768t @77t/s)
2025-12-16 08:55:40,851 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 12.0s: 3598c @299c/s (845ch, ~900t @75t/s)
2025-12-16 08:55:42,856 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 14.0s: 3953c @282c/s (988ch, ~988t @70t/s)
2025-12-16 08:55:44,869 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 16.1s: 4472c @279c/s (1132ch, ~1118t @70t/s)
2025-12-16 08:55:46,874 - src.llm.client - INFO - [lab:e27b3b] ğŸ“Š 18.1s: 5170c @286c/s (1275ch, ~1292t @72t/s)
2025-12-16 08:55:47,515 - src.llm.client - INFO - [lab:e27b3b] âœ“ Done 19.61s: 5405c (~810w @276c/s)
2025-12-16 08:55:47,515 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-16 08:55:47,515 - src.generate.formats.labs - INFO -     - Length: 5508 chars, 826 words
2025-12-16 08:55:47,515 - src.generate.formats.labs - INFO -     - Procedure: 13 steps
2025-12-16 08:55:47,515 - src.generate.formats.labs - INFO -     - Safety: 7 warnings
2025-12-16 08:55:47,515 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-16 08:55:47,518 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:55:47,518 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-16 08:55:47,518 - src.generate.formats.study_notes - INFO - Generating study notes for: Applications & Future Directions (Session 15)
2025-12-16 08:55:47,518 - src.llm.client - INFO - [stu:5aee0f] ğŸš€ stu | m=gemma3:4b | p=4438c | t=120s
2025-12-16 08:55:47,519 - src.llm.client - INFO - [stu:5aee0f] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:55:47,519 - src.llm.client - INFO - [stu:5aee0f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:55:47,520 - src.llm.client - INFO - [stu:5aee0f] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8079 bytes, prompt=4438 chars
2025-12-16 08:55:47,520 - src.llm.client - INFO - [stu:5aee0f] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:55:48,741 - src.llm.request_handler - INFO - [stu:5aee0f] âœ“ Done 1.22s
2025-12-16 08:55:48,741 - src.llm.client - INFO - [stu:5aee0f] âœ… HTTP 200 in 1.22s
2025-12-16 08:55:48,741 - src.llm.client - INFO - [stu:5aee0f] ğŸ“¡ Stream active (200)
2025-12-16 08:55:48,741 - src.llm.client - INFO - [stu:5aee0f] Starting stream parsing, waiting for first chunk...
2025-12-16 08:55:50,755 - src.llm.client - INFO - [stu:5aee0f] ğŸ“Š 2.0s: 838c @416c/s (143ch, ~210t @104t/s)
2025-12-16 08:55:52,760 - src.llm.client - INFO - [stu:5aee0f] ğŸ“Š 4.0s: 1667c @415c/s (286ch, ~417t @104t/s)
2025-12-16 08:55:54,768 - src.llm.client - INFO - [stu:5aee0f] ğŸ“Š 6.0s: 2466c @409c/s (428ch, ~616t @102t/s)
2025-12-16 08:55:56,777 - src.llm.client - INFO - [stu:5aee0f] ğŸ“Š 8.0s: 3256c @405c/s (571ch, ~814t @101t/s)
2025-12-16 08:55:57,391 - src.llm.client - INFO - [stu:5aee0f] âœ“ Done 9.87s: 3482c (~484w @353c/s)
2025-12-16 08:55:57,391 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-16 08:55:57,391 - src.generate.formats.study_notes - INFO -     - Length: 3549 chars, 495 words
2025-12-16 08:55:57,391 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-16 08:55:57,391 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-16 08:55:57,391 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 0 bullets
2025-12-16 08:55:57,391 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-16 08:55:57,393 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:55:57,393 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-16 08:55:57,393 - src.generate.formats.diagrams - INFO - Generating diagram for: Synthesis of Concepts (Applications & Future Directions)
2025-12-16 08:55:57,393 - src.generate.formats.diagrams - INFO - Generating diagram for: Future Research (Applications & Future Directions)
2025-12-16 08:55:57,393 - src.llm.client - INFO - [dia:ae6707] ğŸš€ dia | m=gemma3:4b | p=5759c | t=120s
2025-12-16 08:55:57,394 - src.llm.client - INFO - [dia:292319] ğŸš€ dia | m=gemma3:4b | p=5747c | t=120s
2025-12-16 08:55:57,394 - src.llm.client - INFO - [dia:ae6707] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:55:57,394 - src.llm.client - INFO - [dia:292319] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-16 08:55:57,394 - src.llm.client - INFO - [dia:ae6707] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:55:57,394 - src.llm.client - INFO - [dia:292319] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:55:57,396 - src.llm.client - INFO - [dia:292319] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11056 bytes, prompt=5747 chars
2025-12-16 08:55:57,396 - src.llm.client - INFO - [dia:ae6707] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11068 bytes, prompt=5759 chars
2025-12-16 08:55:57,396 - src.llm.client - INFO - [dia:292319] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:55:57,396 - src.llm.client - INFO - [dia:ae6707] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-16 08:55:59,129 - src.llm.request_handler - INFO - [dia:292319] âœ“ Done 1.73s
2025-12-16 08:55:59,129 - src.llm.client - INFO - [dia:292319] âœ… HTTP 200 in 1.73s
2025-12-16 08:55:59,129 - src.llm.client - INFO - [dia:292319] ğŸ“¡ Stream active (200)
2025-12-16 08:55:59,129 - src.llm.client - INFO - [dia:292319] Starting stream parsing, waiting for first chunk...
2025-12-16 08:56:01,137 - src.llm.client - INFO - [dia:292319] ğŸ“Š 2.0s: 555c @276c/s (142ch, ~139t @69t/s)
2025-12-16 08:56:03,146 - src.llm.client - INFO - [dia:292319] ğŸ“Š 4.0s: 1022c @254c/s (286ch, ~256t @64t/s)
2025-12-16 08:56:03,837 - src.llm.client - INFO - [dia:292319] âœ“ Done 6.44s: 1131c (~147w @176c/s)
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Future Research (Applications & Future Directions):
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING - [WARNING] Only 9 nodes found (require at least 10, need 1 more - add more nodes to the diagram) âš ï¸
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 9 nodes found (require at least 10, need 1 more - add more nodes to the diagram) ğŸ”´
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -     Context: Module 15 Session 15
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 9 nodes found (require at least 10, need 1 more - add more nodes to the diagram) ğŸ”´
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -     Context: Module 15 Session 15
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-16 08:56:03,838 - src.generate.formats.diagrams - WARNING -   Retry attempt 1/1 for diagram: Future Research (Applications & Future Directions)
2025-12-16 08:56:03,839 - src.generate.formats.diagrams - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-16 08:56:05,348 - src.llm.request_handler - INFO - [dia:ae6707] âœ“ Done 7.95s
2025-12-16 08:56:05,348 - src.llm.client - INFO - [dia:ae6707] âœ… HTTP 200 in 7.95s
2025-12-16 08:56:05,348 - src.llm.client - INFO - [dia:ae6707] ğŸ“¡ Stream active (200)
2025-12-16 08:56:05,348 - src.llm.client - INFO - [dia:ae6707] Starting stream parsing, waiting for first chunk...
2025-12-16 08:56:07,353 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 2.0s: 549c @274c/s (143ch, ~137t @68t/s)
2025-12-16 08:56:09,357 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 4.0s: 927c @231c/s (286ch, ~232t @58t/s)
2025-12-16 08:56:11,360 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 6.0s: 1069c @178c/s (428ch, ~267t @44t/s)
2025-12-16 08:56:13,367 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 8.0s: 1212c @151c/s (571ch, ~303t @38t/s)
2025-12-16 08:56:15,369 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 10.0s: 1354c @135c/s (713ch, ~338t @34t/s)
2025-12-16 08:56:17,383 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 12.0s: 1497c @124c/s (856ch, ~374t @31t/s)
2025-12-16 08:56:19,389 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 14.0s: 1639c @117c/s (998ch, ~410t @29t/s)
2025-12-16 08:56:21,398 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 16.0s: 1777c @111c/s (1136ch, ~444t @28t/s)
2025-12-16 08:56:23,402 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 18.1s: 1918c @106c/s (1277ch, ~480t @27t/s)
2025-12-16 08:56:25,413 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 20.1s: 2060c @103c/s (1419ch, ~515t @26t/s)
2025-12-16 08:56:27,422 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 22.1s: 2202c @100c/s (1561ch, ~550t @25t/s)
2025-12-16 08:56:29,423 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 24.1s: 2343c @97c/s (1702ch, ~586t @24t/s)
2025-12-16 08:56:31,436 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 26.1s: 2485c @95c/s (1844ch, ~621t @24t/s)
2025-12-16 08:56:33,438 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 28.1s: 2626c @93c/s (1985ch, ~656t @23t/s)
2025-12-16 08:56:35,448 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 30.1s: 2768c @92c/s (2127ch, ~692t @23t/s)
2025-12-16 08:56:37,450 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 32.1s: 2909c @91c/s (2268ch, ~727t @23t/s)
2025-12-16 08:56:39,455 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 34.1s: 3049c @89c/s (2408ch, ~762t @22t/s)
2025-12-16 08:56:41,463 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 36.1s: 3186c @88c/s (2545ch, ~796t @22t/s)
2025-12-16 08:56:43,476 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 38.1s: 3326c @87c/s (2685ch, ~832t @22t/s)
2025-12-16 08:56:45,483 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 40.1s: 3468c @86c/s (2827ch, ~867t @22t/s)
2025-12-16 08:56:47,497 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 42.1s: 3610c @86c/s (2969ch, ~902t @21t/s)
2025-12-16 08:56:49,498 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 44.2s: 3751c @85c/s (3110ch, ~938t @21t/s)
2025-12-16 08:56:51,501 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 46.2s: 3892c @84c/s (3251ch, ~973t @21t/s)
2025-12-16 08:56:53,515 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 48.2s: 4033c @84c/s (3392ch, ~1008t @21t/s)
2025-12-16 08:56:55,517 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 50.2s: 4171c @83c/s (3530ch, ~1043t @21t/s)
2025-12-16 08:56:57,528 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 52.2s: 4307c @83c/s (3666ch, ~1077t @21t/s)
2025-12-16 08:56:59,538 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 54.2s: 4440c @82c/s (3799ch, ~1110t @20t/s)
2025-12-16 08:57:01,539 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 56.2s: 4575c @81c/s (3934ch, ~1144t @20t/s)
2025-12-16 08:57:03,547 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 58.2s: 4712c @81c/s (4071ch, ~1178t @20t/s)
2025-12-16 08:57:05,560 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 60.2s: 5025c @83c/s (4209ch, ~1256t @21t/s)
2025-12-16 08:57:07,573 - src.llm.client - INFO - [dia:ae6707] ğŸ“Š 62.2s: 5610c @90c/s (4346ch, ~1402t @23t/s)
2025-12-16 08:57:09,472 - src.llm.client - INFO - [dia:ae6707] âœ“ Done 72.08s: 6137c (~345w @85c/s)
2025-12-16 08:57:09,472 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Synthesis of Concepts (Applications & Future Directions):
2025-12-16 08:57:09,472 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-16 08:57:09,472 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-16 08:57:09,472 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-16 08:57:09,472 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-16 08:57:09,473 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-16 08:57:09,473 - src.generate.formats.diagrams - INFO -     - Length: 727 chars (cleaned: 727 chars)
2025-12-16 08:57:09,473 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-16 08:57:09,473 - src.generate.formats.diagrams - INFO - [OK] Elements: 38 total (nodes: 9, connections: 29) âœ“
2025-12-16 08:57:09,473 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-16 08:57:09,473 - src.generate.formats.diagrams - INFO - Generated diagram: 727 characters
2025-12-16 08:57:09,473 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-16 08:57:09,473 - src.generate.formats.questions - INFO - Generating 10 questions for: Applications & Future Directions (Session 15)
2025-12-16 08:57:09,473 - src.llm.client - INFO - [qst:dc0ff8] ğŸš€ qst | m=gemma3:4b | p=7329c | t=150s
2025-12-16 08:57:09,474 - src.llm.client - INFO - [qst:dc0ff8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-16 08:57:09,474 - src.llm.client - INFO - [qst:dc0ff8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-16 08:57:09,475 - src.llm.client - INFO - [qst:dc0ff8] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11015 bytes, prompt=7329 chars
2025-12-16 08:57:09,475 - src.llm.client - INFO - [qst:dc0ff8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-16 08:57:11,492 - src.llm.request_handler - INFO - [qst:dc0ff8] âœ“ Done 2.02s
2025-12-16 08:57:11,492 - src.llm.client - INFO - [qst:dc0ff8] âœ… HTTP 200 in 2.02s
2025-12-16 08:57:11,492 - src.llm.client - INFO - [qst:dc0ff8] ğŸ“¡ Stream active (200)
2025-12-16 08:57:11,492 - src.llm.client - INFO - [qst:dc0ff8] Starting stream parsing, waiting for first chunk...
2025-12-16 08:57:13,502 - src.llm.client - INFO - [qst:dc0ff8] ğŸ“Š 2.0s: 703c @350c/s (139ch, ~176t @87t/s)
2025-12-16 08:57:15,503 - src.llm.client - INFO - [qst:dc0ff8] ğŸ“Š 4.0s: 1333c @332c/s (275ch, ~333t @83t/s)
2025-12-16 08:57:17,511 - src.llm.client - INFO - [qst:dc0ff8] ğŸ“Š 6.0s: 2001c @332c/s (408ch, ~500t @83t/s)
2025-12-16 08:57:19,525 - src.llm.client - INFO - [qst:dc0ff8] ğŸ“Š 8.0s: 2666c @332c/s (542ch, ~666t @83t/s)
2025-12-16 08:57:21,529 - src.llm.client - INFO - [qst:dc0ff8] ğŸ“Š 10.0s: 3319c @331c/s (678ch, ~830t @83t/s)
2025-12-16 08:57:23,613 - src.llm.client - INFO - [qst:dc0ff8] ğŸ“Š 12.1s: 4010c @331c/s (806ch, ~1002t @83t/s)
2025-12-16 08:57:24,551 - src.llm.client - INFO - [qst:dc0ff8] âœ“ Done 15.08s: 4384c (~614w @291c/s)
2025-12-16 08:57:24,552 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 0, 'total_fixes': 3}
2025-12-16 08:57:24,552 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-16 08:57:24,553 - src.generate.formats.questions - INFO - [NEEDS REVIEW] Questions generated âš ï¸
2025-12-16 08:57:24,553 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-16 08:57:24,553 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-16 08:57:24,553 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-16 08:57:24,553 - src.generate.formats.questions - INFO - [WARNING] Question marks: 11 total, 10 questions with '?' âš ï¸
2025-12-16 08:57:24,553 - src.generate.formats.questions - INFO -     - Question length: avg 12.0 words (range: 8-16)
2025-12-16 08:57:24,553 - src.generate.formats.questions - INFO -     - MC explanations: 2/5 have proper length (20-50 words)
2025-12-16 08:57:24,553 - src.generate.formats.questions - WARNING - [WARNING] Explanation length: 3 MC explanations are too short (<20 words - explanations should be 2-3 sentences, roughly 20-50 words) âš ï¸
2025-12-16 08:57:24,553 - src.generate.formats.questions - WARNING - [WARNING] Explanation quality: 3 MC explanations may be too short or too long (target: 2-3 sentences, 20-50 words) âš ï¸
2025-12-16 08:57:24,555 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 15 completed
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - QUALITY SCORE SUMMARY
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - Average Quality Score: 98.2/100
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - Overall Quality: excellent
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - Quality Distribution: {'excellent': 14, 'good': 1}
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - WARNING - Cross-session consistency: 4 issues found
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO -   Recommendation: Consider adding intermediate sessions to bridge 3 concept gaps
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO -   Recommendation: Consider reorganizing 1 related topic pairs that are far apart
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - [ALL COMPLIANT] Primary Materials Generation - Summary âœ…
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO -   Items Processed: 15
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT] Successful: 15
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO -     - [ERROR] Failed: 0
2025-12-16 08:57:24,557 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -   Compliance Breakdown:
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT]: 15
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - [NEEDS REVIEW]: 0
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - [CRITICAL]: 0
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -   Issue Statistics:
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - Total Issues: 0
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - Critical Errors: 0
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - Warnings: 0
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO - 
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -   Recommendations:
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - All content generated successfully
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO -     - No issues detected
2025-12-16 08:57:24,558 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-16 08:57:24,558 - generate_primary - INFO - 
================================================================================
2025-12-16 08:57:24,558 - generate_primary - INFO - PRIMARY MATERIALS COMPLETE
2025-12-16 08:57:24,558 - generate_primary - INFO - ================================================================================
2025-12-16 08:57:24,558 - generate_primary - INFO - Total sessions processed: 15
2025-12-16 08:57:24,558 - generate_primary - INFO - Successful: 15
2025-12-16 08:57:24,558 - generate_primary - INFO - Failed: 0
2025-12-16 08:57:24,558 - generate_primary - INFO - 
================================================================================
2025-12-16 08:57:24,558 - generate_primary - INFO - EXIT CODE: 0 (SUCCESS)
2025-12-16 08:57:24,558 - generate_primary - INFO - ================================================================================
2025-12-16 08:57:24,558 - generate_primary - INFO - All sessions processed successfully with no critical issues
2025-12-16 08:57:24,558 - generate_primary - INFO - ================================================================================
