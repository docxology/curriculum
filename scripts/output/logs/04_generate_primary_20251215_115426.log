2025-12-15 11:54:26,442 - root - INFO - Logging to file: /Users/mini/Documents/GitHub/curriculum/scripts/output/logs/04_generate_primary_20251215_115426.log
2025-12-15 11:54:26,442 - generate_primary - INFO - 
2025-12-15 11:54:26,442 - generate_primary - INFO - ğŸ“š STAGE 04: PRIMARY MATERIALS (Session-Based)
2025-12-15 11:54:26,442 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-15 11:54:26,442 - generate_primary - INFO - Generating materials PER SESSION (not per module)
2025-12-15 11:54:26,442 - generate_primary - INFO - Output structure: output/modules/module_XX/session_YY/[material].md
2025-12-15 11:54:26,442 - generate_primary - INFO - 
2025-12-15 11:54:26,442 - src.config.loader - INFO - Initialized ConfigLoader with directory: /Users/mini/Documents/GitHub/curriculum/config
2025-12-15 11:54:26,443 - src.config.loader - INFO - Course configuration validated successfully
2025-12-15 11:54:26,455 - src.config.loader - INFO - All configurations validated successfully
2025-12-15 11:54:26,455 - generate_primary - INFO - PRIMARY ARTIFACTS GENERATED PER SESSION:
2025-12-15 11:54:26,455 - generate_primary - INFO -   1. lecture.md - Comprehensive instructional content
2025-12-15 11:54:26,455 - generate_primary - INFO -   2. lab.md - Laboratory exercise with procedures
2025-12-15 11:54:26,455 - generate_primary - INFO -   3. study_notes.md - Concise session summary
2025-12-15 11:54:26,455 - generate_primary - INFO -   4. diagram_1.mmd, diagram_2.mmd, ... (up to 4 diagrams)
2025-12-15 11:54:26,455 - generate_primary - INFO -   5. questions.md - Comprehension assessment questions
2025-12-15 11:54:26,455 - generate_primary - INFO - 
2025-12-15 11:54:26,455 - generate_primary - INFO - 
2025-12-15 11:54:26,455 - generate_primary - INFO - âš™ï¸ CONFIGURATION
2025-12-15 11:54:26,455 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-15 11:54:26,455 - generate_primary - INFO -   â€¢ Diagrams per Session: 4
2025-12-15 11:54:26,455 - generate_primary - INFO -   â€¢ Log File: output/logs/04_generate_primary_20251215_115426.log
2025-12-15 11:54:26,455 - generate_primary - INFO - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-15 11:54:26,456 - src.config.loader - INFO - Found most recent outline: /Users/mini/Documents/GitHub/curriculum/scripts/output/active_inference/outlines/course_outline_20251215_115426.json
2025-12-15 11:54:26,456 - generate_primary - INFO - Using most recent outline: /Users/mini/Documents/GitHub/curriculum/scripts/output/active_inference/outlines/course_outline_20251215_115426.json
2025-12-15 11:54:26,456 - generate_primary - INFO - 
2025-12-15 11:54:26,456 - generate_primary - INFO - Processing ALL modules from outline
2025-12-15 11:54:26,456 - src.generate.orchestration.pipeline - INFO - Initializing Educational Course Generator pipeline...
2025-12-15 11:54:26,456 - src.llm.client - INFO - Initialized OllamaClient: model=gemma3:4b, url=http://localhost:11434/api/generate
2025-12-15 11:54:26,456 - src.generate.stages.stage1_outline - INFO - Initialized OutlineGenerator
2025-12-15 11:54:26,456 - src.generate.orchestration.pipeline - INFO - Pipeline initialized successfully
2025-12-15 11:54:26,456 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 11:54:26,456 - src.generate.orchestration.pipeline - INFO - STAGE 2: Generating Primary Content (Session-Based)
2025-12-15 11:54:26,456 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 11:54:26,456 - src.config.loader - INFO - Found most recent outline: /Users/mini/Documents/GitHub/curriculum/scripts/output/active_inference/outlines/course_outline_20251215_115426.json
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO - Processing 10 modules with session-based generation
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO - Using course-specific output directory: output/active_inference/
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO - Module 1: Introduction to Active Inference (1 sessions)
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO - 
[1/20] Session 1: Perception & Action Basics
2025-12-15 11:54:26,457 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 11:54:26,457 - src.generate.formats.lectures - INFO - Generating lecture for: Introduction to Active Inference (Session 1/20)
2025-12-15 11:54:26,457 - src.llm.client - INFO - [lec:0868a5] ğŸš€ lec | m=gemma3:4b | p=3137c | t=180s
2025-12-15 11:54:26,457 - src.llm.client - INFO - [lec:0868a5] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 11:54:26,457 - src.llm.client - INFO - [lec:0868a5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:54:26,462 - src.llm.client - INFO - [lec:0868a5] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6771 bytes, prompt=3137 chars
2025-12-15 11:54:26,462 - src.llm.client - INFO - [lec:0868a5] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 11:54:28,330 - src.llm.request_handler - INFO - [lec:0868a5] âœ“ Done 1.87s
2025-12-15 11:54:28,330 - src.llm.client - INFO - [lec:0868a5] âœ… HTTP 200 in 1.87s
2025-12-15 11:54:28,330 - src.llm.client - INFO - [lec:0868a5] ğŸ“¡ Stream active (200)
2025-12-15 11:54:28,330 - src.llm.client - INFO - [lec:0868a5] Starting stream parsing, waiting for first chunk...
2025-12-15 11:54:30,358 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 2.0s: 404c @199c/s (71ch, ~101t @50t/s)
2025-12-15 11:54:32,363 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 4.0s: 825c @205c/s (141ch, ~206t @51t/s)
2025-12-15 11:54:34,376 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 6.0s: 1100c @182c/s (211ch, ~275t @45t/s)
2025-12-15 11:54:36,379 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 8.0s: 1436c @178c/s (280ch, ~359t @45t/s)
2025-12-15 11:54:38,384 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 10.1s: 1741c @173c/s (349ch, ~435t @43t/s)
2025-12-15 11:54:40,390 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 12.1s: 2072c @172c/s (418ch, ~518t @43t/s)
2025-12-15 11:54:42,395 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 14.1s: 2469c @176c/s (487ch, ~617t @44t/s)
2025-12-15 11:54:44,401 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 16.1s: 2730c @170c/s (556ch, ~682t @42t/s)
2025-12-15 11:54:46,411 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 18.1s: 3087c @171c/s (625ch, ~772t @43t/s)
2025-12-15 11:54:48,424 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 20.1s: 3445c @171c/s (694ch, ~861t @43t/s)
2025-12-15 11:54:50,436 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 22.1s: 3793c @172c/s (763ch, ~948t @43t/s)
2025-12-15 11:54:52,460 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 24.1s: 4115c @171c/s (831ch, ~1029t @43t/s)
2025-12-15 11:54:54,473 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 26.1s: 4454c @170c/s (900ch, ~1114t @43t/s)
2025-12-15 11:54:56,485 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 28.2s: 4809c @171c/s (969ch, ~1202t @43t/s)
2025-12-15 11:54:58,497 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 30.2s: 5161c @171c/s (1038ch, ~1290t @43t/s)
2025-12-15 11:55:00,517 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 32.2s: 5506c @171c/s (1106ch, ~1376t @43t/s)
2025-12-15 11:55:02,531 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 34.2s: 5877c @172c/s (1175ch, ~1469t @43t/s)
2025-12-15 11:55:04,547 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 36.2s: 6220c @172c/s (1244ch, ~1555t @43t/s)
2025-12-15 11:55:06,569 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 38.2s: 6607c @173c/s (1313ch, ~1652t @43t/s)
2025-12-15 11:55:08,596 - src.llm.client - INFO - [lec:0868a5] ğŸ“Š 40.3s: 7018c @174c/s (1382ch, ~1754t @44t/s)
2025-12-15 11:55:09,079 - src.llm.client - INFO - [lec:0868a5] âœ“ Done 42.62s: 7078c (~1090w @166c/s)
2025-12-15 11:55:09,080 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 11:55:09,080 - src.generate.formats.lectures - INFO -     - Length: 7205 chars, 1108 words
2025-12-15 11:55:09,080 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 11:55:09,080 - src.generate.formats.lectures - INFO -     - Structure: 7 sections, 2 subsections
2025-12-15 11:55:09,080 - src.generate.formats.lectures - INFO -     - Content: 7 examples, 0 terms defined
2025-12-15 11:55:09,080 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 11:55:09,085 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 11:55:09,085 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 11:55:09,085 - src.generate.formats.labs - INFO - Generating lab 1 for: Introduction to Active Inference (Session 1)
2025-12-15 11:55:09,085 - src.llm.client - INFO - [lab:fe3c6b] ğŸš€ lab | m=gemma3:4b | p=3373c | t=150s
2025-12-15 11:55:09,085 - src.llm.client - INFO - [lab:fe3c6b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 11:55:09,085 - src.llm.client - INFO - [lab:fe3c6b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:55:09,087 - src.llm.client - INFO - [lab:fe3c6b] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3827 bytes, prompt=3373 chars
2025-12-15 11:55:09,087 - src.llm.client - INFO - [lab:fe3c6b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 11:55:10,747 - src.llm.request_handler - INFO - [lab:fe3c6b] âœ“ Done 1.66s
2025-12-15 11:55:10,747 - src.llm.client - INFO - [lab:fe3c6b] âœ… HTTP 200 in 1.66s
2025-12-15 11:55:10,747 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“¡ Stream active (200)
2025-12-15 11:55:10,747 - src.llm.client - INFO - [lab:fe3c6b] Starting stream parsing, waiting for first chunk...
2025-12-15 11:55:12,773 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 2.0s: 342c @169c/s (71ch, ~86t @42t/s)
2025-12-15 11:55:14,801 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 4.1s: 766c @189c/s (142ch, ~192t @47t/s)
2025-12-15 11:55:16,811 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 6.1s: 1149c @189c/s (212ch, ~287t @47t/s)
2025-12-15 11:55:18,828 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 8.1s: 1384c @171c/s (282ch, ~346t @43t/s)
2025-12-15 11:55:20,834 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 10.1s: 1577c @156c/s (351ch, ~394t @39t/s)
2025-12-15 11:55:22,836 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 12.1s: 1845c @153c/s (420ch, ~461t @38t/s)
2025-12-15 11:55:24,843 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 14.1s: 2164c @154c/s (489ch, ~541t @38t/s)
2025-12-15 11:55:26,855 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 16.1s: 2460c @153c/s (558ch, ~615t @38t/s)
2025-12-15 11:55:28,865 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 18.1s: 2748c @152c/s (627ch, ~687t @38t/s)
2025-12-15 11:55:30,880 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 20.1s: 3058c @152c/s (696ch, ~764t @38t/s)
2025-12-15 11:55:32,893 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 22.1s: 3363c @152c/s (765ch, ~841t @38t/s)
2025-12-15 11:55:34,908 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 24.2s: 3628c @150c/s (834ch, ~907t @38t/s)
2025-12-15 11:55:36,928 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 26.2s: 3841c @147c/s (903ch, ~960t @37t/s)
2025-12-15 11:55:38,952 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 28.2s: 4191c @149c/s (972ch, ~1048t @37t/s)
2025-12-15 11:55:40,969 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 30.2s: 4534c @150c/s (1041ch, ~1134t @38t/s)
2025-12-15 11:55:42,986 - src.llm.client - INFO - [lab:fe3c6b] ğŸ“Š 32.2s: 4940c @153c/s (1110ch, ~1235t @38t/s)
2025-12-15 11:55:44,476 - src.llm.client - INFO - [lab:fe3c6b] âœ“ Done 35.39s: 5213c (~776w @147c/s)
2025-12-15 11:55:44,477 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 11:55:44,477 - src.generate.formats.labs - INFO -     - Length: 5307 chars, 791 words
2025-12-15 11:55:44,477 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-15 11:55:44,477 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 11:55:44,477 - src.generate.formats.labs - INFO -     - Data tables: 4
2025-12-15 11:55:44,480 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 11:55:44,480 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 11:55:44,480 - src.generate.formats.study_notes - INFO - Generating study notes for: Introduction to Active Inference (Session 1)
2025-12-15 11:55:44,481 - src.llm.client - INFO - [stu:a3158d] ğŸš€ stu | m=gemma3:4b | p=4504c | t=120s
2025-12-15 11:55:44,481 - src.llm.client - INFO - [stu:a3158d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 11:55:44,481 - src.llm.client - INFO - [stu:a3158d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:55:44,482 - src.llm.client - INFO - [stu:a3158d] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8174 bytes, prompt=4504 chars
2025-12-15 11:55:44,482 - src.llm.client - INFO - [stu:a3158d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 11:55:46,878 - src.llm.request_handler - INFO - [stu:a3158d] âœ“ Done 2.40s
2025-12-15 11:55:46,878 - src.llm.client - INFO - [stu:a3158d] âœ… HTTP 200 in 2.40s
2025-12-15 11:55:46,878 - src.llm.client - INFO - [stu:a3158d] ğŸ“¡ Stream active (200)
2025-12-15 11:55:46,878 - src.llm.client - INFO - [stu:a3158d] Starting stream parsing, waiting for first chunk...
2025-12-15 11:55:48,883 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 2.0s: 358c @179c/s (69ch, ~90t @45t/s)
2025-12-15 11:55:50,895 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 4.0s: 738c @184c/s (138ch, ~184t @46t/s)
2025-12-15 11:55:52,905 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 6.0s: 1076c @179c/s (206ch, ~269t @45t/s)
2025-12-15 11:55:54,921 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 8.0s: 1424c @177c/s (275ch, ~356t @44t/s)
2025-12-15 11:55:56,938 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 10.1s: 1733c @172c/s (344ch, ~433t @43t/s)
2025-12-15 11:55:58,959 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 12.1s: 2040c @169c/s (413ch, ~510t @42t/s)
2025-12-15 11:56:00,984 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 14.1s: 2340c @166c/s (481ch, ~585t @41t/s)
2025-12-15 11:56:03,003 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 16.1s: 2677c @166c/s (550ch, ~669t @42t/s)
2025-12-15 11:56:05,027 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 18.1s: 2996c @165c/s (619ch, ~749t @41t/s)
2025-12-15 11:56:07,048 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 20.2s: 3359c @167c/s (688ch, ~840t @42t/s)
2025-12-15 11:56:09,068 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 22.2s: 3727c @168c/s (757ch, ~932t @42t/s)
2025-12-15 11:56:11,091 - src.llm.client - INFO - [stu:a3158d] ğŸ“Š 24.2s: 4072c @168c/s (826ch, ~1018t @42t/s)
2025-12-15 11:56:12,001 - src.llm.client - INFO - [stu:a3158d] âœ“ Done 27.52s: 4254c (~632w @155c/s)
2025-12-15 11:56:12,002 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 11:56:12,002 - src.generate.formats.study_notes - INFO -     - Length: 4321 chars, 643 words
2025-12-15 11:56:12,002 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 11:56:12,002 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-15 11:56:12,002 - src.generate.formats.study_notes - INFO -     - Structure: 4 sections, 4 bullets
2025-12-15 11:56:12,002 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 11:56:12,004 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 11:56:12,005 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 11:56:12,005 - src.generate.formats.diagrams - INFO - Generating diagram for: Sensory Input (Introduction to Active Inference)
2025-12-15 11:56:12,005 - src.generate.formats.diagrams - INFO - Generating diagram for: Generative Models (Introduction to Active Inference)
2025-12-15 11:56:12,005 - src.llm.client - INFO - [dia:d4966b] ğŸš€ dia | m=gemma3:4b | p=5759c | t=120s
2025-12-15 11:56:12,005 - src.llm.client - INFO - [dia:d4966b] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 11:56:12,005 - src.llm.client - INFO - [dia:290451] ğŸš€ dia | m=gemma3:4b | p=5751c | t=120s
2025-12-15 11:56:12,005 - src.generate.formats.diagrams - INFO - Generating diagram for: Motor Action (Introduction to Active Inference)
2025-12-15 11:56:12,005 - src.llm.client - INFO - [dia:d4966b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:56:12,006 - src.llm.client - INFO - [dia:f1fbda] ğŸš€ dia | m=gemma3:4b | p=5749c | t=120s
2025-12-15 11:56:12,005 - src.llm.client - INFO - [dia:290451] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 11:56:12,006 - src.llm.client - INFO - [dia:f1fbda] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 11:56:12,006 - src.llm.client - INFO - [dia:290451] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:56:12,006 - src.llm.client - INFO - [dia:f1fbda] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:56:12,007 - src.llm.client - INFO - [dia:d4966b] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11068 bytes, prompt=5759 chars
2025-12-15 11:56:12,008 - src.llm.client - INFO - [dia:f1fbda] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11058 bytes, prompt=5749 chars
2025-12-15 11:56:12,008 - src.llm.client - INFO - [dia:290451] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11060 bytes, prompt=5751 chars
2025-12-15 11:56:12,008 - src.llm.client - INFO - [dia:d4966b] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 11:56:12,008 - src.llm.client - INFO - [dia:f1fbda] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 11:56:12,008 - src.llm.client - INFO - [dia:290451] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 11:56:15,228 - src.llm.request_handler - INFO - [dia:d4966b] âœ“ Done 3.22s
2025-12-15 11:56:15,229 - src.llm.client - INFO - [dia:d4966b] âœ… HTTP 200 in 3.22s
2025-12-15 11:56:15,229 - src.llm.client - INFO - [dia:d4966b] ğŸ“¡ Stream active (200)
2025-12-15 11:56:15,229 - src.llm.client - INFO - [dia:d4966b] Starting stream parsing, waiting for first chunk...
2025-12-15 11:56:17,243 - src.llm.client - INFO - [dia:d4966b] ğŸ“Š 2.0s: 209c @104c/s (69ch, ~52t @26t/s)
2025-12-15 11:56:19,256 - src.llm.client - INFO - [dia:d4966b] ğŸ“Š 4.0s: 420c @104c/s (138ch, ~105t @26t/s)
2025-12-15 11:56:21,271 - src.llm.client - INFO - [dia:d4966b] ğŸ“Š 6.0s: 606c @100c/s (207ch, ~152t @25t/s)
2025-12-15 11:56:23,286 - src.llm.client - INFO - [dia:d4966b] ğŸ“Š 8.1s: 770c @96c/s (276ch, ~192t @24t/s)
2025-12-15 11:56:25,300 - src.llm.client - INFO - [dia:d4966b] ğŸ“Š 10.1s: 937c @93c/s (345ch, ~234t @23t/s)
2025-12-15 11:56:25,648 - src.llm.client - INFO - [dia:d4966b] âœ“ Done 13.64s: 955c (~116w @70c/s)
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Generative Models (Introduction to Active Inference):
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO -     - Length: 498 chars (cleaned: 498 chars)
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO - [OK] Elements: 36 total (nodes: 14, connections: 22) âœ“
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 11:56:25,649 - src.generate.formats.diagrams - INFO - Generated diagram: 498 characters
2025-12-15 11:56:28,696 - src.llm.request_handler - INFO - [dia:290451] âœ“ Done 16.69s
2025-12-15 11:56:28,696 - src.llm.client - INFO - [dia:290451] âœ… HTTP 200 in 16.69s
2025-12-15 11:56:28,696 - src.llm.client - INFO - [dia:290451] ğŸ“¡ Stream active (200)
2025-12-15 11:56:28,696 - src.llm.client - INFO - [dia:290451] Starting stream parsing, waiting for first chunk...
2025-12-15 11:56:30,717 - src.llm.client - INFO - [dia:290451] ğŸ“Š 2.0s: 259c @128c/s (69ch, ~65t @32t/s)
2025-12-15 11:56:32,730 - src.llm.client - INFO - [dia:290451] ğŸ“Š 4.0s: 482c @119c/s (138ch, ~120t @30t/s)
2025-12-15 11:56:34,739 - src.llm.client - INFO - [dia:290451] ğŸ“Š 6.0s: 700c @116c/s (207ch, ~175t @29t/s)
2025-12-15 11:56:36,749 - src.llm.client - INFO - [dia:290451] ğŸ“Š 8.1s: 848c @105c/s (276ch, ~212t @26t/s)
2025-12-15 11:56:38,763 - src.llm.client - INFO - [dia:290451] ğŸ“Š 10.1s: 999c @99c/s (345ch, ~250t @25t/s)
2025-12-15 11:56:39,998 - src.llm.client - INFO - [dia:290451] âœ“ Done 27.99s: 1088c (~131w @39c/s)
2025-12-15 11:56:39,998 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Sensory Input (Introduction to Active Inference):
2025-12-15 11:56:39,998 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 11:56:39,998 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 11:56:39,998 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 11:56:39,998 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - INFO -     - Length: 666 chars (cleaned: 666 chars)
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 35 total (nodes: 13, connections: 22) âš ï¸
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-15 11:56:39,999 - src.generate.formats.diagrams - INFO - Generated diagram: 666 characters
2025-12-15 11:56:43,058 - src.llm.request_handler - INFO - [dia:f1fbda] âœ“ Done 31.05s
2025-12-15 11:56:43,059 - src.llm.client - INFO - [dia:f1fbda] âœ… HTTP 200 in 31.05s
2025-12-15 11:56:43,059 - src.llm.client - INFO - [dia:f1fbda] ğŸ“¡ Stream active (200)
2025-12-15 11:56:43,059 - src.llm.client - INFO - [dia:f1fbda] Starting stream parsing, waiting for first chunk...
2025-12-15 11:56:45,075 - src.llm.client - INFO - [dia:f1fbda] ğŸ“Š 2.0s: 269c @133c/s (69ch, ~67t @33t/s)
2025-12-15 11:56:47,080 - src.llm.client - INFO - [dia:f1fbda] ğŸ“Š 4.0s: 493c @123c/s (138ch, ~123t @31t/s)
2025-12-15 11:56:49,084 - src.llm.client - INFO - [dia:f1fbda] ğŸ“Š 6.0s: 707c @117c/s (207ch, ~177t @29t/s)
2025-12-15 11:56:49,807 - src.llm.client - INFO - [dia:f1fbda] âœ“ Done 37.80s: 758c (~107w @20c/s)
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Motor Action (Introduction to Active Inference):
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO -     - Length: 709 chars (cleaned: 709 chars)
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO - [OK] Elements: 40 total (nodes: 12, connections: 28) âœ“
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 11:56:49,808 - src.generate.formats.diagrams - INFO - Generated diagram: 709 characters
2025-12-15 11:56:49,809 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 11:56:49,809 - src.generate.formats.questions - INFO - Generating 10 questions for: Introduction to Active Inference (Session 1)
2025-12-15 11:56:49,809 - src.llm.client - INFO - [qst:d84059] ğŸš€ qst | m=gemma3:4b | p=7379c | t=150s
2025-12-15 11:56:49,809 - src.llm.client - INFO - [qst:d84059] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 11:56:49,809 - src.llm.client - INFO - [qst:d84059] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:56:49,810 - src.llm.client - INFO - [qst:d84059] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11127 bytes, prompt=7379 chars
2025-12-15 11:56:49,810 - src.llm.client - INFO - [qst:d84059] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 11:56:53,612 - src.llm.request_handler - INFO - [qst:d84059] âœ“ Done 3.80s
2025-12-15 11:56:53,612 - src.llm.client - INFO - [qst:d84059] âœ… HTTP 200 in 3.80s
2025-12-15 11:56:53,612 - src.llm.client - INFO - [qst:d84059] ğŸ“¡ Stream active (200)
2025-12-15 11:56:53,612 - src.llm.client - INFO - [qst:d84059] Starting stream parsing, waiting for first chunk...
2025-12-15 11:56:55,617 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 2.0s: 359c @179c/s (69ch, ~90t @45t/s)
2025-12-15 11:56:57,620 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 4.0s: 746c @186c/s (138ch, ~186t @47t/s)
2025-12-15 11:56:59,639 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 6.0s: 1074c @178c/s (207ch, ~268t @45t/s)
2025-12-15 11:57:01,663 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 8.1s: 1429c @178c/s (276ch, ~357t @44t/s)
2025-12-15 11:57:03,672 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 10.1s: 1815c @180c/s (345ch, ~454t @45t/s)
2025-12-15 11:57:05,684 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 12.1s: 2178c @180c/s (414ch, ~544t @45t/s)
2025-12-15 11:57:07,700 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 14.1s: 2552c @181c/s (483ch, ~638t @45t/s)
2025-12-15 11:57:09,712 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 16.1s: 2887c @179c/s (552ch, ~722t @45t/s)
2025-12-15 11:57:11,729 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 18.1s: 3280c @181c/s (621ch, ~820t @45t/s)
2025-12-15 11:57:13,755 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 20.1s: 3656c @182c/s (690ch, ~914t @45t/s)
2025-12-15 11:57:15,757 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 22.1s: 4039c @182c/s (758ch, ~1010t @46t/s)
2025-12-15 11:57:17,759 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 24.1s: 4368c @181c/s (826ch, ~1092t @45t/s)
2025-12-15 11:57:19,765 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 26.2s: 4726c @181c/s (894ch, ~1182t @45t/s)
2025-12-15 11:57:21,767 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 28.2s: 5142c @183c/s (962ch, ~1286t @46t/s)
2025-12-15 11:57:23,773 - src.llm.client - INFO - [qst:d84059] ğŸ“Š 30.2s: 5528c @183c/s (1030ch, ~1382t @46t/s)
2025-12-15 11:57:24,640 - src.llm.client - INFO - [qst:d84059] âœ“ Done 34.83s: 5704c (~802w @164c/s)
2025-12-15 11:57:24,641 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 2, 'total_fixes': 3}
2025-12-15 11:57:24,641 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-15 11:57:24,641 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 2 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 11:57:24,641 - src.generate.formats.questions - WARNING -     Context: Module 1 Session 1
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 5 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -     Context: Module 1 Session 1
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Introduction to Active Inference (Session 1)
2025-12-15 11:57:24,642 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 11:57:24,644 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 11:57:24,646 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 1 completed
2025-12-15 11:57:24,646 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 11:57:24,646 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 11:57:24,646 - src.generate.orchestration.pipeline - INFO - Module 2: Bayesian Mechanics (2 sessions)
2025-12-15 11:57:24,646 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 11:57:24,646 - src.generate.orchestration.pipeline - INFO - 
[2/20] Session 2: Probability Theory Review
2025-12-15 11:57:24,646 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 11:57:24,646 - src.generate.formats.lectures - INFO - Generating lecture for: Bayesian Mechanics (Session 2/20)
2025-12-15 11:57:24,646 - src.llm.client - INFO - [lec:b5148c] ğŸš€ lec | m=gemma3:4b | p=3055c | t=180s
2025-12-15 11:57:24,646 - src.llm.client - INFO - [lec:b5148c] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 11:57:24,646 - src.llm.client - INFO - [lec:b5148c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:57:24,648 - src.llm.client - INFO - [lec:b5148c] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6691 bytes, prompt=3055 chars
2025-12-15 11:57:24,648 - src.llm.client - INFO - [lec:b5148c] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 11:57:26,440 - src.llm.request_handler - INFO - [lec:b5148c] âœ“ Done 1.79s
2025-12-15 11:57:26,440 - src.llm.client - INFO - [lec:b5148c] âœ… HTTP 200 in 1.79s
2025-12-15 11:57:26,440 - src.llm.client - INFO - [lec:b5148c] ğŸ“¡ Stream active (200)
2025-12-15 11:57:26,440 - src.llm.client - INFO - [lec:b5148c] Starting stream parsing, waiting for first chunk...
2025-12-15 11:57:28,441 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 2.0s: 427c @213c/s (70ch, ~107t @53t/s)
2025-12-15 11:57:30,446 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 4.0s: 802c @200c/s (140ch, ~200t @50t/s)
2025-12-15 11:57:32,460 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 6.0s: 1160c @193c/s (210ch, ~290t @48t/s)
2025-12-15 11:57:34,483 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 8.0s: 1441c @179c/s (280ch, ~360t @45t/s)
2025-12-15 11:57:36,502 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 10.1s: 1719c @171c/s (349ch, ~430t @43t/s)
2025-12-15 11:57:38,513 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 12.1s: 1942c @161c/s (418ch, ~486t @40t/s)
2025-12-15 11:57:40,516 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 14.1s: 2183c @155c/s (487ch, ~546t @39t/s)
2025-12-15 11:57:42,516 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 16.1s: 2511c @156c/s (556ch, ~628t @39t/s)
2025-12-15 11:57:44,543 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 18.1s: 2792c @154c/s (626ch, ~698t @39t/s)
2025-12-15 11:57:46,571 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 20.1s: 2978c @148c/s (696ch, ~744t @37t/s)
2025-12-15 11:57:48,572 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 22.1s: 3258c @147c/s (765ch, ~814t @37t/s)
2025-12-15 11:57:50,578 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 24.1s: 3580c @148c/s (834ch, ~895t @37t/s)
2025-12-15 11:57:52,588 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 26.1s: 3800c @145c/s (902ch, ~950t @36t/s)
2025-12-15 11:57:54,589 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 28.1s: 4083c @145c/s (971ch, ~1021t @36t/s)
2025-12-15 11:57:56,600 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 30.2s: 4393c @146c/s (1040ch, ~1098t @36t/s)
2025-12-15 11:57:58,609 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 32.2s: 4638c @144c/s (1109ch, ~1160t @36t/s)
2025-12-15 11:58:00,619 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 34.2s: 4848c @142c/s (1178ch, ~1212t @35t/s)
2025-12-15 11:58:02,629 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 36.2s: 5191c @143c/s (1247ch, ~1298t @36t/s)
2025-12-15 11:58:04,645 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 38.2s: 5600c @147c/s (1316ch, ~1400t @37t/s)
2025-12-15 11:58:06,662 - src.llm.client - INFO - [lec:b5148c] ğŸ“Š 40.2s: 6024c @150c/s (1385ch, ~1506t @37t/s)
2025-12-15 11:58:06,790 - src.llm.client - INFO - [lec:b5148c] âœ“ Done 42.14s: 6032c (~944w @143c/s)
2025-12-15 11:58:06,791 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 11:58:06,792 - src.generate.formats.lectures - INFO -     - Length: 6123 chars, 955 words
2025-12-15 11:58:06,792 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 11:58:06,792 - src.generate.formats.lectures - INFO -     - Structure: 6 sections, 0 subsections
2025-12-15 11:58:06,792 - src.generate.formats.lectures - INFO -     - Content: 8 examples, 3 terms defined
2025-12-15 11:58:06,792 - src.generate.formats.lectures - WARNING - [WARNING] Word count (955) below minimum 1000 (need 45 more words - consider regenerating or expanding content) âš ï¸
2025-12-15 11:58:06,792 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 11:58:06,792 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 11:58:06,792 - src.generate.formats.lectures - INFO - Quality score: 90.5/100 (excellent)
2025-12-15 11:58:06,795 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 11:58:06,795 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 11:58:06,795 - src.generate.formats.labs - INFO - Generating lab 2 for: Bayesian Mechanics (Session 2)
2025-12-15 11:58:06,795 - src.llm.client - INFO - [lab:0456c1] ğŸš€ lab | m=gemma3:4b | p=3336c | t=150s
2025-12-15 11:58:06,795 - src.llm.client - INFO - [lab:0456c1] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 11:58:06,795 - src.llm.client - INFO - [lab:0456c1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:58:06,797 - src.llm.client - INFO - [lab:0456c1] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3753 bytes, prompt=3336 chars
2025-12-15 11:58:06,797 - src.llm.client - INFO - [lab:0456c1] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 11:58:08,520 - src.llm.request_handler - INFO - [lab:0456c1] âœ“ Done 1.72s
2025-12-15 11:58:08,520 - src.llm.client - INFO - [lab:0456c1] âœ… HTTP 200 in 1.72s
2025-12-15 11:58:08,520 - src.llm.client - INFO - [lab:0456c1] ğŸ“¡ Stream active (200)
2025-12-15 11:58:08,520 - src.llm.client - INFO - [lab:0456c1] Starting stream parsing, waiting for first chunk...
2025-12-15 11:58:10,543 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 2.0s: 348c @172c/s (71ch, ~87t @43t/s)
2025-12-15 11:58:12,545 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 4.0s: 779c @194c/s (141ch, ~195t @48t/s)
2025-12-15 11:58:14,548 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 6.0s: 1159c @192c/s (211ch, ~290t @48t/s)
2025-12-15 11:58:16,563 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 8.0s: 1408c @175c/s (281ch, ~352t @44t/s)
2025-12-15 11:58:18,587 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 10.1s: 1709c @170c/s (351ch, ~427t @42t/s)
2025-12-15 11:58:20,589 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 12.1s: 2012c @167c/s (420ch, ~503t @42t/s)
2025-12-15 11:58:22,602 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 14.1s: 2245c @159c/s (488ch, ~561t @40t/s)
2025-12-15 11:58:24,628 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 16.1s: 2524c @157c/s (558ch, ~631t @39t/s)
2025-12-15 11:58:26,656 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 18.1s: 2873c @158c/s (628ch, ~718t @40t/s)
2025-12-15 11:58:28,657 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 20.1s: 3273c @163c/s (697ch, ~818t @41t/s)
2025-12-15 11:58:30,657 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 22.1s: 3539c @160c/s (766ch, ~885t @40t/s)
2025-12-15 11:58:32,664 - src.llm.client - INFO - [lab:0456c1] ğŸ“Š 24.1s: 3858c @160c/s (835ch, ~964t @40t/s)
2025-12-15 11:58:34,368 - src.llm.client - INFO - [lab:0456c1] âœ“ Done 27.57s: 4123c (~584w @150c/s)
2025-12-15 11:58:34,368 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 11:58:34,368 - src.generate.formats.labs - INFO -     - Length: 4210 chars, 597 words
2025-12-15 11:58:34,368 - src.generate.formats.labs - INFO -     - Procedure: 10 steps
2025-12-15 11:58:34,368 - src.generate.formats.labs - INFO -     - Safety: 5 warnings
2025-12-15 11:58:34,368 - src.generate.formats.labs - INFO -     - Data tables: 12
2025-12-15 11:58:34,371 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 11:58:34,371 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 11:58:34,371 - src.generate.formats.study_notes - INFO - Generating study notes for: Bayesian Mechanics (Session 2)
2025-12-15 11:58:34,371 - src.llm.client - INFO - [stu:9727de] ğŸš€ stu | m=gemma3:4b | p=4444c | t=120s
2025-12-15 11:58:34,371 - src.llm.client - INFO - [stu:9727de] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 11:58:34,371 - src.llm.client - INFO - [stu:9727de] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:58:34,373 - src.llm.client - INFO - [stu:9727de] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8076 bytes, prompt=4444 chars
2025-12-15 11:58:34,373 - src.llm.client - INFO - [stu:9727de] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 11:58:36,770 - src.llm.request_handler - INFO - [stu:9727de] âœ“ Done 2.40s
2025-12-15 11:58:36,770 - src.llm.client - INFO - [stu:9727de] âœ… HTTP 200 in 2.40s
2025-12-15 11:58:36,770 - src.llm.client - INFO - [stu:9727de] ğŸ“¡ Stream active (200)
2025-12-15 11:58:36,770 - src.llm.client - INFO - [stu:9727de] Starting stream parsing, waiting for first chunk...
2025-12-15 11:58:38,780 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 2.0s: 417c @207c/s (69ch, ~104t @52t/s)
2025-12-15 11:58:40,804 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 4.0s: 835c @207c/s (139ch, ~209t @52t/s)
2025-12-15 11:58:42,830 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 6.1s: 1181c @195c/s (209ch, ~295t @49t/s)
2025-12-15 11:58:44,831 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 8.1s: 1543c @191c/s (278ch, ~386t @48t/s)
2025-12-15 11:58:46,836 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 10.1s: 1756c @174c/s (347ch, ~439t @44t/s)
2025-12-15 11:58:48,844 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 12.1s: 2019c @167c/s (416ch, ~505t @42t/s)
2025-12-15 11:58:50,853 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 14.1s: 2410c @171c/s (485ch, ~602t @43t/s)
2025-12-15 11:58:52,861 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 16.1s: 2758c @171c/s (553ch, ~690t @43t/s)
2025-12-15 11:58:54,869 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 18.1s: 3175c @175c/s (622ch, ~794t @44t/s)
2025-12-15 11:58:56,882 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 20.1s: 3480c @173c/s (691ch, ~870t @43t/s)
2025-12-15 11:58:58,887 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 22.1s: 3855c @174c/s (760ch, ~964t @44t/s)
2025-12-15 11:59:00,893 - src.llm.client - INFO - [stu:9727de] ğŸ“Š 24.1s: 4237c @176c/s (829ch, ~1059t @44t/s)
2025-12-15 11:59:01,421 - src.llm.client - INFO - [stu:9727de] âœ“ Done 27.05s: 4338c (~622w @160c/s)
2025-12-15 11:59:01,421 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 11:59:01,421 - src.generate.formats.study_notes - INFO -     - Length: 4391 chars, 631 words
2025-12-15 11:59:01,421 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 11:59:01,421 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-15 11:59:01,421 - src.generate.formats.study_notes - INFO -     - Structure: 5 sections, 9 bullets
2025-12-15 11:59:01,421 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 11:59:01,423 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 11:59:01,424 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 11:59:01,424 - src.generate.formats.diagrams - INFO - Generating diagram for: Joint Probability (Bayesian Mechanics)
2025-12-15 11:59:01,424 - src.generate.formats.diagrams - INFO - Generating diagram for: Marginal Probability (Bayesian Mechanics)
2025-12-15 11:59:01,424 - src.llm.client - INFO - [dia:4c81de] ğŸš€ dia | m=gemma3:4b | p=5750c | t=120s
2025-12-15 11:59:01,424 - src.llm.client - INFO - [dia:4c81de] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 11:59:01,424 - src.llm.client - INFO - [dia:4c81de] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:59:01,424 - src.llm.client - INFO - [dia:a90868] ğŸš€ dia | m=gemma3:4b | p=5744c | t=120s
2025-12-15 11:59:01,425 - src.llm.client - INFO - [dia:a90868] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 11:59:01,425 - src.llm.client - INFO - [dia:a90868] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 11:59:01,426 - src.llm.client - INFO - [dia:4c81de] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11059 bytes, prompt=5750 chars
2025-12-15 11:59:01,426 - src.llm.client - INFO - [dia:4c81de] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 11:59:01,426 - src.llm.client - INFO - [dia:a90868] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11053 bytes, prompt=5744 chars
2025-12-15 11:59:01,427 - src.llm.client - INFO - [dia:a90868] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 11:59:04,640 - src.llm.request_handler - INFO - [dia:4c81de] âœ“ Done 3.21s
2025-12-15 11:59:04,640 - src.llm.client - INFO - [dia:4c81de] âœ… HTTP 200 in 3.21s
2025-12-15 11:59:04,640 - src.llm.client - INFO - [dia:4c81de] ğŸ“¡ Stream active (200)
2025-12-15 11:59:04,640 - src.llm.client - INFO - [dia:4c81de] Starting stream parsing, waiting for first chunk...
2025-12-15 11:59:06,644 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 2.0s: 190c @95c/s (69ch, ~48t @24t/s)
2025-12-15 11:59:08,647 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 4.0s: 383c @96c/s (138ch, ~96t @24t/s)
2025-12-15 11:59:10,676 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 6.0s: 581c @96c/s (208ch, ~145t @24t/s)
2025-12-15 11:59:12,705 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 8.1s: 777c @96c/s (278ch, ~194t @24t/s)
2025-12-15 11:59:14,733 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 10.1s: 969c @96c/s (348ch, ~242t @24t/s)
2025-12-15 11:59:16,735 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 12.1s: 1160c @96c/s (417ch, ~290t @24t/s)
2025-12-15 11:59:18,735 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 14.1s: 1364c @97c/s (486ch, ~341t @24t/s)
2025-12-15 11:59:20,741 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 16.1s: 1564c @97c/s (555ch, ~391t @24t/s)
2025-12-15 11:59:22,754 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 18.1s: 1767c @98c/s (624ch, ~442t @24t/s)
2025-12-15 11:59:24,767 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 20.1s: 1963c @98c/s (693ch, ~491t @24t/s)
2025-12-15 11:59:26,783 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 22.1s: 2158c @97c/s (762ch, ~540t @24t/s)
2025-12-15 11:59:28,801 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 24.2s: 2355c @97c/s (831ch, ~589t @24t/s)
2025-12-15 11:59:30,819 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 26.2s: 2558c @98c/s (900ch, ~640t @24t/s)
2025-12-15 11:59:32,841 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 28.2s: 2755c @98c/s (969ch, ~689t @24t/s)
2025-12-15 11:59:34,863 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 30.2s: 2949c @98c/s (1038ch, ~737t @24t/s)
2025-12-15 11:59:36,867 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 32.2s: 3147c @98c/s (1106ch, ~787t @24t/s)
2025-12-15 11:59:38,878 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 34.2s: 3326c @97c/s (1174ch, ~832t @24t/s)
2025-12-15 11:59:40,886 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 36.2s: 3508c @97c/s (1242ch, ~877t @24t/s)
2025-12-15 11:59:42,895 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 38.3s: 3699c @97c/s (1310ch, ~925t @24t/s)
2025-12-15 11:59:44,903 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 40.3s: 3877c @96c/s (1378ch, ~969t @24t/s)
2025-12-15 11:59:46,911 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 42.3s: 4071c @96c/s (1446ch, ~1018t @24t/s)
2025-12-15 11:59:48,922 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 44.3s: 4266c @96c/s (1514ch, ~1066t @24t/s)
2025-12-15 11:59:50,932 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 46.3s: 4447c @96c/s (1582ch, ~1112t @24t/s)
2025-12-15 11:59:52,947 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 48.3s: 4639c @96c/s (1649ch, ~1160t @24t/s)
2025-12-15 11:59:54,965 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 50.3s: 4815c @96c/s (1717ch, ~1204t @24t/s)
2025-12-15 11:59:56,975 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 52.3s: 4995c @95c/s (1785ch, ~1249t @24t/s)
2025-12-15 11:59:58,986 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 54.3s: 5193c @96c/s (1853ch, ~1298t @24t/s)
2025-12-15 12:00:00,999 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 56.4s: 5394c @96c/s (1921ch, ~1348t @24t/s)
2025-12-15 12:00:03,025 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 58.4s: 5578c @96c/s (1989ch, ~1394t @24t/s)
2025-12-15 12:00:05,043 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 60.4s: 5762c @95c/s (2057ch, ~1440t @24t/s)
2025-12-15 12:00:07,063 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 62.4s: 5960c @95c/s (2125ch, ~1490t @24t/s)
2025-12-15 12:00:09,083 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 64.4s: 6145c @95c/s (2193ch, ~1536t @24t/s)
2025-12-15 12:00:11,103 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 66.5s: 6341c @95c/s (2261ch, ~1585t @24t/s)
2025-12-15 12:00:13,126 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 68.5s: 6520c @95c/s (2329ch, ~1630t @24t/s)
2025-12-15 12:00:15,149 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 70.5s: 6719c @95c/s (2397ch, ~1680t @24t/s)
2025-12-15 12:00:17,173 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 72.5s: 6918c @95c/s (2465ch, ~1730t @24t/s)
2025-12-15 12:00:19,197 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 74.6s: 7108c @95c/s (2533ch, ~1777t @24t/s)
2025-12-15 12:00:21,224 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 76.6s: 7293c @95c/s (2601ch, ~1823t @24t/s)
2025-12-15 12:00:23,233 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 78.6s: 7478c @95c/s (2668ch, ~1870t @24t/s)
2025-12-15 12:00:25,237 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 80.6s: 7669c @95c/s (2736ch, ~1917t @24t/s)
2025-12-15 12:00:27,242 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 82.6s: 7863c @95c/s (2804ch, ~1966t @24t/s)
2025-12-15 12:00:29,248 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 84.6s: 8041c @95c/s (2872ch, ~2010t @24t/s)
2025-12-15 12:00:31,252 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 86.6s: 8247c @95c/s (2940ch, ~2062t @24t/s)
2025-12-15 12:00:33,258 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 88.6s: 8436c @95c/s (3008ch, ~2109t @24t/s)
2025-12-15 12:00:35,265 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 90.6s: 8646c @95c/s (3076ch, ~2162t @24t/s)
2025-12-15 12:00:37,276 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 92.6s: 8849c @96c/s (3144ch, ~2212t @24t/s)
2025-12-15 12:00:39,287 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 94.6s: 9038c @95c/s (3212ch, ~2260t @24t/s)
2025-12-15 12:00:41,297 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 96.7s: 9233c @96c/s (3279ch, ~2308t @24t/s)
2025-12-15 12:00:43,311 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 98.7s: 9420c @95c/s (3346ch, ~2355t @24t/s)
2025-12-15 12:00:45,330 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 100.7s: 9606c @95c/s (3414ch, ~2402t @24t/s)
2025-12-15 12:00:47,343 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 102.7s: 9800c @95c/s (3482ch, ~2450t @24t/s)
2025-12-15 12:00:49,359 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 104.7s: 9989c @95c/s (3550ch, ~2497t @24t/s)
2025-12-15 12:00:51,367 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 106.7s: 10183c @95c/s (3617ch, ~2546t @24t/s)
2025-12-15 12:00:53,393 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 108.8s: 10384c @95c/s (3685ch, ~2596t @24t/s)
2025-12-15 12:00:55,413 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 110.8s: 10580c @96c/s (3753ch, ~2645t @24t/s)
2025-12-15 12:00:57,435 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 112.8s: 10785c @96c/s (3821ch, ~2696t @24t/s)
2025-12-15 12:00:59,456 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 114.8s: 10985c @96c/s (3889ch, ~2746t @24t/s)
2025-12-15 12:01:01,431 - src.llm.request_handler - ERROR - [dia:a90868] â±ï¸ Read timeout 120.00s (limit: 120s)
2025-12-15 12:01:01,458 - src.llm.request_handler - INFO - [dia:a90868] Troubleshooting suggestions:
2025-12-15 12:01:01,458 - src.llm.request_handler - INFO - [dia:a90868]   - Request timed out. Consider:
2025-12-15 12:01:01,458 - src.llm.request_handler - INFO - [dia:a90868]   -   - Increasing timeout in config/llm_config.yaml
2025-12-15 12:01:01,458 - src.llm.request_handler - INFO - [dia:a90868]   -   - Using a faster/smaller model
2025-12-15 12:01:01,458 - src.llm.request_handler - INFO - [dia:a90868]   -   - Reducing num_predict parameter
2025-12-15 12:01:01,458 - src.llm.request_handler - INFO - [dia:a90868]   -   - Checking system resources (CPU/memory)
2025-12-15 12:01:01,458 - src.llm.client - ERROR - [dia:a90868] Read timeout after 120.03s (limit: 120s) - Ollama received request but didn't start generating. Operation: diagram. Model: gemma3:4b. This may indicate: (1) Model is too slow for this timeout, (2) System resources are constrained, (3) Model is hung. Solutions: (1) Increase timeout in config/llm_config.yaml (current: 120s), (2) Use a faster model, (3) Check Ollama logs: ollama logs, (4) Restart Ollama service.
2025-12-15 12:01:01,458 - src.generate.orchestration.pipeline - WARNING -   Transient error in diagram 1 generation (attempt 1/3): [dia:a90868] Read timeout after 120.03s (limit: 120s) - Ollama received request but didn't start generating. Operation: diagram. Model: gemma3:4b. This may indicate: (1) Model is too slow for this timeout, (2) System resources are constrained, (3) Model is hung. Solutions: (1) Increase timeout in config/llm_config.yaml (current: 120s), (2) Use a faster model, (3) Check Ollama logs: ollama logs, (4) Restart Ollama service.. Retrying in 2.0s...
2025-12-15 12:01:01,482 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 116.8s: 11170c @96c/s (3957ch, ~2792t @24t/s)
2025-12-15 12:01:03,459 - src.generate.formats.diagrams - INFO - Generating diagram for: Joint Probability (Bayesian Mechanics)
2025-12-15 12:01:03,460 - src.llm.client - INFO - [dia:b979a7] ğŸš€ dia | m=gemma3:4b | p=5744c | t=120s
2025-12-15 12:01:03,460 - src.llm.client - INFO - [dia:b979a7] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:01:03,460 - src.llm.client - INFO - [dia:b979a7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:01:03,464 - src.llm.client - INFO - [dia:b979a7] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11053 bytes, prompt=5744 chars
2025-12-15 12:01:03,464 - src.llm.client - INFO - [dia:b979a7] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:01:03,513 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 118.9s: 11359c @96c/s (4025ch, ~2840t @24t/s)
2025-12-15 12:01:05,514 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 120.9s: 11552c @96c/s (4092ch, ~2888t @24t/s)
2025-12-15 12:01:07,517 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 122.9s: 11739c @96c/s (4159ch, ~2935t @24t/s)
2025-12-15 12:01:09,544 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 124.9s: 11939c @96c/s (4227ch, ~2985t @24t/s)
2025-12-15 12:01:11,545 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 126.9s: 12127c @96c/s (4294ch, ~3032t @24t/s)
2025-12-15 12:01:13,549 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 128.9s: 12328c @96c/s (4361ch, ~3082t @24t/s)
2025-12-15 12:01:15,552 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 130.9s: 12523c @96c/s (4428ch, ~3131t @24t/s)
2025-12-15 12:01:17,573 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 132.9s: 12705c @96c/s (4494ch, ~3176t @24t/s)
2025-12-15 12:01:19,583 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 134.9s: 12889c @96c/s (4561ch, ~3222t @24t/s)
2025-12-15 12:01:21,592 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 137.0s: 13086c @96c/s (4628ch, ~3272t @24t/s)
2025-12-15 12:01:23,611 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 139.0s: 13279c @96c/s (4695ch, ~3320t @24t/s)
2025-12-15 12:01:25,634 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 141.0s: 13480c @96c/s (4764ch, ~3370t @24t/s)
2025-12-15 12:01:27,651 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 143.0s: 13684c @96c/s (4833ch, ~3421t @24t/s)
2025-12-15 12:01:28,644 - src.llm.client - INFO - [dia:4c81de] Stream making progress - extending timeout by 60.0s (new limit: 240.0s, max: 240.0s)
2025-12-15 12:01:29,667 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 145.0s: 13872c @96c/s (4902ch, ~3468t @24t/s)
2025-12-15 12:01:31,690 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 147.0s: 14070c @96c/s (4971ch, ~3518t @24t/s)
2025-12-15 12:01:33,696 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 149.1s: 14261c @96c/s (5039ch, ~3565t @24t/s)
2025-12-15 12:01:35,702 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 151.1s: 14440c @96c/s (5107ch, ~3610t @24t/s)
2025-12-15 12:01:37,711 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 153.1s: 14645c @96c/s (5175ch, ~3661t @24t/s)
2025-12-15 12:01:39,719 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 155.1s: 14830c @96c/s (5243ch, ~3708t @24t/s)
2025-12-15 12:01:41,721 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 157.1s: 15040c @96c/s (5310ch, ~3760t @24t/s)
2025-12-15 12:01:43,725 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 159.1s: 15243c @96c/s (5377ch, ~3811t @24t/s)
2025-12-15 12:01:45,740 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 161.1s: 15430c @96c/s (5445ch, ~3858t @24t/s)
2025-12-15 12:01:47,750 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 163.1s: 15628c @96c/s (5512ch, ~3907t @24t/s)
2025-12-15 12:01:49,755 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 165.1s: 15806c @96c/s (5577ch, ~3952t @24t/s)
2025-12-15 12:01:51,760 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 167.1s: 15989c @96c/s (5642ch, ~3997t @24t/s)
2025-12-15 12:01:53,765 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 169.1s: 16186c @96c/s (5708ch, ~4046t @24t/s)
2025-12-15 12:01:55,794 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 171.2s: 16368c @96c/s (5775ch, ~4092t @24t/s)
2025-12-15 12:01:57,823 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 173.2s: 16565c @96c/s (5842ch, ~4141t @24t/s)
2025-12-15 12:01:59,850 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 175.2s: 16756c @96c/s (5909ch, ~4189t @24t/s)
2025-12-15 12:02:01,852 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 177.2s: 16937c @96c/s (5975ch, ~4234t @24t/s)
2025-12-15 12:02:03,854 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 179.2s: 17132c @96c/s (6041ch, ~4283t @24t/s)
2025-12-15 12:02:05,859 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 181.2s: 17319c @96c/s (6107ch, ~4330t @24t/s)
2025-12-15 12:02:07,866 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 183.2s: 17496c @95c/s (6173ch, ~4374t @24t/s)
2025-12-15 12:02:09,871 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 185.2s: 17656c @95c/s (6239ch, ~4414t @24t/s)
2025-12-15 12:02:11,878 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 187.2s: 17855c @95c/s (6305ch, ~4464t @24t/s)
2025-12-15 12:02:13,893 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 189.3s: 18029c @95c/s (6371ch, ~4507t @24t/s)
2025-12-15 12:02:15,900 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 191.3s: 18211c @95c/s (6435ch, ~4553t @24t/s)
2025-12-15 12:02:17,929 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 193.3s: 18403c @95c/s (6500ch, ~4601t @24t/s)
2025-12-15 12:02:19,938 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 195.3s: 18570c @95c/s (6564ch, ~4642t @24t/s)
2025-12-15 12:02:21,939 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 197.3s: 18760c @95c/s (6628ch, ~4690t @24t/s)
2025-12-15 12:02:23,940 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 199.3s: 18943c @95c/s (6692ch, ~4736t @24t/s)
2025-12-15 12:02:25,965 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 201.3s: 19112c @95c/s (6756ch, ~4778t @24t/s)
2025-12-15 12:02:27,995 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 203.4s: 19275c @95c/s (6822ch, ~4819t @24t/s)
2025-12-15 12:02:30,025 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 205.4s: 19473c @95c/s (6888ch, ~4868t @24t/s)
2025-12-15 12:02:32,054 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 207.4s: 19648c @95c/s (6954ch, ~4912t @24t/s)
2025-12-15 12:02:34,057 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 209.4s: 19831c @95c/s (7019ch, ~4958t @24t/s)
2025-12-15 12:02:36,065 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 211.4s: 20021c @95c/s (7084ch, ~5005t @24t/s)
2025-12-15 12:02:38,092 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 213.5s: 20195c @95c/s (7149ch, ~5049t @24t/s)
2025-12-15 12:02:40,100 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 215.5s: 20387c @95c/s (7214ch, ~5097t @24t/s)
2025-12-15 12:02:42,105 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 217.5s: 20571c @95c/s (7279ch, ~5143t @24t/s)
2025-12-15 12:02:44,111 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 219.5s: 20744c @95c/s (7344ch, ~5186t @24t/s)
2025-12-15 12:02:46,119 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 221.5s: 20903c @94c/s (7409ch, ~5226t @24t/s)
2025-12-15 12:02:48,130 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 223.5s: 21097c @94c/s (7474ch, ~5274t @24t/s)
2025-12-15 12:02:50,143 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 225.5s: 21272c @94c/s (7539ch, ~5318t @24t/s)
2025-12-15 12:02:52,155 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 227.5s: 21452c @94c/s (7603ch, ~5363t @24t/s)
2025-12-15 12:02:54,170 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 229.5s: 21641c @94c/s (7668ch, ~5410t @24t/s)
2025-12-15 12:02:56,184 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 231.5s: 21817c @94c/s (7733ch, ~5454t @24t/s)
2025-12-15 12:02:58,199 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 233.6s: 21998c @94c/s (7798ch, ~5500t @24t/s)
2025-12-15 12:03:00,220 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 235.6s: 22189c @94c/s (7863ch, ~5547t @24t/s)
2025-12-15 12:03:02,243 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 237.6s: 22365c @94c/s (7928ch, ~5591t @24t/s)
2025-12-15 12:03:03,469 - src.llm.request_handler - ERROR - [dia:b979a7] â±ï¸ Read timeout 120.01s (limit: 120s)
2025-12-15 12:03:03,496 - src.llm.request_handler - INFO - [dia:b979a7] Troubleshooting suggestions:
2025-12-15 12:03:03,496 - src.llm.request_handler - INFO - [dia:b979a7]   - Request timed out. Consider:
2025-12-15 12:03:03,496 - src.llm.request_handler - INFO - [dia:b979a7]   -   - Increasing timeout in config/llm_config.yaml
2025-12-15 12:03:03,496 - src.llm.request_handler - INFO - [dia:b979a7]   -   - Using a faster/smaller model
2025-12-15 12:03:03,496 - src.llm.request_handler - INFO - [dia:b979a7]   -   - Reducing num_predict parameter
2025-12-15 12:03:03,496 - src.llm.request_handler - INFO - [dia:b979a7]   -   - Checking system resources (CPU/memory)
2025-12-15 12:03:03,497 - src.llm.client - ERROR - [dia:b979a7] Read timeout after 120.03s (limit: 120s) - Ollama received request but didn't start generating. Operation: diagram. Model: gemma3:4b. This may indicate: (1) Model is too slow for this timeout, (2) System resources are constrained, (3) Model is hung. Solutions: (1) Increase timeout in config/llm_config.yaml (current: 120s), (2) Use a faster model, (3) Check Ollama logs: ollama logs, (4) Restart Ollama service.
2025-12-15 12:03:03,497 - src.generate.orchestration.pipeline - WARNING -   Transient error in diagram 1 generation (attempt 2/3): [dia:b979a7] Read timeout after 120.03s (limit: 120s) - Ollama received request but didn't start generating. Operation: diagram. Model: gemma3:4b. This may indicate: (1) Model is too slow for this timeout, (2) System resources are constrained, (3) Model is hung. Solutions: (1) Increase timeout in config/llm_config.yaml (current: 120s), (2) Use a faster model, (3) Check Ollama logs: ollama logs, (4) Restart Ollama service.. Retrying in 4.0s...
2025-12-15 12:03:04,267 - src.llm.client - INFO - [dia:4c81de] ğŸ“Š 239.6s: 22527c @94c/s (7993ch, ~5632t @24t/s)
2025-12-15 12:03:04,671 - src.llm.client - ERROR - [dia:4c81de] Stream timeout: 240.03s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 8006 chunks, 753842 bytes, 22559 chars (~5640 tokens) before timeout. Performance: 94.0 chars/s, ~23.5 tok/s. Generation was slow (94.0 chars/s, ~23.5 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.
2025-12-15 12:03:04,672 - src.generate.orchestration.pipeline - WARNING -   Transient error in diagram 2 generation (attempt 1/3): [dia:4c81de] Stream timeout: 240.03s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 8006 chunks, 753842 bytes, 22559 chars (~5640 tokens) before timeout. Performance: 94.0 chars/s, ~23.5 tok/s. Generation was slow (94.0 chars/s, ~23.5 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.. Retrying in 2.0s...
2025-12-15 12:03:06,677 - src.generate.formats.diagrams - INFO - Generating diagram for: Marginal Probability (Bayesian Mechanics)
2025-12-15 12:03:06,678 - src.llm.client - INFO - [dia:52ce01] ğŸš€ dia | m=gemma3:4b | p=5750c | t=120s
2025-12-15 12:03:06,678 - src.llm.client - INFO - [dia:52ce01] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:03:06,678 - src.llm.client - INFO - [dia:52ce01] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:03:06,680 - src.llm.client - INFO - [dia:52ce01] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11059 bytes, prompt=5750 chars
2025-12-15 12:03:06,680 - src.llm.client - INFO - [dia:52ce01] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:03:07,502 - src.generate.formats.diagrams - INFO - Generating diagram for: Joint Probability (Bayesian Mechanics)
2025-12-15 12:03:07,502 - src.llm.client - INFO - [dia:f0db78] ğŸš€ dia | m=gemma3:4b | p=5744c | t=120s
2025-12-15 12:03:07,502 - src.llm.client - INFO - [dia:f0db78] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:03:07,502 - src.llm.client - INFO - [dia:f0db78] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:03:07,504 - src.llm.client - INFO - [dia:f0db78] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11053 bytes, prompt=5744 chars
2025-12-15 12:03:07,504 - src.llm.client - INFO - [dia:f0db78] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:03:10,073 - src.llm.request_handler - INFO - [dia:52ce01] âœ“ Done 3.39s
2025-12-15 12:03:10,073 - src.llm.client - INFO - [dia:52ce01] âœ… HTTP 200 in 3.39s
2025-12-15 12:03:10,073 - src.llm.client - INFO - [dia:52ce01] ğŸ“¡ Stream active (200)
2025-12-15 12:03:10,073 - src.llm.client - INFO - [dia:52ce01] Starting stream parsing, waiting for first chunk...
2025-12-15 12:03:12,075 - src.llm.client - INFO - [dia:52ce01] ğŸ“Š 2.0s: 188c @94c/s (66ch, ~47t @23t/s)
2025-12-15 12:03:14,076 - src.llm.client - INFO - [dia:52ce01] ğŸ“Š 4.0s: 384c @96c/s (132ch, ~96t @24t/s)
2025-12-15 12:03:16,080 - src.llm.client - INFO - [dia:52ce01] ğŸ“Š 6.0s: 589c @98c/s (198ch, ~147t @25t/s)
2025-12-15 12:03:18,082 - src.llm.client - INFO - [dia:52ce01] ğŸ“Š 8.0s: 761c @95c/s (264ch, ~190t @24t/s)
2025-12-15 12:03:20,088 - src.llm.client - INFO - [dia:52ce01] ğŸ“Š 10.0s: 919c @92c/s (330ch, ~230t @23t/s)
2025-12-15 12:03:20,465 - src.llm.client - INFO - [dia:52ce01] âœ“ Done 13.79s: 938c (~128w @68c/s)
2025-12-15 12:03:20,465 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Marginal Probability (Bayesian Mechanics):
2025-12-15 12:03:20,465 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:03:20,465 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:03:20,465 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:03:20,466 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:03:20,466 - src.generate.formats.diagrams - INFO -     - Length: 616 chars (cleaned: 616 chars)
2025-12-15 12:03:20,466 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:03:20,466 - src.generate.formats.diagrams - INFO - [OK] Elements: 39 total (nodes: 16, connections: 23) âœ“
2025-12-15 12:03:20,466 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:03:20,466 - src.generate.formats.diagrams - INFO - Generated diagram: 616 characters
2025-12-15 12:03:23,901 - src.llm.request_handler - INFO - [dia:f0db78] âœ“ Done 16.40s
2025-12-15 12:03:23,901 - src.llm.client - INFO - [dia:f0db78] âœ… HTTP 200 in 16.40s
2025-12-15 12:03:23,901 - src.llm.client - INFO - [dia:f0db78] ğŸ“¡ Stream active (200)
2025-12-15 12:03:23,901 - src.llm.client - INFO - [dia:f0db78] Starting stream parsing, waiting for first chunk...
2025-12-15 12:03:25,910 - src.llm.client - INFO - [dia:f0db78] ğŸ“Š 2.0s: 239c @119c/s (66ch, ~60t @30t/s)
2025-12-15 12:03:27,915 - src.llm.client - INFO - [dia:f0db78] ğŸ“Š 4.0s: 487c @121c/s (132ch, ~122t @30t/s)
2025-12-15 12:03:29,944 - src.llm.client - INFO - [dia:f0db78] ğŸ“Š 6.0s: 720c @119c/s (199ch, ~180t @30t/s)
2025-12-15 12:03:31,971 - src.llm.client - INFO - [dia:f0db78] ğŸ“Š 8.1s: 959c @119c/s (266ch, ~240t @30t/s)
2025-12-15 12:03:33,971 - src.llm.client - INFO - [dia:f0db78] ğŸ“Š 10.1s: 1193c @118c/s (332ch, ~298t @30t/s)
2025-12-15 12:03:34,950 - src.llm.client - INFO - [dia:f0db78] âœ“ Done 27.45s: 1277c (~183w @47c/s)
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Joint Probability (Bayesian Mechanics):
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - INFO -     - Length: 397 chars (cleaned: 397 chars)
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:03:34,950 - src.generate.formats.diagrams - INFO - [OK] Elements: 23 total (nodes: 10, connections: 13) âœ“
2025-12-15 12:03:34,951 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:03:34,951 - src.generate.formats.diagrams - INFO - Generated diagram: 397 characters
2025-12-15 12:03:34,951 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:03:34,951 - src.generate.formats.questions - INFO - Generating 10 questions for: Bayesian Mechanics (Session 2)
2025-12-15 12:03:34,951 - src.llm.client - INFO - [qst:d6164d] ğŸš€ qst | m=gemma3:4b | p=7335c | t=150s
2025-12-15 12:03:34,951 - src.llm.client - INFO - [qst:d6164d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:03:34,951 - src.llm.client - INFO - [qst:d6164d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:03:34,952 - src.llm.client - INFO - [qst:d6164d] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11017 bytes, prompt=7335 chars
2025-12-15 12:03:34,952 - src.llm.client - INFO - [qst:d6164d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:03:38,979 - src.llm.request_handler - INFO - [qst:d6164d] âœ“ Done 4.03s
2025-12-15 12:03:38,979 - src.llm.client - INFO - [qst:d6164d] âœ… HTTP 200 in 4.03s
2025-12-15 12:03:38,979 - src.llm.client - INFO - [qst:d6164d] ğŸ“¡ Stream active (200)
2025-12-15 12:03:38,979 - src.llm.client - INFO - [qst:d6164d] Starting stream parsing, waiting for first chunk...
2025-12-15 12:03:40,981 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 2.0s: 303c @151c/s (66ch, ~76t @38t/s)
2025-12-15 12:03:42,986 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 4.0s: 651c @163c/s (132ch, ~163t @41t/s)
2025-12-15 12:03:44,986 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 6.0s: 1013c @169c/s (198ch, ~253t @42t/s)
2025-12-15 12:03:46,990 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 8.0s: 1187c @148c/s (264ch, ~297t @37t/s)
2025-12-15 12:03:48,995 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 10.0s: 1426c @142c/s (330ch, ~356t @36t/s)
2025-12-15 12:03:51,006 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 12.0s: 1793c @149c/s (396ch, ~448t @37t/s)
2025-12-15 12:03:53,016 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 14.0s: 2175c @155c/s (461ch, ~544t @39t/s)
2025-12-15 12:03:55,029 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 16.0s: 2365c @147c/s (527ch, ~591t @37t/s)
2025-12-15 12:03:57,046 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 18.1s: 2564c @142c/s (593ch, ~641t @35t/s)
2025-12-15 12:03:59,062 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 20.1s: 2954c @147c/s (659ch, ~738t @37t/s)
2025-12-15 12:04:01,090 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 22.1s: 3341c @151c/s (724ch, ~835t @38t/s)
2025-12-15 12:04:03,110 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 24.1s: 3710c @154c/s (790ch, ~928t @38t/s)
2025-12-15 12:04:05,136 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 26.2s: 3970c @152c/s (856ch, ~992t @38t/s)
2025-12-15 12:04:07,141 - src.llm.client - INFO - [qst:d6164d] ğŸ“Š 28.2s: 4188c @149c/s (921ch, ~1047t @37t/s)
2025-12-15 12:04:08,420 - src.llm.client - INFO - [qst:d6164d] âœ“ Done 33.47s: 4456c (~652w @133c/s)
2025-12-15 12:04:08,421 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 5 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 3, 'total_fixes': 5}
2025-12-15 12:04:08,421 - src.generate.formats.questions - INFO - Applied 5 auto-fixes to questions
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 3 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -     Context: Module 2 Session 2
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 3 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -     Context: Module 2 Session 2
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:04:08,421 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Bayesian Mechanics (Session 2)
2025-12-15 12:04:08,422 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:04:08,424 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:04:08,425 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 2 completed
2025-12-15 12:04:08,425 - src.generate.orchestration.pipeline - INFO - 
[3/20] Session 3: Bayesian Inference
2025-12-15 12:04:08,425 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:04:08,425 - src.generate.formats.lectures - INFO - Generating lecture for: Bayesian Mechanics (Session 3/20)
2025-12-15 12:04:08,426 - src.llm.client - INFO - [lec:e4728a] ğŸš€ lec | m=gemma3:4b | p=3043c | t=180s
2025-12-15 12:04:08,426 - src.llm.client - INFO - [lec:e4728a] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:04:08,426 - src.llm.client - INFO - [lec:e4728a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:04:08,427 - src.llm.client - INFO - [lec:e4728a] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6678 bytes, prompt=3043 chars
2025-12-15 12:04:08,427 - src.llm.client - INFO - [lec:e4728a] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:04:10,482 - src.llm.request_handler - INFO - [lec:e4728a] âœ“ Done 2.05s
2025-12-15 12:04:10,482 - src.llm.client - INFO - [lec:e4728a] âœ… HTTP 200 in 2.05s
2025-12-15 12:04:10,482 - src.llm.client - INFO - [lec:e4728a] ğŸ“¡ Stream active (200)
2025-12-15 12:04:10,482 - src.llm.client - INFO - [lec:e4728a] Starting stream parsing, waiting for first chunk...
2025-12-15 12:04:12,510 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 2.0s: 407c @201c/s (68ch, ~102t @50t/s)
2025-12-15 12:04:14,514 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 4.0s: 798c @198c/s (135ch, ~200t @49t/s)
2025-12-15 12:04:16,528 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 6.0s: 1160c @192c/s (202ch, ~290t @48t/s)
2025-12-15 12:04:18,546 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 8.1s: 1480c @184c/s (269ch, ~370t @46t/s)
2025-12-15 12:04:20,571 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 10.1s: 1717c @170c/s (336ch, ~429t @43t/s)
2025-12-15 12:04:22,592 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 12.1s: 1940c @160c/s (403ch, ~485t @40t/s)
2025-12-15 12:04:24,616 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 14.1s: 2261c @160c/s (470ch, ~565t @40t/s)
2025-12-15 12:04:26,619 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 16.1s: 2455c @152c/s (536ch, ~614t @38t/s)
2025-12-15 12:04:28,619 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 18.1s: 2806c @155c/s (602ch, ~702t @39t/s)
2025-12-15 12:04:30,648 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 20.2s: 3154c @156c/s (669ch, ~788t @39t/s)
2025-12-15 12:04:32,650 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 22.2s: 3366c @152c/s (735ch, ~842t @38t/s)
2025-12-15 12:04:34,650 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 24.2s: 3673c @152c/s (801ch, ~918t @38t/s)
2025-12-15 12:04:36,656 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 26.2s: 3914c @150c/s (867ch, ~978t @37t/s)
2025-12-15 12:04:38,667 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 28.2s: 4220c @150c/s (933ch, ~1055t @37t/s)
2025-12-15 12:04:40,672 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 30.2s: 4573c @151c/s (999ch, ~1143t @38t/s)
2025-12-15 12:04:42,678 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 32.2s: 4891c @152c/s (1065ch, ~1223t @38t/s)
2025-12-15 12:04:44,686 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 34.2s: 5260c @154c/s (1131ch, ~1315t @38t/s)
2025-12-15 12:04:46,695 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 36.2s: 5665c @156c/s (1197ch, ~1416t @39t/s)
2025-12-15 12:04:48,721 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 38.2s: 6007c @157c/s (1263ch, ~1502t @39t/s)
2025-12-15 12:04:50,734 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 40.3s: 6364c @158c/s (1329ch, ~1591t @40t/s)
2025-12-15 12:04:52,747 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 42.3s: 6712c @159c/s (1394ch, ~1678t @40t/s)
2025-12-15 12:04:54,761 - src.llm.client - INFO - [lec:e4728a] ğŸ“Š 44.3s: 7059c @159c/s (1460ch, ~1765t @40t/s)
2025-12-15 12:04:55,771 - src.llm.client - INFO - [lec:e4728a] âœ“ Done 47.35s: 7259c (~1107w @153c/s)
2025-12-15 12:04:55,772 - src.generate.formats.lectures - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:04:55,772 - src.generate.formats.lectures - WARNING -     [CRITICAL] Issue 1: Only 4 examples found (require 5-15, need 1 more - add concrete examples)
2025-12-15 12:04:55,772 - src.generate.formats.lectures - WARNING -   Retry attempt 1/1 for lecture: Bayesian Mechanics (Session 3)
2025-12-15 12:04:55,772 - src.generate.formats.lectures - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:04:55,776 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:04:55,776 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:04:55,776 - src.generate.formats.labs - INFO - Generating lab 3 for: Bayesian Mechanics (Session 3)
2025-12-15 12:04:55,777 - src.llm.client - INFO - [lab:eeb6d4] ğŸš€ lab | m=gemma3:4b | p=3329c | t=150s
2025-12-15 12:04:55,777 - src.llm.client - INFO - [lab:eeb6d4] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:04:55,777 - src.llm.client - INFO - [lab:eeb6d4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:04:55,778 - src.llm.client - INFO - [lab:eeb6d4] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3767 bytes, prompt=3329 chars
2025-12-15 12:04:55,778 - src.llm.client - INFO - [lab:eeb6d4] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:04:57,585 - src.llm.request_handler - INFO - [lab:eeb6d4] âœ“ Done 1.81s
2025-12-15 12:04:57,585 - src.llm.client - INFO - [lab:eeb6d4] âœ… HTTP 200 in 1.81s
2025-12-15 12:04:57,585 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“¡ Stream active (200)
2025-12-15 12:04:57,585 - src.llm.client - INFO - [lab:eeb6d4] Starting stream parsing, waiting for first chunk...
2025-12-15 12:04:59,611 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 2.0s: 283c @140c/s (68ch, ~71t @35t/s)
2025-12-15 12:05:01,612 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 4.0s: 657c @163c/s (135ch, ~164t @41t/s)
2025-12-15 12:05:03,618 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 6.0s: 1001c @166c/s (202ch, ~250t @41t/s)
2025-12-15 12:05:05,635 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 8.0s: 1336c @166c/s (269ch, ~334t @41t/s)
2025-12-15 12:05:07,661 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 10.1s: 1623c @161c/s (336ch, ~406t @40t/s)
2025-12-15 12:05:09,683 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 12.1s: 1938c @160c/s (403ch, ~484t @40t/s)
2025-12-15 12:05:11,707 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 14.1s: 2279c @161c/s (470ch, ~570t @40t/s)
2025-12-15 12:05:13,732 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 16.1s: 2620c @162c/s (537ch, ~655t @41t/s)
2025-12-15 12:05:15,762 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 18.2s: 2854c @157c/s (604ch, ~714t @39t/s)
2025-12-15 12:05:17,764 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 20.2s: 3149c @156c/s (670ch, ~787t @39t/s)
2025-12-15 12:05:19,766 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 22.2s: 3476c @157c/s (736ch, ~869t @39t/s)
2025-12-15 12:05:21,770 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 24.2s: 3808c @157c/s (802ch, ~952t @39t/s)
2025-12-15 12:05:23,773 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 26.2s: 4015c @153c/s (868ch, ~1004t @38t/s)
2025-12-15 12:05:25,776 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 28.2s: 4325c @153c/s (934ch, ~1081t @38t/s)
2025-12-15 12:05:27,785 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 30.2s: 4646c @154c/s (1000ch, ~1162t @38t/s)
2025-12-15 12:05:29,791 - src.llm.client - INFO - [lab:eeb6d4] ğŸ“Š 32.2s: 5034c @156c/s (1066ch, ~1258t @39t/s)
2025-12-15 12:05:30,319 - src.llm.client - INFO - [lab:eeb6d4] âœ“ Done 34.54s: 5119c (~722w @148c/s)
2025-12-15 12:05:30,320 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:05:30,320 - src.generate.formats.labs - INFO -     - Length: 5214 chars, 736 words
2025-12-15 12:05:30,320 - src.generate.formats.labs - INFO -     - Procedure: 6 steps
2025-12-15 12:05:30,320 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 12:05:30,320 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-15 12:05:30,322 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:05:30,322 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:05:30,322 - src.generate.formats.study_notes - INFO - Generating study notes for: Bayesian Mechanics (Session 3)
2025-12-15 12:05:30,323 - src.llm.client - INFO - [stu:eccddf] ğŸš€ stu | m=gemma3:4b | p=4441c | t=120s
2025-12-15 12:05:30,323 - src.llm.client - INFO - [stu:eccddf] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:05:30,323 - src.llm.client - INFO - [stu:eccddf] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:05:30,324 - src.llm.client - INFO - [stu:eccddf] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8094 bytes, prompt=4441 chars
2025-12-15 12:05:30,324 - src.llm.client - INFO - [stu:eccddf] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:05:33,235 - src.llm.request_handler - INFO - [stu:eccddf] âœ“ Done 2.91s
2025-12-15 12:05:33,235 - src.llm.client - INFO - [stu:eccddf] âœ… HTTP 200 in 2.91s
2025-12-15 12:05:33,235 - src.llm.client - INFO - [stu:eccddf] ğŸ“¡ Stream active (200)
2025-12-15 12:05:33,235 - src.llm.client - INFO - [stu:eccddf] Starting stream parsing, waiting for first chunk...
2025-12-15 12:05:35,241 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 2.0s: 395c @197c/s (66ch, ~99t @49t/s)
2025-12-15 12:05:37,245 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 4.0s: 790c @197c/s (132ch, ~198t @49t/s)
2025-12-15 12:05:39,253 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 6.0s: 1153c @192c/s (198ch, ~288t @48t/s)
2025-12-15 12:05:41,254 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 8.0s: 1491c @186c/s (264ch, ~373t @46t/s)
2025-12-15 12:05:43,254 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 10.0s: 1765c @176c/s (330ch, ~441t @44t/s)
2025-12-15 12:05:45,256 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 12.0s: 2091c @174c/s (396ch, ~523t @43t/s)
2025-12-15 12:05:47,259 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 14.0s: 2356c @168c/s (462ch, ~589t @42t/s)
2025-12-15 12:05:49,264 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 16.0s: 2647c @165c/s (528ch, ~662t @41t/s)
2025-12-15 12:05:51,273 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 18.0s: 3018c @167c/s (594ch, ~754t @42t/s)
2025-12-15 12:05:53,283 - src.llm.client - INFO - [stu:eccddf] ğŸ“Š 20.0s: 3403c @170c/s (659ch, ~851t @42t/s)
2025-12-15 12:05:54,936 - src.llm.client - INFO - [stu:eccddf] âœ“ Done 24.61s: 3719c (~526w @151c/s)
2025-12-15 12:05:54,936 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:05:54,937 - src.generate.formats.study_notes - INFO -     - Length: 3772 chars, 535 words
2025-12-15 12:05:54,937 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:05:54,937 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-15 12:05:54,937 - src.generate.formats.study_notes - INFO -     - Structure: 4 sections, 1 bullets
2025-12-15 12:05:54,937 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:05:54,938 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:05:54,938 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:05:54,938 - src.generate.formats.diagrams - INFO - Generating diagram for: Prior, Likelihood, Posterior (Bayesian Mechanics)
2025-12-15 12:05:54,938 - src.llm.client - INFO - [dia:cf27cd] ğŸš€ dia | m=gemma3:4b | p=5759c | t=120s
2025-12-15 12:05:54,938 - src.llm.client - INFO - [dia:cf27cd] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:05:54,938 - src.llm.client - INFO - [dia:cf27cd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:05:54,940 - src.llm.client - INFO - [dia:cf27cd] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11068 bytes, prompt=5759 chars
2025-12-15 12:05:54,940 - src.llm.client - INFO - [dia:cf27cd] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:05:58,458 - src.llm.request_handler - INFO - [dia:cf27cd] âœ“ Done 3.52s
2025-12-15 12:05:58,458 - src.llm.client - INFO - [dia:cf27cd] âœ… HTTP 200 in 3.52s
2025-12-15 12:05:58,458 - src.llm.client - INFO - [dia:cf27cd] ğŸ“¡ Stream active (200)
2025-12-15 12:05:58,458 - src.llm.client - INFO - [dia:cf27cd] Starting stream parsing, waiting for first chunk...
2025-12-15 12:06:00,474 - src.llm.client - INFO - [dia:cf27cd] ğŸ“Š 2.0s: 210c @104c/s (65ch, ~52t @26t/s)
2025-12-15 12:06:02,501 - src.llm.client - INFO - [dia:cf27cd] ğŸ“Š 4.0s: 438c @108c/s (132ch, ~110t @27t/s)
2025-12-15 12:06:04,529 - src.llm.client - INFO - [dia:cf27cd] ğŸ“Š 6.1s: 604c @100c/s (199ch, ~151t @25t/s)
2025-12-15 12:06:05,106 - src.llm.client - INFO - [dia:cf27cd] âœ“ Done 10.17s: 644c (~95w @63c/s)
2025-12-15 12:06:05,106 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Prior, Likelihood, Posterior (Bayesian Mechanics):
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - WARNING - [WARNING] Only 9 nodes found (require at least 10, need 1 more - add more nodes to the diagram) âš ï¸
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 9 nodes found (require at least 10, need 1 more - add more nodes to the diagram) ğŸ”´
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - WARNING -     Context: Module 2 Session 3
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - WARNING - [CRITICAL] Content Issue: Only 9 nodes found (require at least 10, need 1 more - add more nodes to the diagram) ğŸ”´
2025-12-15 12:06:05,107 - src.generate.formats.diagrams - WARNING -     Context: Module 2 Session 3
2025-12-15 12:06:05,108 - src.generate.formats.diagrams - WARNING -     Impact: Diagram lacks sufficient nodes for meaningful visualization
2025-12-15 12:06:05,108 - src.generate.formats.diagrams - WARNING -     Recommendation: Add more nodes to the diagram (minimum 10 nodes required)
2025-12-15 12:06:05,108 - src.generate.formats.diagrams - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:06:05,108 - src.generate.formats.diagrams - WARNING -   Retry attempt 1/1 for diagram: Prior, Likelihood, Posterior (Bayesian Mechanics)
2025-12-15 12:06:05,108 - src.generate.formats.diagrams - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:06:05,108 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:06:05,108 - src.generate.formats.questions - INFO - Generating 10 questions for: Bayesian Mechanics (Session 3)
2025-12-15 12:06:05,109 - src.llm.client - INFO - [qst:9828a3] ğŸš€ qst | m=gemma3:4b | p=7320c | t=150s
2025-12-15 12:06:05,109 - src.llm.client - INFO - [qst:9828a3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:06:05,109 - src.llm.client - INFO - [qst:9828a3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:06:05,110 - src.llm.client - INFO - [qst:9828a3] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11038 bytes, prompt=7320 chars
2025-12-15 12:06:05,110 - src.llm.client - INFO - [qst:9828a3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:06:09,173 - src.llm.request_handler - INFO - [qst:9828a3] âœ“ Done 4.06s
2025-12-15 12:06:09,174 - src.llm.client - INFO - [qst:9828a3] âœ… HTTP 200 in 4.06s
2025-12-15 12:06:09,174 - src.llm.client - INFO - [qst:9828a3] ğŸ“¡ Stream active (200)
2025-12-15 12:06:09,174 - src.llm.client - INFO - [qst:9828a3] Starting stream parsing, waiting for first chunk...
2025-12-15 12:06:11,179 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 2.0s: 303c @151c/s (66ch, ~76t @38t/s)
2025-12-15 12:06:13,208 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 4.0s: 640c @159c/s (133ch, ~160t @40t/s)
2025-12-15 12:06:15,209 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 6.0s: 966c @160c/s (199ch, ~242t @40t/s)
2025-12-15 12:06:17,210 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 8.0s: 1272c @158c/s (265ch, ~318t @40t/s)
2025-12-15 12:06:19,212 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 10.0s: 1593c @159c/s (331ch, ~398t @40t/s)
2025-12-15 12:06:21,223 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 12.0s: 1913c @159c/s (397ch, ~478t @40t/s)
2025-12-15 12:06:23,233 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 14.1s: 2271c @162c/s (463ch, ~568t @40t/s)
2025-12-15 12:06:25,245 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 16.1s: 2635c @164c/s (529ch, ~659t @41t/s)
2025-12-15 12:06:27,260 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 18.1s: 3036c @168c/s (595ch, ~759t @42t/s)
2025-12-15 12:06:29,276 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 20.1s: 3413c @170c/s (661ch, ~853t @42t/s)
2025-12-15 12:06:31,299 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 22.1s: 3783c @171c/s (727ch, ~946t @43t/s)
2025-12-15 12:06:33,321 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 24.1s: 4193c @174c/s (793ch, ~1048t @43t/s)
2025-12-15 12:06:35,347 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 26.2s: 4603c @176c/s (859ch, ~1151t @44t/s)
2025-12-15 12:06:37,352 - src.llm.client - INFO - [qst:9828a3] ğŸ“Š 28.2s: 4933c @175c/s (924ch, ~1233t @44t/s)
2025-12-15 12:06:38,170 - src.llm.client - INFO - [qst:9828a3] âœ“ Done 33.06s: 5072c (~707w @153c/s)
2025-12-15 12:06:38,171 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 1, 'total_fixes': 2}
2025-12-15 12:06:38,171 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 2 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -     Context: Module 2 Session 3
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 5 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -     Context: Module 2 Session 3
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Bayesian Mechanics (Session 3)
2025-12-15 12:06:38,171 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:06:38,174 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:06:38,175 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 3 completed
2025-12-15 12:06:38,176 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:06:38,176 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:06:38,176 - src.generate.orchestration.pipeline - INFO - Module 3: Variational Inference (2 sessions)
2025-12-15 12:06:38,176 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:06:38,176 - src.generate.orchestration.pipeline - INFO - 
[4/20] Session 4: Approximation Techniques
2025-12-15 12:06:38,176 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:06:38,176 - src.generate.formats.lectures - INFO - Generating lecture for: Variational Inference (Session 4/20)
2025-12-15 12:06:38,176 - src.llm.client - INFO - [lec:43ab49] ğŸš€ lec | m=gemma3:4b | p=3067c | t=180s
2025-12-15 12:06:38,176 - src.llm.client - INFO - [lec:43ab49] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:06:38,176 - src.llm.client - INFO - [lec:43ab49] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:06:38,177 - src.llm.client - INFO - [lec:43ab49] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6698 bytes, prompt=3067 chars
2025-12-15 12:06:38,177 - src.llm.client - INFO - [lec:43ab49] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:06:40,797 - src.llm.request_handler - INFO - [lec:43ab49] âœ“ Done 2.62s
2025-12-15 12:06:40,797 - src.llm.client - INFO - [lec:43ab49] âœ… HTTP 200 in 2.62s
2025-12-15 12:06:40,797 - src.llm.client - INFO - [lec:43ab49] ğŸ“¡ Stream active (200)
2025-12-15 12:06:40,797 - src.llm.client - INFO - [lec:43ab49] Starting stream parsing, waiting for first chunk...
2025-12-15 12:06:42,826 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 2.0s: 436c @215c/s (68ch, ~109t @54t/s)
2025-12-15 12:06:44,831 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 4.0s: 785c @195c/s (135ch, ~196t @49t/s)
2025-12-15 12:06:46,833 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 6.0s: 1024c @170c/s (201ch, ~256t @42t/s)
2025-12-15 12:06:48,852 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 8.1s: 1281c @159c/s (268ch, ~320t @40t/s)
2025-12-15 12:06:50,879 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 10.1s: 1615c @160c/s (333ch, ~404t @40t/s)
2025-12-15 12:06:52,879 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 12.1s: 1970c @163c/s (398ch, ~492t @41t/s)
2025-12-15 12:06:54,907 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 14.1s: 2277c @161c/s (461ch, ~569t @40t/s)
2025-12-15 12:06:56,920 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 16.1s: 2596c @161c/s (527ch, ~649t @40t/s)
2025-12-15 12:06:58,926 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 18.1s: 2936c @162c/s (593ch, ~734t @40t/s)
2025-12-15 12:07:00,929 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 20.1s: 3286c @163c/s (659ch, ~822t @41t/s)
2025-12-15 12:07:02,932 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 22.1s: 3486c @157c/s (725ch, ~872t @39t/s)
2025-12-15 12:07:04,939 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 24.1s: 3667c @152c/s (791ch, ~917t @38t/s)
2025-12-15 12:07:06,952 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 26.2s: 4008c @153c/s (857ch, ~1002t @38t/s)
2025-12-15 12:07:08,965 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 28.2s: 4324c @154c/s (923ch, ~1081t @38t/s)
2025-12-15 12:07:10,976 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 30.2s: 4642c @154c/s (989ch, ~1160t @38t/s)
2025-12-15 12:07:12,987 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 32.2s: 4975c @155c/s (1055ch, ~1244t @39t/s)
2025-12-15 12:07:14,997 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 34.2s: 5322c @156c/s (1121ch, ~1330t @39t/s)
2025-12-15 12:07:17,012 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 36.2s: 5685c @157c/s (1187ch, ~1421t @39t/s)
2025-12-15 12:07:19,026 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 38.2s: 6073c @159c/s (1253ch, ~1518t @40t/s)
2025-12-15 12:07:21,048 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 40.3s: 6528c @162c/s (1319ch, ~1632t @41t/s)
2025-12-15 12:07:23,069 - src.llm.client - INFO - [lec:43ab49] ğŸ“Š 42.3s: 6950c @164c/s (1385ch, ~1738t @41t/s)
2025-12-15 12:07:23,805 - src.llm.client - INFO - [lec:43ab49] âœ“ Done 45.63s: 7096c (~1028w @156c/s)
2025-12-15 12:07:23,806 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:07:23,806 - src.generate.formats.lectures - INFO -     - Length: 7186 chars, 1039 words
2025-12-15 12:07:23,806 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:07:23,806 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:07:23,806 - src.generate.formats.lectures - INFO -     - Content: 8 examples, 0 terms defined
2025-12-15 12:07:23,806 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:07:23,810 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:07:23,810 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:07:23,810 - src.generate.formats.labs - INFO - Generating lab 4 for: Variational Inference (Session 4)
2025-12-15 12:07:23,810 - src.llm.client - INFO - [lab:1dabf7] ğŸš€ lab | m=gemma3:4b | p=3342c | t=150s
2025-12-15 12:07:23,810 - src.llm.client - INFO - [lab:1dabf7] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:07:23,810 - src.llm.client - INFO - [lab:1dabf7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:07:23,812 - src.llm.client - INFO - [lab:1dabf7] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3807 bytes, prompt=3342 chars
2025-12-15 12:07:23,812 - src.llm.client - INFO - [lab:1dabf7] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:07:25,642 - src.llm.request_handler - INFO - [lab:1dabf7] âœ“ Done 1.83s
2025-12-15 12:07:25,642 - src.llm.client - INFO - [lab:1dabf7] âœ… HTTP 200 in 1.83s
2025-12-15 12:07:25,642 - src.llm.client - INFO - [lab:1dabf7] ğŸ“¡ Stream active (200)
2025-12-15 12:07:25,642 - src.llm.client - INFO - [lab:1dabf7] Starting stream parsing, waiting for first chunk...
2025-12-15 12:07:27,645 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 2.0s: 303c @151c/s (67ch, ~76t @38t/s)
2025-12-15 12:07:29,649 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 4.0s: 715c @178c/s (134ch, ~179t @45t/s)
2025-12-15 12:07:31,658 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 6.0s: 1085c @180c/s (201ch, ~271t @45t/s)
2025-12-15 12:07:33,675 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 8.0s: 1417c @176c/s (268ch, ~354t @44t/s)
2025-12-15 12:07:35,695 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 10.1s: 1688c @168c/s (334ch, ~422t @42t/s)
2025-12-15 12:07:37,707 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 12.1s: 1859c @154c/s (400ch, ~465t @39t/s)
2025-12-15 12:07:39,713 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 14.1s: 2010c @143c/s (466ch, ~502t @36t/s)
2025-12-15 12:07:41,743 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 16.1s: 2331c @145c/s (533ch, ~583t @36t/s)
2025-12-15 12:07:43,746 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 18.1s: 2667c @147c/s (599ch, ~667t @37t/s)
2025-12-15 12:07:45,750 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 20.1s: 2876c @143c/s (665ch, ~719t @36t/s)
2025-12-15 12:07:47,754 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 22.1s: 3148c @142c/s (731ch, ~787t @36t/s)
2025-12-15 12:07:49,758 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 24.1s: 3416c @142c/s (797ch, ~854t @35t/s)
2025-12-15 12:07:51,769 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 26.1s: 3637c @139c/s (862ch, ~909t @35t/s)
2025-12-15 12:07:53,776 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 28.1s: 3897c @139c/s (928ch, ~974t @35t/s)
2025-12-15 12:07:55,784 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 30.1s: 4349c @144c/s (994ch, ~1087t @36t/s)
2025-12-15 12:07:57,792 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 32.1s: 4651c @145c/s (1060ch, ~1163t @36t/s)
2025-12-15 12:07:59,802 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 34.2s: 4929c @144c/s (1126ch, ~1232t @36t/s)
2025-12-15 12:08:01,815 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 36.2s: 5268c @146c/s (1192ch, ~1317t @36t/s)
2025-12-15 12:08:03,830 - src.llm.client - INFO - [lab:1dabf7] ğŸ“Š 38.2s: 5621c @147c/s (1258ch, ~1405t @37t/s)
2025-12-15 12:08:05,339 - src.llm.client - INFO - [lab:1dabf7] âœ“ Done 41.53s: 5898c (~825w @142c/s)
2025-12-15 12:08:05,340 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:08:05,340 - src.generate.formats.labs - INFO -     - Length: 5988 chars, 839 words
2025-12-15 12:08:05,340 - src.generate.formats.labs - INFO -     - Procedure: 7 steps
2025-12-15 12:08:05,340 - src.generate.formats.labs - INFO -     - Safety: 2 warnings
2025-12-15 12:08:05,340 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-15 12:08:05,342 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:08:05,342 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:08:05,342 - src.generate.formats.study_notes - INFO - Generating study notes for: Variational Inference (Session 4)
2025-12-15 12:08:05,343 - src.llm.client - INFO - [stu:54ac02] ğŸš€ stu | m=gemma3:4b | p=4449c | t=120s
2025-12-15 12:08:05,343 - src.llm.client - INFO - [stu:54ac02] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:08:05,343 - src.llm.client - INFO - [stu:54ac02] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:08:05,344 - src.llm.client - INFO - [stu:54ac02] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8129 bytes, prompt=4449 chars
2025-12-15 12:08:05,344 - src.llm.client - INFO - [stu:54ac02] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:08:07,998 - src.llm.request_handler - INFO - [stu:54ac02] âœ“ Done 2.65s
2025-12-15 12:08:07,998 - src.llm.client - INFO - [stu:54ac02] âœ… HTTP 200 in 2.65s
2025-12-15 12:08:07,998 - src.llm.client - INFO - [stu:54ac02] ğŸ“¡ Stream active (200)
2025-12-15 12:08:07,998 - src.llm.client - INFO - [stu:54ac02] Starting stream parsing, waiting for first chunk...
2025-12-15 12:08:10,002 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 2.0s: 363c @181c/s (66ch, ~91t @45t/s)
2025-12-15 12:08:12,029 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 4.0s: 635c @158c/s (133ch, ~159t @39t/s)
2025-12-15 12:08:14,032 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 6.0s: 948c @157c/s (199ch, ~237t @39t/s)
2025-12-15 12:08:16,037 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 8.0s: 1191c @148c/s (265ch, ~298t @37t/s)
2025-12-15 12:08:18,045 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 10.0s: 1522c @151c/s (331ch, ~380t @38t/s)
2025-12-15 12:08:20,052 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 12.1s: 1895c @157c/s (397ch, ~474t @39t/s)
2025-12-15 12:08:22,064 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 14.1s: 2231c @159c/s (463ch, ~558t @40t/s)
2025-12-15 12:08:24,075 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 16.1s: 2577c @160c/s (529ch, ~644t @40t/s)
2025-12-15 12:08:26,088 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 18.1s: 2863c @158c/s (595ch, ~716t @40t/s)
2025-12-15 12:08:28,101 - src.llm.client - INFO - [stu:54ac02] ğŸ“Š 20.1s: 3170c @158c/s (661ch, ~792t @39t/s)
2025-12-15 12:08:28,787 - src.llm.client - INFO - [stu:54ac02] âœ“ Done 23.44s: 3283c (~472w @140c/s)
2025-12-15 12:08:28,787 - src.generate.formats.study_notes - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:08:28,787 - src.generate.formats.study_notes - WARNING -     [CRITICAL] Issue 1: Only 1 key concepts highlighted (require 3-10, need 2 more - format concepts as **Concept Name:** in bullet points)
2025-12-15 12:08:28,787 - src.generate.formats.study_notes - WARNING -   Retry attempt 1/1 for study notes: Variational Inference (Session 4)
2025-12-15 12:08:28,787 - src.generate.formats.study_notes - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:08:28,789 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:08:28,790 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:08:28,790 - src.generate.formats.diagrams - INFO - Generating diagram for: Mean Field Approximation (Variational Inference)
2025-12-15 12:08:28,790 - src.generate.formats.diagrams - INFO - Generating diagram for: Evidence Lower Bound (Variational Inference)
2025-12-15 12:08:28,790 - src.llm.client - INFO - [dia:5d6d42] ğŸš€ dia | m=gemma3:4b | p=5752c | t=120s
2025-12-15 12:08:28,790 - src.llm.client - INFO - [dia:5d6d42] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:08:28,790 - src.llm.client - INFO - [dia:5d6d42] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:08:28,790 - src.llm.client - INFO - [dia:a70d58] ğŸš€ dia | m=gemma3:4b | p=5760c | t=120s
2025-12-15 12:08:28,791 - src.llm.client - INFO - [dia:a70d58] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:08:28,791 - src.llm.client - INFO - [dia:a70d58] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:08:28,792 - src.llm.client - INFO - [dia:a70d58] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11069 bytes, prompt=5760 chars
2025-12-15 12:08:28,792 - src.llm.client - INFO - [dia:5d6d42] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11061 bytes, prompt=5752 chars
2025-12-15 12:08:28,792 - src.llm.client - INFO - [dia:5d6d42] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:08:28,792 - src.llm.client - INFO - [dia:a70d58] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:08:32,347 - src.llm.request_handler - INFO - [dia:a70d58] âœ“ Done 3.55s
2025-12-15 12:08:32,347 - src.llm.client - INFO - [dia:a70d58] âœ… HTTP 200 in 3.55s
2025-12-15 12:08:32,347 - src.llm.client - INFO - [dia:a70d58] ğŸ“¡ Stream active (200)
2025-12-15 12:08:32,347 - src.llm.client - INFO - [dia:a70d58] Starting stream parsing, waiting for first chunk...
2025-12-15 12:08:34,364 - src.llm.client - INFO - [dia:a70d58] ğŸ“Š 2.0s: 221c @110c/s (66ch, ~55t @27t/s)
2025-12-15 12:08:36,370 - src.llm.client - INFO - [dia:a70d58] ğŸ“Š 4.0s: 451c @112c/s (132ch, ~113t @28t/s)
2025-12-15 12:08:38,380 - src.llm.client - INFO - [dia:a70d58] ğŸ“Š 6.0s: 678c @112c/s (198ch, ~170t @28t/s)
2025-12-15 12:08:40,389 - src.llm.client - INFO - [dia:a70d58] ğŸ“Š 8.0s: 924c @115c/s (264ch, ~231t @29t/s)
2025-12-15 12:08:42,391 - src.llm.client - INFO - [dia:a70d58] ğŸ“Š 10.0s: 1091c @109c/s (330ch, ~273t @27t/s)
2025-12-15 12:08:44,401 - src.llm.client - INFO - [dia:a70d58] ğŸ“Š 12.1s: 1251c @104c/s (396ch, ~313t @26t/s)
2025-12-15 12:08:46,373 - src.llm.client - INFO - [dia:a70d58] âœ“ Done 17.58s: 1402c (~176w @80c/s)
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Mean Field Approximation (Variational Inference):
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO -     - Length: 945 chars (cleaned: 945 chars)
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO - [OK] Elements: 51 total (nodes: 26, connections: 25) âœ“
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:08:46,374 - src.generate.formats.diagrams - INFO - Generated diagram: 945 characters
2025-12-15 12:08:49,682 - src.llm.request_handler - INFO - [dia:5d6d42] âœ“ Done 20.89s
2025-12-15 12:08:49,682 - src.llm.client - INFO - [dia:5d6d42] âœ… HTTP 200 in 20.89s
2025-12-15 12:08:49,682 - src.llm.client - INFO - [dia:5d6d42] ğŸ“¡ Stream active (200)
2025-12-15 12:08:49,682 - src.llm.client - INFO - [dia:5d6d42] Starting stream parsing, waiting for first chunk...
2025-12-15 12:08:51,705 - src.llm.client - INFO - [dia:5d6d42] ğŸ“Š 2.0s: 202c @100c/s (65ch, ~50t @25t/s)
2025-12-15 12:08:53,710 - src.llm.client - INFO - [dia:5d6d42] ğŸ“Š 4.0s: 399c @99c/s (131ch, ~100t @25t/s)
2025-12-15 12:08:55,716 - src.llm.client - INFO - [dia:5d6d42] ğŸ“Š 6.0s: 615c @102c/s (197ch, ~154t @25t/s)
2025-12-15 12:08:57,719 - src.llm.client - INFO - [dia:5d6d42] ğŸ“Š 8.0s: 768c @96c/s (263ch, ~192t @24t/s)
2025-12-15 12:08:59,099 - src.llm.client - INFO - [dia:5d6d42] âœ“ Done 30.31s: 874c (~119w @29c/s)
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Evidence Lower Bound (Variational Inference):
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO -     - Length: 587 chars (cleaned: 587 chars)
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO - [OK] Elements: 36 total (nodes: 15, connections: 21) âœ“
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:08:59,099 - src.generate.formats.diagrams - INFO - Generated diagram: 587 characters
2025-12-15 12:08:59,100 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:08:59,100 - src.generate.formats.questions - INFO - Generating 10 questions for: Variational Inference (Session 4)
2025-12-15 12:08:59,100 - src.llm.client - INFO - [qst:df73b0] ğŸš€ qst | m=gemma3:4b | p=7341c | t=150s
2025-12-15 12:08:59,100 - src.llm.client - INFO - [qst:df73b0] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:08:59,100 - src.llm.client - INFO - [qst:df73b0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:08:59,101 - src.llm.client - INFO - [qst:df73b0] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11066 bytes, prompt=7341 chars
2025-12-15 12:08:59,101 - src.llm.client - INFO - [qst:df73b0] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:09:03,288 - src.llm.request_handler - INFO - [qst:df73b0] âœ“ Done 4.19s
2025-12-15 12:09:03,289 - src.llm.client - INFO - [qst:df73b0] âœ… HTTP 200 in 4.19s
2025-12-15 12:09:03,289 - src.llm.client - INFO - [qst:df73b0] ğŸ“¡ Stream active (200)
2025-12-15 12:09:03,289 - src.llm.client - INFO - [qst:df73b0] Starting stream parsing, waiting for first chunk...
2025-12-15 12:09:05,301 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 2.0s: 290c @144c/s (66ch, ~72t @36t/s)
2025-12-15 12:09:07,317 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 4.0s: 644c @160c/s (132ch, ~161t @40t/s)
2025-12-15 12:09:09,326 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 6.0s: 993c @164c/s (198ch, ~248t @41t/s)
2025-12-15 12:09:11,335 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 8.0s: 1339c @166c/s (264ch, ~335t @42t/s)
2025-12-15 12:09:13,342 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 10.1s: 1635c @163c/s (330ch, ~409t @41t/s)
2025-12-15 12:09:15,356 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 12.1s: 1981c @164c/s (396ch, ~495t @41t/s)
2025-12-15 12:09:17,376 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 14.1s: 2318c @165c/s (462ch, ~580t @41t/s)
2025-12-15 12:09:19,401 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 16.1s: 2620c @163c/s (528ch, ~655t @41t/s)
2025-12-15 12:09:21,422 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 18.1s: 2975c @164c/s (594ch, ~744t @41t/s)
2025-12-15 12:09:23,442 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 20.2s: 3256c @162c/s (660ch, ~814t @40t/s)
2025-12-15 12:09:25,469 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 22.2s: 3587c @162c/s (726ch, ~897t @40t/s)
2025-12-15 12:09:27,473 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 24.2s: 3953c @163c/s (791ch, ~988t @41t/s)
2025-12-15 12:09:29,479 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 26.2s: 4259c @163c/s (856ch, ~1065t @41t/s)
2025-12-15 12:09:31,480 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 28.2s: 4556c @162c/s (921ch, ~1139t @40t/s)
2025-12-15 12:09:33,481 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 30.2s: 4879c @162c/s (986ch, ~1220t @40t/s)
2025-12-15 12:09:35,484 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 32.2s: 5196c @161c/s (1051ch, ~1299t @40t/s)
2025-12-15 12:09:37,500 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 34.2s: 5501c @161c/s (1116ch, ~1375t @40t/s)
2025-12-15 12:09:39,517 - src.llm.client - INFO - [qst:df73b0] ğŸ“Š 36.2s: 5802c @160c/s (1181ch, ~1450t @40t/s)
2025-12-15 12:09:40,268 - src.llm.client - INFO - [qst:df73b0] âœ“ Done 41.17s: 5911c (~870w @144c/s)
2025-12-15 12:09:40,269 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 0, 'total_fixes': 3}
2025-12-15 12:09:40,269 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-15 12:09:40,269 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 8 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:09:40,269 - src.generate.formats.questions - WARNING -     Context: Module 3 Session 4
2025-12-15 12:09:40,269 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:09:40,269 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:09:40,269 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:09:40,269 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Variational Inference (Session 4)
2025-12-15 12:09:40,269 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:09:40,272 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:09:40,274 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 4 completed
2025-12-15 12:09:40,274 - src.generate.orchestration.pipeline - INFO - 
[5/20] Session 5: Variational Free Energy
2025-12-15 12:09:40,274 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:09:40,274 - src.generate.formats.lectures - INFO - Generating lecture for: Variational Inference (Session 5/20)
2025-12-15 12:09:40,274 - src.llm.client - INFO - [lec:c44b36] ğŸš€ lec | m=gemma3:4b | p=3019c | t=180s
2025-12-15 12:09:40,274 - src.llm.client - INFO - [lec:c44b36] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:09:40,274 - src.llm.client - INFO - [lec:c44b36] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:09:40,275 - src.llm.client - INFO - [lec:c44b36] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6650 bytes, prompt=3019 chars
2025-12-15 12:09:40,275 - src.llm.client - INFO - [lec:c44b36] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:09:42,262 - src.llm.request_handler - INFO - [lec:c44b36] âœ“ Done 1.99s
2025-12-15 12:09:42,262 - src.llm.client - INFO - [lec:c44b36] âœ… HTTP 200 in 1.99s
2025-12-15 12:09:42,262 - src.llm.client - INFO - [lec:c44b36] ğŸ“¡ Stream active (200)
2025-12-15 12:09:42,262 - src.llm.client - INFO - [lec:c44b36] Starting stream parsing, waiting for first chunk...
2025-12-15 12:09:44,268 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 2.0s: 411c @205c/s (67ch, ~103t @51t/s)
2025-12-15 12:09:46,278 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 4.0s: 789c @197c/s (134ch, ~197t @49t/s)
2025-12-15 12:09:48,292 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 6.0s: 1100c @182c/s (201ch, ~275t @46t/s)
2025-12-15 12:09:50,319 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 8.1s: 1468c @182c/s (268ch, ~367t @46t/s)
2025-12-15 12:09:52,342 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 10.1s: 1648c @163c/s (334ch, ~412t @41t/s)
2025-12-15 12:09:54,369 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 12.1s: 1933c @160c/s (401ch, ~483t @40t/s)
2025-12-15 12:09:56,397 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 14.1s: 2224c @157c/s (468ch, ~556t @39t/s)
2025-12-15 12:09:58,426 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 16.2s: 2519c @156c/s (535ch, ~630t @39t/s)
2025-12-15 12:10:00,433 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 18.2s: 2817c @155c/s (601ch, ~704t @39t/s)
2025-12-15 12:10:02,437 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 20.2s: 3167c @157c/s (667ch, ~792t @39t/s)
2025-12-15 12:10:04,443 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 22.2s: 3548c @160c/s (733ch, ~887t @40t/s)
2025-12-15 12:10:06,452 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 24.2s: 3859c @160c/s (799ch, ~965t @40t/s)
2025-12-15 12:10:08,464 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 26.2s: 4086c @156c/s (865ch, ~1022t @39t/s)
2025-12-15 12:10:10,477 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 28.2s: 4344c @154c/s (931ch, ~1086t @38t/s)
2025-12-15 12:10:12,494 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 30.2s: 4689c @155c/s (997ch, ~1172t @39t/s)
2025-12-15 12:10:14,495 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 32.2s: 5011c @155c/s (1062ch, ~1253t @39t/s)
2025-12-15 12:10:16,497 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 34.2s: 5368c @157c/s (1127ch, ~1342t @39t/s)
2025-12-15 12:10:18,511 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 36.2s: 5676c @157c/s (1193ch, ~1419t @39t/s)
2025-12-15 12:10:20,532 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 38.3s: 6037c @158c/s (1259ch, ~1509t @39t/s)
2025-12-15 12:10:22,551 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 40.3s: 6390c @159c/s (1325ch, ~1598t @40t/s)
2025-12-15 12:10:24,572 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 42.3s: 6723c @159c/s (1391ch, ~1681t @40t/s)
2025-12-15 12:10:26,595 - src.llm.client - INFO - [lec:c44b36] ğŸ“Š 44.3s: 7128c @161c/s (1457ch, ~1782t @40t/s)
2025-12-15 12:10:27,086 - src.llm.client - INFO - [lec:c44b36] âœ“ Done 46.81s: 7221c (~1069w @154c/s)
2025-12-15 12:10:27,088 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:10:27,088 - src.generate.formats.lectures - INFO -     - Length: 7291 chars, 1079 words
2025-12-15 12:10:27,088 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:10:27,088 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:10:27,088 - src.generate.formats.lectures - INFO -     - Content: 10 examples, 0 terms defined
2025-12-15 12:10:27,088 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:10:27,092 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:10:27,092 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:10:27,092 - src.generate.formats.labs - INFO - Generating lab 5 for: Variational Inference (Session 5)
2025-12-15 12:10:27,092 - src.llm.client - INFO - [lab:02455f] ğŸš€ lab | m=gemma3:4b | p=3300c | t=150s
2025-12-15 12:10:27,092 - src.llm.client - INFO - [lab:02455f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:10:27,092 - src.llm.client - INFO - [lab:02455f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:10:27,093 - src.llm.client - INFO - [lab:02455f] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3797 bytes, prompt=3300 chars
2025-12-15 12:10:27,093 - src.llm.client - INFO - [lab:02455f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:10:28,953 - src.llm.request_handler - INFO - [lab:02455f] âœ“ Done 1.86s
2025-12-15 12:10:28,953 - src.llm.client - INFO - [lab:02455f] âœ… HTTP 200 in 1.86s
2025-12-15 12:10:28,953 - src.llm.client - INFO - [lab:02455f] ğŸ“¡ Stream active (200)
2025-12-15 12:10:28,953 - src.llm.client - INFO - [lab:02455f] Starting stream parsing, waiting for first chunk...
2025-12-15 12:10:30,961 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 2.0s: 364c @181c/s (67ch, ~91t @45t/s)
2025-12-15 12:10:32,962 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 4.0s: 715c @178c/s (134ch, ~179t @45t/s)
2025-12-15 12:10:34,970 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 6.0s: 1046c @174c/s (201ch, ~262t @43t/s)
2025-12-15 12:10:36,989 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 8.0s: 1333c @166c/s (268ch, ~333t @41t/s)
2025-12-15 12:10:38,992 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 10.0s: 1637c @163c/s (334ch, ~409t @41t/s)
2025-12-15 12:10:40,996 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 12.0s: 1896c @157c/s (400ch, ~474t @39t/s)
2025-12-15 12:10:42,996 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 14.0s: 2189c @156c/s (466ch, ~547t @39t/s)
2025-12-15 12:10:44,999 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 16.0s: 2529c @158c/s (532ch, ~632t @39t/s)
2025-12-15 12:10:47,004 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 18.1s: 2820c @156c/s (598ch, ~705t @39t/s)
2025-12-15 12:10:49,013 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 20.1s: 3026c @151c/s (664ch, ~756t @38t/s)
2025-12-15 12:10:51,027 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 22.1s: 3366c @152c/s (730ch, ~842t @38t/s)
2025-12-15 12:10:53,043 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 24.1s: 3645c @151c/s (795ch, ~911t @38t/s)
2025-12-15 12:10:55,049 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 26.1s: 3960c @152c/s (861ch, ~990t @38t/s)
2025-12-15 12:10:57,056 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 28.1s: 4185c @149c/s (927ch, ~1046t @37t/s)
2025-12-15 12:10:59,068 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 30.1s: 4476c @149c/s (993ch, ~1119t @37t/s)
2025-12-15 12:11:01,081 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 32.1s: 4773c @149c/s (1059ch, ~1193t @37t/s)
2025-12-15 12:11:03,091 - src.llm.client - INFO - [lab:02455f] ğŸ“Š 34.1s: 5089c @149c/s (1125ch, ~1272t @37t/s)
2025-12-15 12:11:04,816 - src.llm.client - INFO - [lab:02455f] âœ“ Done 37.72s: 5382c (~784w @143c/s)
2025-12-15 12:11:04,817 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:11:04,817 - src.generate.formats.labs - INFO -     - Length: 5467 chars, 797 words
2025-12-15 12:11:04,817 - src.generate.formats.labs - INFO -     - Procedure: 15 steps
2025-12-15 12:11:04,817 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 12:11:04,817 - src.generate.formats.labs - INFO -     - Data tables: 5
2025-12-15 12:11:04,820 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:11:04,820 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:11:04,820 - src.generate.formats.study_notes - INFO - Generating study notes for: Variational Inference (Session 5)
2025-12-15 12:11:04,821 - src.llm.client - INFO - [stu:2cde29] ğŸš€ stu | m=gemma3:4b | p=4415c | t=120s
2025-12-15 12:11:04,821 - src.llm.client - INFO - [stu:2cde29] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:11:04,821 - src.llm.client - INFO - [stu:2cde29] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:11:04,822 - src.llm.client - INFO - [stu:2cde29] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8127 bytes, prompt=4415 chars
2025-12-15 12:11:04,822 - src.llm.client - INFO - [stu:2cde29] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:11:07,816 - src.llm.request_handler - INFO - [stu:2cde29] âœ“ Done 2.99s
2025-12-15 12:11:07,817 - src.llm.client - INFO - [stu:2cde29] âœ… HTTP 200 in 2.99s
2025-12-15 12:11:07,817 - src.llm.client - INFO - [stu:2cde29] ğŸ“¡ Stream active (200)
2025-12-15 12:11:07,817 - src.llm.client - INFO - [stu:2cde29] Starting stream parsing, waiting for first chunk...
2025-12-15 12:11:09,818 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 2.0s: 393c @196c/s (66ch, ~98t @49t/s)
2025-12-15 12:11:11,846 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 4.0s: 724c @180c/s (133ch, ~181t @45t/s)
2025-12-15 12:11:13,846 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 6.0s: 1006c @167c/s (199ch, ~252t @42t/s)
2025-12-15 12:11:15,854 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 8.0s: 1355c @169c/s (265ch, ~339t @42t/s)
2025-12-15 12:11:17,866 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 10.0s: 1764c @176c/s (331ch, ~441t @44t/s)
2025-12-15 12:11:19,880 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 12.1s: 2170c @180c/s (397ch, ~542t @45t/s)
2025-12-15 12:11:21,893 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 14.1s: 2477c @176c/s (463ch, ~619t @44t/s)
2025-12-15 12:11:23,904 - src.llm.client - INFO - [stu:2cde29] ğŸ“Š 16.1s: 2723c @169c/s (529ch, ~681t @42t/s)
2025-12-15 12:11:25,552 - src.llm.client - INFO - [stu:2cde29] âœ“ Done 20.73s: 3020c (~411w @146c/s)
2025-12-15 12:11:25,553 - src.generate.formats.study_notes - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:11:25,553 - src.generate.formats.study_notes - WARNING -     [CRITICAL] Issue 1: Only 1 key concepts highlighted (require 3-10, need 2 more - format concepts as **Concept Name:** in bullet points)
2025-12-15 12:11:25,553 - src.generate.formats.study_notes - WARNING -   Retry attempt 1/1 for study notes: Variational Inference (Session 5)
2025-12-15 12:11:25,553 - src.generate.formats.study_notes - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:11:25,554 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:11:25,555 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:11:25,555 - src.generate.formats.diagrams - INFO - Generating diagram for: ELF Formulation (Variational Inference)
2025-12-15 12:11:25,555 - src.generate.formats.diagrams - INFO - Generating diagram for: Optimization (Variational Inference)
2025-12-15 12:11:25,555 - src.llm.client - INFO - [dia:667b31] ğŸš€ dia | m=gemma3:4b | p=5741c | t=120s
2025-12-15 12:11:25,555 - src.llm.client - INFO - [dia:52a04f] ğŸš€ dia | m=gemma3:4b | p=5735c | t=120s
2025-12-15 12:11:25,555 - src.llm.client - INFO - [dia:667b31] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:11:25,555 - src.llm.client - INFO - [dia:52a04f] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:11:25,555 - src.llm.client - INFO - [dia:52a04f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:11:25,555 - src.llm.client - INFO - [dia:667b31] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:11:25,557 - src.llm.client - INFO - [dia:667b31] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11050 bytes, prompt=5741 chars
2025-12-15 12:11:25,557 - src.llm.client - INFO - [dia:52a04f] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11044 bytes, prompt=5735 chars
2025-12-15 12:11:25,557 - src.llm.client - INFO - [dia:667b31] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:11:25,557 - src.llm.client - INFO - [dia:52a04f] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:11:29,181 - src.llm.request_handler - INFO - [dia:52a04f] âœ“ Done 3.62s
2025-12-15 12:11:29,181 - src.llm.client - INFO - [dia:52a04f] âœ… HTTP 200 in 3.62s
2025-12-15 12:11:29,181 - src.llm.client - INFO - [dia:52a04f] ğŸ“¡ Stream active (200)
2025-12-15 12:11:29,181 - src.llm.client - INFO - [dia:52a04f] Starting stream parsing, waiting for first chunk...
2025-12-15 12:11:31,197 - src.llm.client - INFO - [dia:52a04f] ğŸ“Š 2.0s: 217c @108c/s (66ch, ~54t @27t/s)
2025-12-15 12:11:33,206 - src.llm.client - INFO - [dia:52a04f] ğŸ“Š 4.0s: 406c @101c/s (132ch, ~102t @25t/s)
2025-12-15 12:11:35,212 - src.llm.client - INFO - [dia:52a04f] ğŸ“Š 6.0s: 580c @96c/s (198ch, ~145t @24t/s)
2025-12-15 12:11:37,224 - src.llm.client - INFO - [dia:52a04f] ğŸ“Š 8.0s: 741c @92c/s (264ch, ~185t @23t/s)
2025-12-15 12:11:37,637 - src.llm.client - INFO - [dia:52a04f] âœ“ Done 12.08s: 769c (~110w @64c/s)
2025-12-15 12:11:37,637 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Optimization (Variational Inference):
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO -     - Length: 516 chars (cleaned: 516 chars)
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO - [OK] Elements: 39 total (nodes: 14, connections: 25) âœ“
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:11:37,638 - src.generate.formats.diagrams - INFO - Generated diagram: 516 characters
2025-12-15 12:11:40,989 - src.llm.request_handler - INFO - [dia:667b31] âœ“ Done 15.43s
2025-12-15 12:11:40,989 - src.llm.client - INFO - [dia:667b31] âœ… HTTP 200 in 15.43s
2025-12-15 12:11:40,989 - src.llm.client - INFO - [dia:667b31] ğŸ“¡ Stream active (200)
2025-12-15 12:11:40,989 - src.llm.client - INFO - [dia:667b31] Starting stream parsing, waiting for first chunk...
2025-12-15 12:11:43,000 - src.llm.client - INFO - [dia:667b31] ğŸ“Š 2.0s: 218c @108c/s (66ch, ~54t @27t/s)
2025-12-15 12:11:45,005 - src.llm.client - INFO - [dia:667b31] ğŸ“Š 4.0s: 420c @105c/s (132ch, ~105t @26t/s)
2025-12-15 12:11:47,009 - src.llm.client - INFO - [dia:667b31] ğŸ“Š 6.0s: 629c @104c/s (198ch, ~157t @26t/s)
2025-12-15 12:11:49,013 - src.llm.client - INFO - [dia:667b31] ğŸ“Š 8.0s: 813c @101c/s (264ch, ~203t @25t/s)
2025-12-15 12:11:49,362 - src.llm.client - INFO - [dia:667b31] âœ“ Done 23.81s: 831c (~126w @35c/s)
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for ELF Formulation (Variational Inference):
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO -     - Length: 722 chars (cleaned: 722 chars)
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO - [OK] Elements: 47 total (nodes: 15, connections: 32) âœ“
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:11:49,362 - src.generate.formats.diagrams - INFO - Generated diagram: 722 characters
2025-12-15 12:11:49,363 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:11:49,363 - src.generate.formats.questions - INFO - Generating 10 questions for: Variational Inference (Session 5)
2025-12-15 12:11:49,363 - src.llm.client - INFO - [qst:fba52b] ğŸš€ qst | m=gemma3:4b | p=7304c | t=150s
2025-12-15 12:11:49,363 - src.llm.client - INFO - [qst:fba52b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:11:49,363 - src.llm.client - INFO - [qst:fba52b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:11:49,364 - src.llm.client - INFO - [qst:fba52b] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11081 bytes, prompt=7304 chars
2025-12-15 12:11:49,364 - src.llm.client - INFO - [qst:fba52b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:11:53,578 - src.llm.request_handler - INFO - [qst:fba52b] âœ“ Done 4.21s
2025-12-15 12:11:53,579 - src.llm.client - INFO - [qst:fba52b] âœ… HTTP 200 in 4.21s
2025-12-15 12:11:53,579 - src.llm.client - INFO - [qst:fba52b] ğŸ“¡ Stream active (200)
2025-12-15 12:11:53,579 - src.llm.client - INFO - [qst:fba52b] Starting stream parsing, waiting for first chunk...
2025-12-15 12:11:55,592 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 2.0s: 303c @151c/s (66ch, ~76t @38t/s)
2025-12-15 12:11:57,606 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 4.0s: 633c @157c/s (132ch, ~158t @39t/s)
2025-12-15 12:11:59,619 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 6.0s: 996c @165c/s (198ch, ~249t @41t/s)
2025-12-15 12:12:01,628 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 8.0s: 1288c @160c/s (264ch, ~322t @40t/s)
2025-12-15 12:12:03,635 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 10.1s: 1623c @161c/s (330ch, ~406t @40t/s)
2025-12-15 12:12:05,650 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 12.1s: 1960c @162c/s (396ch, ~490t @41t/s)
2025-12-15 12:12:07,673 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 14.1s: 2300c @163c/s (462ch, ~575t @41t/s)
2025-12-15 12:12:09,692 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 16.1s: 2670c @166c/s (528ch, ~668t @41t/s)
2025-12-15 12:12:11,710 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 18.1s: 3069c @169c/s (594ch, ~767t @42t/s)
2025-12-15 12:12:13,730 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 20.2s: 3415c @169c/s (660ch, ~854t @42t/s)
2025-12-15 12:12:15,755 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 22.2s: 3758c @169c/s (726ch, ~940t @42t/s)
2025-12-15 12:12:17,780 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 24.2s: 4137c @171c/s (792ch, ~1034t @43t/s)
2025-12-15 12:12:19,793 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 26.2s: 4508c @172c/s (857ch, ~1127t @43t/s)
2025-12-15 12:12:21,795 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 28.2s: 4899c @174c/s (922ch, ~1225t @43t/s)
2025-12-15 12:12:23,798 - src.llm.client - INFO - [qst:fba52b] ğŸ“Š 30.2s: 5264c @174c/s (987ch, ~1316t @44t/s)
2025-12-15 12:12:24,431 - src.llm.client - INFO - [qst:fba52b] âœ“ Done 35.07s: 5380c (~733w @153c/s)
2025-12-15 12:12:24,431 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 4 question format issues: {'format_standardized': 0, 'question_marks_added': 4, 'mc_options_fixed': 0, 'total_fixes': 4}
2025-12-15 12:12:24,431 - src.generate.formats.questions - INFO - Applied 4 auto-fixes to questions
2025-12-15 12:12:24,432 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 2 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:12:24,432 - src.generate.formats.questions - WARNING -     Context: Module 3 Session 5
2025-12-15 12:12:24,432 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:12:24,432 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:12:24,432 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:12:24,432 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Variational Inference (Session 5)
2025-12-15 12:12:24,432 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:12:24,434 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:12:24,436 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 5 completed
2025-12-15 12:12:24,436 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:12:24,436 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:12:24,436 - src.generate.orchestration.pipeline - INFO - Module 4: Hierarchical Generative Models (2 sessions)
2025-12-15 12:12:24,436 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:12:24,436 - src.generate.orchestration.pipeline - INFO - 
[6/20] Session 6: Recurrent Predictive Models
2025-12-15 12:12:24,437 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:12:24,437 - src.generate.formats.lectures - INFO - Generating lecture for: Hierarchical Generative Models (Session 6/20)
2025-12-15 12:12:24,437 - src.llm.client - INFO - [lec:b5b194] ğŸš€ lec | m=gemma3:4b | p=3083c | t=180s
2025-12-15 12:12:24,437 - src.llm.client - INFO - [lec:b5b194] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:12:24,437 - src.llm.client - INFO - [lec:b5b194] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:12:24,438 - src.llm.client - INFO - [lec:b5b194] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6714 bytes, prompt=3083 chars
2025-12-15 12:12:24,438 - src.llm.client - INFO - [lec:b5b194] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:12:27,126 - src.llm.request_handler - INFO - [lec:b5b194] âœ“ Done 2.69s
2025-12-15 12:12:27,126 - src.llm.client - INFO - [lec:b5b194] âœ… HTTP 200 in 2.69s
2025-12-15 12:12:27,127 - src.llm.client - INFO - [lec:b5b194] ğŸ“¡ Stream active (200)
2025-12-15 12:12:27,127 - src.llm.client - INFO - [lec:b5b194] Starting stream parsing, waiting for first chunk...
2025-12-15 12:12:29,135 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 2.0s: 404c @201c/s (67ch, ~101t @50t/s)
2025-12-15 12:12:31,149 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 4.0s: 847c @211c/s (134ch, ~212t @53t/s)
2025-12-15 12:12:33,166 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 6.0s: 1169c @194c/s (201ch, ~292t @48t/s)
2025-12-15 12:12:35,166 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 8.0s: 1524c @190c/s (267ch, ~381t @47t/s)
2025-12-15 12:12:37,172 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 10.0s: 1912c @190c/s (332ch, ~478t @48t/s)
2025-12-15 12:12:39,200 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 12.1s: 2294c @190c/s (399ch, ~574t @48t/s)
2025-12-15 12:12:41,202 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 14.1s: 2649c @188c/s (465ch, ~662t @47t/s)
2025-12-15 12:12:43,203 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 16.1s: 3021c @188c/s (531ch, ~755t @47t/s)
2025-12-15 12:12:45,208 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 18.1s: 3416c @189c/s (597ch, ~854t @47t/s)
2025-12-15 12:12:47,216 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 20.1s: 3762c @187c/s (663ch, ~940t @47t/s)
2025-12-15 12:12:49,223 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 22.1s: 4143c @188c/s (729ch, ~1036t @47t/s)
2025-12-15 12:12:51,238 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 24.1s: 4527c @188c/s (795ch, ~1132t @47t/s)
2025-12-15 12:12:53,245 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 26.1s: 4886c @187c/s (860ch, ~1222t @47t/s)
2025-12-15 12:12:55,253 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 28.1s: 5042c @179c/s (926ch, ~1260t @45t/s)
2025-12-15 12:12:57,263 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 30.1s: 5348c @177c/s (992ch, ~1337t @44t/s)
2025-12-15 12:12:59,278 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 32.2s: 5725c @178c/s (1058ch, ~1431t @45t/s)
2025-12-15 12:13:01,292 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 34.2s: 6079c @178c/s (1124ch, ~1520t @44t/s)
2025-12-15 12:13:03,303 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 36.2s: 6426c @178c/s (1190ch, ~1606t @44t/s)
2025-12-15 12:13:05,309 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 38.2s: 6747c @177c/s (1255ch, ~1687t @44t/s)
2025-12-15 12:13:07,334 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 40.2s: 7133c @177c/s (1321ch, ~1783t @44t/s)
2025-12-15 12:13:09,357 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 42.2s: 7547c @179c/s (1387ch, ~1887t @45t/s)
2025-12-15 12:13:11,381 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 44.3s: 7863c @178c/s (1453ch, ~1966t @44t/s)
2025-12-15 12:13:13,406 - src.llm.client - INFO - [lec:b5b194] ğŸ“Š 46.3s: 8307c @179c/s (1519ch, ~2077t @45t/s)
2025-12-15 12:13:14,086 - src.llm.client - INFO - [lec:b5b194] âœ“ Done 49.65s: 8428c (~1222w @170c/s)
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO -     - Length: 8529 chars, 1234 words
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO -     - Content: 14 examples, 3 terms defined
2025-12-15 12:13:14,087 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 12:13:14,087 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-15 12:13:14,091 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:13:14,091 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:13:14,091 - src.generate.formats.labs - INFO - Generating lab 6 for: Hierarchical Generative Models (Session 6)
2025-12-15 12:13:14,092 - src.llm.client - INFO - [lab:42f4cc] ğŸš€ lab | m=gemma3:4b | p=3342c | t=150s
2025-12-15 12:13:14,092 - src.llm.client - INFO - [lab:42f4cc] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:13:14,092 - src.llm.client - INFO - [lab:42f4cc] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:13:14,093 - src.llm.client - INFO - [lab:42f4cc] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3773 bytes, prompt=3342 chars
2025-12-15 12:13:14,093 - src.llm.client - INFO - [lab:42f4cc] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:13:15,828 - src.llm.request_handler - INFO - [lab:42f4cc] âœ“ Done 1.74s
2025-12-15 12:13:15,829 - src.llm.client - INFO - [lab:42f4cc] âœ… HTTP 200 in 1.74s
2025-12-15 12:13:15,829 - src.llm.client - INFO - [lab:42f4cc] ğŸ“¡ Stream active (200)
2025-12-15 12:13:15,829 - src.llm.client - INFO - [lab:42f4cc] Starting stream parsing, waiting for first chunk...
2025-12-15 12:13:17,850 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 2.0s: 343c @170c/s (68ch, ~86t @42t/s)
2025-12-15 12:13:19,876 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 4.0s: 760c @188c/s (136ch, ~190t @47t/s)
2025-12-15 12:13:21,884 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 6.1s: 1093c @181c/s (203ch, ~273t @45t/s)
2025-12-15 12:13:23,895 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 8.1s: 1434c @178c/s (270ch, ~358t @44t/s)
2025-12-15 12:13:25,917 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 10.1s: 1719c @170c/s (337ch, ~430t @43t/s)
2025-12-15 12:13:27,945 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 12.1s: 1934c @160c/s (404ch, ~484t @40t/s)
2025-12-15 12:13:29,970 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 14.1s: 2266c @160c/s (471ch, ~566t @40t/s)
2025-12-15 12:13:31,999 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 16.2s: 2615c @162c/s (538ch, ~654t @40t/s)
2025-12-15 12:13:33,999 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 18.2s: 2927c @161c/s (604ch, ~732t @40t/s)
2025-12-15 12:13:36,008 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 20.2s: 3231c @160c/s (670ch, ~808t @40t/s)
2025-12-15 12:13:38,017 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 22.2s: 3540c @160c/s (736ch, ~885t @40t/s)
2025-12-15 12:13:40,021 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 24.2s: 3770c @156c/s (802ch, ~942t @39t/s)
2025-12-15 12:13:42,026 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 26.2s: 4130c @158c/s (868ch, ~1032t @39t/s)
2025-12-15 12:13:44,033 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 28.2s: 4478c @159c/s (934ch, ~1120t @40t/s)
2025-12-15 12:13:46,048 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 30.2s: 4795c @159c/s (1000ch, ~1199t @40t/s)
2025-12-15 12:13:48,057 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 32.2s: 5098c @158c/s (1066ch, ~1274t @40t/s)
2025-12-15 12:13:50,070 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 34.2s: 5444c @159c/s (1132ch, ~1361t @40t/s)
2025-12-15 12:13:52,085 - src.llm.client - INFO - [lab:42f4cc] ğŸ“Š 36.3s: 5812c @160c/s (1197ch, ~1453t @40t/s)
2025-12-15 12:13:53,103 - src.llm.client - INFO - [lab:42f4cc] âœ“ Done 39.01s: 5942c (~782w @152c/s)
2025-12-15 12:13:53,104 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:13:53,104 - src.generate.formats.labs - INFO -     - Length: 6037 chars, 796 words
2025-12-15 12:13:53,104 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-15 12:13:53,104 - src.generate.formats.labs - INFO -     - Safety: 10 warnings
2025-12-15 12:13:53,104 - src.generate.formats.labs - INFO -     - Data tables: 9
2025-12-15 12:13:53,106 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:13:53,107 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:13:53,107 - src.generate.formats.study_notes - INFO - Generating study notes for: Hierarchical Generative Models (Session 6)
2025-12-15 12:13:53,107 - src.llm.client - INFO - [stu:cb9787] ğŸš€ stu | m=gemma3:4b | p=4459c | t=120s
2025-12-15 12:13:53,107 - src.llm.client - INFO - [stu:cb9787] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:13:53,107 - src.llm.client - INFO - [stu:cb9787] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:13:53,108 - src.llm.client - INFO - [stu:cb9787] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8105 bytes, prompt=4459 chars
2025-12-15 12:13:53,108 - src.llm.client - INFO - [stu:cb9787] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:13:55,956 - src.llm.request_handler - INFO - [stu:cb9787] âœ“ Done 2.85s
2025-12-15 12:13:55,956 - src.llm.client - INFO - [stu:cb9787] âœ… HTTP 200 in 2.85s
2025-12-15 12:13:55,956 - src.llm.client - INFO - [stu:cb9787] ğŸ“¡ Stream active (200)
2025-12-15 12:13:55,956 - src.llm.client - INFO - [stu:cb9787] Starting stream parsing, waiting for first chunk...
2025-12-15 12:13:57,975 - src.llm.client - INFO - [stu:cb9787] ğŸ“Š 2.0s: 353c @175c/s (66ch, ~88t @44t/s)
2025-12-15 12:14:00,001 - src.llm.client - INFO - [stu:cb9787] ğŸ“Š 4.0s: 732c @181c/s (132ch, ~183t @45t/s)
2025-12-15 12:14:02,020 - src.llm.client - INFO - [stu:cb9787] ğŸ“Š 6.1s: 1090c @180c/s (198ch, ~272t @45t/s)
2025-12-15 12:14:04,020 - src.llm.client - INFO - [stu:cb9787] ğŸ“Š 8.1s: 1447c @179c/s (264ch, ~362t @45t/s)
2025-12-15 12:14:06,025 - src.llm.client - INFO - [stu:cb9787] ğŸ“Š 10.1s: 1829c @182c/s (330ch, ~457t @45t/s)
2025-12-15 12:14:08,031 - src.llm.client - INFO - [stu:cb9787] ğŸ“Š 12.1s: 2209c @183c/s (396ch, ~552t @46t/s)
2025-12-15 12:14:10,034 - src.llm.client - INFO - [stu:cb9787] ğŸ“Š 14.1s: 2575c @183c/s (462ch, ~644t @46t/s)
2025-12-15 12:14:11,652 - src.llm.client - INFO - [stu:cb9787] âœ“ Done 18.55s: 2811c (~388w @152c/s)
2025-12-15 12:14:11,652 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:14:11,652 - src.generate.formats.study_notes - INFO -     - Length: 2876 chars, 398 words
2025-12-15 12:14:11,652 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:14:11,652 - src.generate.formats.study_notes - INFO -     - Key concepts: 7
2025-12-15 12:14:11,652 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 6 bullets
2025-12-15 12:14:11,652 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:14:11,654 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:14:11,655 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:14:11,655 - src.generate.formats.diagrams - INFO - Generating diagram for: Recurrent Connections (Hierarchical Generative Models)
2025-12-15 12:14:11,655 - src.generate.formats.diagrams - INFO - Generating diagram for: Prediction Error (Hierarchical Generative Models)
2025-12-15 12:14:11,656 - src.llm.client - INFO - [dia:753363] ğŸš€ dia | m=gemma3:4b | p=5756c | t=120s
2025-12-15 12:14:11,656 - src.llm.client - INFO - [dia:753363] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:14:11,656 - src.llm.client - INFO - [dia:753363] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:14:11,656 - src.llm.client - INFO - [dia:7bde3a] ğŸš€ dia | m=gemma3:4b | p=5766c | t=120s
2025-12-15 12:14:11,656 - src.llm.client - INFO - [dia:7bde3a] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:14:11,656 - src.llm.client - INFO - [dia:7bde3a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:14:11,658 - src.llm.client - INFO - [dia:753363] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11065 bytes, prompt=5756 chars
2025-12-15 12:14:11,658 - src.llm.client - INFO - [dia:7bde3a] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11075 bytes, prompt=5766 chars
2025-12-15 12:14:11,658 - src.llm.client - INFO - [dia:7bde3a] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:14:11,658 - src.llm.client - INFO - [dia:753363] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:14:15,291 - src.llm.request_handler - INFO - [dia:753363] âœ“ Done 3.63s
2025-12-15 12:14:15,291 - src.llm.client - INFO - [dia:753363] âœ… HTTP 200 in 3.63s
2025-12-15 12:14:15,291 - src.llm.client - INFO - [dia:753363] ğŸ“¡ Stream active (200)
2025-12-15 12:14:15,291 - src.llm.client - INFO - [dia:753363] Starting stream parsing, waiting for first chunk...
2025-12-15 12:14:17,309 - src.llm.client - INFO - [dia:753363] ğŸ“Š 2.0s: 226c @112c/s (66ch, ~56t @28t/s)
2025-12-15 12:14:19,321 - src.llm.client - INFO - [dia:753363] ğŸ“Š 4.0s: 434c @108c/s (132ch, ~108t @27t/s)
2025-12-15 12:14:21,329 - src.llm.client - INFO - [dia:753363] ğŸ“Š 6.0s: 631c @104c/s (198ch, ~158t @26t/s)
2025-12-15 12:14:23,337 - src.llm.client - INFO - [dia:753363] ğŸ“Š 8.0s: 813c @101c/s (264ch, ~203t @25t/s)
2025-12-15 12:14:25,344 - src.llm.client - INFO - [dia:753363] ğŸ“Š 10.1s: 972c @97c/s (330ch, ~243t @24t/s)
2025-12-15 12:14:27,069 - src.llm.client - INFO - [dia:753363] âœ“ Done 15.41s: 1102c (~147w @71c/s)
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Prediction Error (Hierarchical Generative Models):
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO -     - Length: 747 chars (cleaned: 747 chars)
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO - [OK] Elements: 52 total (nodes: 19, connections: 33) âœ“
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:14:27,069 - src.generate.formats.diagrams - INFO - Generated diagram: 747 characters
2025-12-15 12:14:30,428 - src.llm.request_handler - INFO - [dia:7bde3a] âœ“ Done 18.77s
2025-12-15 12:14:30,429 - src.llm.client - INFO - [dia:7bde3a] âœ… HTTP 200 in 18.77s
2025-12-15 12:14:30,429 - src.llm.client - INFO - [dia:7bde3a] ğŸ“¡ Stream active (200)
2025-12-15 12:14:30,429 - src.llm.client - INFO - [dia:7bde3a] Starting stream parsing, waiting for first chunk...
2025-12-15 12:14:32,442 - src.llm.client - INFO - [dia:7bde3a] ğŸ“Š 2.0s: 227c @113c/s (66ch, ~57t @28t/s)
2025-12-15 12:14:34,448 - src.llm.client - INFO - [dia:7bde3a] ğŸ“Š 4.0s: 439c @109c/s (132ch, ~110t @27t/s)
2025-12-15 12:14:36,456 - src.llm.client - INFO - [dia:7bde3a] ğŸ“Š 6.0s: 623c @103c/s (198ch, ~156t @26t/s)
2025-12-15 12:14:38,270 - src.llm.client - INFO - [dia:7bde3a] âœ“ Done 26.61s: 751c (~104w @28c/s)
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Recurrent Connections (Hierarchical Generative Models):
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO -     - Length: 554 chars (cleaned: 554 chars)
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO - [OK] Elements: 39 total (nodes: 14, connections: 25) âœ“
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:14:38,271 - src.generate.formats.diagrams - INFO - Generated diagram: 554 characters
2025-12-15 12:14:38,272 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:14:38,272 - src.generate.formats.questions - INFO - Generating 10 questions for: Hierarchical Generative Models (Session 6)
2025-12-15 12:14:38,272 - src.llm.client - INFO - [qst:9a2c3e] ğŸš€ qst | m=gemma3:4b | p=7345c | t=150s
2025-12-15 12:14:38,272 - src.llm.client - INFO - [qst:9a2c3e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:14:38,272 - src.llm.client - INFO - [qst:9a2c3e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:14:38,273 - src.llm.client - INFO - [qst:9a2c3e] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11019 bytes, prompt=7345 chars
2025-12-15 12:14:38,273 - src.llm.client - INFO - [qst:9a2c3e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:14:42,290 - src.llm.request_handler - INFO - [qst:9a2c3e] âœ“ Done 4.02s
2025-12-15 12:14:42,290 - src.llm.client - INFO - [qst:9a2c3e] âœ… HTTP 200 in 4.02s
2025-12-15 12:14:42,290 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“¡ Stream active (200)
2025-12-15 12:14:42,290 - src.llm.client - INFO - [qst:9a2c3e] Starting stream parsing, waiting for first chunk...
2025-12-15 12:14:44,309 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 2.0s: 336c @166c/s (66ch, ~84t @42t/s)
2025-12-15 12:14:46,318 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 4.0s: 687c @171c/s (132ch, ~172t @43t/s)
2025-12-15 12:14:48,327 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 6.0s: 1008c @167c/s (198ch, ~252t @42t/s)
2025-12-15 12:14:50,335 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 8.0s: 1356c @169c/s (264ch, ~339t @42t/s)
2025-12-15 12:14:52,338 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 10.0s: 1673c @167c/s (328ch, ~418t @42t/s)
2025-12-15 12:14:54,345 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 12.1s: 2006c @166c/s (394ch, ~502t @42t/s)
2025-12-15 12:14:56,359 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 14.1s: 2346c @167c/s (460ch, ~586t @42t/s)
2025-12-15 12:14:58,373 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 16.1s: 2696c @168c/s (526ch, ~674t @42t/s)
2025-12-15 12:15:00,390 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 18.1s: 3046c @168c/s (592ch, ~762t @42t/s)
2025-12-15 12:15:02,409 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 20.1s: 3452c @172c/s (658ch, ~863t @43t/s)
2025-12-15 12:15:04,432 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 22.1s: 3799c @172c/s (724ch, ~950t @43t/s)
2025-12-15 12:15:06,454 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 24.2s: 4175c @173c/s (790ch, ~1044t @43t/s)
2025-12-15 12:15:08,485 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 26.2s: 4544c @173c/s (856ch, ~1136t @43t/s)
2025-12-15 12:15:10,485 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 28.2s: 4892c @174c/s (921ch, ~1223t @43t/s)
2025-12-15 12:15:12,489 - src.llm.client - INFO - [qst:9a2c3e] ğŸ“Š 30.2s: 5199c @172c/s (986ch, ~1300t @43t/s)
2025-12-15 12:15:13,554 - src.llm.client - INFO - [qst:9a2c3e] âœ“ Done 35.28s: 5389c (~759w @153c/s)
2025-12-15 12:15:13,555 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 0, 'total_fixes': 1}
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO - [NEEDS REVIEW] Questions generated âš ï¸
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO -     - Multiple choice: 6 (valid structure: 6, with 4 options: 6)
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 6
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO - [WARNING] Question marks: 16 total, 10 questions with '?' âš ï¸
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO -     - Question length: avg 14.9 words (range: 12-18)
2025-12-15 12:15:13,555 - src.generate.formats.questions - INFO -     - MC explanations: 6/6 have proper length (20-50 words)
2025-12-15 12:15:13,555 - src.generate.formats.questions - WARNING - [WARNING] Question mark ratio: 1.6 question marks per question (may indicate multiple questions combined or excessive punctuation) âš ï¸
2025-12-15 12:15:13,558 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:15:13,560 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 6 completed
2025-12-15 12:15:13,560 - src.generate.orchestration.pipeline - INFO - 
[7/20] Session 7: Deep Predictive Processing
2025-12-15 12:15:13,560 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:15:13,560 - src.generate.formats.lectures - INFO - Generating lecture for: Hierarchical Generative Models (Session 7/20)
2025-12-15 12:15:13,560 - src.llm.client - INFO - [lec:6f6efa] ğŸš€ lec | m=gemma3:4b | p=3058c | t=180s
2025-12-15 12:15:13,560 - src.llm.client - INFO - [lec:6f6efa] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:15:13,560 - src.llm.client - INFO - [lec:6f6efa] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:15:13,562 - src.llm.client - INFO - [lec:6f6efa] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6689 bytes, prompt=3058 chars
2025-12-15 12:15:13,562 - src.llm.client - INFO - [lec:6f6efa] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:15:21,847 - src.llm.request_handler - INFO - [lec:6f6efa] âœ“ Done 8.29s
2025-12-15 12:15:21,847 - src.llm.client - INFO - [lec:6f6efa] âœ… HTTP 200 in 8.29s
2025-12-15 12:15:21,847 - src.llm.client - INFO - [lec:6f6efa] ğŸ“¡ Stream active (200)
2025-12-15 12:15:21,847 - src.llm.client - INFO - [lec:6f6efa] Starting stream parsing, waiting for first chunk...
2025-12-15 12:15:23,857 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 2.0s: 395c @197c/s (67ch, ~99t @49t/s)
2025-12-15 12:15:25,867 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 4.0s: 770c @192c/s (134ch, ~192t @48t/s)
2025-12-15 12:15:27,881 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 6.0s: 1157c @192c/s (201ch, ~289t @48t/s)
2025-12-15 12:15:29,907 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 8.1s: 1530c @190c/s (268ch, ~382t @47t/s)
2025-12-15 12:15:31,938 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 10.1s: 1873c @186c/s (335ch, ~468t @46t/s)
2025-12-15 12:15:33,945 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 12.1s: 2227c @184c/s (401ch, ~557t @46t/s)
2025-12-15 12:15:35,975 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 14.1s: 2601c @184c/s (468ch, ~650t @46t/s)
2025-12-15 12:15:37,980 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 16.1s: 2965c @184c/s (534ch, ~741t @46t/s)
2025-12-15 12:15:39,981 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 18.1s: 3340c @184c/s (600ch, ~835t @46t/s)
2025-12-15 12:15:41,985 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 20.1s: 3652c @181c/s (666ch, ~913t @45t/s)
2025-12-15 12:15:43,988 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 22.1s: 4023c @182c/s (732ch, ~1006t @45t/s)
2025-12-15 12:15:45,996 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 24.1s: 4379c @181c/s (798ch, ~1095t @45t/s)
2025-12-15 12:15:48,003 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 26.2s: 4689c @179c/s (864ch, ~1172t @45t/s)
2025-12-15 12:15:50,016 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 28.2s: 5021c @178c/s (930ch, ~1255t @45t/s)
2025-12-15 12:15:52,030 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 30.2s: 5394c @179c/s (995ch, ~1348t @45t/s)
2025-12-15 12:15:54,042 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 32.2s: 5770c @179c/s (1061ch, ~1442t @45t/s)
2025-12-15 12:15:56,055 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 34.2s: 6139c @179c/s (1127ch, ~1535t @45t/s)
2025-12-15 12:15:58,064 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 36.2s: 6502c @180c/s (1193ch, ~1626t @45t/s)
2025-12-15 12:16:00,077 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 38.2s: 6837c @179c/s (1259ch, ~1709t @45t/s)
2025-12-15 12:16:02,100 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 40.3s: 7257c @180c/s (1325ch, ~1814t @45t/s)
2025-12-15 12:16:04,121 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 42.3s: 7620c @180c/s (1391ch, ~1905t @45t/s)
2025-12-15 12:16:06,146 - src.llm.client - INFO - [lec:6f6efa] ğŸ“Š 44.3s: 8014c @181c/s (1457ch, ~2004t @45t/s)
2025-12-15 12:16:07,802 - src.llm.client - INFO - [lec:6f6efa] âœ“ Done 54.24s: 8303c (~1215w @153c/s)
2025-12-15 12:16:07,803 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 12:16:07,803 - src.generate.formats.lectures - INFO -     - Length: 8393 chars, 1227 words
2025-12-15 12:16:07,803 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:16:07,803 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:16:07,804 - src.generate.formats.lectures - INFO -     - Content: 16 examples, 3 terms defined
2025-12-15 12:16:07,804 - src.generate.formats.lectures - WARNING - [WARNING] Too many examples (16, maximum 15, 1 excess - consider consolidating or removing less critical examples) âš ï¸
2025-12-15 12:16:07,804 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 12:16:07,804 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 12:16:07,804 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-15 12:16:07,807 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:16:07,808 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:16:07,808 - src.generate.formats.labs - INFO - Generating lab 7 for: Hierarchical Generative Models (Session 7)
2025-12-15 12:16:07,808 - src.llm.client - INFO - [lab:4aa4ff] ğŸš€ lab | m=gemma3:4b | p=3322c | t=150s
2025-12-15 12:16:07,808 - src.llm.client - INFO - [lab:4aa4ff] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:16:07,808 - src.llm.client - INFO - [lab:4aa4ff] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:16:07,809 - src.llm.client - INFO - [lab:4aa4ff] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3733 bytes, prompt=3322 chars
2025-12-15 12:16:07,809 - src.llm.client - INFO - [lab:4aa4ff] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:16:09,627 - src.llm.request_handler - INFO - [lab:4aa4ff] âœ“ Done 1.82s
2025-12-15 12:16:09,628 - src.llm.client - INFO - [lab:4aa4ff] âœ… HTTP 200 in 1.82s
2025-12-15 12:16:09,628 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“¡ Stream active (200)
2025-12-15 12:16:09,628 - src.llm.client - INFO - [lab:4aa4ff] Starting stream parsing, waiting for first chunk...
2025-12-15 12:16:11,658 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 2.0s: 318c @157c/s (68ch, ~80t @39t/s)
2025-12-15 12:16:13,686 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 4.1s: 677c @167c/s (136ch, ~169t @42t/s)
2025-12-15 12:16:15,690 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 6.1s: 1042c @172c/s (203ch, ~260t @43t/s)
2025-12-15 12:16:17,702 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 8.1s: 1318c @163c/s (270ch, ~330t @41t/s)
2025-12-15 12:16:19,720 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 10.1s: 1556c @154c/s (337ch, ~389t @39t/s)
2025-12-15 12:16:21,727 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 12.1s: 1744c @144c/s (403ch, ~436t @36t/s)
2025-12-15 12:16:23,729 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 14.1s: 1941c @138c/s (469ch, ~485t @34t/s)
2025-12-15 12:16:25,733 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 16.1s: 2295c @142c/s (535ch, ~574t @36t/s)
2025-12-15 12:16:27,734 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 18.1s: 2563c @142c/s (601ch, ~641t @35t/s)
2025-12-15 12:16:29,742 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 20.1s: 2871c @143c/s (667ch, ~718t @36t/s)
2025-12-15 12:16:31,748 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 22.1s: 3114c @141c/s (733ch, ~778t @35t/s)
2025-12-15 12:16:33,763 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 24.1s: 3348c @139c/s (799ch, ~837t @35t/s)
2025-12-15 12:16:35,771 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 26.1s: 3684c @141c/s (865ch, ~921t @35t/s)
2025-12-15 12:16:37,788 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 28.2s: 4067c @144c/s (931ch, ~1017t @36t/s)
2025-12-15 12:16:39,803 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 30.2s: 4383c @145c/s (997ch, ~1096t @36t/s)
2025-12-15 12:16:41,815 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 32.2s: 4794c @149c/s (1063ch, ~1198t @37t/s)
2025-12-15 12:16:43,830 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 34.2s: 5115c @150c/s (1129ch, ~1279t @37t/s)
2025-12-15 12:16:45,840 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 36.2s: 5491c @152c/s (1195ch, ~1373t @38t/s)
2025-12-15 12:16:47,850 - src.llm.client - INFO - [lab:4aa4ff] ğŸ“Š 38.2s: 5874c @154c/s (1261ch, ~1468t @38t/s)
2025-12-15 12:16:48,730 - src.llm.client - INFO - [lab:4aa4ff] âœ“ Done 40.92s: 6050c (~776w @148c/s)
2025-12-15 12:16:48,731 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:16:48,731 - src.generate.formats.labs - INFO -     - Length: 6141 chars, 789 words
2025-12-15 12:16:48,731 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-15 12:16:48,731 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 12:16:48,731 - src.generate.formats.labs - INFO -     - Data tables: 5
2025-12-15 12:16:48,734 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:16:48,736 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:16:48,737 - src.generate.formats.study_notes - INFO - Generating study notes for: Hierarchical Generative Models (Session 7)
2025-12-15 12:16:48,737 - src.llm.client - INFO - [stu:d7c5d8] ğŸš€ stu | m=gemma3:4b | p=4447c | t=120s
2025-12-15 12:16:48,737 - src.llm.client - INFO - [stu:d7c5d8] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:16:48,737 - src.llm.client - INFO - [stu:d7c5d8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:16:48,739 - src.llm.client - INFO - [stu:d7c5d8] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8073 bytes, prompt=4447 chars
2025-12-15 12:16:48,739 - src.llm.client - INFO - [stu:d7c5d8] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:16:51,606 - src.llm.request_handler - INFO - [stu:d7c5d8] âœ“ Done 2.87s
2025-12-15 12:16:51,606 - src.llm.client - INFO - [stu:d7c5d8] âœ… HTTP 200 in 2.87s
2025-12-15 12:16:51,606 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“¡ Stream active (200)
2025-12-15 12:16:51,606 - src.llm.client - INFO - [stu:d7c5d8] Starting stream parsing, waiting for first chunk...
2025-12-15 12:16:53,607 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 2.0s: 384c @192c/s (64ch, ~96t @48t/s)
2025-12-15 12:16:55,636 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 4.0s: 757c @188c/s (131ch, ~189t @47t/s)
2025-12-15 12:16:57,665 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 6.1s: 1135c @187c/s (198ch, ~284t @47t/s)
2025-12-15 12:16:59,668 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 8.1s: 1530c @190c/s (264ch, ~382t @47t/s)
2025-12-15 12:17:01,670 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 10.1s: 1858c @185c/s (330ch, ~464t @46t/s)
2025-12-15 12:17:03,673 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 12.1s: 2282c @189c/s (396ch, ~570t @47t/s)
2025-12-15 12:17:05,678 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 14.1s: 2656c @189c/s (462ch, ~664t @47t/s)
2025-12-15 12:17:07,686 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 16.1s: 2985c @186c/s (528ch, ~746t @46t/s)
2025-12-15 12:17:09,696 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 18.1s: 3301c @182c/s (594ch, ~825t @46t/s)
2025-12-15 12:17:11,739 - src.llm.client - INFO - [stu:d7c5d8] ğŸ“Š 20.1s: 3698c @184c/s (660ch, ~924t @46t/s)
2025-12-15 12:17:11,740 - src.llm.client - INFO - [stu:d7c5d8] âœ“ Done 23.00s: 3698c (~501w @161c/s)
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO - [NEEDS REVIEW] Study notes generated âš ï¸
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO -     - Length: 3763 chars, 511 words
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO -     - Key concepts: 12
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 5 bullets
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - WARNING - [WARNING] Too many key concepts (12, maximum 10, 2 excess - consolidate related concepts or remove less critical ones) âš ï¸
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 12:17:11,740 - src.generate.formats.study_notes - INFO - Quality score: 98.0/100 (excellent)
2025-12-15 12:17:11,742 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:17:11,742 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:17:11,742 - src.generate.formats.diagrams - INFO - Generating diagram for: Autoencoders (Hierarchical Generative Models)
2025-12-15 12:17:11,742 - src.generate.formats.diagrams - INFO - Generating diagram for: Convolutional Models (Hierarchical Generative Models)
2025-12-15 12:17:11,742 - src.llm.client - INFO - [dia:c0a454] ğŸš€ dia | m=gemma3:4b | p=5763c | t=120s
2025-12-15 12:17:11,743 - src.llm.client - INFO - [dia:c0a454] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:17:11,743 - src.llm.client - INFO - [dia:c0a454] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:17:11,742 - src.llm.client - INFO - [dia:8f5788] ğŸš€ dia | m=gemma3:4b | p=5747c | t=120s
2025-12-15 12:17:11,743 - src.llm.client - INFO - [dia:8f5788] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:17:11,743 - src.llm.client - INFO - [dia:8f5788] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:17:11,744 - src.llm.client - INFO - [dia:c0a454] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11072 bytes, prompt=5763 chars
2025-12-15 12:17:11,744 - src.llm.client - INFO - [dia:c0a454] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:17:11,744 - src.llm.client - INFO - [dia:8f5788] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11056 bytes, prompt=5747 chars
2025-12-15 12:17:11,745 - src.llm.client - INFO - [dia:8f5788] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:17:15,381 - src.llm.request_handler - INFO - [dia:c0a454] âœ“ Done 3.64s
2025-12-15 12:17:15,381 - src.llm.client - INFO - [dia:c0a454] âœ… HTTP 200 in 3.64s
2025-12-15 12:17:15,381 - src.llm.client - INFO - [dia:c0a454] ğŸ“¡ Stream active (200)
2025-12-15 12:17:15,381 - src.llm.client - INFO - [dia:c0a454] Starting stream parsing, waiting for first chunk...
2025-12-15 12:17:17,400 - src.llm.client - INFO - [dia:c0a454] ğŸ“Š 2.0s: 239c @118c/s (66ch, ~60t @30t/s)
2025-12-15 12:17:19,420 - src.llm.client - INFO - [dia:c0a454] ğŸ“Š 4.0s: 472c @117c/s (132ch, ~118t @29t/s)
2025-12-15 12:17:21,430 - src.llm.client - INFO - [dia:c0a454] ğŸ“Š 6.0s: 685c @113c/s (198ch, ~171t @28t/s)
2025-12-15 12:17:23,213 - src.llm.client - INFO - [dia:c0a454] âœ“ Done 11.47s: 798c (~107w @70c/s)
2025-12-15 12:17:23,213 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Convolutional Models (Hierarchical Generative Models):
2025-12-15 12:17:23,213 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:17:23,213 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-15 12:17:23,213 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:17:23,213 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:17:23,214 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:17:23,214 - src.generate.formats.diagrams - INFO -     - Length: 625 chars (cleaned: 625 chars)
2025-12-15 12:17:23,214 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:17:23,214 - src.generate.formats.diagrams - INFO - [OK] Elements: 33 total (nodes: 13, connections: 20) âœ“
2025-12-15 12:17:23,214 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-15 12:17:23,214 - src.generate.formats.diagrams - INFO - Generated diagram: 625 characters
2025-12-15 12:17:26,573 - src.llm.request_handler - INFO - [dia:8f5788] âœ“ Done 14.83s
2025-12-15 12:17:26,573 - src.llm.client - INFO - [dia:8f5788] âœ… HTTP 200 in 14.83s
2025-12-15 12:17:26,573 - src.llm.client - INFO - [dia:8f5788] ğŸ“¡ Stream active (200)
2025-12-15 12:17:26,574 - src.llm.client - INFO - [dia:8f5788] Starting stream parsing, waiting for first chunk...
2025-12-15 12:17:28,593 - src.llm.client - INFO - [dia:8f5788] ğŸ“Š 2.0s: 185c @92c/s (66ch, ~46t @23t/s)
2025-12-15 12:17:30,603 - src.llm.client - INFO - [dia:8f5788] ğŸ“Š 4.0s: 422c @105c/s (132ch, ~106t @26t/s)
2025-12-15 12:17:32,607 - src.llm.client - INFO - [dia:8f5788] ğŸ“Š 6.0s: 622c @103c/s (198ch, ~156t @26t/s)
2025-12-15 12:17:34,611 - src.llm.client - INFO - [dia:8f5788] ğŸ“Š 8.0s: 798c @99c/s (264ch, ~200t @25t/s)
2025-12-15 12:17:34,866 - src.llm.client - INFO - [dia:8f5788] âœ“ Done 23.12s: 813c (~115w @35c/s)
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Autoencoders (Hierarchical Generative Models):
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - INFO -     - Length: 540 chars (cleaned: 540 chars)
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:17:34,867 - src.generate.formats.diagrams - INFO - [OK] Elements: 37 total (nodes: 15, connections: 22) âœ“
2025-12-15 12:17:34,868 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:17:34,868 - src.generate.formats.diagrams - INFO - Generated diagram: 540 characters
2025-12-15 12:17:34,868 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:17:34,868 - src.generate.formats.questions - INFO - Generating 10 questions for: Hierarchical Generative Models (Session 7)
2025-12-15 12:17:34,868 - src.llm.client - INFO - [qst:abb18b] ğŸš€ qst | m=gemma3:4b | p=7329c | t=150s
2025-12-15 12:17:34,868 - src.llm.client - INFO - [qst:abb18b] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:17:34,868 - src.llm.client - INFO - [qst:abb18b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:17:34,869 - src.llm.client - INFO - [qst:abb18b] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11015 bytes, prompt=7329 chars
2025-12-15 12:17:34,869 - src.llm.client - INFO - [qst:abb18b] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:17:39,097 - src.llm.request_handler - INFO - [qst:abb18b] âœ“ Done 4.23s
2025-12-15 12:17:39,097 - src.llm.client - INFO - [qst:abb18b] âœ… HTTP 200 in 4.23s
2025-12-15 12:17:39,097 - src.llm.client - INFO - [qst:abb18b] ğŸ“¡ Stream active (200)
2025-12-15 12:17:39,097 - src.llm.client - INFO - [qst:abb18b] Starting stream parsing, waiting for first chunk...
2025-12-15 12:17:41,117 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 2.0s: 303c @150c/s (66ch, ~76t @38t/s)
2025-12-15 12:17:43,129 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 4.0s: 619c @154c/s (132ch, ~155t @38t/s)
2025-12-15 12:17:45,139 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 6.0s: 974c @161c/s (198ch, ~244t @40t/s)
2025-12-15 12:17:47,152 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 8.1s: 1276c @158c/s (264ch, ~319t @40t/s)
2025-12-15 12:17:49,160 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 10.1s: 1638c @163c/s (330ch, ~410t @41t/s)
2025-12-15 12:17:51,177 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 12.1s: 1975c @163c/s (396ch, ~494t @41t/s)
2025-12-15 12:17:53,196 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 14.1s: 2315c @164c/s (461ch, ~579t @41t/s)
2025-12-15 12:17:55,216 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 16.1s: 2632c @163c/s (527ch, ~658t @41t/s)
2025-12-15 12:17:57,234 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 18.1s: 2962c @163c/s (593ch, ~740t @41t/s)
2025-12-15 12:17:59,260 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 20.2s: 3320c @165c/s (659ch, ~830t @41t/s)
2025-12-15 12:18:01,282 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 22.2s: 3679c @166c/s (725ch, ~920t @41t/s)
2025-12-15 12:18:03,307 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 24.2s: 4050c @167c/s (791ch, ~1012t @42t/s)
2025-12-15 12:18:05,337 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 26.2s: 4423c @169c/s (857ch, ~1106t @42t/s)
2025-12-15 12:18:07,424 - src.llm.client - INFO - [qst:abb18b] ğŸ“Š 28.3s: 4757c @168c/s (922ch, ~1189t @42t/s)
2025-12-15 12:18:07,424 - src.llm.client - INFO - [qst:abb18b] âœ“ Done 32.56s: 4757c (~665w @146c/s)
2025-12-15 12:18:07,425 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 0, 'total_fixes': 2}
2025-12-15 12:18:07,425 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -     Context: Module 4 Session 7
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 3 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -     Context: Module 4 Session 7
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Hierarchical Generative Models (Session 7)
2025-12-15 12:18:07,425 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:18:07,428 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:18:07,430 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 7 completed
2025-12-15 12:18:07,430 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:18:07,430 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:18:07,430 - src.generate.orchestration.pipeline - INFO - Module 5: Precision Weighting & Attention (2 sessions)
2025-12-15 12:18:07,430 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:18:07,430 - src.generate.orchestration.pipeline - INFO - 
[8/20] Session 8: Dynamic Priors
2025-12-15 12:18:07,430 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:18:07,430 - src.generate.formats.lectures - INFO - Generating lecture for: Precision Weighting & Attention (Session 8/20)
2025-12-15 12:18:07,430 - src.llm.client - INFO - [lec:082996] ğŸš€ lec | m=gemma3:4b | p=3029c | t=180s
2025-12-15 12:18:07,430 - src.llm.client - INFO - [lec:082996] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:18:07,430 - src.llm.client - INFO - [lec:082996] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:18:07,431 - src.llm.client - INFO - [lec:082996] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6659 bytes, prompt=3029 chars
2025-12-15 12:18:07,431 - src.llm.client - INFO - [lec:082996] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:19:04,025 - src.llm.request_handler - INFO - [lec:082996] âœ“ Done 56.59s
2025-12-15 12:19:04,025 - src.llm.client - INFO - [lec:082996] âœ… HTTP 200 in 56.59s
2025-12-15 12:19:04,025 - src.llm.client - INFO - [lec:082996] ğŸ“¡ Stream active (200)
2025-12-15 12:19:04,025 - src.llm.client - INFO - [lec:082996] Starting stream parsing, waiting for first chunk...
2025-12-15 12:19:06,037 - src.llm.client - INFO - [lec:082996] ğŸ“Š 2.0s: 395c @196c/s (67ch, ~99t @49t/s)
2025-12-15 12:19:08,050 - src.llm.client - INFO - [lec:082996] ğŸ“Š 4.0s: 739c @184c/s (134ch, ~185t @46t/s)
2025-12-15 12:19:10,063 - src.llm.client - INFO - [lec:082996] ğŸ“Š 6.0s: 1078c @179c/s (201ch, ~270t @45t/s)
2025-12-15 12:19:12,086 - src.llm.client - INFO - [lec:082996] ğŸ“Š 8.1s: 1413c @175c/s (268ch, ~353t @44t/s)
2025-12-15 12:19:14,114 - src.llm.client - INFO - [lec:082996] ğŸ“Š 10.1s: 1747c @173c/s (335ch, ~437t @43t/s)
2025-12-15 12:19:16,144 - src.llm.client - INFO - [lec:082996] ğŸ“Š 12.1s: 2102c @173c/s (402ch, ~526t @43t/s)
2025-12-15 12:19:18,144 - src.llm.client - INFO - [lec:082996] ğŸ“Š 14.1s: 2415c @171c/s (468ch, ~604t @43t/s)
2025-12-15 12:19:20,149 - src.llm.client - INFO - [lec:082996] ğŸ“Š 16.1s: 2738c @170c/s (534ch, ~684t @42t/s)
2025-12-15 12:19:22,157 - src.llm.client - INFO - [lec:082996] ğŸ“Š 18.1s: 3075c @170c/s (600ch, ~769t @42t/s)
2025-12-15 12:19:24,163 - src.llm.client - INFO - [lec:082996] ğŸ“Š 20.1s: 3266c @162c/s (666ch, ~816t @41t/s)
2025-12-15 12:19:26,169 - src.llm.client - INFO - [lec:082996] ğŸ“Š 22.1s: 3497c @158c/s (732ch, ~874t @39t/s)
2025-12-15 12:19:28,175 - src.llm.client - INFO - [lec:082996] ğŸ“Š 24.2s: 3766c @156c/s (798ch, ~942t @39t/s)
2025-12-15 12:19:30,184 - src.llm.client - INFO - [lec:082996] ğŸ“Š 26.2s: 4119c @157c/s (864ch, ~1030t @39t/s)
2025-12-15 12:19:32,197 - src.llm.client - INFO - [lec:082996] ğŸ“Š 28.2s: 4475c @159c/s (930ch, ~1119t @40t/s)
2025-12-15 12:19:34,206 - src.llm.client - INFO - [lec:082996] ğŸ“Š 30.2s: 4782c @158c/s (996ch, ~1196t @40t/s)
2025-12-15 12:19:36,221 - src.llm.client - INFO - [lec:082996] ğŸ“Š 32.2s: 5193c @161c/s (1062ch, ~1298t @40t/s)
2025-12-15 12:19:38,244 - src.llm.client - INFO - [lec:082996] ğŸ“Š 34.2s: 5550c @162c/s (1128ch, ~1388t @41t/s)
2025-12-15 12:19:40,256 - src.llm.client - INFO - [lec:082996] ğŸ“Š 36.2s: 5908c @163c/s (1194ch, ~1477t @41t/s)
2025-12-15 12:19:42,271 - src.llm.client - INFO - [lec:082996] ğŸ“Š 38.2s: 6308c @165c/s (1260ch, ~1577t @41t/s)
2025-12-15 12:19:44,288 - src.llm.client - INFO - [lec:082996] ğŸ“Š 40.3s: 6666c @166c/s (1326ch, ~1666t @41t/s)
2025-12-15 12:19:46,307 - src.llm.client - INFO - [lec:082996] ğŸ“Š 42.3s: 7036c @166c/s (1392ch, ~1759t @42t/s)
2025-12-15 12:19:48,329 - src.llm.client - INFO - [lec:082996] ğŸ“Š 44.3s: 7452c @168c/s (1458ch, ~1863t @42t/s)
2025-12-15 12:19:49,617 - src.llm.client - INFO - [lec:082996] âœ“ Done 102.19s: 7709c (~1173w @75c/s)
2025-12-15 12:19:49,618 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:19:49,618 - src.generate.formats.lectures - INFO -     - Length: 7809 chars, 1187 words
2025-12-15 12:19:49,618 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:19:49,618 - src.generate.formats.lectures - INFO -     - Structure: 7 sections, 0 subsections
2025-12-15 12:19:49,618 - src.generate.formats.lectures - INFO -     - Content: 11 examples, 1 terms defined
2025-12-15 12:19:49,618 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:19:49,622 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:19:49,623 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:19:49,623 - src.generate.formats.labs - INFO - Generating lab 8 for: Precision Weighting & Attention (Session 8)
2025-12-15 12:19:49,623 - src.llm.client - INFO - [lab:8ffcc0] ğŸš€ lab | m=gemma3:4b | p=3337c | t=150s
2025-12-15 12:19:49,623 - src.llm.client - INFO - [lab:8ffcc0] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:19:49,623 - src.llm.client - INFO - [lab:8ffcc0] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:19:49,624 - src.llm.client - INFO - [lab:8ffcc0] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3789 bytes, prompt=3337 chars
2025-12-15 12:19:49,624 - src.llm.client - INFO - [lab:8ffcc0] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:19:51,499 - src.llm.request_handler - INFO - [lab:8ffcc0] âœ“ Done 1.88s
2025-12-15 12:19:51,500 - src.llm.client - INFO - [lab:8ffcc0] âœ… HTTP 200 in 1.88s
2025-12-15 12:19:51,500 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“¡ Stream active (200)
2025-12-15 12:19:51,500 - src.llm.client - INFO - [lab:8ffcc0] Starting stream parsing, waiting for first chunk...
2025-12-15 12:19:53,513 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 2.0s: 276c @137c/s (66ch, ~69t @34t/s)
2025-12-15 12:19:55,514 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 4.0s: 655c @163c/s (133ch, ~164t @41t/s)
2025-12-15 12:19:57,519 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 6.0s: 1014c @168c/s (200ch, ~254t @42t/s)
2025-12-15 12:19:59,532 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 8.0s: 1326c @165c/s (267ch, ~332t @41t/s)
2025-12-15 12:20:01,556 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 10.1s: 1588c @158c/s (334ch, ~397t @39t/s)
2025-12-15 12:20:03,586 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 12.1s: 1795c @149c/s (401ch, ~449t @37t/s)
2025-12-15 12:20:05,615 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 14.1s: 2061c @146c/s (468ch, ~515t @37t/s)
2025-12-15 12:20:07,625 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 16.1s: 2356c @146c/s (534ch, ~589t @37t/s)
2025-12-15 12:20:09,633 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 18.1s: 2615c @144c/s (600ch, ~654t @36t/s)
2025-12-15 12:20:11,643 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 20.1s: 2819c @140c/s (666ch, ~705t @35t/s)
2025-12-15 12:20:13,651 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 22.2s: 3077c @139c/s (732ch, ~769t @35t/s)
2025-12-15 12:20:15,659 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 24.2s: 3259c @135c/s (798ch, ~815t @34t/s)
2025-12-15 12:20:17,668 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 26.2s: 3463c @132c/s (864ch, ~866t @33t/s)
2025-12-15 12:20:19,676 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 28.2s: 3739c @133c/s (930ch, ~935t @33t/s)
2025-12-15 12:20:21,695 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 30.2s: 4164c @138c/s (996ch, ~1041t @34t/s)
2025-12-15 12:20:23,707 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 32.2s: 4599c @143c/s (1062ch, ~1150t @36t/s)
2025-12-15 12:20:25,720 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 34.2s: 4895c @143c/s (1128ch, ~1224t @36t/s)
2025-12-15 12:20:27,728 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 36.2s: 5159c @142c/s (1194ch, ~1290t @36t/s)
2025-12-15 12:20:29,737 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 38.2s: 5444c @142c/s (1260ch, ~1361t @36t/s)
2025-12-15 12:20:31,752 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 40.3s: 5747c @143c/s (1326ch, ~1437t @36t/s)
2025-12-15 12:20:33,770 - src.llm.client - INFO - [lab:8ffcc0] ğŸ“Š 42.3s: 6161c @146c/s (1392ch, ~1540t @36t/s)
2025-12-15 12:20:33,917 - src.llm.client - INFO - [lab:8ffcc0] âœ“ Done 44.29s: 6172c (~857w @139c/s)
2025-12-15 12:20:33,918 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:20:33,918 - src.generate.formats.labs - INFO -     - Length: 6278 chars, 873 words
2025-12-15 12:20:33,918 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-15 12:20:33,918 - src.generate.formats.labs - INFO -     - Safety: 5 warnings
2025-12-15 12:20:33,918 - src.generate.formats.labs - INFO -     - Data tables: 12
2025-12-15 12:20:33,922 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:20:33,922 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:20:33,922 - src.generate.formats.study_notes - INFO - Generating study notes for: Precision Weighting & Attention (Session 8)
2025-12-15 12:20:33,922 - src.llm.client - INFO - [stu:10a389] ğŸš€ stu | m=gemma3:4b | p=4441c | t=120s
2025-12-15 12:20:33,922 - src.llm.client - INFO - [stu:10a389] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:20:33,922 - src.llm.client - INFO - [stu:10a389] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:20:33,923 - src.llm.client - INFO - [stu:10a389] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8108 bytes, prompt=4441 chars
2025-12-15 12:20:33,924 - src.llm.client - INFO - [stu:10a389] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:20:36,884 - src.llm.request_handler - INFO - [stu:10a389] âœ“ Done 2.96s
2025-12-15 12:20:36,884 - src.llm.client - INFO - [stu:10a389] âœ… HTTP 200 in 2.96s
2025-12-15 12:20:36,884 - src.llm.client - INFO - [stu:10a389] ğŸ“¡ Stream active (200)
2025-12-15 12:20:36,885 - src.llm.client - INFO - [stu:10a389] Starting stream parsing, waiting for first chunk...
2025-12-15 12:20:38,890 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 2.0s: 336c @168c/s (66ch, ~84t @42t/s)
2025-12-15 12:20:40,891 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 4.0s: 710c @177c/s (132ch, ~178t @44t/s)
2025-12-15 12:20:42,917 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 6.0s: 1119c @186c/s (199ch, ~280t @46t/s)
2025-12-15 12:20:44,918 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 8.0s: 1402c @175c/s (265ch, ~350t @44t/s)
2025-12-15 12:20:46,924 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 10.0s: 1825c @182c/s (331ch, ~456t @45t/s)
2025-12-15 12:20:48,932 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 12.0s: 2235c @186c/s (397ch, ~559t @46t/s)
2025-12-15 12:20:50,945 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 14.1s: 2640c @188c/s (463ch, ~660t @47t/s)
2025-12-15 12:20:52,964 - src.llm.client - INFO - [stu:10a389] ğŸ“Š 16.1s: 2974c @185c/s (528ch, ~744t @46t/s)
2025-12-15 12:20:54,317 - src.llm.client - INFO - [stu:10a389] âœ“ Done 20.40s: 3188c (~439w @156c/s)
2025-12-15 12:20:54,318 - src.generate.formats.study_notes - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:20:54,318 - src.generate.formats.study_notes - WARNING -     [CRITICAL] Issue 1: Only 1 key concepts highlighted (require 3-10, need 2 more - format concepts as **Concept Name:** in bullet points)
2025-12-15 12:20:54,318 - src.generate.formats.study_notes - WARNING -   Retry attempt 1/1 for study notes: Precision Weighting & Attention (Session 8)
2025-12-15 12:20:54,318 - src.generate.formats.study_notes - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:20:54,319 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:20:54,320 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:20:54,320 - src.generate.formats.diagrams - INFO - Generating diagram for: Learning Precision Weights (Precision Weighting & Attention)
2025-12-15 12:20:54,320 - src.llm.client - INFO - [dia:e1afc2] ğŸš€ dia | m=gemma3:4b | p=5764c | t=120s
2025-12-15 12:20:54,320 - src.llm.client - INFO - [dia:e1afc2] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:20:54,320 - src.llm.client - INFO - [dia:e1afc2] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:20:54,321 - src.llm.client - INFO - [dia:e1afc2] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11073 bytes, prompt=5764 chars
2025-12-15 12:20:54,321 - src.llm.client - INFO - [dia:e1afc2] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:20:57,959 - src.llm.request_handler - INFO - [dia:e1afc2] âœ“ Done 3.64s
2025-12-15 12:20:57,959 - src.llm.client - INFO - [dia:e1afc2] âœ… HTTP 200 in 3.64s
2025-12-15 12:20:57,959 - src.llm.client - INFO - [dia:e1afc2] ğŸ“¡ Stream active (200)
2025-12-15 12:20:57,959 - src.llm.client - INFO - [dia:e1afc2] Starting stream parsing, waiting for first chunk...
2025-12-15 12:20:59,975 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 2.0s: 344c @171c/s (66ch, ~86t @43t/s)
2025-12-15 12:21:01,989 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 4.0s: 676c @168c/s (132ch, ~169t @42t/s)
2025-12-15 12:21:03,992 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 6.0s: 1037c @172c/s (198ch, ~259t @43t/s)
2025-12-15 12:21:05,997 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 8.0s: 1374c @171c/s (264ch, ~344t @43t/s)
2025-12-15 12:21:08,003 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 10.0s: 1712c @170c/s (330ch, ~428t @43t/s)
2025-12-15 12:21:10,007 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 12.0s: 2057c @171c/s (396ch, ~514t @43t/s)
2025-12-15 12:21:12,018 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 14.1s: 2401c @171c/s (462ch, ~600t @43t/s)
2025-12-15 12:21:14,022 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 16.1s: 2750c @171c/s (528ch, ~688t @43t/s)
2025-12-15 12:21:16,034 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 18.1s: 3098c @171c/s (594ch, ~774t @43t/s)
2025-12-15 12:21:18,047 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 20.1s: 3438c @171c/s (660ch, ~860t @43t/s)
2025-12-15 12:21:20,064 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 22.1s: 3790c @171c/s (726ch, ~948t @43t/s)
2025-12-15 12:21:22,084 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 24.1s: 4132c @171c/s (792ch, ~1033t @43t/s)
2025-12-15 12:21:24,104 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 26.1s: 4477c @171c/s (858ch, ~1119t @43t/s)
2025-12-15 12:21:26,105 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 28.1s: 4817c @171c/s (923ch, ~1204t @43t/s)
2025-12-15 12:21:28,131 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 30.2s: 5166c @171c/s (989ch, ~1292t @43t/s)
2025-12-15 12:21:30,156 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 32.2s: 5511c @171c/s (1055ch, ~1378t @43t/s)
2025-12-15 12:21:32,184 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 34.2s: 5853c @171c/s (1121ch, ~1463t @43t/s)
2025-12-15 12:21:34,191 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 36.2s: 6197c @171c/s (1186ch, ~1549t @43t/s)
2025-12-15 12:21:36,203 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 38.2s: 6540c @171c/s (1251ch, ~1635t @43t/s)
2025-12-15 12:21:38,213 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 40.3s: 6878c @171c/s (1316ch, ~1720t @43t/s)
2025-12-15 12:21:40,219 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 42.3s: 7217c @171c/s (1381ch, ~1804t @43t/s)
2025-12-15 12:21:42,229 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 44.3s: 7557c @171c/s (1446ch, ~1889t @43t/s)
2025-12-15 12:21:44,238 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 46.3s: 7897c @171c/s (1511ch, ~1974t @43t/s)
2025-12-15 12:21:46,258 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 48.3s: 8239c @171c/s (1576ch, ~2060t @43t/s)
2025-12-15 12:21:48,259 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 50.3s: 8561c @170c/s (1638ch, ~2140t @43t/s)
2025-12-15 12:21:50,271 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 52.3s: 8910c @170c/s (1704ch, ~2228t @43t/s)
2025-12-15 12:21:52,272 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 54.3s: 9251c @170c/s (1769ch, ~2313t @43t/s)
2025-12-15 12:21:54,285 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 56.3s: 9593c @170c/s (1835ch, ~2398t @43t/s)
2025-12-15 12:21:56,304 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 58.3s: 9950c @171c/s (1902ch, ~2488t @43t/s)
2025-12-15 12:21:58,329 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 60.4s: 10297c @171c/s (1969ch, ~2574t @43t/s)
2025-12-15 12:22:00,352 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 62.4s: 10647c @171c/s (2036ch, ~2662t @43t/s)
2025-12-15 12:22:02,379 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 64.4s: 11000c @171c/s (2103ch, ~2750t @43t/s)
2025-12-15 12:22:04,402 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 66.4s: 11349c @171c/s (2170ch, ~2837t @43t/s)
2025-12-15 12:22:06,427 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 68.5s: 11701c @171c/s (2237ch, ~2925t @43t/s)
2025-12-15 12:22:08,454 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 70.5s: 12052c @171c/s (2304ch, ~3013t @43t/s)
2025-12-15 12:22:10,480 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 72.5s: 12406c @171c/s (2371ch, ~3102t @43t/s)
2025-12-15 12:22:12,507 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 74.5s: 12756c @171c/s (2438ch, ~3189t @43t/s)
2025-12-15 12:22:14,533 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 76.6s: 13108c @171c/s (2505ch, ~3277t @43t/s)
2025-12-15 12:22:16,535 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 78.6s: 13448c @171c/s (2571ch, ~3362t @43t/s)
2025-12-15 12:22:18,548 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 80.6s: 13800c @171c/s (2637ch, ~3450t @43t/s)
2025-12-15 12:22:20,552 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 82.6s: 14147c @171c/s (2704ch, ~3537t @43t/s)
2025-12-15 12:22:22,563 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 84.6s: 14497c @171c/s (2771ch, ~3624t @43t/s)
2025-12-15 12:22:24,574 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 86.6s: 14850c @171c/s (2838ch, ~3712t @43t/s)
2025-12-15 12:22:26,585 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 88.6s: 15199c @171c/s (2905ch, ~3800t @43t/s)
2025-12-15 12:22:28,596 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 90.6s: 15551c @172c/s (2972ch, ~3888t @43t/s)
2025-12-15 12:22:30,608 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 92.6s: 15902c @172c/s (3039ch, ~3976t @43t/s)
2025-12-15 12:22:32,621 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 94.7s: 16256c @172c/s (3106ch, ~4064t @43t/s)
2025-12-15 12:22:34,637 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 96.7s: 16606c @172c/s (3173ch, ~4152t @43t/s)
2025-12-15 12:22:36,646 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 98.7s: 16951c @172c/s (3239ch, ~4238t @43t/s)
2025-12-15 12:22:38,670 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 100.7s: 17298c @172c/s (3306ch, ~4324t @43t/s)
2025-12-15 12:22:40,688 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 102.7s: 17657c @172c/s (3373ch, ~4414t @43t/s)
2025-12-15 12:22:42,709 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 104.8s: 18001c @172c/s (3440ch, ~4500t @43t/s)
2025-12-15 12:22:44,729 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 106.8s: 18351c @172c/s (3507ch, ~4588t @43t/s)
2025-12-15 12:22:46,753 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 108.8s: 18708c @172c/s (3574ch, ~4677t @43t/s)
2025-12-15 12:22:48,773 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 110.8s: 19053c @172c/s (3641ch, ~4763t @43t/s)
2025-12-15 12:22:50,799 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 112.8s: 19410c @172c/s (3708ch, ~4852t @43t/s)
2025-12-15 12:22:52,827 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 114.9s: 19757c @172c/s (3775ch, ~4939t @43t/s)
2025-12-15 12:22:54,827 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 116.9s: 20106c @172c/s (3841ch, ~5026t @43t/s)
2025-12-15 12:22:56,832 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 118.9s: 20449c @172c/s (3907ch, ~5112t @43t/s)
2025-12-15 12:22:58,861 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 120.9s: 20801c @172c/s (3974ch, ~5200t @43t/s)
2025-12-15 12:23:00,873 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 122.9s: 21143c @172c/s (4040ch, ~5286t @43t/s)
2025-12-15 12:23:02,881 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 124.9s: 21491c @172c/s (4106ch, ~5373t @43t/s)
2025-12-15 12:23:04,884 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 126.9s: 21838c @172c/s (4172ch, ~5460t @43t/s)
2025-12-15 12:23:06,912 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 129.0s: 22181c @172c/s (4238ch, ~5545t @43t/s)
2025-12-15 12:23:08,932 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 131.0s: 22527c @172c/s (4304ch, ~5632t @43t/s)
2025-12-15 12:23:10,941 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 133.0s: 22876c @172c/s (4370ch, ~5719t @43t/s)
2025-12-15 12:23:12,951 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 135.0s: 23221c @172c/s (4436ch, ~5805t @43t/s)
2025-12-15 12:23:14,959 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 137.0s: 23563c @172c/s (4502ch, ~5891t @43t/s)
2025-12-15 12:23:16,972 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 139.0s: 23911c @172c/s (4568ch, ~5978t @43t/s)
2025-12-15 12:23:18,981 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 141.0s: 24258c @172c/s (4634ch, ~6064t @43t/s)
2025-12-15 12:23:20,994 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 143.0s: 24601c @172c/s (4700ch, ~6150t @43t/s)
2025-12-15 12:23:21,978 - src.llm.client - INFO - [dia:e1afc2] Stream making progress - extending timeout by 60.0s (new limit: 240.0s, max: 240.0s)
2025-12-15 12:23:23,017 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 145.1s: 24959c @172c/s (4768ch, ~6240t @43t/s)
2025-12-15 12:23:25,038 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 147.1s: 25318c @172c/s (4836ch, ~6330t @43t/s)
2025-12-15 12:23:27,061 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 149.1s: 25671c @172c/s (4904ch, ~6418t @43t/s)
2025-12-15 12:23:29,091 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 151.1s: 26027c @172c/s (4972ch, ~6507t @43t/s)
2025-12-15 12:23:31,098 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 153.1s: 26377c @172c/s (5039ch, ~6594t @43t/s)
2025-12-15 12:23:33,107 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 155.1s: 26730c @172c/s (5106ch, ~6682t @43t/s)
2025-12-15 12:23:35,116 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 157.2s: 27079c @172c/s (5173ch, ~6770t @43t/s)
2025-12-15 12:23:37,129 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 159.2s: 27431c @172c/s (5240ch, ~6858t @43t/s)
2025-12-15 12:23:39,147 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 161.2s: 27782c @172c/s (5307ch, ~6946t @43t/s)
2025-12-15 12:23:41,169 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 163.2s: 28136c @172c/s (5374ch, ~7034t @43t/s)
2025-12-15 12:23:43,188 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 165.2s: 28486c @172c/s (5441ch, ~7122t @43t/s)
2025-12-15 12:23:45,205 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 167.2s: 28838c @172c/s (5508ch, ~7210t @43t/s)
2025-12-15 12:23:47,227 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 169.3s: 29187c @172c/s (5575ch, ~7297t @43t/s)
2025-12-15 12:23:49,253 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 171.3s: 29538c @172c/s (5642ch, ~7384t @43t/s)
2025-12-15 12:23:51,255 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 173.3s: 29881c @172c/s (5708ch, ~7470t @43t/s)
2025-12-15 12:23:53,280 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 175.3s: 30231c @172c/s (5775ch, ~7558t @43t/s)
2025-12-15 12:23:55,303 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 177.3s: 30588c @172c/s (5842ch, ~7647t @43t/s)
2025-12-15 12:23:57,307 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 179.3s: 30929c @172c/s (5908ch, ~7732t @43t/s)
2025-12-15 12:23:59,333 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 181.4s: 31281c @172c/s (5975ch, ~7820t @43t/s)
2025-12-15 12:24:01,349 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 183.4s: 31628c @172c/s (6041ch, ~7907t @43t/s)
2025-12-15 12:24:03,379 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 185.4s: 31977c @172c/s (6108ch, ~7994t @43t/s)
2025-12-15 12:24:05,409 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 187.5s: 32329c @172c/s (6175ch, ~8082t @43t/s)
2025-12-15 12:24:07,417 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 189.5s: 32678c @172c/s (6241ch, ~8170t @43t/s)
2025-12-15 12:24:09,419 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 191.5s: 33019c @172c/s (6307ch, ~8255t @43t/s)
2025-12-15 12:24:11,426 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 193.5s: 33367c @172c/s (6373ch, ~8342t @43t/s)
2025-12-15 12:24:13,429 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 195.5s: 33717c @172c/s (6439ch, ~8429t @43t/s)
2025-12-15 12:24:15,438 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 197.5s: 34057c @172c/s (6505ch, ~8514t @43t/s)
2025-12-15 12:24:17,450 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 199.5s: 34406c @172c/s (6571ch, ~8602t @43t/s)
2025-12-15 12:24:19,458 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 201.5s: 34749c @172c/s (6637ch, ~8687t @43t/s)
2025-12-15 12:24:21,473 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 203.5s: 35098c @172c/s (6703ch, ~8774t @43t/s)
2025-12-15 12:24:23,478 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 205.5s: 35439c @172c/s (6769ch, ~8860t @43t/s)
2025-12-15 12:24:25,487 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 207.5s: 35791c @172c/s (6836ch, ~8948t @43t/s)
2025-12-15 12:24:27,495 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 209.5s: 36142c @172c/s (6903ch, ~9036t @43t/s)
2025-12-15 12:24:29,503 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 211.5s: 36496c @173c/s (6970ch, ~9124t @43t/s)
2025-12-15 12:24:31,515 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 213.6s: 36846c @173c/s (7037ch, ~9212t @43t/s)
2025-12-15 12:24:33,529 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 215.6s: 37198c @173c/s (7104ch, ~9300t @43t/s)
2025-12-15 12:24:35,541 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 217.6s: 37547c @173c/s (7171ch, ~9387t @43t/s)
2025-12-15 12:24:37,560 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 219.6s: 37898c @173c/s (7238ch, ~9474t @43t/s)
2025-12-15 12:24:39,572 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 221.6s: 38247c @173c/s (7305ch, ~9562t @43t/s)
2025-12-15 12:24:41,591 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 223.6s: 38599c @173c/s (7372ch, ~9650t @43t/s)
2025-12-15 12:24:43,618 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 225.7s: 38951c @173c/s (7439ch, ~9738t @43t/s)
2025-12-15 12:24:45,634 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 227.7s: 39298c @173c/s (7506ch, ~9824t @43t/s)
2025-12-15 12:24:47,656 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 229.7s: 39657c @173c/s (7573ch, ~9914t @43t/s)
2025-12-15 12:24:49,676 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 231.7s: 40001c @173c/s (7640ch, ~10000t @43t/s)
2025-12-15 12:24:51,700 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 233.7s: 40351c @173c/s (7707ch, ~10088t @43t/s)
2025-12-15 12:24:53,723 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 235.8s: 40708c @173c/s (7774ch, ~10177t @43t/s)
2025-12-15 12:24:55,749 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 237.8s: 41053c @173c/s (7841ch, ~10263t @43t/s)
2025-12-15 12:24:57,776 - src.llm.client - INFO - [dia:e1afc2] ğŸ“Š 239.8s: 41410c @173c/s (7908ch, ~10352t @43t/s)
2025-12-15 12:24:57,988 - src.llm.client - ERROR - [dia:e1afc2] Stream timeout: 240.03s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 7915 chunks, 767626 bytes, 41446 chars (~10362 tokens) before timeout. Performance: 172.7 chars/s, ~43.2 tok/s. Generation was slow (172.7 chars/s, ~43.2 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.
2025-12-15 12:24:57,989 - src.generate.orchestration.pipeline - WARNING -   Transient error in diagram 1 generation (attempt 1/3): [dia:e1afc2] Stream timeout: 240.03s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 7915 chunks, 767626 bytes, 41446 chars (~10362 tokens) before timeout. Performance: 172.7 chars/s, ~43.2 tok/s. Generation was slow (172.7 chars/s, ~43.2 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.. Retrying in 2.0s...
2025-12-15 12:24:59,998 - src.generate.formats.diagrams - INFO - Generating diagram for: Learning Precision Weights (Precision Weighting & Attention)
2025-12-15 12:24:59,998 - src.llm.client - INFO - [dia:a43606] ğŸš€ dia | m=gemma3:4b | p=5764c | t=120s
2025-12-15 12:24:59,999 - src.llm.client - INFO - [dia:a43606] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:24:59,999 - src.llm.client - INFO - [dia:a43606] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:25:00,002 - src.llm.client - INFO - [dia:a43606] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11073 bytes, prompt=5764 chars
2025-12-15 12:25:00,002 - src.llm.client - INFO - [dia:a43606] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:25:03,486 - src.llm.request_handler - INFO - [dia:a43606] âœ“ Done 3.48s
2025-12-15 12:25:03,487 - src.llm.client - INFO - [dia:a43606] âœ… HTTP 200 in 3.48s
2025-12-15 12:25:03,487 - src.llm.client - INFO - [dia:a43606] ğŸ“¡ Stream active (200)
2025-12-15 12:25:03,487 - src.llm.client - INFO - [dia:a43606] Starting stream parsing, waiting for first chunk...
2025-12-15 12:25:05,498 - src.llm.client - INFO - [dia:a43606] ğŸ“Š 2.0s: 238c @118c/s (68ch, ~60t @30t/s)
2025-12-15 12:25:07,515 - src.llm.client - INFO - [dia:a43606] ğŸ“Š 4.0s: 475c @118c/s (136ch, ~119t @29t/s)
2025-12-15 12:25:09,520 - src.llm.client - INFO - [dia:a43606] ğŸ“Š 6.0s: 692c @115c/s (204ch, ~173t @29t/s)
2025-12-15 12:25:11,528 - src.llm.client - INFO - [dia:a43606] ğŸ“Š 8.0s: 886c @110c/s (272ch, ~222t @28t/s)
2025-12-15 12:25:11,747 - src.llm.client - INFO - [dia:a43606] âœ“ Done 11.75s: 895c (~128w @76c/s)
2025-12-15 12:25:11,747 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Learning Precision Weights (Precision Weighting & Attention):
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO -     - Length: 772 chars (cleaned: 772 chars)
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO - [OK] Elements: 43 total (nodes: 14, connections: 29) âœ“
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:25:11,748 - src.generate.formats.diagrams - INFO - Generated diagram: 772 characters
2025-12-15 12:25:11,748 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:25:11,748 - src.generate.formats.questions - INFO - Generating 10 questions for: Precision Weighting & Attention (Session 8)
2025-12-15 12:25:11,748 - src.llm.client - INFO - [qst:46fc48] ğŸš€ qst | m=gemma3:4b | p=7330c | t=150s
2025-12-15 12:25:11,748 - src.llm.client - INFO - [qst:46fc48] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:25:11,748 - src.llm.client - INFO - [qst:46fc48] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:25:11,750 - src.llm.client - INFO - [qst:46fc48] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11104 bytes, prompt=7330 chars
2025-12-15 12:25:11,750 - src.llm.client - INFO - [qst:46fc48] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:25:15,904 - src.llm.request_handler - INFO - [qst:46fc48] âœ“ Done 4.15s
2025-12-15 12:25:15,904 - src.llm.client - INFO - [qst:46fc48] âœ… HTTP 200 in 4.15s
2025-12-15 12:25:15,904 - src.llm.client - INFO - [qst:46fc48] ğŸ“¡ Stream active (200)
2025-12-15 12:25:15,904 - src.llm.client - INFO - [qst:46fc48] Starting stream parsing, waiting for first chunk...
2025-12-15 12:25:17,918 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 2.0s: 322c @160c/s (68ch, ~80t @40t/s)
2025-12-15 12:25:19,928 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 4.0s: 647c @161c/s (136ch, ~162t @40t/s)
2025-12-15 12:25:21,942 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 6.0s: 1014c @168c/s (204ch, ~254t @42t/s)
2025-12-15 12:25:23,951 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 8.0s: 1375c @171c/s (272ch, ~344t @43t/s)
2025-12-15 12:25:25,959 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 10.1s: 1719c @171c/s (340ch, ~430t @43t/s)
2025-12-15 12:25:27,980 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 12.1s: 2063c @171c/s (408ch, ~516t @43t/s)
2025-12-15 12:25:29,998 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 14.1s: 2418c @172c/s (476ch, ~604t @43t/s)
2025-12-15 12:25:32,018 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 16.1s: 2758c @171c/s (544ch, ~690t @43t/s)
2025-12-15 12:25:34,038 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 18.1s: 3102c @171c/s (612ch, ~776t @43t/s)
2025-12-15 12:25:36,040 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 20.1s: 3488c @173c/s (679ch, ~872t @43t/s)
2025-12-15 12:25:38,068 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 22.2s: 3835c @173c/s (747ch, ~959t @43t/s)
2025-12-15 12:25:40,072 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 24.2s: 4166c @172c/s (814ch, ~1042t @43t/s)
2025-12-15 12:25:42,082 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 26.2s: 4516c @173c/s (881ch, ~1129t @43t/s)
2025-12-15 12:25:44,113 - src.llm.client - INFO - [qst:46fc48] ğŸ“Š 28.2s: 4879c @173c/s (949ch, ~1220t @43t/s)
2025-12-15 12:25:46,027 - src.llm.client - INFO - [qst:46fc48] âœ“ Done 34.28s: 5254c (~768w @153c/s)
2025-12-15 12:25:46,028 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 5 question format issues: {'format_standardized': 0, 'question_marks_added': 4, 'mc_options_fixed': 1, 'total_fixes': 5}
2025-12-15 12:25:46,028 - src.generate.formats.questions - INFO - Applied 5 auto-fixes to questions
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 2 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING -     Context: Module 5 Session 8
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 5 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING -     Context: Module 5 Session 8
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:25:46,028 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:25:46,029 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Precision Weighting & Attention (Session 8)
2025-12-15 12:25:46,029 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:25:46,031 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:25:46,033 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 8 completed
2025-12-15 12:25:46,033 - src.generate.orchestration.pipeline - INFO - 
[9/20] Session 9: Attention Mechanisms
2025-12-15 12:25:46,033 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:25:46,033 - src.generate.formats.lectures - INFO - Generating lecture for: Precision Weighting & Attention (Session 9/20)
2025-12-15 12:25:46,033 - src.llm.client - INFO - [lec:192ff3] ğŸš€ lec | m=gemma3:4b | p=3056c | t=180s
2025-12-15 12:25:46,033 - src.llm.client - INFO - [lec:192ff3] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:25:46,033 - src.llm.client - INFO - [lec:192ff3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:25:46,034 - src.llm.client - INFO - [lec:192ff3] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6687 bytes, prompt=3056 chars
2025-12-15 12:25:46,034 - src.llm.client - INFO - [lec:192ff3] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:25:48,042 - src.llm.request_handler - INFO - [lec:192ff3] âœ“ Done 2.01s
2025-12-15 12:25:48,042 - src.llm.client - INFO - [lec:192ff3] âœ… HTTP 200 in 2.01s
2025-12-15 12:25:48,042 - src.llm.client - INFO - [lec:192ff3] ğŸ“¡ Stream active (200)
2025-12-15 12:25:48,042 - src.llm.client - INFO - [lec:192ff3] Starting stream parsing, waiting for first chunk...
2025-12-15 12:25:50,055 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 2.0s: 365c @181c/s (69ch, ~91t @45t/s)
2025-12-15 12:25:52,070 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 4.0s: 745c @185c/s (138ch, ~186t @46t/s)
2025-12-15 12:25:54,085 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 6.0s: 1094c @181c/s (207ch, ~274t @45t/s)
2025-12-15 12:25:56,112 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 8.1s: 1472c @182c/s (276ch, ~368t @46t/s)
2025-12-15 12:25:58,112 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 10.1s: 1817c @180c/s (344ch, ~454t @45t/s)
2025-12-15 12:26:00,119 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 12.1s: 2202c @182c/s (412ch, ~550t @46t/s)
2025-12-15 12:26:02,132 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 14.1s: 2531c @180c/s (480ch, ~633t @45t/s)
2025-12-15 12:26:04,134 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 16.1s: 2936c @182c/s (548ch, ~734t @46t/s)
2025-12-15 12:26:06,138 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 18.1s: 3308c @183c/s (616ch, ~827t @46t/s)
2025-12-15 12:26:08,145 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 20.1s: 3671c @183c/s (684ch, ~918t @46t/s)
2025-12-15 12:26:10,150 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 22.1s: 4056c @183c/s (752ch, ~1014t @46t/s)
2025-12-15 12:26:12,161 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 24.1s: 4424c @183c/s (820ch, ~1106t @46t/s)
2025-12-15 12:26:14,168 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 26.1s: 4808c @184c/s (888ch, ~1202t @46t/s)
2025-12-15 12:26:16,178 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 28.1s: 5187c @184c/s (956ch, ~1297t @46t/s)
2025-12-15 12:26:18,202 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 30.2s: 5503c @182c/s (1024ch, ~1376t @46t/s)
2025-12-15 12:26:20,216 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 32.2s: 5904c @184c/s (1092ch, ~1476t @46t/s)
2025-12-15 12:26:22,234 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 34.2s: 6292c @184c/s (1160ch, ~1573t @46t/s)
2025-12-15 12:26:24,246 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 36.2s: 6675c @184c/s (1228ch, ~1669t @46t/s)
2025-12-15 12:26:26,263 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 38.2s: 7078c @185c/s (1296ch, ~1770t @46t/s)
2025-12-15 12:26:28,283 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 40.2s: 7417c @184c/s (1364ch, ~1854t @46t/s)
2025-12-15 12:26:30,300 - src.llm.client - INFO - [lec:192ff3] ğŸ“Š 42.3s: 7834c @185c/s (1432ch, ~1958t @46t/s)
2025-12-15 12:26:32,000 - src.llm.client - INFO - [lec:192ff3] âœ“ Done 45.97s: 8189c (~1202w @178c/s)
2025-12-15 12:26:32,001 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 12:26:32,001 - src.generate.formats.lectures - INFO -     - Length: 8283 chars, 1215 words
2025-12-15 12:26:32,001 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:26:32,001 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:26:32,002 - src.generate.formats.lectures - INFO -     - Content: 17 examples, 0 terms defined
2025-12-15 12:26:32,002 - src.generate.formats.lectures - WARNING - [WARNING] Too many examples (17, maximum 15, 2 excess - consider consolidating or removing less critical examples) âš ï¸
2025-12-15 12:26:32,002 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 12:26:32,002 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 12:26:32,002 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-15 12:26:32,005 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:26:32,006 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:26:32,006 - src.generate.formats.labs - INFO - Generating lab 9 for: Precision Weighting & Attention (Session 9)
2025-12-15 12:26:32,006 - src.llm.client - INFO - [lab:1e17e5] ğŸš€ lab | m=gemma3:4b | p=3339c | t=150s
2025-12-15 12:26:32,006 - src.llm.client - INFO - [lab:1e17e5] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:26:32,006 - src.llm.client - INFO - [lab:1e17e5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:26:32,009 - src.llm.client - INFO - [lab:1e17e5] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3792 bytes, prompt=3339 chars
2025-12-15 12:26:32,009 - src.llm.client - INFO - [lab:1e17e5] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:26:33,796 - src.llm.request_handler - INFO - [lab:1e17e5] âœ“ Done 1.79s
2025-12-15 12:26:33,797 - src.llm.client - INFO - [lab:1e17e5] âœ… HTTP 200 in 1.79s
2025-12-15 12:26:33,797 - src.llm.client - INFO - [lab:1e17e5] ğŸ“¡ Stream active (200)
2025-12-15 12:26:33,797 - src.llm.client - INFO - [lab:1e17e5] Starting stream parsing, waiting for first chunk...
2025-12-15 12:26:35,818 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 2.0s: 340c @168c/s (70ch, ~85t @42t/s)
2025-12-15 12:26:37,826 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 4.0s: 755c @187c/s (139ch, ~189t @47t/s)
2025-12-15 12:26:39,828 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 6.0s: 1078c @179c/s (208ch, ~270t @45t/s)
2025-12-15 12:26:41,843 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 8.0s: 1384c @172c/s (277ch, ~346t @43t/s)
2025-12-15 12:26:43,866 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 10.1s: 1605c @159c/s (346ch, ~401t @40t/s)
2025-12-15 12:26:45,892 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 12.1s: 1848c @153c/s (415ch, ~462t @38t/s)
2025-12-15 12:26:47,905 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 14.1s: 2172c @154c/s (483ch, ~543t @38t/s)
2025-12-15 12:26:49,934 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 16.1s: 2515c @156c/s (552ch, ~629t @39t/s)
2025-12-15 12:26:51,963 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 18.2s: 2795c @154c/s (620ch, ~699t @38t/s)
2025-12-15 12:26:53,985 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 20.2s: 3007c @149c/s (688ch, ~752t @37t/s)
2025-12-15 12:26:55,989 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 22.2s: 3307c @149c/s (756ch, ~827t @37t/s)
2025-12-15 12:26:57,992 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 24.2s: 3623c @150c/s (824ch, ~906t @37t/s)
2025-12-15 12:26:59,996 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 26.2s: 3966c @151c/s (892ch, ~992t @38t/s)
2025-12-15 12:27:02,016 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 28.2s: 4278c @152c/s (960ch, ~1070t @38t/s)
2025-12-15 12:27:04,023 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 30.2s: 4803c @159c/s (1028ch, ~1201t @40t/s)
2025-12-15 12:27:06,033 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 32.2s: 5306c @165c/s (1096ch, ~1326t @41t/s)
2025-12-15 12:27:08,054 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 34.3s: 5661c @165c/s (1164ch, ~1415t @41t/s)
2025-12-15 12:27:10,067 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 36.3s: 5980c @165c/s (1232ch, ~1495t @41t/s)
2025-12-15 12:27:12,083 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 38.3s: 6312c @165c/s (1300ch, ~1578t @41t/s)
2025-12-15 12:27:14,098 - src.llm.client - INFO - [lab:1e17e5] ğŸ“Š 40.3s: 6657c @165c/s (1368ch, ~1664t @41t/s)
2025-12-15 12:27:15,045 - src.llm.client - INFO - [lab:1e17e5] âœ“ Done 43.04s: 6808c (~871w @158c/s)
2025-12-15 12:27:15,045 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:27:15,045 - src.generate.formats.labs - INFO -     - Length: 6908 chars, 886 words
2025-12-15 12:27:15,045 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-15 12:27:15,045 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 12:27:15,045 - src.generate.formats.labs - INFO -     - Data tables: 13
2025-12-15 12:27:15,049 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:27:15,049 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:27:15,049 - src.generate.formats.study_notes - INFO - Generating study notes for: Precision Weighting & Attention (Session 9)
2025-12-15 12:27:15,050 - src.llm.client - INFO - [stu:3c38d9] ğŸš€ stu | m=gemma3:4b | p=4457c | t=120s
2025-12-15 12:27:15,050 - src.llm.client - INFO - [stu:3c38d9] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:27:15,050 - src.llm.client - INFO - [stu:3c38d9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:27:15,051 - src.llm.client - INFO - [stu:3c38d9] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8125 bytes, prompt=4457 chars
2025-12-15 12:27:15,051 - src.llm.client - INFO - [stu:3c38d9] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:27:17,649 - src.llm.request_handler - INFO - [stu:3c38d9] âœ“ Done 2.60s
2025-12-15 12:27:17,649 - src.llm.client - INFO - [stu:3c38d9] âœ… HTTP 200 in 2.60s
2025-12-15 12:27:17,649 - src.llm.client - INFO - [stu:3c38d9] ğŸ“¡ Stream active (200)
2025-12-15 12:27:17,649 - src.llm.client - INFO - [stu:3c38d9] Starting stream parsing, waiting for first chunk...
2025-12-15 12:27:19,678 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 2.0s: 392c @193c/s (69ch, ~98t @48t/s)
2025-12-15 12:27:21,680 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 4.0s: 751c @186c/s (137ch, ~188t @47t/s)
2025-12-15 12:27:23,684 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 6.0s: 1163c @193c/s (205ch, ~291t @48t/s)
2025-12-15 12:27:25,686 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 8.0s: 1573c @196c/s (273ch, ~393t @49t/s)
2025-12-15 12:27:27,692 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 10.0s: 1953c @194c/s (341ch, ~488t @49t/s)
2025-12-15 12:27:29,694 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 12.0s: 2333c @194c/s (409ch, ~583t @48t/s)
2025-12-15 12:27:31,704 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 14.1s: 2727c @194c/s (477ch, ~682t @49t/s)
2025-12-15 12:27:33,713 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 16.1s: 3053c @190c/s (545ch, ~763t @48t/s)
2025-12-15 12:27:35,724 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 18.1s: 3424c @189c/s (613ch, ~856t @47t/s)
2025-12-15 12:27:37,739 - src.llm.client - INFO - [stu:3c38d9] ğŸ“Š 20.1s: 3754c @187c/s (681ch, ~938t @47t/s)
2025-12-15 12:27:38,976 - src.llm.client - INFO - [stu:3c38d9] âœ“ Done 23.93s: 3949c (~557w @165c/s)
2025-12-15 12:27:38,976 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:27:38,976 - src.generate.formats.study_notes - INFO -     - Length: 4015 chars, 568 words
2025-12-15 12:27:38,976 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:27:38,976 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-15 12:27:38,976 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 2 bullets
2025-12-15 12:27:38,976 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:27:38,978 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:27:38,979 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:27:38,979 - src.generate.formats.diagrams - INFO - Generating diagram for: Selective Perception (Precision Weighting & Attention)
2025-12-15 12:27:38,979 - src.generate.formats.diagrams - INFO - Generating diagram for: Optimal Inference (Precision Weighting & Attention)
2025-12-15 12:27:38,979 - src.llm.client - INFO - [dia:f15fe7] ğŸš€ dia | m=gemma3:4b | p=5752c | t=120s
2025-12-15 12:27:38,979 - src.llm.client - INFO - [dia:f15fe7] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:27:38,979 - src.llm.client - INFO - [dia:18b1c5] ğŸš€ dia | m=gemma3:4b | p=5758c | t=120s
2025-12-15 12:27:38,979 - src.llm.client - INFO - [dia:18b1c5] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:27:38,979 - src.llm.client - INFO - [dia:18b1c5] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:27:38,979 - src.llm.client - INFO - [dia:f15fe7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:27:38,981 - src.llm.client - INFO - [dia:18b1c5] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11067 bytes, prompt=5758 chars
2025-12-15 12:27:38,981 - src.llm.client - INFO - [dia:f15fe7] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11061 bytes, prompt=5752 chars
2025-12-15 12:27:38,981 - src.llm.client - INFO - [dia:f15fe7] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:27:38,981 - src.llm.client - INFO - [dia:18b1c5] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:27:42,863 - src.llm.request_handler - INFO - [dia:18b1c5] âœ“ Done 3.88s
2025-12-15 12:27:42,863 - src.llm.client - INFO - [dia:18b1c5] âœ… HTTP 200 in 3.88s
2025-12-15 12:27:42,863 - src.llm.client - INFO - [dia:18b1c5] ğŸ“¡ Stream active (200)
2025-12-15 12:27:42,863 - src.llm.client - INFO - [dia:18b1c5] Starting stream parsing, waiting for first chunk...
2025-12-15 12:27:44,877 - src.llm.client - INFO - [dia:18b1c5] ğŸ“Š 2.0s: 244c @121c/s (68ch, ~61t @30t/s)
2025-12-15 12:27:46,893 - src.llm.client - INFO - [dia:18b1c5] ğŸ“Š 4.0s: 427c @106c/s (136ch, ~107t @26t/s)
2025-12-15 12:27:48,900 - src.llm.client - INFO - [dia:18b1c5] ğŸ“Š 6.0s: 629c @104c/s (204ch, ~157t @26t/s)
2025-12-15 12:27:50,908 - src.llm.client - INFO - [dia:18b1c5] ğŸ“Š 8.0s: 795c @99c/s (272ch, ~199t @25t/s)
2025-12-15 12:27:51,668 - src.llm.client - INFO - [dia:18b1c5] âœ“ Done 12.69s: 848c (~107w @67c/s)
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Selective Perception (Precision Weighting & Attention):
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO -     - Length: 475 chars (cleaned: 475 chars)
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO - [OK] Elements: 29 total (nodes: 10, connections: 19) âœ“
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:27:51,668 - src.generate.formats.diagrams - INFO - Generated diagram: 475 characters
2025-12-15 12:27:54,960 - src.llm.request_handler - INFO - [dia:f15fe7] âœ“ Done 15.98s
2025-12-15 12:27:54,960 - src.llm.client - INFO - [dia:f15fe7] âœ… HTTP 200 in 15.98s
2025-12-15 12:27:54,960 - src.llm.client - INFO - [dia:f15fe7] ğŸ“¡ Stream active (200)
2025-12-15 12:27:54,960 - src.llm.client - INFO - [dia:f15fe7] Starting stream parsing, waiting for first chunk...
2025-12-15 12:27:56,980 - src.llm.client - INFO - [dia:f15fe7] ğŸ“Š 2.0s: 217c @107c/s (68ch, ~54t @27t/s)
2025-12-15 12:27:58,988 - src.llm.client - INFO - [dia:f15fe7] ğŸ“Š 4.0s: 444c @110c/s (136ch, ~111t @28t/s)
2025-12-15 12:28:01,007 - src.llm.client - INFO - [dia:f15fe7] ğŸ“Š 6.0s: 627c @104c/s (204ch, ~157t @26t/s)
2025-12-15 12:28:03,016 - src.llm.client - INFO - [dia:f15fe7] ğŸ“Š 8.1s: 793c @98c/s (272ch, ~198t @25t/s)
2025-12-15 12:28:03,650 - src.llm.client - INFO - [dia:f15fe7] âœ“ Done 24.67s: 836c (~107w @34c/s)
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Optimal Inference (Precision Weighting & Attention):
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO -     - Length: 515 chars (cleaned: 515 chars)
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO - [OK] Elements: 32 total (nodes: 12, connections: 20) âœ“
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:28:03,650 - src.generate.formats.diagrams - INFO - Generated diagram: 515 characters
2025-12-15 12:28:03,651 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:28:03,651 - src.generate.formats.questions - INFO - Generating 10 questions for: Precision Weighting & Attention (Session 9)
2025-12-15 12:28:03,651 - src.llm.client - INFO - [qst:9c98c7] ğŸš€ qst | m=gemma3:4b | p=7338c | t=150s
2025-12-15 12:28:03,651 - src.llm.client - INFO - [qst:9c98c7] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:28:03,651 - src.llm.client - INFO - [qst:9c98c7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:28:03,652 - src.llm.client - INFO - [qst:9c98c7] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11041 bytes, prompt=7338 chars
2025-12-15 12:28:03,652 - src.llm.client - INFO - [qst:9c98c7] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:28:07,798 - src.llm.request_handler - INFO - [qst:9c98c7] âœ“ Done 4.15s
2025-12-15 12:28:07,798 - src.llm.client - INFO - [qst:9c98c7] âœ… HTTP 200 in 4.15s
2025-12-15 12:28:07,798 - src.llm.client - INFO - [qst:9c98c7] ğŸ“¡ Stream active (200)
2025-12-15 12:28:07,798 - src.llm.client - INFO - [qst:9c98c7] Starting stream parsing, waiting for first chunk...
2025-12-15 12:28:09,813 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 2.0s: 322c @160c/s (68ch, ~80t @40t/s)
2025-12-15 12:28:11,824 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 4.0s: 669c @166c/s (136ch, ~167t @42t/s)
2025-12-15 12:28:13,835 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 6.0s: 1021c @169c/s (204ch, ~255t @42t/s)
2025-12-15 12:28:15,846 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 8.0s: 1378c @171c/s (272ch, ~344t @43t/s)
2025-12-15 12:28:17,855 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 10.1s: 1710c @170c/s (340ch, ~428t @43t/s)
2025-12-15 12:28:19,873 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 12.1s: 2052c @170c/s (408ch, ~513t @42t/s)
2025-12-15 12:28:21,888 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 14.1s: 2389c @170c/s (476ch, ~597t @42t/s)
2025-12-15 12:28:23,906 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 16.1s: 2727c @169c/s (544ch, ~682t @42t/s)
2025-12-15 12:28:25,931 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 18.1s: 3084c @170c/s (612ch, ~771t @43t/s)
2025-12-15 12:28:27,952 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 20.2s: 3406c @169c/s (680ch, ~852t @42t/s)
2025-12-15 12:28:29,958 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 22.2s: 3735c @169c/s (747ch, ~934t @42t/s)
2025-12-15 12:28:31,985 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 24.2s: 4116c @170c/s (815ch, ~1029t @43t/s)
2025-12-15 12:28:34,000 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 26.2s: 4483c @171c/s (881ch, ~1121t @43t/s)
2025-12-15 12:28:36,008 - src.llm.client - INFO - [qst:9c98c7] ğŸ“Š 28.2s: 4827c @171c/s (948ch, ~1207t @43t/s)
2025-12-15 12:28:37,196 - src.llm.client - INFO - [qst:9c98c7] âœ“ Done 33.54s: 5079c (~703w @151c/s)
2025-12-15 12:28:37,196 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 0, 'total_fixes': 3}
2025-12-15 12:28:37,196 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-15 12:28:37,197 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:28:37,197 - src.generate.formats.questions - WARNING -     Context: Module 5 Session 9
2025-12-15 12:28:37,197 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:28:37,197 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:28:37,197 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:28:37,197 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Precision Weighting & Attention (Session 9)
2025-12-15 12:28:37,197 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:28:37,199 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:28:37,201 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 9 completed
2025-12-15 12:28:37,201 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:28:37,201 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:28:37,201 - src.generate.orchestration.pipeline - INFO - Module 6: Policy Selection & Planning (2 sessions)
2025-12-15 12:28:37,201 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:28:37,201 - src.generate.orchestration.pipeline - INFO - 
[10/20] Session 10: Optimal Control Theory
2025-12-15 12:28:37,202 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:28:37,202 - src.generate.formats.lectures - INFO - Generating lecture for: Policy Selection & Planning (Session 10/20)
2025-12-15 12:28:37,202 - src.llm.client - INFO - [lec:f3adf7] ğŸš€ lec | m=gemma3:4b | p=3052c | t=180s
2025-12-15 12:28:37,202 - src.llm.client - INFO - [lec:f3adf7] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:28:37,202 - src.llm.client - INFO - [lec:f3adf7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:28:37,203 - src.llm.client - INFO - [lec:f3adf7] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6683 bytes, prompt=3052 chars
2025-12-15 12:28:37,203 - src.llm.client - INFO - [lec:f3adf7] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:28:39,199 - src.llm.request_handler - INFO - [lec:f3adf7] âœ“ Done 2.00s
2025-12-15 12:28:39,199 - src.llm.client - INFO - [lec:f3adf7] âœ… HTTP 200 in 2.00s
2025-12-15 12:28:39,199 - src.llm.client - INFO - [lec:f3adf7] ğŸ“¡ Stream active (200)
2025-12-15 12:28:39,199 - src.llm.client - INFO - [lec:f3adf7] Starting stream parsing, waiting for first chunk...
2025-12-15 12:28:41,208 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 2.0s: 440c @219c/s (69ch, ~110t @55t/s)
2025-12-15 12:28:43,218 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 4.0s: 821c @204c/s (138ch, ~205t @51t/s)
2025-12-15 12:28:45,233 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 6.0s: 1167c @193c/s (207ch, ~292t @48t/s)
2025-12-15 12:28:47,259 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 8.1s: 1404c @174c/s (276ch, ~351t @44t/s)
2025-12-15 12:28:49,287 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 10.1s: 1692c @168c/s (345ch, ~423t @42t/s)
2025-12-15 12:28:51,289 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 12.1s: 2064c @171c/s (413ch, ~516t @43t/s)
2025-12-15 12:28:53,317 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 14.1s: 2401c @170c/s (482ch, ~600t @43t/s)
2025-12-15 12:28:55,321 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 16.1s: 2628c @163c/s (550ch, ~657t @41t/s)
2025-12-15 12:28:57,331 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 18.1s: 2979c @164c/s (618ch, ~745t @41t/s)
2025-12-15 12:28:59,336 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 20.1s: 3257c @162c/s (686ch, ~814t @40t/s)
2025-12-15 12:29:01,349 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 22.1s: 3552c @160c/s (754ch, ~888t @40t/s)
2025-12-15 12:29:03,357 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 24.2s: 3918c @162c/s (822ch, ~980t @41t/s)
2025-12-15 12:29:05,365 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 26.2s: 4259c @163c/s (890ch, ~1065t @41t/s)
2025-12-15 12:29:07,380 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 28.2s: 4577c @162c/s (958ch, ~1144t @41t/s)
2025-12-15 12:29:09,391 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 30.2s: 4935c @163c/s (1026ch, ~1234t @41t/s)
2025-12-15 12:29:11,405 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 32.2s: 5233c @162c/s (1094ch, ~1308t @41t/s)
2025-12-15 12:29:13,416 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 34.2s: 5486c @160c/s (1162ch, ~1372t @40t/s)
2025-12-15 12:29:15,428 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 36.2s: 5854c @162c/s (1230ch, ~1464t @40t/s)
2025-12-15 12:29:17,449 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 38.2s: 6161c @161c/s (1298ch, ~1540t @40t/s)
2025-12-15 12:29:19,470 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 40.3s: 6472c @161c/s (1366ch, ~1618t @40t/s)
2025-12-15 12:29:21,493 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 42.3s: 6903c @163c/s (1434ch, ~1726t @41t/s)
2025-12-15 12:29:23,521 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 44.3s: 7266c @164c/s (1502ch, ~1816t @41t/s)
2025-12-15 12:29:25,546 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 46.3s: 7685c @166c/s (1570ch, ~1921t @41t/s)
2025-12-15 12:29:27,547 - src.llm.client - INFO - [lec:f3adf7] ğŸ“Š 48.3s: 8063c @167c/s (1637ch, ~2016t @42t/s)
2025-12-15 12:29:29,422 - src.llm.client - INFO - [lec:f3adf7] âœ“ Done 52.22s: 8401c (~1282w @161c/s)
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO -     - Length: 8491 chars, 1295 words
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO -     - Content: 14 examples, 2 terms defined
2025-12-15 12:29:29,423 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 12:29:29,423 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-15 12:29:29,426 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:29:29,427 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:29:29,427 - src.generate.formats.labs - INFO - Generating lab 10 for: Policy Selection & Planning (Session 10)
2025-12-15 12:29:29,427 - src.llm.client - INFO - [lab:095d5a] ğŸš€ lab | m=gemma3:4b | p=3328c | t=150s
2025-12-15 12:29:29,427 - src.llm.client - INFO - [lab:095d5a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:29:29,427 - src.llm.client - INFO - [lab:095d5a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:29:29,428 - src.llm.client - INFO - [lab:095d5a] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3794 bytes, prompt=3328 chars
2025-12-15 12:29:29,428 - src.llm.client - INFO - [lab:095d5a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:29:31,293 - src.llm.request_handler - INFO - [lab:095d5a] âœ“ Done 1.86s
2025-12-15 12:29:31,293 - src.llm.client - INFO - [lab:095d5a] âœ… HTTP 200 in 1.86s
2025-12-15 12:29:31,293 - src.llm.client - INFO - [lab:095d5a] ğŸ“¡ Stream active (200)
2025-12-15 12:29:31,293 - src.llm.client - INFO - [lab:095d5a] Starting stream parsing, waiting for first chunk...
2025-12-15 12:29:33,305 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 2.0s: 309c @154c/s (71ch, ~77t @38t/s)
2025-12-15 12:29:35,308 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 4.0s: 730c @182c/s (140ch, ~182t @45t/s)
2025-12-15 12:29:37,322 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 6.0s: 1064c @176c/s (209ch, ~266t @44t/s)
2025-12-15 12:29:39,339 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 8.0s: 1261c @157c/s (278ch, ~315t @39t/s)
2025-12-15 12:29:41,366 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 10.1s: 1498c @149c/s (347ch, ~374t @37t/s)
2025-12-15 12:29:43,377 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 12.1s: 1771c @147c/s (415ch, ~443t @37t/s)
2025-12-15 12:29:45,404 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 14.1s: 2070c @147c/s (484ch, ~518t @37t/s)
2025-12-15 12:29:47,405 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 16.1s: 2343c @145c/s (552ch, ~586t @36t/s)
2025-12-15 12:29:49,408 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 18.1s: 2593c @143c/s (620ch, ~648t @36t/s)
2025-12-15 12:29:51,414 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 20.1s: 2814c @140c/s (688ch, ~704t @35t/s)
2025-12-15 12:29:53,416 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 22.1s: 3102c @140c/s (756ch, ~776t @35t/s)
2025-12-15 12:29:55,420 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 24.1s: 3315c @137c/s (824ch, ~829t @34t/s)
2025-12-15 12:29:57,427 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 26.1s: 3446c @132c/s (892ch, ~862t @33t/s)
2025-12-15 12:29:59,435 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 28.1s: 3620c @129c/s (960ch, ~905t @32t/s)
2025-12-15 12:30:01,454 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 30.2s: 3948c @131c/s (1028ch, ~987t @33t/s)
2025-12-15 12:30:03,466 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 32.2s: 4216c @131c/s (1096ch, ~1054t @33t/s)
2025-12-15 12:30:05,477 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 34.2s: 4559c @133c/s (1164ch, ~1140t @33t/s)
2025-12-15 12:30:07,495 - src.llm.client - INFO - [lab:095d5a] ğŸ“Š 36.2s: 4923c @136c/s (1232ch, ~1231t @34t/s)
2025-12-15 12:30:08,431 - src.llm.client - INFO - [lab:095d5a] âœ“ Done 39.00s: 5069c (~803w @130c/s)
2025-12-15 12:30:08,431 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:30:08,431 - src.generate.formats.labs - INFO -     - Length: 5160 chars, 818 words
2025-12-15 12:30:08,431 - src.generate.formats.labs - INFO -     - Procedure: 7 steps
2025-12-15 12:30:08,431 - src.generate.formats.labs - INFO -     - Safety: 5 warnings
2025-12-15 12:30:08,431 - src.generate.formats.labs - INFO -     - Data tables: 9
2025-12-15 12:30:08,434 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:30:08,434 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:30:08,434 - src.generate.formats.study_notes - INFO - Generating study notes for: Policy Selection & Planning (Session 10)
2025-12-15 12:30:08,434 - src.llm.client - INFO - [stu:663514] ğŸš€ stu | m=gemma3:4b | p=4443c | t=120s
2025-12-15 12:30:08,435 - src.llm.client - INFO - [stu:663514] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:30:08,435 - src.llm.client - INFO - [stu:663514] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:30:08,436 - src.llm.client - INFO - [stu:663514] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8124 bytes, prompt=4443 chars
2025-12-15 12:30:08,436 - src.llm.client - INFO - [stu:663514] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:30:11,035 - src.llm.request_handler - INFO - [stu:663514] âœ“ Done 2.60s
2025-12-15 12:30:11,035 - src.llm.client - INFO - [stu:663514] âœ… HTTP 200 in 2.60s
2025-12-15 12:30:11,035 - src.llm.client - INFO - [stu:663514] ğŸ“¡ Stream active (200)
2025-12-15 12:30:11,035 - src.llm.client - INFO - [stu:663514] Starting stream parsing, waiting for first chunk...
2025-12-15 12:30:13,045 - src.llm.client - INFO - [stu:663514] ğŸ“Š 2.0s: 419c @209c/s (68ch, ~105t @52t/s)
2025-12-15 12:30:15,050 - src.llm.client - INFO - [stu:663514] ğŸ“Š 4.0s: 816c @203c/s (136ch, ~204t @51t/s)
2025-12-15 12:30:17,061 - src.llm.client - INFO - [stu:663514] ğŸ“Š 6.0s: 1151c @191c/s (204ch, ~288t @48t/s)
2025-12-15 12:30:19,074 - src.llm.client - INFO - [stu:663514] ğŸ“Š 8.0s: 1422c @177c/s (272ch, ~356t @44t/s)
2025-12-15 12:30:21,086 - src.llm.client - INFO - [stu:663514] ğŸ“Š 10.1s: 1716c @171c/s (340ch, ~429t @43t/s)
2025-12-15 12:30:23,096 - src.llm.client - INFO - [stu:663514] ğŸ“Š 12.1s: 2068c @171c/s (408ch, ~517t @43t/s)
2025-12-15 12:30:25,111 - src.llm.client - INFO - [stu:663514] ğŸ“Š 14.1s: 2404c @171c/s (476ch, ~601t @43t/s)
2025-12-15 12:30:27,129 - src.llm.client - INFO - [stu:663514] ğŸ“Š 16.1s: 2718c @169c/s (544ch, ~680t @42t/s)
2025-12-15 12:30:29,146 - src.llm.client - INFO - [stu:663514] ğŸ“Š 18.1s: 3100c @171c/s (612ch, ~775t @43t/s)
2025-12-15 12:30:31,161 - src.llm.client - INFO - [stu:663514] ğŸ“Š 20.1s: 3455c @172c/s (680ch, ~864t @43t/s)
2025-12-15 12:30:32,777 - src.llm.client - INFO - [stu:663514] âœ“ Done 24.34s: 3798c (~544w @156c/s)
2025-12-15 12:30:32,778 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:30:32,778 - src.generate.formats.study_notes - INFO -     - Length: 3860 chars, 555 words
2025-12-15 12:30:32,778 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:30:32,778 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-15 12:30:32,778 - src.generate.formats.study_notes - INFO -     - Structure: 3 sections, 4 bullets
2025-12-15 12:30:32,778 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:30:32,779 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:30:32,779 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:30:32,780 - src.generate.formats.diagrams - INFO - Generating diagram for: Hamiltonian Formalism (Policy Selection & Planning)
2025-12-15 12:30:32,780 - src.generate.formats.diagrams - INFO - Generating diagram for: Cost Functions (Policy Selection & Planning)
2025-12-15 12:30:32,780 - src.llm.client - INFO - [dia:e20489] ğŸš€ dia | m=gemma3:4b | p=5744c | t=120s
2025-12-15 12:30:32,780 - src.llm.client - INFO - [dia:e20489] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:30:32,780 - src.llm.client - INFO - [dia:e20489] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:30:32,780 - src.llm.client - INFO - [dia:1d659f] ğŸš€ dia | m=gemma3:4b | p=5758c | t=120s
2025-12-15 12:30:32,780 - src.llm.client - INFO - [dia:1d659f] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:30:32,780 - src.llm.client - INFO - [dia:1d659f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:30:32,782 - src.llm.client - INFO - [dia:e20489] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11053 bytes, prompt=5744 chars
2025-12-15 12:30:32,782 - src.llm.client - INFO - [dia:1d659f] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11067 bytes, prompt=5758 chars
2025-12-15 12:30:32,782 - src.llm.client - INFO - [dia:1d659f] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:30:32,782 - src.llm.client - INFO - [dia:e20489] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:30:36,640 - src.llm.request_handler - INFO - [dia:e20489] âœ“ Done 3.86s
2025-12-15 12:30:36,640 - src.llm.client - INFO - [dia:e20489] âœ… HTTP 200 in 3.86s
2025-12-15 12:30:36,640 - src.llm.client - INFO - [dia:e20489] ğŸ“¡ Stream active (200)
2025-12-15 12:30:36,640 - src.llm.client - INFO - [dia:e20489] Starting stream parsing, waiting for first chunk...
2025-12-15 12:30:38,662 - src.llm.client - INFO - [dia:e20489] ğŸ“Š 2.0s: 245c @121c/s (68ch, ~61t @30t/s)
2025-12-15 12:30:40,675 - src.llm.client - INFO - [dia:e20489] ğŸ“Š 4.0s: 518c @128c/s (136ch, ~130t @32t/s)
2025-12-15 12:30:42,683 - src.llm.client - INFO - [dia:e20489] ğŸ“Š 6.0s: 780c @129c/s (204ch, ~195t @32t/s)
2025-12-15 12:30:44,689 - src.llm.client - INFO - [dia:e20489] ğŸ“Š 8.0s: 972c @121c/s (272ch, ~243t @30t/s)
2025-12-15 12:30:46,718 - src.llm.client - INFO - [dia:e20489] ğŸ“Š 10.1s: 1117c @111c/s (339ch, ~279t @28t/s)
2025-12-15 12:30:46,718 - src.llm.client - INFO - [dia:e20489] âœ“ Done 13.94s: 1117c (~167w @80c/s)
2025-12-15 12:30:46,718 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Cost Functions (Policy Selection & Planning):
2025-12-15 12:30:46,718 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:30:46,718 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:30:46,718 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:30:46,719 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:30:46,719 - src.generate.formats.diagrams - INFO -     - Length: 1068 chars (cleaned: 1068 chars)
2025-12-15 12:30:46,719 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:30:46,719 - src.generate.formats.diagrams - INFO - [OK] Elements: 70 total (nodes: 26, connections: 44) âœ“
2025-12-15 12:30:46,719 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:30:46,719 - src.generate.formats.diagrams - INFO - Generated diagram: 1068 characters
2025-12-15 12:30:50,013 - src.llm.request_handler - INFO - [dia:1d659f] âœ“ Done 17.23s
2025-12-15 12:30:50,013 - src.llm.client - INFO - [dia:1d659f] âœ… HTTP 200 in 17.23s
2025-12-15 12:30:50,013 - src.llm.client - INFO - [dia:1d659f] ğŸ“¡ Stream active (200)
2025-12-15 12:30:50,013 - src.llm.client - INFO - [dia:1d659f] Starting stream parsing, waiting for first chunk...
2025-12-15 12:30:52,031 - src.llm.client - INFO - [dia:1d659f] ğŸ“Š 2.0s: 259c @128c/s (65ch, ~65t @32t/s)
2025-12-15 12:30:54,050 - src.llm.client - INFO - [dia:1d659f] ğŸ“Š 4.0s: 512c @127c/s (133ch, ~128t @32t/s)
2025-12-15 12:30:56,065 - src.llm.client - INFO - [dia:1d659f] ğŸ“Š 6.1s: 717c @118c/s (201ch, ~179t @30t/s)
2025-12-15 12:30:57,083 - src.llm.client - INFO - [dia:1d659f] âœ“ Done 24.30s: 807c (~121w @33c/s)
2025-12-15 12:30:57,083 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Hamiltonian Formalism (Policy Selection & Planning):
2025-12-15 12:30:57,083 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:30:57,083 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:30:57,083 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:30:57,084 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:30:57,084 - src.generate.formats.diagrams - INFO -     - Length: 758 chars (cleaned: 758 chars)
2025-12-15 12:30:57,084 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:30:57,084 - src.generate.formats.diagrams - INFO - [OK] Elements: 48 total (nodes: 15, connections: 33) âœ“
2025-12-15 12:30:57,084 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:30:57,084 - src.generate.formats.diagrams - INFO - Generated diagram: 758 characters
2025-12-15 12:30:57,084 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:30:57,084 - src.generate.formats.questions - INFO - Generating 10 questions for: Policy Selection & Planning (Session 10)
2025-12-15 12:30:57,084 - src.llm.client - INFO - [qst:c972ee] ğŸš€ qst | m=gemma3:4b | p=7332c | t=150s
2025-12-15 12:30:57,084 - src.llm.client - INFO - [qst:c972ee] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:30:57,084 - src.llm.client - INFO - [qst:c972ee] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:30:57,086 - src.llm.client - INFO - [qst:c972ee] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11071 bytes, prompt=7332 chars
2025-12-15 12:30:57,086 - src.llm.client - INFO - [qst:c972ee] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:31:01,310 - src.llm.request_handler - INFO - [qst:c972ee] âœ“ Done 4.22s
2025-12-15 12:31:01,310 - src.llm.client - INFO - [qst:c972ee] âœ… HTTP 200 in 4.22s
2025-12-15 12:31:01,310 - src.llm.client - INFO - [qst:c972ee] ğŸ“¡ Stream active (200)
2025-12-15 12:31:01,310 - src.llm.client - INFO - [qst:c972ee] Starting stream parsing, waiting for first chunk...
2025-12-15 12:31:03,331 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 2.0s: 315c @156c/s (68ch, ~79t @39t/s)
2025-12-15 12:31:05,347 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 4.0s: 646c @160c/s (136ch, ~162t @40t/s)
2025-12-15 12:31:07,368 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 6.1s: 974c @161c/s (204ch, ~244t @40t/s)
2025-12-15 12:31:09,383 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 8.1s: 1283c @159c/s (272ch, ~321t @40t/s)
2025-12-15 12:31:11,394 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 10.1s: 1634c @162c/s (340ch, ~408t @41t/s)
2025-12-15 12:31:13,408 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 12.1s: 1938c @160c/s (408ch, ~484t @40t/s)
2025-12-15 12:31:15,419 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 14.1s: 2225c @158c/s (476ch, ~556t @39t/s)
2025-12-15 12:31:17,438 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 16.1s: 2579c @160c/s (544ch, ~645t @40t/s)
2025-12-15 12:31:19,458 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 18.1s: 2883c @159c/s (612ch, ~721t @40t/s)
2025-12-15 12:31:21,485 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 20.2s: 3249c @161c/s (680ch, ~812t @40t/s)
2025-12-15 12:31:23,487 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 22.2s: 3606c @163c/s (747ch, ~902t @41t/s)
2025-12-15 12:31:25,516 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 24.2s: 3975c @164c/s (815ch, ~994t @41t/s)
2025-12-15 12:31:27,524 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 26.2s: 4357c @166c/s (882ch, ~1089t @42t/s)
2025-12-15 12:31:29,526 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 28.2s: 4686c @166c/s (949ch, ~1172t @42t/s)
2025-12-15 12:31:31,530 - src.llm.client - INFO - [qst:c972ee] ğŸ“Š 30.2s: 5051c @167c/s (1016ch, ~1263t @42t/s)
2025-12-15 12:31:32,569 - src.llm.client - INFO - [qst:c972ee] âœ“ Done 35.48s: 5253c (~751w @148c/s)
2025-12-15 12:31:32,569 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 5 question format issues: {'format_standardized': 0, 'question_marks_added': 4, 'mc_options_fixed': 1, 'total_fixes': 5}
2025-12-15 12:31:32,569 - src.generate.formats.questions - INFO - Applied 5 auto-fixes to questions
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING -     Context: Module 6 Session 10
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING -     Context: Module 6 Session 10
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:31:32,570 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:31:32,571 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:31:32,571 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Policy Selection & Planning (Session 10)
2025-12-15 12:31:32,571 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:31:32,573 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:31:32,575 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 10 completed
2025-12-15 12:31:32,575 - src.generate.orchestration.pipeline - INFO - 
[11/20] Session 11: Reinforcement Learning Link
2025-12-15 12:31:32,575 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:31:32,575 - src.generate.formats.lectures - INFO - Generating lecture for: Policy Selection & Planning (Session 11/20)
2025-12-15 12:31:32,575 - src.llm.client - INFO - [lec:6db829] ğŸš€ lec | m=gemma3:4b | p=3067c | t=180s
2025-12-15 12:31:32,575 - src.llm.client - INFO - [lec:6db829] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:31:32,575 - src.llm.client - INFO - [lec:6db829] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:31:32,576 - src.llm.client - INFO - [lec:6db829] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6698 bytes, prompt=3067 chars
2025-12-15 12:31:32,576 - src.llm.client - INFO - [lec:6db829] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:31:34,563 - src.llm.request_handler - INFO - [lec:6db829] âœ“ Done 1.99s
2025-12-15 12:31:34,563 - src.llm.client - INFO - [lec:6db829] âœ… HTTP 200 in 1.99s
2025-12-15 12:31:34,563 - src.llm.client - INFO - [lec:6db829] ğŸ“¡ Stream active (200)
2025-12-15 12:31:34,563 - src.llm.client - INFO - [lec:6db829] Starting stream parsing, waiting for first chunk...
2025-12-15 12:31:36,575 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 2.0s: 417c @207c/s (69ch, ~104t @52t/s)
2025-12-15 12:31:38,590 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 4.0s: 775c @192c/s (138ch, ~194t @48t/s)
2025-12-15 12:31:40,606 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 6.0s: 1165c @193c/s (207ch, ~291t @48t/s)
2025-12-15 12:31:42,632 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 8.1s: 1511c @187c/s (276ch, ~378t @47t/s)
2025-12-15 12:31:44,661 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 10.1s: 1811c @179c/s (345ch, ~453t @45t/s)
2025-12-15 12:31:46,661 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 12.1s: 2057c @170c/s (413ch, ~514t @43t/s)
2025-12-15 12:31:48,665 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 14.1s: 2368c @168c/s (481ch, ~592t @42t/s)
2025-12-15 12:31:50,665 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 16.1s: 2696c @167c/s (549ch, ~674t @42t/s)
2025-12-15 12:31:52,668 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 18.1s: 3045c @168c/s (616ch, ~761t @42t/s)
2025-12-15 12:31:54,673 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 20.1s: 3417c @170c/s (684ch, ~854t @42t/s)
2025-12-15 12:31:56,677 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 22.1s: 3651c @165c/s (752ch, ~913t @41t/s)
2025-12-15 12:31:58,684 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 24.1s: 3931c @163c/s (820ch, ~983t @41t/s)
2025-12-15 12:32:00,696 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 26.1s: 4116c @158c/s (888ch, ~1029t @39t/s)
2025-12-15 12:32:02,710 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 28.1s: 4418c @157c/s (956ch, ~1104t @39t/s)
2025-12-15 12:32:04,721 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 30.2s: 4792c @159c/s (1024ch, ~1198t @40t/s)
2025-12-15 12:32:06,732 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 32.2s: 5137c @160c/s (1092ch, ~1284t @40t/s)
2025-12-15 12:32:08,755 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 34.2s: 5500c @161c/s (1160ch, ~1375t @40t/s)
2025-12-15 12:32:10,767 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 36.2s: 5865c @162c/s (1228ch, ~1466t @41t/s)
2025-12-15 12:32:12,769 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 38.2s: 6241c @163c/s (1295ch, ~1560t @41t/s)
2025-12-15 12:32:14,791 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 40.2s: 6604c @164c/s (1363ch, ~1651t @41t/s)
2025-12-15 12:32:16,812 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 42.2s: 6921c @164c/s (1431ch, ~1730t @41t/s)
2025-12-15 12:32:18,834 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 44.3s: 7274c @164c/s (1499ch, ~1818t @41t/s)
2025-12-15 12:32:20,854 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 46.3s: 7676c @166c/s (1567ch, ~1919t @41t/s)
2025-12-15 12:32:22,883 - src.llm.client - INFO - [lec:6db829] ğŸ“Š 48.3s: 8065c @167c/s (1635ch, ~2016t @42t/s)
2025-12-15 12:32:23,400 - src.llm.client - INFO - [lec:6db829] âœ“ Done 50.83s: 8162c (~1268w @161c/s)
2025-12-15 12:32:23,402 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:32:23,402 - src.generate.formats.lectures - INFO -     - Length: 8239 chars, 1281 words
2025-12-15 12:32:23,402 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:32:23,402 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:32:23,402 - src.generate.formats.lectures - INFO -     - Content: 11 examples, 0 terms defined
2025-12-15 12:32:23,402 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:32:23,406 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:32:23,406 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:32:23,406 - src.generate.formats.labs - INFO - Generating lab 11 for: Policy Selection & Planning (Session 11)
2025-12-15 12:32:23,406 - src.llm.client - INFO - [lab:682c38] ğŸš€ lab | m=gemma3:4b | p=3320c | t=150s
2025-12-15 12:32:23,406 - src.llm.client - INFO - [lab:682c38] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:32:23,406 - src.llm.client - INFO - [lab:682c38] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:32:23,407 - src.llm.client - INFO - [lab:682c38] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3777 bytes, prompt=3320 chars
2025-12-15 12:32:23,407 - src.llm.client - INFO - [lab:682c38] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:32:47,907 - src.llm.request_handler - INFO - [lab:682c38] âœ“ Done 24.50s
2025-12-15 12:32:47,907 - src.llm.client - INFO - [lab:682c38] âœ… HTTP 200 in 24.50s
2025-12-15 12:32:47,907 - src.llm.client - INFO - [lab:682c38] ğŸ“¡ Stream active (200)
2025-12-15 12:32:47,908 - src.llm.client - INFO - [lab:682c38] Starting stream parsing, waiting for first chunk...
2025-12-15 12:32:49,936 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 2.0s: 350c @173c/s (70ch, ~88t @43t/s)
2025-12-15 12:32:51,939 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 4.0s: 757c @188c/s (139ch, ~189t @47t/s)
2025-12-15 12:32:53,944 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 6.0s: 1095c @181c/s (207ch, ~274t @45t/s)
2025-12-15 12:32:55,960 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 8.1s: 1389c @172c/s (276ch, ~347t @43t/s)
2025-12-15 12:32:57,987 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 10.1s: 1660c @165c/s (345ch, ~415t @41t/s)
2025-12-15 12:33:00,012 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 12.1s: 2013c @166c/s (414ch, ~503t @42t/s)
2025-12-15 12:33:02,020 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 14.1s: 2366c @168c/s (482ch, ~592t @42t/s)
2025-12-15 12:33:04,020 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 16.1s: 2637c @164c/s (550ch, ~659t @41t/s)
2025-12-15 12:33:06,048 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 18.1s: 2928c @161c/s (619ch, ~732t @40t/s)
2025-12-15 12:33:08,055 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 20.1s: 3181c @158c/s (687ch, ~795t @39t/s)
2025-12-15 12:33:10,057 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 22.1s: 3525c @159c/s (755ch, ~881t @40t/s)
2025-12-15 12:33:12,060 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 24.2s: 3834c @159c/s (823ch, ~958t @40t/s)
2025-12-15 12:33:14,064 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 26.2s: 4163c @159c/s (891ch, ~1041t @40t/s)
2025-12-15 12:33:16,070 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 28.2s: 4450c @158c/s (959ch, ~1112t @40t/s)
2025-12-15 12:33:18,079 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 30.2s: 4800c @159c/s (1027ch, ~1200t @40t/s)
2025-12-15 12:33:20,088 - src.llm.client - INFO - [lab:682c38] ğŸ“Š 32.2s: 5185c @161c/s (1095ch, ~1296t @40t/s)
2025-12-15 12:33:21,523 - src.llm.client - INFO - [lab:682c38] âœ“ Done 58.12s: 5475c (~787w @94c/s)
2025-12-15 12:33:21,523 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:33:21,523 - src.generate.formats.labs - INFO -     - Length: 5571 chars, 802 words
2025-12-15 12:33:21,523 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-15 12:33:21,523 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 12:33:21,523 - src.generate.formats.labs - INFO -     - Data tables: 6
2025-12-15 12:33:21,526 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:33:21,526 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:33:21,526 - src.generate.formats.study_notes - INFO - Generating study notes for: Policy Selection & Planning (Session 11)
2025-12-15 12:33:21,526 - src.llm.client - INFO - [stu:e53579] ğŸš€ stu | m=gemma3:4b | p=4442c | t=120s
2025-12-15 12:33:21,526 - src.llm.client - INFO - [stu:e53579] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:33:21,526 - src.llm.client - INFO - [stu:e53579] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:33:21,528 - src.llm.client - INFO - [stu:e53579] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8114 bytes, prompt=4442 chars
2025-12-15 12:33:21,528 - src.llm.client - INFO - [stu:e53579] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:33:24,130 - src.llm.request_handler - INFO - [stu:e53579] âœ“ Done 2.60s
2025-12-15 12:33:24,131 - src.llm.client - INFO - [stu:e53579] âœ… HTTP 200 in 2.60s
2025-12-15 12:33:24,131 - src.llm.client - INFO - [stu:e53579] ğŸ“¡ Stream active (200)
2025-12-15 12:33:24,131 - src.llm.client - INFO - [stu:e53579] Starting stream parsing, waiting for first chunk...
2025-12-15 12:33:26,137 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 2.0s: 403c @201c/s (68ch, ~101t @50t/s)
2025-12-15 12:33:28,142 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 4.0s: 772c @192c/s (136ch, ~193t @48t/s)
2025-12-15 12:33:30,143 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 6.0s: 1112c @185c/s (204ch, ~278t @46t/s)
2025-12-15 12:33:32,148 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 8.0s: 1465c @183c/s (272ch, ~366t @46t/s)
2025-12-15 12:33:34,156 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 10.0s: 1777c @177c/s (339ch, ~444t @44t/s)
2025-12-15 12:33:36,159 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 12.0s: 2152c @179c/s (407ch, ~538t @45t/s)
2025-12-15 12:33:38,170 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 14.0s: 2541c @181c/s (475ch, ~635t @45t/s)
2025-12-15 12:33:40,175 - src.llm.client - INFO - [stu:e53579] ğŸ“Š 16.0s: 2910c @181c/s (543ch, ~728t @45t/s)
2025-12-15 12:33:41,361 - src.llm.client - INFO - [stu:e53579] âœ“ Done 19.83s: 3104c (~444w @156c/s)
2025-12-15 12:33:41,362 - src.generate.formats.study_notes - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:33:41,362 - src.generate.formats.study_notes - WARNING -     [CRITICAL] Issue 1: Only 2 key concepts highlighted (require 3-10, need 1 more - format concepts as **Concept Name:** in bullet points)
2025-12-15 12:33:41,362 - src.generate.formats.study_notes - WARNING -   Retry attempt 1/1 for study notes: Policy Selection & Planning (Session 11)
2025-12-15 12:33:41,362 - src.generate.formats.study_notes - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:33:41,363 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:33:41,363 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:33:41,364 - src.generate.formats.diagrams - INFO - Generating diagram for: Policy Optimization (Policy Selection & Planning)
2025-12-15 12:33:41,364 - src.generate.formats.diagrams - INFO - Generating diagram for: Reward Functions (Policy Selection & Planning)
2025-12-15 12:33:41,364 - src.llm.client - INFO - [dia:8f7ba9] ğŸš€ dia | m=gemma3:4b | p=5759c | t=120s
2025-12-15 12:33:41,364 - src.llm.client - INFO - [dia:45ae52] ğŸš€ dia | m=gemma3:4b | p=5753c | t=120s
2025-12-15 12:33:41,364 - src.llm.client - INFO - [dia:8f7ba9] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:33:41,365 - src.llm.client - INFO - [dia:8f7ba9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:33:41,365 - src.llm.client - INFO - [dia:45ae52] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:33:41,365 - src.llm.client - INFO - [dia:45ae52] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:33:41,367 - src.llm.client - INFO - [dia:8f7ba9] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11068 bytes, prompt=5759 chars
2025-12-15 12:33:41,367 - src.llm.client - INFO - [dia:45ae52] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11062 bytes, prompt=5753 chars
2025-12-15 12:33:41,367 - src.llm.client - INFO - [dia:8f7ba9] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:33:41,367 - src.llm.client - INFO - [dia:45ae52] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:33:44,938 - src.llm.request_handler - INFO - [dia:45ae52] âœ“ Done 3.57s
2025-12-15 12:33:44,939 - src.llm.client - INFO - [dia:45ae52] âœ… HTTP 200 in 3.57s
2025-12-15 12:33:44,939 - src.llm.client - INFO - [dia:45ae52] ğŸ“¡ Stream active (200)
2025-12-15 12:33:44,939 - src.llm.client - INFO - [dia:45ae52] Starting stream parsing, waiting for first chunk...
2025-12-15 12:33:46,960 - src.llm.client - INFO - [dia:45ae52] ğŸ“Š 2.0s: 239c @118c/s (68ch, ~60t @30t/s)
2025-12-15 12:33:48,966 - src.llm.client - INFO - [dia:45ae52] ğŸ“Š 4.0s: 452c @112c/s (136ch, ~113t @28t/s)
2025-12-15 12:33:50,973 - src.llm.client - INFO - [dia:45ae52] ğŸ“Š 6.0s: 625c @104c/s (204ch, ~156t @26t/s)
2025-12-15 12:33:52,983 - src.llm.client - INFO - [dia:45ae52] ğŸ“Š 8.0s: 804c @100c/s (272ch, ~201t @25t/s)
2025-12-15 12:33:54,926 - src.llm.client - INFO - [dia:45ae52] âœ“ Done 13.56s: 968c (~118w @71c/s)
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Reward Functions (Policy Selection & Planning):
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO -     - Length: 511 chars (cleaned: 511 chars)
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO - [OK] Elements: 34 total (nodes: 14, connections: 20) âœ“
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:33:54,926 - src.generate.formats.diagrams - INFO - Generated diagram: 511 characters
2025-12-15 12:33:58,218 - src.llm.request_handler - INFO - [dia:8f7ba9] âœ“ Done 16.85s
2025-12-15 12:33:58,218 - src.llm.client - INFO - [dia:8f7ba9] âœ… HTTP 200 in 16.85s
2025-12-15 12:33:58,218 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“¡ Stream active (200)
2025-12-15 12:33:58,218 - src.llm.client - INFO - [dia:8f7ba9] Starting stream parsing, waiting for first chunk...
2025-12-15 12:34:00,235 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 2.0s: 320c @159c/s (68ch, ~80t @40t/s)
2025-12-15 12:34:02,257 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 4.0s: 657c @163c/s (136ch, ~164t @41t/s)
2025-12-15 12:34:04,265 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 6.0s: 966c @160c/s (204ch, ~242t @40t/s)
2025-12-15 12:34:06,269 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 8.1s: 1278c @159c/s (272ch, ~320t @40t/s)
2025-12-15 12:34:08,276 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 10.1s: 1604c @159c/s (340ch, ~401t @40t/s)
2025-12-15 12:34:10,282 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 12.1s: 1921c @159c/s (408ch, ~480t @40t/s)
2025-12-15 12:34:12,291 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 14.1s: 2246c @160c/s (476ch, ~562t @40t/s)
2025-12-15 12:34:14,302 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 16.1s: 2570c @160c/s (544ch, ~642t @40t/s)
2025-12-15 12:34:16,313 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 18.1s: 2889c @160c/s (612ch, ~722t @40t/s)
2025-12-15 12:34:18,327 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 20.1s: 3214c @160c/s (680ch, ~804t @40t/s)
2025-12-15 12:34:20,347 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 22.1s: 3530c @160c/s (748ch, ~882t @40t/s)
2025-12-15 12:34:22,368 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 24.2s: 3856c @160c/s (816ch, ~964t @40t/s)
2025-12-15 12:34:24,394 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 26.2s: 4174c @159c/s (884ch, ~1044t @40t/s)
2025-12-15 12:34:26,417 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 28.2s: 4496c @159c/s (952ch, ~1124t @40t/s)
2025-12-15 12:34:28,419 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 30.2s: 4814c @159c/s (1019ch, ~1204t @40t/s)
2025-12-15 12:34:30,446 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 32.2s: 5138c @159c/s (1087ch, ~1284t @40t/s)
2025-12-15 12:34:32,454 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 34.2s: 5455c @159c/s (1154ch, ~1364t @40t/s)
2025-12-15 12:34:34,467 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 36.2s: 5769c @159c/s (1221ch, ~1442t @40t/s)
2025-12-15 12:34:36,469 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 38.3s: 6087c @159c/s (1288ch, ~1522t @40t/s)
2025-12-15 12:34:38,490 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 40.3s: 6405c @159c/s (1355ch, ~1601t @40t/s)
2025-12-15 12:34:40,495 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 42.3s: 6718c @159c/s (1422ch, ~1680t @40t/s)
2025-12-15 12:34:42,511 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 44.3s: 7039c @159c/s (1489ch, ~1760t @40t/s)
2025-12-15 12:34:44,532 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 46.3s: 7356c @159c/s (1556ch, ~1839t @40t/s)
2025-12-15 12:34:46,548 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 48.3s: 7670c @159c/s (1623ch, ~1918t @40t/s)
2025-12-15 12:34:48,567 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 50.3s: 7987c @159c/s (1690ch, ~1997t @40t/s)
2025-12-15 12:34:50,585 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 52.4s: 8305c @159c/s (1757ch, ~2076t @40t/s)
2025-12-15 12:34:52,612 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 54.4s: 8626c @159c/s (1824ch, ~2156t @40t/s)
2025-12-15 12:34:54,634 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 56.4s: 8936c @158c/s (1891ch, ~2234t @40t/s)
2025-12-15 12:34:56,652 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 58.4s: 9257c @158c/s (1958ch, ~2314t @40t/s)
2025-12-15 12:34:58,668 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 60.4s: 9574c @158c/s (2025ch, ~2394t @40t/s)
2025-12-15 12:35:00,691 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 62.5s: 9888c @158c/s (2092ch, ~2472t @40t/s)
2025-12-15 12:35:02,692 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 64.5s: 10201c @158c/s (2158ch, ~2550t @40t/s)
2025-12-15 12:35:04,715 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 66.5s: 10519c @158c/s (2225ch, ~2630t @40t/s)
2025-12-15 12:35:06,738 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 68.5s: 10840c @158c/s (2292ch, ~2710t @40t/s)
2025-12-15 12:35:08,766 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 70.5s: 11150c @158c/s (2359ch, ~2788t @40t/s)
2025-12-15 12:35:10,793 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 72.6s: 11471c @158c/s (2426ch, ~2868t @40t/s)
2025-12-15 12:35:12,822 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 74.6s: 11788c @158c/s (2493ch, ~2947t @40t/s)
2025-12-15 12:35:14,825 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 76.6s: 12098c @158c/s (2559ch, ~3024t @39t/s)
2025-12-15 12:35:16,853 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 78.6s: 12415c @158c/s (2626ch, ~3104t @39t/s)
2025-12-15 12:35:18,876 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 80.7s: 12733c @158c/s (2693ch, ~3183t @39t/s)
2025-12-15 12:35:20,881 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 82.7s: 13054c @158c/s (2760ch, ~3264t @39t/s)
2025-12-15 12:35:22,896 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 84.7s: 13364c @158c/s (2827ch, ~3341t @39t/s)
2025-12-15 12:35:24,906 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 86.7s: 13685c @158c/s (2894ch, ~3421t @39t/s)
2025-12-15 12:35:26,916 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 88.7s: 14002c @158c/s (2961ch, ~3500t @39t/s)
2025-12-15 12:35:28,924 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 90.7s: 14316c @158c/s (3028ch, ~3579t @39t/s)
2025-12-15 12:35:30,937 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 92.7s: 14638c @158c/s (3095ch, ~3660t @39t/s)
2025-12-15 12:35:32,959 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 94.7s: 14951c @158c/s (3162ch, ~3738t @39t/s)
2025-12-15 12:35:34,974 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 96.8s: 15269c @158c/s (3229ch, ~3817t @39t/s)
2025-12-15 12:35:36,990 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 98.8s: 15584c @158c/s (3296ch, ~3896t @39t/s)
2025-12-15 12:35:39,010 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 100.8s: 15903c @158c/s (3363ch, ~3976t @39t/s)
2025-12-15 12:35:41,028 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 102.8s: 16223c @158c/s (3430ch, ~4056t @39t/s)
2025-12-15 12:35:43,049 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 104.8s: 16538c @158c/s (3497ch, ~4134t @39t/s)
2025-12-15 12:35:45,070 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 106.9s: 16856c @158c/s (3564ch, ~4214t @39t/s)
2025-12-15 12:35:47,092 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 108.9s: 17172c @158c/s (3631ch, ~4293t @39t/s)
2025-12-15 12:35:49,119 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 110.9s: 17487c @158c/s (3698ch, ~4372t @39t/s)
2025-12-15 12:35:51,149 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 112.9s: 17807c @158c/s (3765ch, ~4452t @39t/s)
2025-12-15 12:35:53,157 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 114.9s: 18113c @158c/s (3830ch, ~4528t @39t/s)
2025-12-15 12:35:55,158 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 116.9s: 18426c @158c/s (3896ch, ~4606t @39t/s)
2025-12-15 12:35:57,164 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 118.9s: 18739c @158c/s (3962ch, ~4685t @39t/s)
2025-12-15 12:35:59,168 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 120.9s: 19052c @158c/s (4028ch, ~4763t @39t/s)
2025-12-15 12:36:01,173 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 123.0s: 19359c @157c/s (4094ch, ~4840t @39t/s)
2025-12-15 12:36:03,183 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 125.0s: 19672c @157c/s (4160ch, ~4918t @39t/s)
2025-12-15 12:36:05,188 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 127.0s: 19986c @157c/s (4226ch, ~4996t @39t/s)
2025-12-15 12:36:07,195 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 129.0s: 20300c @157c/s (4292ch, ~5075t @39t/s)
2025-12-15 12:36:09,201 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 131.0s: 20609c @157c/s (4358ch, ~5152t @39t/s)
2025-12-15 12:36:11,206 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 133.0s: 20919c @157c/s (4423ch, ~5230t @39t/s)
2025-12-15 12:36:13,215 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 135.0s: 21231c @157c/s (4489ch, ~5308t @39t/s)
2025-12-15 12:36:15,222 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 137.0s: 21542c @157c/s (4555ch, ~5386t @39t/s)
2025-12-15 12:36:17,232 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 139.0s: 21851c @157c/s (4621ch, ~5463t @39t/s)
2025-12-15 12:36:19,260 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 141.0s: 22168c @157c/s (4687ch, ~5542t @39t/s)
2025-12-15 12:36:21,279 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 143.1s: 22489c @157c/s (4755ch, ~5622t @39t/s)
2025-12-15 12:36:22,230 - src.llm.client - INFO - [dia:8f7ba9] Stream making progress - extending timeout by 60.0s (new limit: 240.0s, max: 240.0s)
2025-12-15 12:36:23,303 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 145.1s: 22811c @157c/s (4823ch, ~5703t @39t/s)
2025-12-15 12:36:25,325 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 147.1s: 23133c @157c/s (4891ch, ~5783t @39t/s)
2025-12-15 12:36:27,352 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 149.1s: 23450c @157c/s (4959ch, ~5862t @39t/s)
2025-12-15 12:36:29,355 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 151.1s: 23771c @157c/s (5026ch, ~5943t @39t/s)
2025-12-15 12:36:31,359 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 153.1s: 24088c @157c/s (5093ch, ~6022t @39t/s)
2025-12-15 12:36:33,367 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 155.1s: 24402c @157c/s (5160ch, ~6100t @39t/s)
2025-12-15 12:36:35,376 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 157.2s: 24724c @157c/s (5227ch, ~6181t @39t/s)
2025-12-15 12:36:37,393 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 159.2s: 25037c @157c/s (5294ch, ~6259t @39t/s)
2025-12-15 12:36:39,415 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 161.2s: 25355c @157c/s (5361ch, ~6339t @39t/s)
2025-12-15 12:36:41,436 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 163.2s: 25670c @157c/s (5428ch, ~6418t @39t/s)
2025-12-15 12:36:43,455 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 165.2s: 25989c @157c/s (5495ch, ~6497t @39t/s)
2025-12-15 12:36:45,478 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 167.3s: 26309c @157c/s (5562ch, ~6577t @39t/s)
2025-12-15 12:36:47,500 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 169.3s: 26624c @157c/s (5629ch, ~6656t @39t/s)
2025-12-15 12:36:49,507 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 171.3s: 26938c @157c/s (5695ch, ~6734t @39t/s)
2025-12-15 12:36:51,509 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 173.3s: 27247c @157c/s (5761ch, ~6812t @39t/s)
2025-12-15 12:36:53,529 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 175.3s: 27561c @157c/s (5827ch, ~6890t @39t/s)
2025-12-15 12:36:55,556 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 177.3s: 27874c @157c/s (5894ch, ~6968t @39t/s)
2025-12-15 12:36:57,582 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 179.4s: 28195c @157c/s (5961ch, ~7049t @39t/s)
2025-12-15 12:36:59,584 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 181.4s: 28508c @157c/s (6027ch, ~7127t @39t/s)
2025-12-15 12:37:01,592 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 183.4s: 28818c @157c/s (6093ch, ~7204t @39t/s)
2025-12-15 12:37:03,593 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 185.4s: 29131c @157c/s (6159ch, ~7283t @39t/s)
2025-12-15 12:37:05,594 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 187.4s: 29441c @157c/s (6225ch, ~7360t @39t/s)
2025-12-15 12:37:07,600 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 189.4s: 29754c @157c/s (6291ch, ~7438t @39t/s)
2025-12-15 12:37:09,606 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 191.4s: 30068c @157c/s (6357ch, ~7517t @39t/s)
2025-12-15 12:37:11,609 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 193.4s: 30382c @157c/s (6423ch, ~7596t @39t/s)
2025-12-15 12:37:13,612 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 195.4s: 30691c @157c/s (6489ch, ~7673t @39t/s)
2025-12-15 12:37:15,631 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 197.4s: 31005c @157c/s (6555ch, ~7751t @39t/s)
2025-12-15 12:37:17,639 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 199.4s: 31317c @157c/s (6621ch, ~7829t @39t/s)
2025-12-15 12:37:19,648 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 201.4s: 31628c @157c/s (6687ch, ~7907t @39t/s)
2025-12-15 12:37:21,658 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 203.4s: 31937c @157c/s (6753ch, ~7984t @39t/s)
2025-12-15 12:37:23,668 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 205.4s: 32258c @157c/s (6820ch, ~8064t @39t/s)
2025-12-15 12:37:25,677 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 207.5s: 32575c @157c/s (6887ch, ~8144t @39t/s)
2025-12-15 12:37:27,686 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 209.5s: 32889c @157c/s (6954ch, ~8222t @39t/s)
2025-12-15 12:37:29,696 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 211.5s: 33211c @157c/s (7021ch, ~8303t @39t/s)
2025-12-15 12:37:31,707 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 213.5s: 33524c @157c/s (7088ch, ~8381t @39t/s)
2025-12-15 12:37:33,719 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 215.5s: 33842c @157c/s (7155ch, ~8460t @39t/s)
2025-12-15 12:37:35,729 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 217.5s: 34157c @157c/s (7222ch, ~8539t @39t/s)
2025-12-15 12:37:37,746 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 219.5s: 34476c @157c/s (7289ch, ~8619t @39t/s)
2025-12-15 12:37:39,759 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 221.5s: 34796c @157c/s (7356ch, ~8699t @39t/s)
2025-12-15 12:37:41,776 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 223.6s: 35111c @157c/s (7423ch, ~8778t @39t/s)
2025-12-15 12:37:43,790 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 225.6s: 35429c @157c/s (7490ch, ~8857t @39t/s)
2025-12-15 12:37:45,807 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 227.6s: 35745c @157c/s (7557ch, ~8936t @39t/s)
2025-12-15 12:37:47,827 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 229.6s: 36060c @157c/s (7624ch, ~9015t @39t/s)
2025-12-15 12:37:49,850 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 231.6s: 36380c @157c/s (7691ch, ~9095t @39t/s)
2025-12-15 12:37:51,874 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 233.7s: 36697c @157c/s (7758ch, ~9174t @39t/s)
2025-12-15 12:37:53,887 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 235.7s: 37010c @157c/s (7824ch, ~9252t @39t/s)
2025-12-15 12:37:55,912 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 237.7s: 37325c @157c/s (7891ch, ~9331t @39t/s)
2025-12-15 12:37:57,940 - src.llm.client - INFO - [dia:8f7ba9] ğŸ“Š 239.7s: 37643c @157c/s (7958ch, ~9411t @39t/s)
2025-12-15 12:37:58,244 - src.llm.client - ERROR - [dia:8f7ba9] Stream timeout: 240.03s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 7968 chunks, 769192 bytes, 37686 chars (~9422 tokens) before timeout. Performance: 157.0 chars/s, ~39.3 tok/s. Generation was slow (157.0 chars/s, ~39.3 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.
2025-12-15 12:37:58,245 - src.generate.orchestration.pipeline - WARNING -   Transient error in diagram 1 generation (attempt 1/3): [dia:8f7ba9] Stream timeout: 240.03s elapsed (limit: 240.00s, base timeout: 120.00s). Operation: diagram. Received 7968 chunks, 769192 bytes, 37686 chars (~9422 tokens) before timeout. Performance: 157.0 chars/s, ~39.3 tok/s. Generation was slow (157.0 chars/s, ~39.3 tok/s) but making progress. General solutions: (1) Increase timeout in config/llm_config.yaml, (2) Use a faster model, (3) Check system resources (CPU/memory/GPU), (4) See docs/TROUBLESHOOTING.md for detailed guidance.. Retrying in 2.0s...
2025-12-15 12:38:00,247 - src.generate.formats.diagrams - INFO - Generating diagram for: Policy Optimization (Policy Selection & Planning)
2025-12-15 12:38:00,248 - src.llm.client - INFO - [dia:2dc0e6] ğŸš€ dia | m=gemma3:4b | p=5759c | t=120s
2025-12-15 12:38:00,248 - src.llm.client - INFO - [dia:2dc0e6] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:38:00,248 - src.llm.client - INFO - [dia:2dc0e6] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:38:00,251 - src.llm.client - INFO - [dia:2dc0e6] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11068 bytes, prompt=5759 chars
2025-12-15 12:38:00,251 - src.llm.client - INFO - [dia:2dc0e6] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:38:03,860 - src.llm.request_handler - INFO - [dia:2dc0e6] âœ“ Done 3.61s
2025-12-15 12:38:03,860 - src.llm.client - INFO - [dia:2dc0e6] âœ… HTTP 200 in 3.61s
2025-12-15 12:38:03,861 - src.llm.client - INFO - [dia:2dc0e6] ğŸ“¡ Stream active (200)
2025-12-15 12:38:03,861 - src.llm.client - INFO - [dia:2dc0e6] Starting stream parsing, waiting for first chunk...
2025-12-15 12:38:05,879 - src.llm.client - INFO - [dia:2dc0e6] ğŸ“Š 2.0s: 261c @129c/s (68ch, ~65t @32t/s)
2025-12-15 12:38:07,891 - src.llm.client - INFO - [dia:2dc0e6] ğŸ“Š 4.0s: 509c @126c/s (136ch, ~127t @32t/s)
2025-12-15 12:38:09,901 - src.llm.client - INFO - [dia:2dc0e6] ğŸ“Š 6.0s: 746c @124c/s (204ch, ~186t @31t/s)
2025-12-15 12:38:11,908 - src.llm.client - INFO - [dia:2dc0e6] ğŸ“Š 8.0s: 958c @119c/s (272ch, ~240t @30t/s)
2025-12-15 12:38:12,925 - src.llm.client - INFO - [dia:2dc0e6] âœ“ Done 12.68s: 1071c (~140w @84c/s)
2025-12-15 12:38:12,925 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Policy Optimization (Policy Selection & Planning):
2025-12-15 12:38:12,925 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:38:12,925 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:38:12,925 - src.generate.formats.diagrams - INFO - [FIXED] Removed classDef command (not supported in all renderers) âœ“
2025-12-15 12:38:12,925 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:38:12,926 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:38:12,926 - src.generate.formats.diagrams - INFO -     - Length: 872 chars (cleaned: 872 chars)
2025-12-15 12:38:12,926 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:38:12,926 - src.generate.formats.diagrams - INFO - [OK] Elements: 51 total (nodes: 19, connections: 32) âœ“
2025-12-15 12:38:12,926 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-15 12:38:12,926 - src.generate.formats.diagrams - INFO - Generated diagram: 872 characters
2025-12-15 12:38:12,926 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:38:12,926 - src.generate.formats.questions - INFO - Generating 10 questions for: Policy Selection & Planning (Session 11)
2025-12-15 12:38:12,926 - src.llm.client - INFO - [qst:15fae8] ğŸš€ qst | m=gemma3:4b | p=7319c | t=150s
2025-12-15 12:38:12,926 - src.llm.client - INFO - [qst:15fae8] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:38:12,926 - src.llm.client - INFO - [qst:15fae8] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:38:12,928 - src.llm.client - INFO - [qst:15fae8] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11017 bytes, prompt=7319 chars
2025-12-15 12:38:12,928 - src.llm.client - INFO - [qst:15fae8] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:38:17,006 - src.llm.request_handler - INFO - [qst:15fae8] âœ“ Done 4.08s
2025-12-15 12:38:17,006 - src.llm.client - INFO - [qst:15fae8] âœ… HTTP 200 in 4.08s
2025-12-15 12:38:17,007 - src.llm.client - INFO - [qst:15fae8] ğŸ“¡ Stream active (200)
2025-12-15 12:38:17,007 - src.llm.client - INFO - [qst:15fae8] Starting stream parsing, waiting for first chunk...
2025-12-15 12:38:19,018 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 2.0s: 369c @183c/s (68ch, ~92t @46t/s)
2025-12-15 12:38:21,032 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 4.0s: 735c @183c/s (136ch, ~184t @46t/s)
2025-12-15 12:38:23,041 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 6.0s: 1061c @176c/s (204ch, ~265t @44t/s)
2025-12-15 12:38:25,050 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 8.0s: 1404c @175c/s (272ch, ~351t @44t/s)
2025-12-15 12:38:27,057 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 10.1s: 1808c @180c/s (340ch, ~452t @45t/s)
2025-12-15 12:38:29,065 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 12.1s: 2180c @181c/s (408ch, ~545t @45t/s)
2025-12-15 12:38:31,086 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 14.1s: 2543c @181c/s (476ch, ~636t @45t/s)
2025-12-15 12:38:33,115 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 16.1s: 2856c @177c/s (544ch, ~714t @44t/s)
2025-12-15 12:38:35,131 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 18.1s: 3200c @177c/s (611ch, ~800t @44t/s)
2025-12-15 12:38:37,154 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 20.1s: 3541c @176c/s (679ch, ~885t @44t/s)
2025-12-15 12:38:39,175 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 22.2s: 3915c @177c/s (747ch, ~979t @44t/s)
2025-12-15 12:38:41,204 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 24.2s: 4291c @177c/s (815ch, ~1073t @44t/s)
2025-12-15 12:38:43,231 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 26.2s: 4663c @178c/s (883ch, ~1166t @44t/s)
2025-12-15 12:38:45,261 - src.llm.client - INFO - [qst:15fae8] ğŸ“Š 28.3s: 5035c @178c/s (951ch, ~1259t @45t/s)
2025-12-15 12:38:45,484 - src.llm.client - INFO - [qst:15fae8] âœ“ Done 32.56s: 5054c (~721w @155c/s)
2025-12-15 12:38:45,485 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 0, 'total_fixes': 1}
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO - [COMPLIANT] Questions generated âœ“
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO - [OK] Question marks: 15 total, 10 questions with '?' âœ“
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO -     - Question length: avg 12.5 words (range: 9-16)
2025-12-15 12:38:45,485 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-15 12:38:45,488 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:38:45,490 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 11 completed
2025-12-15 12:38:45,490 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:38:45,490 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:38:45,490 - src.generate.orchestration.pipeline - INFO - Module 7: Model Learning & Structure Learning (2 sessions)
2025-12-15 12:38:45,490 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:38:45,490 - src.generate.orchestration.pipeline - INFO - 
[12/20] Session 12: Parameter Estimation
2025-12-15 12:38:45,490 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:38:45,490 - src.generate.formats.lectures - INFO - Generating lecture for: Model Learning & Structure Learning (Session 12/20)
2025-12-15 12:38:45,490 - src.llm.client - INFO - [lec:9539b3] ğŸš€ lec | m=gemma3:4b | p=3053c | t=180s
2025-12-15 12:38:45,490 - src.llm.client - INFO - [lec:9539b3] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:38:45,490 - src.llm.client - INFO - [lec:9539b3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:38:45,491 - src.llm.client - INFO - [lec:9539b3] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6683 bytes, prompt=3053 chars
2025-12-15 12:38:45,491 - src.llm.client - INFO - [lec:9539b3] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:38:47,498 - src.llm.request_handler - INFO - [lec:9539b3] âœ“ Done 2.01s
2025-12-15 12:38:47,498 - src.llm.client - INFO - [lec:9539b3] âœ… HTTP 200 in 2.01s
2025-12-15 12:38:47,498 - src.llm.client - INFO - [lec:9539b3] ğŸ“¡ Stream active (200)
2025-12-15 12:38:47,498 - src.llm.client - INFO - [lec:9539b3] Starting stream parsing, waiting for first chunk...
2025-12-15 12:38:49,509 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 2.0s: 411c @204c/s (69ch, ~103t @51t/s)
2025-12-15 12:38:51,525 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 4.0s: 770c @191c/s (138ch, ~192t @48t/s)
2025-12-15 12:38:53,543 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 6.0s: 1136c @188c/s (207ch, ~284t @47t/s)
2025-12-15 12:38:55,567 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 8.1s: 1490c @185c/s (276ch, ~372t @46t/s)
2025-12-15 12:38:57,596 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 10.1s: 1859c @184c/s (345ch, ~465t @46t/s)
2025-12-15 12:38:59,625 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 12.1s: 2215c @183c/s (414ch, ~554t @46t/s)
2025-12-15 12:39:01,634 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 14.1s: 2556c @181c/s (482ch, ~639t @45t/s)
2025-12-15 12:39:03,637 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 16.1s: 2904c @180c/s (550ch, ~726t @45t/s)
2025-12-15 12:39:05,639 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 18.1s: 3239c @179c/s (618ch, ~810t @45t/s)
2025-12-15 12:39:07,648 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 20.1s: 3590c @178c/s (686ch, ~898t @45t/s)
2025-12-15 12:39:09,652 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 22.2s: 3905c @176c/s (754ch, ~976t @44t/s)
2025-12-15 12:39:11,666 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 24.2s: 4250c @176c/s (822ch, ~1062t @44t/s)
2025-12-15 12:39:13,672 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 26.2s: 4638c @177c/s (890ch, ~1160t @44t/s)
2025-12-15 12:39:15,679 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 28.2s: 4993c @177c/s (958ch, ~1248t @44t/s)
2025-12-15 12:39:17,691 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 30.2s: 5350c @177c/s (1026ch, ~1338t @44t/s)
2025-12-15 12:39:19,703 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 32.2s: 5661c @176c/s (1094ch, ~1415t @44t/s)
2025-12-15 12:39:21,719 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 34.2s: 6007c @176c/s (1162ch, ~1502t @44t/s)
2025-12-15 12:39:23,734 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 36.2s: 6338c @175c/s (1230ch, ~1584t @44t/s)
2025-12-15 12:39:25,749 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 38.3s: 6567c @172c/s (1298ch, ~1642t @43t/s)
2025-12-15 12:39:27,770 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 40.3s: 6852c @170c/s (1366ch, ~1713t @43t/s)
2025-12-15 12:39:29,789 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 42.3s: 7245c @171c/s (1434ch, ~1811t @43t/s)
2025-12-15 12:39:31,811 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 44.3s: 7617c @172c/s (1502ch, ~1904t @43t/s)
2025-12-15 12:39:33,838 - src.llm.client - INFO - [lec:9539b3] ğŸ“Š 46.3s: 8033c @173c/s (1570ch, ~2008t @43t/s)
2025-12-15 12:39:34,888 - src.llm.client - INFO - [lec:9539b3] âœ“ Done 49.40s: 8249c (~1279w @167c/s)
2025-12-15 12:39:34,889 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:39:34,889 - src.generate.formats.lectures - INFO -     - Length: 8343 chars, 1293 words
2025-12-15 12:39:34,889 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:39:34,889 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:39:34,889 - src.generate.formats.lectures - INFO -     - Content: 8 examples, 0 terms defined
2025-12-15 12:39:34,889 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:39:34,893 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:39:34,893 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:39:34,893 - src.generate.formats.labs - INFO - Generating lab 12 for: Model Learning & Structure Learning (Session 12)
2025-12-15 12:39:34,894 - src.llm.client - INFO - [lab:3cd440] ğŸš€ lab | m=gemma3:4b | p=3334c | t=150s
2025-12-15 12:39:34,894 - src.llm.client - INFO - [lab:3cd440] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:39:34,894 - src.llm.client - INFO - [lab:3cd440] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:39:34,895 - src.llm.client - INFO - [lab:3cd440] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3769 bytes, prompt=3334 chars
2025-12-15 12:39:34,895 - src.llm.client - INFO - [lab:3cd440] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:39:36,997 - src.llm.request_handler - INFO - [lab:3cd440] âœ“ Done 2.10s
2025-12-15 12:39:36,997 - src.llm.client - INFO - [lab:3cd440] âœ… HTTP 200 in 2.10s
2025-12-15 12:39:36,997 - src.llm.client - INFO - [lab:3cd440] ğŸ“¡ Stream active (200)
2025-12-15 12:39:36,997 - src.llm.client - INFO - [lab:3cd440] Starting stream parsing, waiting for first chunk...
2025-12-15 12:39:39,026 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 2.0s: 328c @162c/s (70ch, ~82t @40t/s)
2025-12-15 12:39:41,029 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 4.0s: 745c @185c/s (139ch, ~186t @46t/s)
2025-12-15 12:39:43,036 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 6.0s: 1145c @190c/s (208ch, ~286t @47t/s)
2025-12-15 12:39:45,050 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 8.1s: 1450c @180c/s (277ch, ~362t @45t/s)
2025-12-15 12:39:47,074 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 10.1s: 1763c @175c/s (346ch, ~441t @44t/s)
2025-12-15 12:39:49,101 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 12.1s: 2109c @174c/s (415ch, ~527t @44t/s)
2025-12-15 12:39:51,129 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 14.1s: 2428c @172c/s (484ch, ~607t @43t/s)
2025-12-15 12:39:53,157 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 16.2s: 2733c @169c/s (553ch, ~683t @42t/s)
2025-12-15 12:39:55,161 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 18.2s: 2984c @164c/s (621ch, ~746t @41t/s)
2025-12-15 12:39:57,166 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 20.2s: 3290c @163c/s (689ch, ~822t @41t/s)
2025-12-15 12:39:59,168 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 22.2s: 3514c @158c/s (757ch, ~878t @40t/s)
2025-12-15 12:40:01,180 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 24.2s: 3755c @155c/s (825ch, ~939t @39t/s)
2025-12-15 12:40:03,189 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 26.2s: 4041c @154c/s (893ch, ~1010t @39t/s)
2025-12-15 12:40:05,195 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 28.2s: 4461c @158c/s (961ch, ~1115t @40t/s)
2025-12-15 12:40:07,211 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 30.2s: 4787c @158c/s (1029ch, ~1197t @40t/s)
2025-12-15 12:40:09,216 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 32.2s: 5155c @160c/s (1097ch, ~1289t @40t/s)
2025-12-15 12:40:11,273 - src.llm.client - INFO - [lab:3cd440] ğŸ“Š 34.3s: 5544c @162c/s (1165ch, ~1386t @40t/s)
2025-12-15 12:40:11,273 - src.llm.client - INFO - [lab:3cd440] âœ“ Done 36.38s: 5544c (~781w @152c/s)
2025-12-15 12:40:11,274 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:40:11,274 - src.generate.formats.labs - INFO -     - Length: 5656 chars, 799 words
2025-12-15 12:40:11,274 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-15 12:40:11,274 - src.generate.formats.labs - INFO -     - Safety: 4 warnings
2025-12-15 12:40:11,274 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-15 12:40:11,276 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:40:11,277 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:40:11,277 - src.generate.formats.study_notes - INFO - Generating study notes for: Model Learning & Structure Learning (Session 12)
2025-12-15 12:40:11,277 - src.llm.client - INFO - [stu:bfe704] ğŸš€ stu | m=gemma3:4b | p=4449c | t=120s
2025-12-15 12:40:11,277 - src.llm.client - INFO - [stu:bfe704] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:40:11,277 - src.llm.client - INFO - [stu:bfe704] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:40:11,278 - src.llm.client - INFO - [stu:bfe704] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8099 bytes, prompt=4449 chars
2025-12-15 12:40:11,278 - src.llm.client - INFO - [stu:bfe704] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:40:13,859 - src.llm.request_handler - INFO - [stu:bfe704] âœ“ Done 2.58s
2025-12-15 12:40:13,859 - src.llm.client - INFO - [stu:bfe704] âœ… HTTP 200 in 2.58s
2025-12-15 12:40:13,859 - src.llm.client - INFO - [stu:bfe704] ğŸ“¡ Stream active (200)
2025-12-15 12:40:13,859 - src.llm.client - INFO - [stu:bfe704] Starting stream parsing, waiting for first chunk...
2025-12-15 12:40:15,881 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 2.0s: 402c @199c/s (68ch, ~100t @50t/s)
2025-12-15 12:40:17,886 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 4.0s: 763c @189c/s (136ch, ~191t @47t/s)
2025-12-15 12:40:19,890 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 6.0s: 1117c @185c/s (204ch, ~279t @46t/s)
2025-12-15 12:40:21,896 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 8.0s: 1478c @184c/s (272ch, ~370t @46t/s)
2025-12-15 12:40:23,905 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 10.0s: 1856c @185c/s (340ch, ~464t @46t/s)
2025-12-15 12:40:25,914 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 12.1s: 2174c @180c/s (408ch, ~544t @45t/s)
2025-12-15 12:40:27,926 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 14.1s: 2545c @181c/s (476ch, ~636t @45t/s)
2025-12-15 12:40:29,939 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 16.1s: 2925c @182c/s (544ch, ~731t @45t/s)
2025-12-15 12:40:31,966 - src.llm.client - INFO - [stu:bfe704] ğŸ“Š 18.1s: 3276c @181c/s (612ch, ~819t @45t/s)
2025-12-15 12:40:33,599 - src.llm.client - INFO - [stu:bfe704] âœ“ Done 22.32s: 3579c (~527w @160c/s)
2025-12-15 12:40:33,599 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:40:33,600 - src.generate.formats.study_notes - INFO -     - Length: 3649 chars, 539 words
2025-12-15 12:40:33,600 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:40:33,600 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-15 12:40:33,600 - src.generate.formats.study_notes - INFO -     - Structure: 6 sections, 4 bullets
2025-12-15 12:40:33,600 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:40:33,601 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:40:33,602 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:40:33,602 - src.generate.formats.diagrams - INFO - Generating diagram for: Bayesian Updating of Models (Model Learning & Structure Learning)
2025-12-15 12:40:33,602 - src.llm.client - INFO - [dia:d03fde] ğŸš€ dia | m=gemma3:4b | p=5776c | t=120s
2025-12-15 12:40:33,602 - src.llm.client - INFO - [dia:d03fde] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:40:33,602 - src.llm.client - INFO - [dia:d03fde] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:40:33,603 - src.llm.client - INFO - [dia:d03fde] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11085 bytes, prompt=5776 chars
2025-12-15 12:40:33,603 - src.llm.client - INFO - [dia:d03fde] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:40:37,174 - src.llm.request_handler - INFO - [dia:d03fde] âœ“ Done 3.57s
2025-12-15 12:40:37,174 - src.llm.client - INFO - [dia:d03fde] âœ… HTTP 200 in 3.57s
2025-12-15 12:40:37,174 - src.llm.client - INFO - [dia:d03fde] ğŸ“¡ Stream active (200)
2025-12-15 12:40:37,174 - src.llm.client - INFO - [dia:d03fde] Starting stream parsing, waiting for first chunk...
2025-12-15 12:40:39,188 - src.llm.client - INFO - [dia:d03fde] ğŸ“Š 2.0s: 241c @120c/s (68ch, ~60t @30t/s)
2025-12-15 12:40:41,197 - src.llm.client - INFO - [dia:d03fde] ğŸ“Š 4.0s: 469c @117c/s (136ch, ~117t @29t/s)
2025-12-15 12:40:43,205 - src.llm.client - INFO - [dia:d03fde] ğŸ“Š 6.0s: 638c @106c/s (204ch, ~160t @26t/s)
2025-12-15 12:40:45,214 - src.llm.client - INFO - [dia:d03fde] ğŸ“Š 8.0s: 785c @98c/s (272ch, ~196t @24t/s)
2025-12-15 12:40:46,142 - src.llm.client - INFO - [dia:d03fde] âœ“ Done 12.54s: 844c (~105w @67c/s)
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Bayesian Updating of Models (Model Learning & Structure Learning):
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO -     - Length: 496 chars (cleaned: 496 chars)
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO - [OK] Elements: 30 total (nodes: 12, connections: 18) âœ“
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:40:46,143 - src.generate.formats.diagrams - INFO - Generated diagram: 496 characters
2025-12-15 12:40:46,143 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:40:46,143 - src.generate.formats.questions - INFO - Generating 10 questions for: Model Learning & Structure Learning (Session 12)
2025-12-15 12:40:46,143 - src.llm.client - INFO - [qst:f04a8d] ğŸš€ qst | m=gemma3:4b | p=7325c | t=150s
2025-12-15 12:40:46,144 - src.llm.client - INFO - [qst:f04a8d] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:40:46,144 - src.llm.client - INFO - [qst:f04a8d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:40:46,145 - src.llm.client - INFO - [qst:f04a8d] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11011 bytes, prompt=7325 chars
2025-12-15 12:40:46,145 - src.llm.client - INFO - [qst:f04a8d] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:40:54,534 - src.llm.request_handler - INFO - [qst:f04a8d] âœ“ Done 8.39s
2025-12-15 12:40:54,534 - src.llm.client - INFO - [qst:f04a8d] âœ… HTTP 200 in 8.39s
2025-12-15 12:40:54,534 - src.llm.client - INFO - [qst:f04a8d] ğŸ“¡ Stream active (200)
2025-12-15 12:40:54,534 - src.llm.client - INFO - [qst:f04a8d] Starting stream parsing, waiting for first chunk...
2025-12-15 12:40:56,552 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 2.0s: 322c @160c/s (68ch, ~80t @40t/s)
2025-12-15 12:40:58,558 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 4.0s: 691c @172c/s (136ch, ~173t @43t/s)
2025-12-15 12:41:00,569 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 6.0s: 1050c @174c/s (204ch, ~262t @44t/s)
2025-12-15 12:41:02,585 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 8.1s: 1405c @175c/s (272ch, ~351t @44t/s)
2025-12-15 12:41:04,595 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 10.1s: 1794c @178c/s (340ch, ~448t @45t/s)
2025-12-15 12:41:06,603 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 12.1s: 2182c @181c/s (408ch, ~546t @45t/s)
2025-12-15 12:41:08,625 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 14.1s: 2562c @182c/s (476ch, ~640t @45t/s)
2025-12-15 12:41:10,643 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 16.1s: 2886c @179c/s (544ch, ~722t @45t/s)
2025-12-15 12:41:12,662 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 18.1s: 3290c @181c/s (612ch, ~822t @45t/s)
2025-12-15 12:41:14,681 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 20.1s: 3661c @182c/s (680ch, ~915t @45t/s)
2025-12-15 12:41:16,710 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 22.2s: 4058c @183c/s (748ch, ~1014t @46t/s)
2025-12-15 12:41:18,712 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 24.2s: 4442c @184c/s (815ch, ~1110t @46t/s)
2025-12-15 12:41:20,714 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 26.2s: 4715c @180c/s (882ch, ~1179t @45t/s)
2025-12-15 12:41:22,720 - src.llm.client - INFO - [qst:f04a8d] ğŸ“Š 28.2s: 5072c @180c/s (949ch, ~1268t @45t/s)
2025-12-15 12:41:24,268 - src.llm.client - INFO - [qst:f04a8d] âœ“ Done 38.12s: 5369c (~747w @141c/s)
2025-12-15 12:41:24,269 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 3 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 1, 'total_fixes': 3}
2025-12-15 12:41:24,269 - src.generate.formats.questions - INFO - Applied 3 auto-fixes to questions
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 2 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING -     Context: Module 7 Session 12
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 6 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING -     Context: Module 7 Session 12
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:41:24,269 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:41:24,270 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Model Learning & Structure Learning (Session 12)
2025-12-15 12:41:24,270 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:41:24,272 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:41:24,274 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 12 completed
2025-12-15 12:41:24,274 - src.generate.orchestration.pipeline - INFO - 
[13/20] Session 13: Structure Learning
2025-12-15 12:41:24,274 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:41:24,274 - src.generate.formats.lectures - INFO - Generating lecture for: Model Learning & Structure Learning (Session 13/20)
2025-12-15 12:41:24,275 - src.llm.client - INFO - [lec:c4b7ba] ğŸš€ lec | m=gemma3:4b | p=3042c | t=180s
2025-12-15 12:41:24,275 - src.llm.client - INFO - [lec:c4b7ba] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:41:24,275 - src.llm.client - INFO - [lec:c4b7ba] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:41:24,276 - src.llm.client - INFO - [lec:c4b7ba] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6672 bytes, prompt=3042 chars
2025-12-15 12:41:24,276 - src.llm.client - INFO - [lec:c4b7ba] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:41:26,272 - src.llm.request_handler - INFO - [lec:c4b7ba] âœ“ Done 2.00s
2025-12-15 12:41:26,272 - src.llm.client - INFO - [lec:c4b7ba] âœ… HTTP 200 in 2.00s
2025-12-15 12:41:26,272 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“¡ Stream active (200)
2025-12-15 12:41:26,272 - src.llm.client - INFO - [lec:c4b7ba] Starting stream parsing, waiting for first chunk...
2025-12-15 12:41:28,278 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 2.0s: 437c @218c/s (69ch, ~109t @54t/s)
2025-12-15 12:41:30,291 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 4.0s: 855c @213c/s (138ch, ~214t @53t/s)
2025-12-15 12:41:32,311 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 6.0s: 1222c @202c/s (207ch, ~306t @51t/s)
2025-12-15 12:41:34,313 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 8.0s: 1613c @201c/s (275ch, ~403t @50t/s)
2025-12-15 12:41:36,316 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 10.0s: 1986c @198c/s (343ch, ~496t @49t/s)
2025-12-15 12:41:38,319 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 12.0s: 2357c @196c/s (411ch, ~589t @49t/s)
2025-12-15 12:41:40,344 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 14.1s: 2715c @193c/s (480ch, ~679t @48t/s)
2025-12-15 12:41:42,348 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 16.1s: 3108c @193c/s (548ch, ~777t @48t/s)
2025-12-15 12:41:44,351 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 18.1s: 3484c @193c/s (616ch, ~871t @48t/s)
2025-12-15 12:41:46,359 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 20.1s: 3896c @194c/s (684ch, ~974t @48t/s)
2025-12-15 12:41:48,376 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 22.1s: 4244c @192c/s (752ch, ~1061t @48t/s)
2025-12-15 12:41:50,388 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 24.1s: 4647c @193c/s (820ch, ~1162t @48t/s)
2025-12-15 12:41:52,399 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 26.1s: 5006c @192c/s (888ch, ~1252t @48t/s)
2025-12-15 12:41:54,401 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 28.1s: 5393c @192c/s (955ch, ~1348t @48t/s)
2025-12-15 12:41:56,413 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 30.1s: 5781c @192c/s (1023ch, ~1445t @48t/s)
2025-12-15 12:41:58,428 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 32.2s: 6152c @191c/s (1091ch, ~1538t @48t/s)
2025-12-15 12:42:00,443 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 34.2s: 6554c @192c/s (1159ch, ~1638t @48t/s)
2025-12-15 12:42:02,466 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 36.2s: 6922c @191c/s (1227ch, ~1730t @48t/s)
2025-12-15 12:42:04,483 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 38.2s: 7307c @191c/s (1295ch, ~1827t @48t/s)
2025-12-15 12:42:06,506 - src.llm.client - INFO - [lec:c4b7ba] ğŸ“Š 40.2s: 7728c @192c/s (1363ch, ~1932t @48t/s)
2025-12-15 12:42:08,203 - src.llm.client - INFO - [lec:c4b7ba] âœ“ Done 43.93s: 8094c (~1144w @184c/s)
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO -     - Length: 8191 chars, 1158 words
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO -     - Content: 10 examples, 1 terms defined
2025-12-15 12:42:08,204 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 12:42:08,204 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-15 12:42:08,208 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:42:08,208 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:42:08,208 - src.generate.formats.labs - INFO - Generating lab 13 for: Model Learning & Structure Learning (Session 13)
2025-12-15 12:42:08,208 - src.llm.client - INFO - [lab:3373e4] ğŸš€ lab | m=gemma3:4b | p=3337c | t=150s
2025-12-15 12:42:08,208 - src.llm.client - INFO - [lab:3373e4] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:42:08,208 - src.llm.client - INFO - [lab:3373e4] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:42:08,210 - src.llm.client - INFO - [lab:3373e4] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3751 bytes, prompt=3337 chars
2025-12-15 12:42:08,210 - src.llm.client - INFO - [lab:3373e4] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:42:09,934 - src.llm.request_handler - INFO - [lab:3373e4] âœ“ Done 1.72s
2025-12-15 12:42:09,934 - src.llm.client - INFO - [lab:3373e4] âœ… HTTP 200 in 1.72s
2025-12-15 12:42:09,934 - src.llm.client - INFO - [lab:3373e4] ğŸ“¡ Stream active (200)
2025-12-15 12:42:09,934 - src.llm.client - INFO - [lab:3373e4] Starting stream parsing, waiting for first chunk...
2025-12-15 12:42:11,939 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 2.0s: 325c @162c/s (69ch, ~81t @41t/s)
2025-12-15 12:42:13,943 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 4.0s: 749c @187c/s (138ch, ~187t @47t/s)
2025-12-15 12:42:15,949 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 6.0s: 1135c @189c/s (207ch, ~284t @47t/s)
2025-12-15 12:42:17,961 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 8.0s: 1470c @183c/s (276ch, ~368t @46t/s)
2025-12-15 12:42:19,977 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 10.0s: 1698c @169c/s (345ch, ~424t @42t/s)
2025-12-15 12:42:21,978 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 12.0s: 1934c @161c/s (413ch, ~484t @40t/s)
2025-12-15 12:42:23,979 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 14.0s: 2162c @154c/s (481ch, ~540t @38t/s)
2025-12-15 12:42:25,981 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 16.0s: 2467c @154c/s (549ch, ~617t @38t/s)
2025-12-15 12:42:27,995 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 18.1s: 2776c @154c/s (617ch, ~694t @38t/s)
2025-12-15 12:42:29,998 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 20.1s: 3080c @154c/s (685ch, ~770t @38t/s)
2025-12-15 12:42:32,002 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 22.1s: 3384c @153c/s (753ch, ~846t @38t/s)
2025-12-15 12:42:34,007 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 24.1s: 3679c @153c/s (821ch, ~920t @38t/s)
2025-12-15 12:42:36,013 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 26.1s: 4001c @153c/s (889ch, ~1000t @38t/s)
2025-12-15 12:42:38,023 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 28.1s: 4248c @151c/s (957ch, ~1062t @38t/s)
2025-12-15 12:42:40,028 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 30.1s: 4426c @147c/s (1025ch, ~1106t @37t/s)
2025-12-15 12:42:42,038 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 32.1s: 4609c @144c/s (1093ch, ~1152t @36t/s)
2025-12-15 12:42:44,045 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 34.1s: 4889c @143c/s (1161ch, ~1222t @36t/s)
2025-12-15 12:42:46,048 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 36.1s: 5227c @145c/s (1228ch, ~1307t @36t/s)
2025-12-15 12:42:48,051 - src.llm.client - INFO - [lab:3373e4] ğŸ“Š 38.1s: 5605c @147c/s (1294ch, ~1401t @37t/s)
2025-12-15 12:42:48,051 - src.llm.client - INFO - [lab:3373e4] âœ“ Done 39.84s: 5605c (~856w @141c/s)
2025-12-15 12:42:48,051 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:42:48,051 - src.generate.formats.labs - INFO -     - Length: 5717 chars, 872 words
2025-12-15 12:42:48,051 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-15 12:42:48,052 - src.generate.formats.labs - INFO -     - Safety: 5 warnings
2025-12-15 12:42:48,052 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-15 12:42:48,055 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:42:48,055 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:42:48,055 - src.generate.formats.study_notes - INFO - Generating study notes for: Model Learning & Structure Learning (Session 13)
2025-12-15 12:42:48,055 - src.llm.client - INFO - [stu:0eeba1] ğŸš€ stu | m=gemma3:4b | p=4438c | t=120s
2025-12-15 12:42:48,055 - src.llm.client - INFO - [stu:0eeba1] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:42:48,055 - src.llm.client - INFO - [stu:0eeba1] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:42:48,057 - src.llm.client - INFO - [stu:0eeba1] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8067 bytes, prompt=4438 chars
2025-12-15 12:42:48,057 - src.llm.client - INFO - [stu:0eeba1] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:42:50,579 - src.llm.request_handler - INFO - [stu:0eeba1] âœ“ Done 2.52s
2025-12-15 12:42:50,579 - src.llm.client - INFO - [stu:0eeba1] âœ… HTTP 200 in 2.52s
2025-12-15 12:42:50,579 - src.llm.client - INFO - [stu:0eeba1] ğŸ“¡ Stream active (200)
2025-12-15 12:42:50,579 - src.llm.client - INFO - [stu:0eeba1] Starting stream parsing, waiting for first chunk...
2025-12-15 12:42:52,582 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 2.0s: 409c @204c/s (68ch, ~102t @51t/s)
2025-12-15 12:42:54,603 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 4.0s: 776c @193c/s (136ch, ~194t @48t/s)
2025-12-15 12:42:56,606 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 6.0s: 1152c @191c/s (204ch, ~288t @48t/s)
2025-12-15 12:42:58,610 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 8.0s: 1567c @195c/s (272ch, ~392t @49t/s)
2025-12-15 12:43:00,617 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 10.0s: 1958c @195c/s (340ch, ~490t @49t/s)
2025-12-15 12:43:02,625 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 12.0s: 2382c @198c/s (408ch, ~596t @49t/s)
2025-12-15 12:43:04,626 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 14.0s: 2767c @197c/s (476ch, ~692t @49t/s)
2025-12-15 12:43:06,634 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 16.1s: 3136c @195c/s (544ch, ~784t @49t/s)
2025-12-15 12:43:08,645 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 18.1s: 3511c @194c/s (612ch, ~878t @49t/s)
2025-12-15 12:43:10,651 - src.llm.client - INFO - [stu:0eeba1] ğŸ“Š 20.1s: 3879c @193c/s (680ch, ~970t @48t/s)
2025-12-15 12:43:11,195 - src.llm.client - INFO - [stu:0eeba1] âœ“ Done 23.14s: 3989c (~546w @172c/s)
2025-12-15 12:43:11,195 - src.generate.formats.study_notes - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:43:11,195 - src.generate.formats.study_notes - WARNING -     [CRITICAL] Issue 1: Only 2 key concepts highlighted (require 3-10, need 1 more - format concepts as **Concept Name:** in bullet points)
2025-12-15 12:43:11,196 - src.generate.formats.study_notes - WARNING -   Retry attempt 1/1 for study notes: Model Learning & Structure Learning (Session 13)
2025-12-15 12:43:11,196 - src.generate.formats.study_notes - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:43:11,197 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:43:11,198 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:43:11,198 - src.generate.formats.diagrams - INFO - Generating diagram for: Adding/Removing Connections (Model Learning & Structure Learning)
2025-12-15 12:43:11,198 - src.llm.client - INFO - [dia:a3df9b] ğŸš€ dia | m=gemma3:4b | p=5774c | t=120s
2025-12-15 12:43:11,198 - src.llm.client - INFO - [dia:a3df9b] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:43:11,198 - src.llm.client - INFO - [dia:a3df9b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:43:11,199 - src.llm.client - INFO - [dia:a3df9b] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11083 bytes, prompt=5774 chars
2025-12-15 12:43:11,199 - src.llm.client - INFO - [dia:a3df9b] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:43:14,776 - src.llm.request_handler - INFO - [dia:a3df9b] âœ“ Done 3.58s
2025-12-15 12:43:14,776 - src.llm.client - INFO - [dia:a3df9b] âœ… HTTP 200 in 3.58s
2025-12-15 12:43:14,776 - src.llm.client - INFO - [dia:a3df9b] ğŸ“¡ Stream active (200)
2025-12-15 12:43:14,776 - src.llm.client - INFO - [dia:a3df9b] Starting stream parsing, waiting for first chunk...
2025-12-15 12:43:16,799 - src.llm.client - INFO - [dia:a3df9b] ğŸ“Š 2.0s: 245c @121c/s (68ch, ~61t @30t/s)
2025-12-15 12:43:18,811 - src.llm.client - INFO - [dia:a3df9b] ğŸ“Š 4.0s: 505c @125c/s (136ch, ~126t @31t/s)
2025-12-15 12:43:20,770 - src.llm.client - INFO - [dia:a3df9b] âœ“ Done 9.57s: 634c (~91w @66c/s)
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Adding/Removing Connections (Model Learning & Structure Learning):
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO -     - Length: 489 chars (cleaned: 489 chars)
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO - [OK] Elements: 31 total (nodes: 7, connections: 24) âœ“
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-15 12:43:20,770 - src.generate.formats.diagrams - INFO - Generated diagram: 489 characters
2025-12-15 12:43:20,770 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:43:20,771 - src.generate.formats.questions - INFO - Generating 10 questions for: Model Learning & Structure Learning (Session 13)
2025-12-15 12:43:20,771 - src.llm.client - INFO - [qst:7db143] ğŸš€ qst | m=gemma3:4b | p=7328c | t=150s
2025-12-15 12:43:20,771 - src.llm.client - INFO - [qst:7db143] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:43:20,771 - src.llm.client - INFO - [qst:7db143] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:43:20,772 - src.llm.client - INFO - [qst:7db143] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11004 bytes, prompt=7328 chars
2025-12-15 12:43:20,772 - src.llm.client - INFO - [qst:7db143] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:43:56,661 - src.llm.request_handler - INFO - [qst:7db143] âœ“ Done 35.89s
2025-12-15 12:43:56,661 - src.llm.client - INFO - [qst:7db143] âœ… HTTP 200 in 35.89s
2025-12-15 12:43:56,661 - src.llm.client - INFO - [qst:7db143] ğŸ“¡ Stream active (200)
2025-12-15 12:43:56,661 - src.llm.client - INFO - [qst:7db143] Starting stream parsing, waiting for first chunk...
2025-12-15 12:43:58,683 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 2.0s: 376c @186c/s (68ch, ~94t @47t/s)
2025-12-15 12:44:00,704 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 4.0s: 734c @182c/s (136ch, ~184t @45t/s)
2025-12-15 12:44:02,720 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 6.1s: 1087c @179c/s (204ch, ~272t @45t/s)
2025-12-15 12:44:04,729 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 8.1s: 1450c @180c/s (272ch, ~362t @45t/s)
2025-12-15 12:44:06,737 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 10.1s: 1823c @181c/s (340ch, ~456t @45t/s)
2025-12-15 12:44:08,750 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 12.1s: 2216c @183c/s (408ch, ~554t @46t/s)
2025-12-15 12:44:10,761 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 14.1s: 2554c @181c/s (476ch, ~638t @45t/s)
2025-12-15 12:44:12,778 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 16.1s: 2928c @182c/s (544ch, ~732t @45t/s)
2025-12-15 12:44:14,796 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 18.1s: 3263c @180c/s (612ch, ~816t @45t/s)
2025-12-15 12:44:16,816 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 20.2s: 3624c @180c/s (680ch, ~906t @45t/s)
2025-12-15 12:44:18,838 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 22.2s: 4013c @181c/s (748ch, ~1003t @45t/s)
2025-12-15 12:44:20,860 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 24.2s: 4336c @179c/s (816ch, ~1084t @45t/s)
2025-12-15 12:44:22,889 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 26.2s: 4744c @181c/s (884ch, ~1186t @45t/s)
2025-12-15 12:44:24,919 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 28.3s: 5137c @182c/s (952ch, ~1284t @45t/s)
2025-12-15 12:44:26,919 - src.llm.client - INFO - [qst:7db143] ğŸ“Š 30.3s: 5524c @183c/s (1019ch, ~1381t @46t/s)
2025-12-15 12:44:27,880 - src.llm.client - INFO - [qst:7db143] âœ“ Done 67.11s: 5719c (~797w @85c/s)
2025-12-15 12:44:27,880 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 4 question format issues: {'format_standardized': 0, 'question_marks_added': 4, 'mc_options_fixed': 0, 'total_fixes': 4}
2025-12-15 12:44:27,880 - src.generate.formats.questions - INFO - Applied 4 auto-fixes to questions
2025-12-15 12:44:27,880 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 2 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:44:27,880 - src.generate.formats.questions - WARNING -     Context: Module 7 Session 13
2025-12-15 12:44:27,880 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:44:27,881 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:44:27,881 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:44:27,881 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Model Learning & Structure Learning (Session 13)
2025-12-15 12:44:27,881 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:44:27,883 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:44:27,885 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 13 completed
2025-12-15 12:44:27,885 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:44:27,885 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:44:27,885 - src.generate.orchestration.pipeline - INFO - Module 8: Neuroscientific Evidence (2 sessions)
2025-12-15 12:44:27,885 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:44:27,885 - src.generate.orchestration.pipeline - INFO - 
[14/20] Session 14: Sensory Coding
2025-12-15 12:44:27,885 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:44:27,885 - src.generate.formats.lectures - INFO - Generating lecture for: Neuroscientific Evidence (Session 14/20)
2025-12-15 12:44:27,886 - src.llm.client - INFO - [lec:f5b2b7] ğŸš€ lec | m=gemma3:4b | p=3042c | t=180s
2025-12-15 12:44:27,886 - src.llm.client - INFO - [lec:f5b2b7] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:44:27,886 - src.llm.client - INFO - [lec:f5b2b7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:44:27,887 - src.llm.client - INFO - [lec:f5b2b7] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6673 bytes, prompt=3042 chars
2025-12-15 12:44:27,887 - src.llm.client - INFO - [lec:f5b2b7] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:44:29,887 - src.llm.request_handler - INFO - [lec:f5b2b7] âœ“ Done 2.00s
2025-12-15 12:44:29,887 - src.llm.client - INFO - [lec:f5b2b7] âœ… HTTP 200 in 2.00s
2025-12-15 12:44:29,887 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“¡ Stream active (200)
2025-12-15 12:44:29,887 - src.llm.client - INFO - [lec:f5b2b7] Starting stream parsing, waiting for first chunk...
2025-12-15 12:44:31,898 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 2.0s: 386c @192c/s (69ch, ~96t @48t/s)
2025-12-15 12:44:33,914 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 4.0s: 776c @193c/s (138ch, ~194t @48t/s)
2025-12-15 12:44:35,934 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 6.0s: 1181c @195c/s (207ch, ~295t @49t/s)
2025-12-15 12:44:37,937 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 8.1s: 1562c @194c/s (275ch, ~390t @49t/s)
2025-12-15 12:44:39,966 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 10.1s: 1940c @192c/s (344ch, ~485t @48t/s)
2025-12-15 12:44:41,967 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 12.1s: 2281c @189c/s (412ch, ~570t @47t/s)
2025-12-15 12:44:43,995 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 14.1s: 2669c @189c/s (481ch, ~667t @47t/s)
2025-12-15 12:44:45,996 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 16.1s: 2934c @182c/s (549ch, ~734t @46t/s)
2025-12-15 12:44:47,999 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 18.1s: 3286c @181c/s (617ch, ~822t @45t/s)
2025-12-15 12:44:50,005 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 20.1s: 3621c @180c/s (685ch, ~905t @45t/s)
2025-12-15 12:44:52,024 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 22.1s: 3983c @180c/s (753ch, ~996t @45t/s)
2025-12-15 12:44:54,031 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 24.1s: 4361c @181c/s (821ch, ~1090t @45t/s)
2025-12-15 12:44:56,037 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 26.2s: 4682c @179c/s (889ch, ~1170t @45t/s)
2025-12-15 12:44:58,049 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 28.2s: 5011c @178c/s (957ch, ~1253t @44t/s)
2025-12-15 12:45:00,063 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 30.2s: 5352c @177c/s (1025ch, ~1338t @44t/s)
2025-12-15 12:45:02,083 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 32.2s: 5717c @178c/s (1093ch, ~1429t @44t/s)
2025-12-15 12:45:04,096 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 34.2s: 6051c @177c/s (1161ch, ~1513t @44t/s)
2025-12-15 12:45:06,107 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 36.2s: 6383c @176c/s (1229ch, ~1596t @44t/s)
2025-12-15 12:45:08,131 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 38.2s: 6751c @177c/s (1297ch, ~1688t @44t/s)
2025-12-15 12:45:10,155 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 40.3s: 7109c @177c/s (1365ch, ~1777t @44t/s)
2025-12-15 12:45:12,175 - src.llm.client - INFO - [lec:f5b2b7] ğŸ“Š 42.3s: 7478c @177c/s (1431ch, ~1870t @44t/s)
2025-12-15 12:45:12,175 - src.llm.client - INFO - [lec:f5b2b7] âœ“ Done 44.29s: 7478c (~1105w @169c/s)
2025-12-15 12:45:12,177 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:45:12,177 - src.generate.formats.lectures - INFO -     - Length: 7568 chars, 1116 words
2025-12-15 12:45:12,177 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:45:12,177 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:45:12,177 - src.generate.formats.lectures - INFO -     - Content: 14 examples, 4 terms defined
2025-12-15 12:45:12,177 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:45:12,180 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:45:12,181 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:45:12,181 - src.generate.formats.labs - INFO - Generating lab 14 for: Neuroscientific Evidence (Session 14)
2025-12-15 12:45:12,181 - src.llm.client - INFO - [lab:903cbd] ğŸš€ lab | m=gemma3:4b | p=3337c | t=150s
2025-12-15 12:45:12,181 - src.llm.client - INFO - [lab:903cbd] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:45:12,181 - src.llm.client - INFO - [lab:903cbd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:45:12,182 - src.llm.client - INFO - [lab:903cbd] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3768 bytes, prompt=3337 chars
2025-12-15 12:45:12,182 - src.llm.client - INFO - [lab:903cbd] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:45:14,273 - src.llm.request_handler - INFO - [lab:903cbd] âœ“ Done 2.09s
2025-12-15 12:45:14,273 - src.llm.client - INFO - [lab:903cbd] âœ… HTTP 200 in 2.09s
2025-12-15 12:45:14,273 - src.llm.client - INFO - [lab:903cbd] ğŸ“¡ Stream active (200)
2025-12-15 12:45:14,273 - src.llm.client - INFO - [lab:903cbd] Starting stream parsing, waiting for first chunk...
2025-12-15 12:45:16,274 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 2.0s: 330c @165c/s (69ch, ~82t @41t/s)
2025-12-15 12:45:18,278 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 4.0s: 742c @185c/s (138ch, ~186t @46t/s)
2025-12-15 12:45:20,282 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 6.0s: 1092c @182c/s (207ch, ~273t @45t/s)
2025-12-15 12:45:22,296 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 8.0s: 1451c @181c/s (276ch, ~363t @45t/s)
2025-12-15 12:45:24,317 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 10.0s: 1702c @169c/s (345ch, ~426t @42t/s)
2025-12-15 12:45:26,344 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 12.1s: 1980c @164c/s (414ch, ~495t @41t/s)
2025-12-15 12:45:28,346 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 14.1s: 2217c @158c/s (482ch, ~554t @39t/s)
2025-12-15 12:45:30,373 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 16.1s: 2547c @158c/s (551ch, ~637t @40t/s)
2025-12-15 12:45:32,377 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 18.1s: 2839c @157c/s (619ch, ~710t @39t/s)
2025-12-15 12:45:34,382 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 20.1s: 3108c @155c/s (687ch, ~777t @39t/s)
2025-12-15 12:45:36,383 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 22.1s: 3369c @152c/s (755ch, ~842t @38t/s)
2025-12-15 12:45:38,395 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 24.1s: 3650c @151c/s (823ch, ~912t @38t/s)
2025-12-15 12:45:40,398 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 26.1s: 3977c @152c/s (891ch, ~994t @38t/s)
2025-12-15 12:45:42,408 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 28.1s: 4435c @158c/s (959ch, ~1109t @39t/s)
2025-12-15 12:45:44,417 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 30.1s: 4782c @159c/s (1027ch, ~1196t @40t/s)
2025-12-15 12:45:46,426 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 32.2s: 5097c @159c/s (1095ch, ~1274t @40t/s)
2025-12-15 12:45:48,441 - src.llm.client - INFO - [lab:903cbd] ğŸ“Š 34.2s: 5486c @161c/s (1163ch, ~1372t @40t/s)
2025-12-15 12:45:50,439 - src.llm.client - INFO - [lab:903cbd] âœ“ Done 38.26s: 5859c (~780w @153c/s)
2025-12-15 12:45:50,440 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:45:50,440 - src.generate.formats.labs - INFO -     - Length: 5950 chars, 793 words
2025-12-15 12:45:50,440 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-15 12:45:50,440 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 12:45:50,440 - src.generate.formats.labs - INFO -     - Data tables: 12
2025-12-15 12:45:50,443 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:45:50,443 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:45:50,443 - src.generate.formats.study_notes - INFO - Generating study notes for: Neuroscientific Evidence (Session 14)
2025-12-15 12:45:50,443 - src.llm.client - INFO - [stu:7a5851] ğŸš€ stu | m=gemma3:4b | p=4459c | t=120s
2025-12-15 12:45:50,443 - src.llm.client - INFO - [stu:7a5851] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:45:50,443 - src.llm.client - INFO - [stu:7a5851] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:45:50,444 - src.llm.client - INFO - [stu:7a5851] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8105 bytes, prompt=4459 chars
2025-12-15 12:45:50,444 - src.llm.client - INFO - [stu:7a5851] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:45:52,976 - src.llm.request_handler - INFO - [stu:7a5851] âœ“ Done 2.53s
2025-12-15 12:45:52,976 - src.llm.client - INFO - [stu:7a5851] âœ… HTTP 200 in 2.53s
2025-12-15 12:45:52,976 - src.llm.client - INFO - [stu:7a5851] ğŸ“¡ Stream active (200)
2025-12-15 12:45:52,976 - src.llm.client - INFO - [stu:7a5851] Starting stream parsing, waiting for first chunk...
2025-12-15 12:45:54,986 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 2.0s: 409c @203c/s (68ch, ~102t @51t/s)
2025-12-15 12:45:56,992 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 4.0s: 810c @202c/s (136ch, ~202t @50t/s)
2025-12-15 12:45:58,994 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 6.0s: 1201c @200c/s (204ch, ~300t @50t/s)
2025-12-15 12:46:01,004 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 8.0s: 1583c @197c/s (272ch, ~396t @49t/s)
2025-12-15 12:46:03,015 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 10.0s: 1987c @198c/s (340ch, ~497t @49t/s)
2025-12-15 12:46:05,017 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 12.0s: 2354c @195c/s (408ch, ~588t @49t/s)
2025-12-15 12:46:07,022 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 14.0s: 2674c @190c/s (476ch, ~668t @48t/s)
2025-12-15 12:46:09,029 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 16.1s: 3068c @191c/s (544ch, ~767t @48t/s)
2025-12-15 12:46:11,036 - src.llm.client - INFO - [stu:7a5851] ğŸ“Š 18.1s: 3379c @187c/s (612ch, ~845t @47t/s)
2025-12-15 12:46:11,835 - src.llm.client - INFO - [stu:7a5851] âœ“ Done 21.39s: 3512c (~485w @164c/s)
2025-12-15 12:46:11,835 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:46:11,835 - src.generate.formats.study_notes - INFO -     - Length: 3571 chars, 494 words
2025-12-15 12:46:11,836 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:46:11,836 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-15 12:46:11,836 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 6 bullets
2025-12-15 12:46:11,836 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:46:11,837 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:46:11,837 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:46:11,838 - src.generate.formats.diagrams - INFO - Generating diagram for: Cortical Representations (Neuroscientific Evidence)
2025-12-15 12:46:11,838 - src.llm.client - INFO - [dia:8aaeaf] ğŸš€ dia | m=gemma3:4b | p=5753c | t=120s
2025-12-15 12:46:11,838 - src.llm.client - INFO - [dia:8aaeaf] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:46:11,838 - src.llm.client - INFO - [dia:8aaeaf] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:46:11,838 - src.generate.formats.diagrams - INFO - Generating diagram for: Predictive Coding (Neuroscientific Evidence)
2025-12-15 12:46:11,838 - src.llm.client - INFO - [dia:7dcc29] ğŸš€ dia | m=gemma3:4b | p=5739c | t=120s
2025-12-15 12:46:11,838 - src.llm.client - INFO - [dia:7dcc29] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:46:11,838 - src.llm.client - INFO - [dia:7dcc29] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:46:11,840 - src.llm.client - INFO - [dia:8aaeaf] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11062 bytes, prompt=5753 chars
2025-12-15 12:46:11,840 - src.llm.client - INFO - [dia:8aaeaf] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:46:11,840 - src.llm.client - INFO - [dia:7dcc29] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11048 bytes, prompt=5739 chars
2025-12-15 12:46:11,840 - src.llm.client - INFO - [dia:7dcc29] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:46:15,416 - src.llm.request_handler - INFO - [dia:8aaeaf] âœ“ Done 3.58s
2025-12-15 12:46:15,416 - src.llm.client - INFO - [dia:8aaeaf] âœ… HTTP 200 in 3.58s
2025-12-15 12:46:15,416 - src.llm.client - INFO - [dia:8aaeaf] ğŸ“¡ Stream active (200)
2025-12-15 12:46:15,417 - src.llm.client - INFO - [dia:8aaeaf] Starting stream parsing, waiting for first chunk...
2025-12-15 12:46:17,433 - src.llm.client - INFO - [dia:8aaeaf] ğŸ“Š 2.0s: 248c @123c/s (68ch, ~62t @31t/s)
2025-12-15 12:46:19,460 - src.llm.client - INFO - [dia:8aaeaf] ğŸ“Š 4.0s: 477c @118c/s (136ch, ~119t @29t/s)
2025-12-15 12:46:21,471 - src.llm.client - INFO - [dia:8aaeaf] ğŸ“Š 6.1s: 711c @117c/s (204ch, ~178t @29t/s)
2025-12-15 12:46:23,484 - src.llm.client - INFO - [dia:8aaeaf] ğŸ“Š 8.1s: 965c @120c/s (272ch, ~241t @30t/s)
2025-12-15 12:46:25,490 - src.llm.client - INFO - [dia:8aaeaf] ğŸ“Š 10.1s: 1244c @123c/s (340ch, ~311t @31t/s)
2025-12-15 12:46:27,348 - src.llm.client - INFO - [dia:8aaeaf] âœ“ Done 15.51s: 1406c (~175w @91c/s)
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Cortical Representations (Neuroscientific Evidence):
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO -     - Length: 1289 chars (cleaned: 1289 chars)
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO - [OK] Elements: 55 total (nodes: 27, connections: 28) âœ“
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:46:27,348 - src.generate.formats.diagrams - INFO - Generated diagram: 1289 characters
2025-12-15 12:46:30,637 - src.llm.request_handler - INFO - [dia:7dcc29] âœ“ Done 18.80s
2025-12-15 12:46:30,637 - src.llm.client - INFO - [dia:7dcc29] âœ… HTTP 200 in 18.80s
2025-12-15 12:46:30,637 - src.llm.client - INFO - [dia:7dcc29] ğŸ“¡ Stream active (200)
2025-12-15 12:46:30,637 - src.llm.client - INFO - [dia:7dcc29] Starting stream parsing, waiting for first chunk...
2025-12-15 12:46:32,663 - src.llm.client - INFO - [dia:7dcc29] ğŸ“Š 2.0s: 253c @125c/s (68ch, ~63t @31t/s)
2025-12-15 12:46:34,675 - src.llm.client - INFO - [dia:7dcc29] ğŸ“Š 4.0s: 507c @126c/s (136ch, ~127t @31t/s)
2025-12-15 12:46:36,680 - src.llm.client - INFO - [dia:7dcc29] ğŸ“Š 6.0s: 739c @122c/s (204ch, ~185t @31t/s)
2025-12-15 12:46:38,690 - src.llm.client - INFO - [dia:7dcc29] ğŸ“Š 8.1s: 922c @115c/s (272ch, ~230t @29t/s)
2025-12-15 12:46:40,694 - src.llm.client - INFO - [dia:7dcc29] ğŸ“Š 10.1s: 1082c @108c/s (340ch, ~270t @27t/s)
2025-12-15 12:46:42,704 - src.llm.client - INFO - [dia:7dcc29] ğŸ“Š 12.1s: 1230c @102c/s (408ch, ~308t @25t/s)
2025-12-15 12:46:44,710 - src.llm.client - INFO - [dia:7dcc29] ğŸ“Š 14.1s: 1379c @98c/s (476ch, ~345t @24t/s)
2025-12-15 12:46:45,380 - src.llm.client - INFO - [dia:7dcc29] âœ“ Done 33.54s: 1422c (~149w @42c/s)
2025-12-15 12:46:45,380 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Predictive Coding (Neuroscientific Evidence):
2025-12-15 12:46:45,380 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:46:45,380 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:46:45,380 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:46:45,381 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:46:45,381 - src.generate.formats.diagrams - INFO -     - Length: 852 chars (cleaned: 852 chars)
2025-12-15 12:46:45,381 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:46:45,381 - src.generate.formats.diagrams - INFO - [OK] Elements: 42 total (nodes: 16, connections: 26) âœ“
2025-12-15 12:46:45,381 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:46:45,381 - src.generate.formats.diagrams - INFO - Generated diagram: 852 characters
2025-12-15 12:46:45,381 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:46:45,381 - src.generate.formats.questions - INFO - Generating 10 questions for: Neuroscientific Evidence (Session 14)
2025-12-15 12:46:45,381 - src.llm.client - INFO - [qst:a9a549] ğŸš€ qst | m=gemma3:4b | p=7338c | t=150s
2025-12-15 12:46:45,381 - src.llm.client - INFO - [qst:a9a549] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:46:45,381 - src.llm.client - INFO - [qst:a9a549] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:46:45,383 - src.llm.client - INFO - [qst:a9a549] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11014 bytes, prompt=7338 chars
2025-12-15 12:46:45,383 - src.llm.client - INFO - [qst:a9a549] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:46:49,507 - src.llm.request_handler - INFO - [qst:a9a549] âœ“ Done 4.12s
2025-12-15 12:46:49,507 - src.llm.client - INFO - [qst:a9a549] âœ… HTTP 200 in 4.12s
2025-12-15 12:46:49,507 - src.llm.client - INFO - [qst:a9a549] ğŸ“¡ Stream active (200)
2025-12-15 12:46:49,507 - src.llm.client - INFO - [qst:a9a549] Starting stream parsing, waiting for first chunk...
2025-12-15 12:46:51,522 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 2.0s: 345c @171c/s (68ch, ~86t @43t/s)
2025-12-15 12:46:53,549 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 4.0s: 699c @173c/s (136ch, ~175t @43t/s)
2025-12-15 12:46:55,571 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 6.1s: 1049c @173c/s (204ch, ~262t @43t/s)
2025-12-15 12:46:57,579 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 8.1s: 1393c @173c/s (272ch, ~348t @43t/s)
2025-12-15 12:46:59,587 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 10.1s: 1738c @172c/s (340ch, ~434t @43t/s)
2025-12-15 12:47:01,604 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 12.1s: 2040c @169c/s (408ch, ~510t @42t/s)
2025-12-15 12:47:03,625 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 14.1s: 2376c @168c/s (476ch, ~594t @42t/s)
2025-12-15 12:47:05,637 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 16.1s: 2717c @168c/s (544ch, ~679t @42t/s)
2025-12-15 12:47:07,659 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 18.2s: 3121c @172c/s (612ch, ~780t @43t/s)
2025-12-15 12:47:09,678 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 20.2s: 3506c @174c/s (680ch, ~876t @43t/s)
2025-12-15 12:47:11,699 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 22.2s: 3881c @175c/s (748ch, ~970t @44t/s)
2025-12-15 12:47:13,700 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 24.2s: 4264c @176c/s (815ch, ~1066t @44t/s)
2025-12-15 12:47:15,727 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 26.2s: 4673c @178c/s (883ch, ~1168t @45t/s)
2025-12-15 12:47:17,728 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 28.2s: 5088c @180c/s (950ch, ~1272t @45t/s)
2025-12-15 12:47:19,729 - src.llm.client - INFO - [qst:a9a549] ğŸ“Š 30.2s: 5501c @182c/s (1017ch, ~1375t @46t/s)
2025-12-15 12:47:20,048 - src.llm.client - INFO - [qst:a9a549] âœ“ Done 34.67s: 5547c (~755w @160c/s)
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -     Context: Module 8 Session 14
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -     Context: Module 8 Session 14
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Neuroscientific Evidence (Session 14)
2025-12-15 12:47:20,049 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:47:20,052 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:47:20,053 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 14 completed
2025-12-15 12:47:20,053 - src.generate.orchestration.pipeline - INFO - 
[15/20] Session 15: Motor Control
2025-12-15 12:47:20,054 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:47:20,054 - src.generate.formats.lectures - INFO - Generating lecture for: Neuroscientific Evidence (Session 15/20)
2025-12-15 12:47:20,054 - src.llm.client - INFO - [lec:736bbd] ğŸš€ lec | m=gemma3:4b | p=3020c | t=180s
2025-12-15 12:47:20,054 - src.llm.client - INFO - [lec:736bbd] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:47:20,054 - src.llm.client - INFO - [lec:736bbd] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:47:20,055 - src.llm.client - INFO - [lec:736bbd] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6650 bytes, prompt=3020 chars
2025-12-15 12:47:20,055 - src.llm.client - INFO - [lec:736bbd] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:47:22,046 - src.llm.request_handler - INFO - [lec:736bbd] âœ“ Done 1.99s
2025-12-15 12:47:22,046 - src.llm.client - INFO - [lec:736bbd] âœ… HTTP 200 in 1.99s
2025-12-15 12:47:22,046 - src.llm.client - INFO - [lec:736bbd] ğŸ“¡ Stream active (200)
2025-12-15 12:47:22,047 - src.llm.client - INFO - [lec:736bbd] Starting stream parsing, waiting for first chunk...
2025-12-15 12:47:24,059 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 2.0s: 405c @201c/s (69ch, ~101t @50t/s)
2025-12-15 12:47:26,071 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 4.0s: 781c @194c/s (138ch, ~195t @49t/s)
2025-12-15 12:47:28,074 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 6.0s: 1132c @188c/s (206ch, ~283t @47t/s)
2025-12-15 12:47:30,100 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 8.1s: 1475c @183c/s (275ch, ~369t @46t/s)
2025-12-15 12:47:32,106 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 10.1s: 1863c @185c/s (343ch, ~466t @46t/s)
2025-12-15 12:47:34,112 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 12.1s: 2237c @185c/s (411ch, ~559t @46t/s)
2025-12-15 12:47:36,114 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 14.1s: 2593c @184c/s (479ch, ~648t @46t/s)
2025-12-15 12:47:38,121 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 16.1s: 2974c @185c/s (547ch, ~744t @46t/s)
2025-12-15 12:47:40,124 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 18.1s: 3326c @184c/s (615ch, ~832t @46t/s)
2025-12-15 12:47:42,130 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 20.1s: 3687c @184c/s (683ch, ~922t @46t/s)
2025-12-15 12:47:44,137 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 22.1s: 4028c @182c/s (751ch, ~1007t @46t/s)
2025-12-15 12:47:46,146 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 24.1s: 4381c @182c/s (819ch, ~1095t @45t/s)
2025-12-15 12:47:48,154 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 26.1s: 4712c @180c/s (887ch, ~1178t @45t/s)
2025-12-15 12:47:50,161 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 28.1s: 5098c @181c/s (955ch, ~1274t @45t/s)
2025-12-15 12:47:52,173 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 30.1s: 5502c @183c/s (1023ch, ~1376t @46t/s)
2025-12-15 12:47:54,176 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 32.1s: 5866c @183c/s (1090ch, ~1466t @46t/s)
2025-12-15 12:47:56,191 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 34.1s: 6197c @181c/s (1158ch, ~1549t @45t/s)
2025-12-15 12:47:58,206 - src.llm.client - INFO - [lec:736bbd] ğŸ“Š 36.2s: 6603c @183c/s (1226ch, ~1651t @46t/s)
2025-12-15 12:47:59,714 - src.llm.client - INFO - [lec:736bbd] âœ“ Done 39.66s: 6872c (~1028w @173c/s)
2025-12-15 12:47:59,715 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:47:59,715 - src.generate.formats.lectures - INFO -     - Length: 6968 chars, 1040 words
2025-12-15 12:47:59,715 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:47:59,715 - src.generate.formats.lectures - INFO -     - Structure: 5 sections, 2 subsections
2025-12-15 12:47:59,715 - src.generate.formats.lectures - INFO -     - Content: 9 examples, 4 terms defined
2025-12-15 12:47:59,715 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:47:59,719 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:47:59,719 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:47:59,719 - src.generate.formats.labs - INFO - Generating lab 15 for: Neuroscientific Evidence (Session 15)
2025-12-15 12:47:59,719 - src.llm.client - INFO - [lab:b8d4d3] ğŸš€ lab | m=gemma3:4b | p=3336c | t=150s
2025-12-15 12:47:59,719 - src.llm.client - INFO - [lab:b8d4d3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:47:59,719 - src.llm.client - INFO - [lab:b8d4d3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:47:59,721 - src.llm.client - INFO - [lab:b8d4d3] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3776 bytes, prompt=3336 chars
2025-12-15 12:47:59,721 - src.llm.client - INFO - [lab:b8d4d3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:48:01,526 - src.llm.request_handler - INFO - [lab:b8d4d3] âœ“ Done 1.80s
2025-12-15 12:48:01,526 - src.llm.client - INFO - [lab:b8d4d3] âœ… HTTP 200 in 1.81s
2025-12-15 12:48:01,526 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“¡ Stream active (200)
2025-12-15 12:48:01,526 - src.llm.client - INFO - [lab:b8d4d3] Starting stream parsing, waiting for first chunk...
2025-12-15 12:48:03,529 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 2.0s: 327c @163c/s (69ch, ~82t @41t/s)
2025-12-15 12:48:05,531 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 4.0s: 765c @191c/s (138ch, ~191t @48t/s)
2025-12-15 12:48:07,542 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 6.0s: 1149c @191c/s (207ch, ~287t @48t/s)
2025-12-15 12:48:09,556 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 8.0s: 1462c @182c/s (276ch, ~366t @46t/s)
2025-12-15 12:48:11,581 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 10.1s: 1738c @173c/s (345ch, ~434t @43t/s)
2025-12-15 12:48:13,582 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 12.1s: 2003c @166c/s (413ch, ~501t @42t/s)
2025-12-15 12:48:15,608 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 14.1s: 2349c @167c/s (482ch, ~587t @42t/s)
2025-12-15 12:48:17,637 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 16.1s: 2644c @164c/s (551ch, ~661t @41t/s)
2025-12-15 12:48:19,638 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 18.1s: 2981c @165c/s (619ch, ~745t @41t/s)
2025-12-15 12:48:21,644 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 20.1s: 3321c @165c/s (687ch, ~830t @41t/s)
2025-12-15 12:48:23,649 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 22.1s: 3629c @164c/s (755ch, ~907t @41t/s)
2025-12-15 12:48:25,654 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 24.1s: 3886c @161c/s (823ch, ~972t @40t/s)
2025-12-15 12:48:27,661 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 26.1s: 4010c @153c/s (891ch, ~1002t @38t/s)
2025-12-15 12:48:29,670 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 28.1s: 4124c @147c/s (959ch, ~1031t @37t/s)
2025-12-15 12:48:31,680 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 30.2s: 4338c @144c/s (1027ch, ~1084t @36t/s)
2025-12-15 12:48:33,704 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 32.2s: 4674c @145c/s (1094ch, ~1168t @36t/s)
2025-12-15 12:48:35,719 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 34.2s: 5075c @148c/s (1162ch, ~1269t @37t/s)
2025-12-15 12:48:37,739 - src.llm.client - INFO - [lab:b8d4d3] ğŸ“Š 36.2s: 5487c @152c/s (1230ch, ~1372t @38t/s)
2025-12-15 12:48:39,622 - src.llm.client - INFO - [lab:b8d4d3] âœ“ Done 39.90s: 5890c (~876w @148c/s)
2025-12-15 12:48:39,623 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:48:39,623 - src.generate.formats.labs - INFO -     - Length: 5991 chars, 891 words
2025-12-15 12:48:39,623 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-15 12:48:39,623 - src.generate.formats.labs - INFO -     - Safety: 7 warnings
2025-12-15 12:48:39,623 - src.generate.formats.labs - INFO -     - Data tables: 14
2025-12-15 12:48:39,626 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:48:39,626 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:48:39,626 - src.generate.formats.study_notes - INFO - Generating study notes for: Neuroscientific Evidence (Session 15)
2025-12-15 12:48:39,626 - src.llm.client - INFO - [stu:f5a92d] ğŸš€ stu | m=gemma3:4b | p=4435c | t=120s
2025-12-15 12:48:39,627 - src.llm.client - INFO - [stu:f5a92d] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:48:39,627 - src.llm.client - INFO - [stu:f5a92d] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:48:39,628 - src.llm.client - INFO - [stu:f5a92d] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8090 bytes, prompt=4435 chars
2025-12-15 12:48:39,628 - src.llm.client - INFO - [stu:f5a92d] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:48:42,170 - src.llm.request_handler - INFO - [stu:f5a92d] âœ“ Done 2.54s
2025-12-15 12:48:42,170 - src.llm.client - INFO - [stu:f5a92d] âœ… HTTP 200 in 2.54s
2025-12-15 12:48:42,170 - src.llm.client - INFO - [stu:f5a92d] ğŸ“¡ Stream active (200)
2025-12-15 12:48:42,170 - src.llm.client - INFO - [stu:f5a92d] Starting stream parsing, waiting for first chunk...
2025-12-15 12:48:44,173 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 2.0s: 399c @199c/s (68ch, ~100t @50t/s)
2025-12-15 12:48:46,174 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 4.0s: 812c @203c/s (136ch, ~203t @51t/s)
2025-12-15 12:48:48,175 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 6.0s: 1166c @194c/s (204ch, ~292t @49t/s)
2025-12-15 12:48:50,182 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 8.0s: 1542c @192c/s (272ch, ~386t @48t/s)
2025-12-15 12:48:52,191 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 10.0s: 1866c @186c/s (340ch, ~466t @47t/s)
2025-12-15 12:48:54,203 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 12.0s: 2224c @185c/s (408ch, ~556t @46t/s)
2025-12-15 12:48:56,213 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 14.0s: 2546c @181c/s (476ch, ~636t @45t/s)
2025-12-15 12:48:58,227 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 16.1s: 2876c @179c/s (544ch, ~719t @45t/s)
2025-12-15 12:49:00,242 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 18.1s: 3207c @177c/s (612ch, ~802t @44t/s)
2025-12-15 12:49:02,264 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 20.1s: 3546c @176c/s (680ch, ~886t @44t/s)
2025-12-15 12:49:04,279 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 22.1s: 3928c @178c/s (748ch, ~982t @44t/s)
2025-12-15 12:49:06,305 - src.llm.client - INFO - [stu:f5a92d] ğŸ“Š 24.1s: 4215c @175c/s (815ch, ~1054t @44t/s)
2025-12-15 12:49:06,305 - src.llm.client - INFO - [stu:f5a92d] âœ“ Done 26.68s: 4215c (~575w @158c/s)
2025-12-15 12:49:06,306 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:49:06,306 - src.generate.formats.study_notes - INFO -     - Length: 4274 chars, 584 words
2025-12-15 12:49:06,306 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:49:06,306 - src.generate.formats.study_notes - INFO -     - Key concepts: 5
2025-12-15 12:49:06,306 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 20 bullets
2025-12-15 12:49:06,306 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:49:06,308 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:49:06,308 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:49:06,308 - src.generate.formats.diagrams - INFO - Generating diagram for: Internal Models of Movement (Neuroscientific Evidence)
2025-12-15 12:49:06,308 - src.llm.client - INFO - [dia:8f199f] ğŸš€ dia | m=gemma3:4b | p=5758c | t=120s
2025-12-15 12:49:06,308 - src.llm.client - INFO - [dia:8f199f] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:49:06,308 - src.llm.client - INFO - [dia:8f199f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:49:06,310 - src.llm.client - INFO - [dia:8f199f] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11067 bytes, prompt=5758 chars
2025-12-15 12:49:06,310 - src.llm.client - INFO - [dia:8f199f] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:49:09,863 - src.llm.request_handler - INFO - [dia:8f199f] âœ“ Done 3.55s
2025-12-15 12:49:09,863 - src.llm.client - INFO - [dia:8f199f] âœ… HTTP 200 in 3.55s
2025-12-15 12:49:09,863 - src.llm.client - INFO - [dia:8f199f] ğŸ“¡ Stream active (200)
2025-12-15 12:49:09,863 - src.llm.client - INFO - [dia:8f199f] Starting stream parsing, waiting for first chunk...
2025-12-15 12:49:11,886 - src.llm.client - INFO - [dia:8f199f] ğŸ“Š 2.0s: 219c @108c/s (68ch, ~55t @27t/s)
2025-12-15 12:49:13,897 - src.llm.client - INFO - [dia:8f199f] ğŸ“Š 4.0s: 445c @110c/s (136ch, ~111t @28t/s)
2025-12-15 12:49:15,904 - src.llm.client - INFO - [dia:8f199f] ğŸ“Š 6.0s: 637c @105c/s (204ch, ~159t @26t/s)
2025-12-15 12:49:17,912 - src.llm.client - INFO - [dia:8f199f] ğŸ“Š 8.0s: 799c @99c/s (272ch, ~200t @25t/s)
2025-12-15 12:49:19,920 - src.llm.client - INFO - [dia:8f199f] ğŸ“Š 10.1s: 946c @94c/s (340ch, ~236t @24t/s)
2025-12-15 12:49:21,931 - src.llm.client - INFO - [dia:8f199f] ğŸ“Š 12.1s: 1094c @91c/s (408ch, ~274t @23t/s)
2025-12-15 12:49:23,940 - src.llm.client - INFO - [dia:8f199f] ğŸ“Š 14.1s: 1241c @88c/s (476ch, ~310t @22t/s)
2025-12-15 12:49:25,352 - src.llm.client - INFO - [dia:8f199f] âœ“ Done 19.04s: 1335c (~158w @70c/s)
2025-12-15 12:49:25,352 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Internal Models of Movement (Neuroscientific Evidence):
2025-12-15 12:49:25,352 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:49:25,352 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:49:25,352 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:49:25,353 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:49:25,353 - src.generate.formats.diagrams - INFO -     - Length: 728 chars (cleaned: 728 chars)
2025-12-15 12:49:25,353 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:49:25,353 - src.generate.formats.diagrams - INFO - [OK] Elements: 37 total (nodes: 18, connections: 19) âœ“
2025-12-15 12:49:25,353 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:49:25,353 - src.generate.formats.diagrams - INFO - Generated diagram: 728 characters
2025-12-15 12:49:25,353 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:49:25,353 - src.generate.formats.questions - INFO - Generating 10 questions for: Neuroscientific Evidence (Session 15)
2025-12-15 12:49:25,353 - src.llm.client - INFO - [qst:8d450c] ğŸš€ qst | m=gemma3:4b | p=7327c | t=150s
2025-12-15 12:49:25,353 - src.llm.client - INFO - [qst:8d450c] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:49:25,353 - src.llm.client - INFO - [qst:8d450c] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:49:25,354 - src.llm.client - INFO - [qst:8d450c] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11006 bytes, prompt=7327 chars
2025-12-15 12:49:25,354 - src.llm.client - INFO - [qst:8d450c] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:51:29,361 - src.llm.request_handler - INFO - [qst:8d450c] âœ“ Done 124.01s
2025-12-15 12:51:29,361 - src.llm.client - INFO - [qst:8d450c] âœ… HTTP 200 in 124.01s
2025-12-15 12:51:29,362 - src.llm.client - INFO - [qst:8d450c] ğŸ“¡ Stream active (200)
2025-12-15 12:51:29,362 - src.llm.client - INFO - [qst:8d450c] Starting stream parsing, waiting for first chunk...
2025-12-15 12:51:31,377 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 2.0s: 322c @160c/s (68ch, ~80t @40t/s)
2025-12-15 12:51:33,389 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 4.0s: 650c @161c/s (136ch, ~162t @40t/s)
2025-12-15 12:51:35,396 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 6.0s: 1010c @167c/s (204ch, ~252t @42t/s)
2025-12-15 12:51:37,412 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 8.0s: 1378c @171c/s (272ch, ~344t @43t/s)
2025-12-15 12:51:39,419 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 10.1s: 1704c @169c/s (340ch, ~426t @42t/s)
2025-12-15 12:51:41,426 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 12.1s: 2053c @170c/s (408ch, ~513t @43t/s)
2025-12-15 12:51:43,444 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 14.1s: 2395c @170c/s (476ch, ~599t @43t/s)
2025-12-15 12:51:45,459 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 16.1s: 2731c @170c/s (544ch, ~683t @42t/s)
2025-12-15 12:51:47,484 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 18.1s: 3119c @172c/s (612ch, ~780t @43t/s)
2025-12-15 12:51:49,506 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 20.1s: 3484c @173c/s (680ch, ~871t @43t/s)
2025-12-15 12:51:51,526 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 22.2s: 3861c @174c/s (748ch, ~965t @44t/s)
2025-12-15 12:51:53,549 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 24.2s: 4268c @176c/s (816ch, ~1067t @44t/s)
2025-12-15 12:51:55,568 - src.llm.client - INFO - [qst:8d450c] ğŸ“Š 26.2s: 4650c @177c/s (883ch, ~1162t @44t/s)
2025-12-15 12:51:56,059 - src.llm.client - INFO - [qst:8d450c] âœ“ Done 150.71s: 4737c (~670w @31c/s)
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -     Context: Module 8 Session 15
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -     Context: Module 8 Session 15
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Neuroscientific Evidence (Session 15)
2025-12-15 12:51:56,060 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:51:56,063 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:51:56,064 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 15 completed
2025-12-15 12:51:56,064 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:51:56,065 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:51:56,065 - src.generate.orchestration.pipeline - INFO - Module 9: Applications: AI & Robotics (2 sessions)
2025-12-15 12:51:56,065 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:51:56,065 - src.generate.orchestration.pipeline - INFO - 
[16/20] Session 16: Robot Navigation
2025-12-15 12:51:56,065 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:51:56,065 - src.generate.formats.lectures - INFO - Generating lecture for: Applications: AI & Robotics (Session 16/20)
2025-12-15 12:51:56,065 - src.llm.client - INFO - [lec:7627de] ğŸš€ lec | m=gemma3:4b | p=3052c | t=180s
2025-12-15 12:51:56,065 - src.llm.client - INFO - [lec:7627de] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:51:56,065 - src.llm.client - INFO - [lec:7627de] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:51:56,066 - src.llm.client - INFO - [lec:7627de] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6683 bytes, prompt=3052 chars
2025-12-15 12:51:56,066 - src.llm.client - INFO - [lec:7627de] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:52:04,867 - src.llm.request_handler - INFO - [lec:7627de] âœ“ Done 8.80s
2025-12-15 12:52:04,868 - src.llm.client - INFO - [lec:7627de] âœ… HTTP 200 in 8.80s
2025-12-15 12:52:04,868 - src.llm.client - INFO - [lec:7627de] ğŸ“¡ Stream active (200)
2025-12-15 12:52:04,868 - src.llm.client - INFO - [lec:7627de] Starting stream parsing, waiting for first chunk...
2025-12-15 12:52:06,874 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 2.0s: 448c @223c/s (69ch, ~112t @56t/s)
2025-12-15 12:52:08,877 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 4.0s: 863c @215c/s (137ch, ~216t @54t/s)
2025-12-15 12:52:10,897 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 6.0s: 1234c @205c/s (206ch, ~308t @51t/s)
2025-12-15 12:52:12,898 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 8.0s: 1659c @207c/s (274ch, ~415t @52t/s)
2025-12-15 12:52:14,927 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 10.1s: 2078c @207c/s (343ch, ~520t @52t/s)
2025-12-15 12:52:16,929 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 12.1s: 2446c @203c/s (411ch, ~612t @51t/s)
2025-12-15 12:52:18,930 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 14.1s: 2783c @198c/s (479ch, ~696t @49t/s)
2025-12-15 12:52:20,933 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 16.1s: 3145c @196c/s (547ch, ~786t @49t/s)
2025-12-15 12:52:22,938 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 18.1s: 3547c @196c/s (615ch, ~887t @49t/s)
2025-12-15 12:52:24,945 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 20.1s: 3911c @195c/s (683ch, ~978t @49t/s)
2025-12-15 12:52:26,954 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 22.1s: 4252c @193c/s (751ch, ~1063t @48t/s)
2025-12-15 12:52:28,961 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 24.1s: 4618c @192c/s (819ch, ~1154t @48t/s)
2025-12-15 12:52:30,970 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 26.1s: 4983c @191c/s (887ch, ~1246t @48t/s)
2025-12-15 12:52:32,982 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 28.1s: 5363c @191c/s (955ch, ~1341t @48t/s)
2025-12-15 12:52:34,993 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 30.1s: 5743c @191c/s (1023ch, ~1436t @48t/s)
2025-12-15 12:52:37,014 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 32.1s: 6128c @191c/s (1091ch, ~1532t @48t/s)
2025-12-15 12:52:39,029 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 34.2s: 6527c @191c/s (1159ch, ~1632t @48t/s)
2025-12-15 12:52:41,045 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 36.2s: 6929c @192c/s (1227ch, ~1732t @48t/s)
2025-12-15 12:52:43,064 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 38.2s: 7307c @191c/s (1295ch, ~1827t @48t/s)
2025-12-15 12:52:45,069 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 40.2s: 7642c @190c/s (1362ch, ~1910t @48t/s)
2025-12-15 12:52:47,093 - src.llm.client - INFO - [lec:7627de] ğŸ“Š 42.2s: 8057c @191c/s (1430ch, ~2014t @48t/s)
2025-12-15 12:52:48,911 - src.llm.client - INFO - [lec:7627de] âœ“ Done 52.85s: 8439c (~1211w @160c/s)
2025-12-15 12:52:48,912 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:52:48,913 - src.generate.formats.lectures - INFO -     - Length: 8525 chars, 1224 words
2025-12-15 12:52:48,913 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:52:48,913 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 12:52:48,913 - src.generate.formats.lectures - INFO -     - Content: 12 examples, 0 terms defined
2025-12-15 12:52:48,913 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:52:48,917 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:52:48,917 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:52:48,917 - src.generate.formats.labs - INFO - Generating lab 16 for: Applications: AI & Robotics (Session 16)
2025-12-15 12:52:48,917 - src.llm.client - INFO - [lab:ce6ceb] ğŸš€ lab | m=gemma3:4b | p=3343c | t=150s
2025-12-15 12:52:48,917 - src.llm.client - INFO - [lab:ce6ceb] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:52:48,917 - src.llm.client - INFO - [lab:ce6ceb] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:52:48,918 - src.llm.client - INFO - [lab:ce6ceb] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3754 bytes, prompt=3343 chars
2025-12-15 12:52:48,918 - src.llm.client - INFO - [lab:ce6ceb] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:52:50,625 - src.llm.request_handler - INFO - [lab:ce6ceb] âœ“ Done 1.71s
2025-12-15 12:52:50,625 - src.llm.client - INFO - [lab:ce6ceb] âœ… HTTP 200 in 1.71s
2025-12-15 12:52:50,625 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“¡ Stream active (200)
2025-12-15 12:52:50,625 - src.llm.client - INFO - [lab:ce6ceb] Starting stream parsing, waiting for first chunk...
2025-12-15 12:52:52,650 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 2.0s: 331c @164c/s (70ch, ~83t @41t/s)
2025-12-15 12:52:54,673 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 4.0s: 759c @188c/s (139ch, ~190t @47t/s)
2025-12-15 12:52:56,680 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 6.1s: 1088c @180c/s (208ch, ~272t @45t/s)
2025-12-15 12:52:58,690 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 8.1s: 1463c @181c/s (277ch, ~366t @45t/s)
2025-12-15 12:53:00,714 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 10.1s: 1739c @172c/s (346ch, ~435t @43t/s)
2025-12-15 12:53:02,719 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 12.1s: 1994c @165c/s (414ch, ~498t @41t/s)
2025-12-15 12:53:04,747 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 14.1s: 2353c @167c/s (483ch, ~588t @42t/s)
2025-12-15 12:53:06,747 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 16.1s: 2677c @166c/s (551ch, ~669t @42t/s)
2025-12-15 12:53:08,752 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 18.1s: 2927c @161c/s (619ch, ~732t @40t/s)
2025-12-15 12:53:10,757 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 20.1s: 3190c @158c/s (687ch, ~798t @40t/s)
2025-12-15 12:53:12,763 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 22.1s: 3431c @155c/s (755ch, ~858t @39t/s)
2025-12-15 12:53:14,767 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 24.1s: 3634c @151c/s (823ch, ~908t @38t/s)
2025-12-15 12:53:16,774 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 26.1s: 3910c @150c/s (890ch, ~978t @37t/s)
2025-12-15 12:53:18,781 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 28.2s: 4178c @148c/s (958ch, ~1044t @37t/s)
2025-12-15 12:53:20,790 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 30.2s: 4374c @145c/s (1026ch, ~1094t @36t/s)
2025-12-15 12:53:22,800 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 32.2s: 4489c @140c/s (1094ch, ~1122t @35t/s)
2025-12-15 12:53:24,814 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 34.2s: 4802c @140c/s (1162ch, ~1200t @35t/s)
2025-12-15 12:53:26,828 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 36.2s: 5131c @142c/s (1230ch, ~1283t @35t/s)
2025-12-15 12:53:28,844 - src.llm.client - INFO - [lab:ce6ceb] ğŸ“Š 38.2s: 5517c @144c/s (1298ch, ~1379t @36t/s)
2025-12-15 12:53:29,522 - src.llm.client - INFO - [lab:ce6ceb] âœ“ Done 40.60s: 5631c (~828w @139c/s)
2025-12-15 12:53:29,522 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:53:29,522 - src.generate.formats.labs - INFO -     - Length: 5731 chars, 843 words
2025-12-15 12:53:29,522 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-15 12:53:29,522 - src.generate.formats.labs - INFO -     - Safety: 8 warnings
2025-12-15 12:53:29,522 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-15 12:53:29,526 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:53:29,526 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:53:29,526 - src.generate.formats.study_notes - INFO - Generating study notes for: Applications: AI & Robotics (Session 16)
2025-12-15 12:53:29,526 - src.llm.client - INFO - [stu:1fc5a3] ğŸš€ stu | m=gemma3:4b | p=4452c | t=120s
2025-12-15 12:53:29,526 - src.llm.client - INFO - [stu:1fc5a3] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:53:29,526 - src.llm.client - INFO - [stu:1fc5a3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:53:29,528 - src.llm.client - INFO - [stu:1fc5a3] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8078 bytes, prompt=4452 chars
2025-12-15 12:53:29,528 - src.llm.client - INFO - [stu:1fc5a3] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:53:32,240 - src.llm.request_handler - INFO - [stu:1fc5a3] âœ“ Done 2.71s
2025-12-15 12:53:32,240 - src.llm.client - INFO - [stu:1fc5a3] âœ… HTTP 200 in 2.71s
2025-12-15 12:53:32,240 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“¡ Stream active (200)
2025-12-15 12:53:32,240 - src.llm.client - INFO - [stu:1fc5a3] Starting stream parsing, waiting for first chunk...
2025-12-15 12:53:34,256 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 2.0s: 396c @196c/s (67ch, ~99t @49t/s)
2025-12-15 12:53:36,283 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 4.0s: 731c @181c/s (135ch, ~183t @45t/s)
2025-12-15 12:53:38,289 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 6.0s: 1033c @171c/s (193ch, ~258t @43t/s)
2025-12-15 12:53:40,300 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 8.1s: 1374c @170c/s (252ch, ~344t @43t/s)
2025-12-15 12:53:42,326 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 10.1s: 1720c @171c/s (315ch, ~430t @43t/s)
2025-12-15 12:53:44,337 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 12.1s: 2051c @170c/s (381ch, ~513t @42t/s)
2025-12-15 12:53:46,357 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 14.1s: 2419c @171c/s (450ch, ~605t @43t/s)
2025-12-15 12:53:48,369 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 16.1s: 2773c @172c/s (516ch, ~693t @43t/s)
2025-12-15 12:53:50,396 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 18.2s: 3148c @173c/s (584ch, ~787t @43t/s)
2025-12-15 12:53:52,416 - src.llm.client - INFO - [stu:1fc5a3] ğŸ“Š 20.2s: 3502c @174c/s (650ch, ~876t @43t/s)
2025-12-15 12:53:52,417 - src.llm.client - INFO - [stu:1fc5a3] âœ“ Done 22.89s: 3502c (~501w @153c/s)
2025-12-15 12:53:52,417 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:53:52,417 - src.generate.formats.study_notes - INFO -     - Length: 3564 chars, 512 words
2025-12-15 12:53:52,417 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:53:52,417 - src.generate.formats.study_notes - INFO -     - Key concepts: 6
2025-12-15 12:53:52,417 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 3 bullets
2025-12-15 12:53:52,417 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:53:52,419 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:53:52,419 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:53:52,420 - src.generate.formats.diagrams - INFO - Generating diagram for: Autonomous Exploration (Applications: AI & Robotics)
2025-12-15 12:53:52,420 - src.llm.client - INFO - [dia:c4c954] ğŸš€ dia | m=gemma3:4b | p=5754c | t=120s
2025-12-15 12:53:52,420 - src.llm.client - INFO - [dia:c4c954] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:53:52,420 - src.llm.client - INFO - [dia:c4c954] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:53:52,420 - src.generate.formats.diagrams - INFO - Generating diagram for: Perception-Action Loops (Applications: AI & Robotics)
2025-12-15 12:53:52,420 - src.llm.client - INFO - [dia:29c1c7] ğŸš€ dia | m=gemma3:4b | p=5756c | t=120s
2025-12-15 12:53:52,420 - src.llm.client - INFO - [dia:29c1c7] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:53:52,421 - src.llm.client - INFO - [dia:29c1c7] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:53:52,422 - src.llm.client - INFO - [dia:29c1c7] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11065 bytes, prompt=5756 chars
2025-12-15 12:53:52,422 - src.llm.client - INFO - [dia:29c1c7] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:53:52,422 - src.llm.client - INFO - [dia:c4c954] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11063 bytes, prompt=5754 chars
2025-12-15 12:53:52,422 - src.llm.client - INFO - [dia:c4c954] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:53:55,874 - src.llm.request_handler - INFO - [dia:29c1c7] âœ“ Done 3.45s
2025-12-15 12:53:55,874 - src.llm.client - INFO - [dia:29c1c7] âœ… HTTP 200 in 3.45s
2025-12-15 12:53:55,874 - src.llm.client - INFO - [dia:29c1c7] ğŸ“¡ Stream active (200)
2025-12-15 12:53:55,874 - src.llm.client - INFO - [dia:29c1c7] Starting stream parsing, waiting for first chunk...
2025-12-15 12:53:57,878 - src.llm.client - INFO - [dia:29c1c7] ğŸ“Š 2.0s: 254c @127c/s (67ch, ~64t @32t/s)
2025-12-15 12:53:59,902 - src.llm.client - INFO - [dia:29c1c7] ğŸ“Š 4.0s: 460c @114c/s (135ch, ~115t @29t/s)
2025-12-15 12:54:01,922 - src.llm.client - INFO - [dia:29c1c7] ğŸ“Š 6.0s: 652c @108c/s (203ch, ~163t @27t/s)
2025-12-15 12:54:03,945 - src.llm.client - INFO - [dia:29c1c7] ğŸ“Š 8.1s: 801c @99c/s (270ch, ~200t @25t/s)
2025-12-15 12:54:04,902 - src.llm.client - INFO - [dia:29c1c7] âœ“ Done 12.48s: 858c (~102w @69c/s)
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Perception-Action Loops (Applications: AI & Robotics):
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO -     - Length: 584 chars (cleaned: 584 chars)
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO - [OK] Elements: 36 total (nodes: 11, connections: 25) âœ“
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:54:04,903 - src.generate.formats.diagrams - INFO - Generated diagram: 584 characters
2025-12-15 12:54:08,266 - src.llm.request_handler - INFO - [dia:c4c954] âœ“ Done 15.84s
2025-12-15 12:54:08,267 - src.llm.client - INFO - [dia:c4c954] âœ… HTTP 200 in 15.84s
2025-12-15 12:54:08,267 - src.llm.client - INFO - [dia:c4c954] ğŸ“¡ Stream active (200)
2025-12-15 12:54:08,267 - src.llm.client - INFO - [dia:c4c954] Starting stream parsing, waiting for first chunk...
2025-12-15 12:54:10,273 - src.llm.client - INFO - [dia:c4c954] ğŸ“Š 2.0s: 249c @124c/s (66ch, ~62t @31t/s)
2025-12-15 12:54:12,273 - src.llm.client - INFO - [dia:c4c954] ğŸ“Š 4.0s: 467c @117c/s (132ch, ~117t @29t/s)
2025-12-15 12:54:14,301 - src.llm.client - INFO - [dia:c4c954] ğŸ“Š 6.0s: 692c @115c/s (199ch, ~173t @29t/s)
2025-12-15 12:54:16,324 - src.llm.client - INFO - [dia:c4c954] ğŸ“Š 8.1s: 897c @111c/s (266ch, ~224t @28t/s)
2025-12-15 12:54:17,365 - src.llm.client - INFO - [dia:c4c954] âœ“ Done 24.95s: 1005c (~131w @40c/s)
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Autonomous Exploration (Applications: AI & Robotics):
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO - [FIXED] Removed classDef command (not supported in all renderers) âœ“
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO -     - Length: 840 chars (cleaned: 840 chars)
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO - [OK] Elements: 41 total (nodes: 16, connections: 25) âœ“
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-15 12:54:17,366 - src.generate.formats.diagrams - INFO - Generated diagram: 840 characters
2025-12-15 12:54:17,367 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:54:17,367 - src.generate.formats.questions - INFO - Generating 10 questions for: Applications: AI & Robotics (Session 16)
2025-12-15 12:54:17,367 - src.llm.client - INFO - [qst:c2d888] ğŸš€ qst | m=gemma3:4b | p=7338c | t=150s
2025-12-15 12:54:17,367 - src.llm.client - INFO - [qst:c2d888] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:54:17,367 - src.llm.client - INFO - [qst:c2d888] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:54:17,368 - src.llm.client - INFO - [qst:c2d888] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11016 bytes, prompt=7338 chars
2025-12-15 12:54:17,368 - src.llm.client - INFO - [qst:c2d888] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:54:21,379 - src.llm.request_handler - INFO - [qst:c2d888] âœ“ Done 4.01s
2025-12-15 12:54:21,379 - src.llm.client - INFO - [qst:c2d888] âœ… HTTP 200 in 4.01s
2025-12-15 12:54:21,379 - src.llm.client - INFO - [qst:c2d888] ğŸ“¡ Stream active (200)
2025-12-15 12:54:21,379 - src.llm.client - INFO - [qst:c2d888] Starting stream parsing, waiting for first chunk...
2025-12-15 12:54:23,386 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 2.0s: 303c @151c/s (66ch, ~76t @38t/s)
2025-12-15 12:54:25,386 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 4.0s: 630c @157c/s (132ch, ~158t @39t/s)
2025-12-15 12:54:27,415 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 6.0s: 938c @155c/s (199ch, ~234t @39t/s)
2025-12-15 12:54:29,440 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 8.1s: 1266c @157c/s (266ch, ~316t @39t/s)
2025-12-15 12:54:31,466 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 10.1s: 1622c @161c/s (333ch, ~406t @40t/s)
2025-12-15 12:54:33,472 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 12.1s: 1932c @160c/s (399ch, ~483t @40t/s)
2025-12-15 12:54:35,501 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 14.1s: 2283c @162c/s (466ch, ~571t @40t/s)
2025-12-15 12:54:37,513 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 16.1s: 2604c @161c/s (531ch, ~651t @40t/s)
2025-12-15 12:54:39,515 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 18.1s: 2975c @164c/s (597ch, ~744t @41t/s)
2025-12-15 12:54:41,520 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 20.1s: 3372c @167c/s (663ch, ~843t @42t/s)
2025-12-15 12:54:43,529 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 22.1s: 3759c @170c/s (729ch, ~940t @42t/s)
2025-12-15 12:54:45,539 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 24.2s: 4090c @169c/s (795ch, ~1022t @42t/s)
2025-12-15 12:54:47,555 - src.llm.client - INFO - [qst:c2d888] ğŸ“Š 26.2s: 4404c @168c/s (861ch, ~1101t @42t/s)
2025-12-15 12:54:49,524 - src.llm.client - INFO - [qst:c2d888] âœ“ Done 32.16s: 4686c (~685w @146c/s)
2025-12-15 12:54:49,524 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 2, 'mc_options_fixed': 0, 'total_fixes': 2}
2025-12-15 12:54:49,524 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-15 12:54:49,525 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 1 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:54:49,525 - src.generate.formats.questions - WARNING -     Context: Module 9 Session 16
2025-12-15 12:54:49,525 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:54:49,525 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:54:49,525 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 1 issues
2025-12-15 12:54:49,525 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Applications: AI & Robotics (Session 16)
2025-12-15 12:54:49,525 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:54:49,527 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:54:49,529 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 16 completed
2025-12-15 12:54:49,529 - src.generate.orchestration.pipeline - INFO - 
[17/20] Session 17: Deep Learning & Active Inference
2025-12-15 12:54:49,529 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:54:49,529 - src.generate.formats.lectures - INFO - Generating lecture for: Applications: AI & Robotics (Session 17/20)
2025-12-15 12:54:49,530 - src.llm.client - INFO - [lec:bcaf54] ğŸš€ lec | m=gemma3:4b | p=3073c | t=180s
2025-12-15 12:54:49,530 - src.llm.client - INFO - [lec:bcaf54] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:54:49,530 - src.llm.client - INFO - [lec:bcaf54] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:54:49,531 - src.llm.client - INFO - [lec:bcaf54] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6703 bytes, prompt=3073 chars
2025-12-15 12:54:49,531 - src.llm.client - INFO - [lec:bcaf54] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:54:52,181 - src.llm.request_handler - INFO - [lec:bcaf54] âœ“ Done 2.65s
2025-12-15 12:54:52,181 - src.llm.client - INFO - [lec:bcaf54] âœ… HTTP 200 in 2.65s
2025-12-15 12:54:52,181 - src.llm.client - INFO - [lec:bcaf54] ğŸ“¡ Stream active (200)
2025-12-15 12:54:52,181 - src.llm.client - INFO - [lec:bcaf54] Starting stream parsing, waiting for first chunk...
2025-12-15 12:54:54,181 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 2.0s: 394c @197c/s (67ch, ~98t @49t/s)
2025-12-15 12:54:56,210 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 4.0s: 755c @187c/s (135ch, ~189t @47t/s)
2025-12-15 12:54:58,220 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 6.0s: 1058c @175c/s (202ch, ~264t @44t/s)
2025-12-15 12:55:00,239 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 8.1s: 1358c @169c/s (269ch, ~340t @42t/s)
2025-12-15 12:55:02,258 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 10.1s: 1750c @174c/s (336ch, ~438t @43t/s)
2025-12-15 12:55:04,279 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 12.1s: 2093c @173c/s (403ch, ~523t @43t/s)
2025-12-15 12:55:06,300 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 14.1s: 2416c @171c/s (470ch, ~604t @43t/s)
2025-12-15 12:55:08,329 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 16.1s: 2752c @170c/s (537ch, ~688t @43t/s)
2025-12-15 12:55:10,356 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 18.2s: 3108c @171c/s (604ch, ~777t @43t/s)
2025-12-15 12:55:12,381 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 20.2s: 3463c @171c/s (671ch, ~866t @43t/s)
2025-12-15 12:55:14,406 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 22.2s: 3803c @171c/s (738ch, ~951t @43t/s)
2025-12-15 12:55:16,435 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 24.3s: 4165c @172c/s (805ch, ~1041t @43t/s)
2025-12-15 12:55:18,435 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 26.3s: 4523c @172c/s (871ch, ~1131t @43t/s)
2025-12-15 12:55:20,441 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 28.3s: 4875c @173c/s (937ch, ~1219t @43t/s)
2025-12-15 12:55:22,441 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 30.3s: 5258c @174c/s (1003ch, ~1314t @43t/s)
2025-12-15 12:55:24,469 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 32.3s: 5540c @172c/s (1070ch, ~1385t @43t/s)
2025-12-15 12:55:26,473 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 34.3s: 5890c @172c/s (1136ch, ~1472t @43t/s)
2025-12-15 12:55:28,475 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 36.3s: 6243c @172c/s (1202ch, ~1561t @43t/s)
2025-12-15 12:55:30,480 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 38.3s: 6585c @172c/s (1268ch, ~1646t @43t/s)
2025-12-15 12:55:32,494 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 40.3s: 6970c @173c/s (1334ch, ~1742t @43t/s)
2025-12-15 12:55:34,507 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 42.3s: 7364c @174c/s (1400ch, ~1841t @43t/s)
2025-12-15 12:55:36,520 - src.llm.client - INFO - [lec:bcaf54] ğŸ“Š 44.3s: 7780c @175c/s (1466ch, ~1945t @44t/s)
2025-12-15 12:55:37,921 - src.llm.client - INFO - [lec:bcaf54] âœ“ Done 48.39s: 8060c (~1221w @167c/s)
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO -     - Length: 8141 chars, 1234 words
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO -     - Structure: 7 sections, 2 subsections
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO -     - Content: 19 examples, 0 terms defined
2025-12-15 12:55:37,922 - src.generate.formats.lectures - WARNING - [WARNING] Too many examples (19, maximum 15, 4 excess - consider consolidating or removing less critical examples) âš ï¸
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 12:55:37,922 - src.generate.formats.lectures - INFO - Quality score: 98.0/100 (excellent)
2025-12-15 12:55:37,926 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:55:37,926 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:55:37,926 - src.generate.formats.labs - INFO - Generating lab 17 for: Applications: AI & Robotics (Session 17)
2025-12-15 12:55:37,926 - src.llm.client - INFO - [lab:6bc8b3] ğŸš€ lab | m=gemma3:4b | p=3329c | t=150s
2025-12-15 12:55:37,926 - src.llm.client - INFO - [lab:6bc8b3] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:55:37,926 - src.llm.client - INFO - [lab:6bc8b3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:55:37,927 - src.llm.client - INFO - [lab:6bc8b3] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3784 bytes, prompt=3329 chars
2025-12-15 12:55:37,928 - src.llm.client - INFO - [lab:6bc8b3] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:55:39,831 - src.llm.request_handler - INFO - [lab:6bc8b3] âœ“ Done 1.90s
2025-12-15 12:55:39,831 - src.llm.client - INFO - [lab:6bc8b3] âœ… HTTP 200 in 1.90s
2025-12-15 12:55:39,831 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“¡ Stream active (200)
2025-12-15 12:55:39,831 - src.llm.client - INFO - [lab:6bc8b3] Starting stream parsing, waiting for first chunk...
2025-12-15 12:55:41,853 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 2.0s: 283c @140c/s (68ch, ~71t @35t/s)
2025-12-15 12:55:43,853 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 4.0s: 647c @161c/s (135ch, ~162t @40t/s)
2025-12-15 12:55:45,880 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 6.0s: 985c @163c/s (203ch, ~246t @41t/s)
2025-12-15 12:55:47,885 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 8.1s: 1283c @159c/s (270ch, ~321t @40t/s)
2025-12-15 12:55:49,899 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 10.1s: 1535c @152c/s (337ch, ~384t @38t/s)
2025-12-15 12:55:51,919 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 12.1s: 1791c @148c/s (404ch, ~448t @37t/s)
2025-12-15 12:55:53,936 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 14.1s: 2042c @145c/s (471ch, ~510t @36t/s)
2025-12-15 12:55:55,951 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 16.1s: 2368c @147c/s (538ch, ~592t @37t/s)
2025-12-15 12:55:57,972 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 18.1s: 2665c @147c/s (605ch, ~666t @37t/s)
2025-12-15 12:55:59,994 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 20.2s: 2909c @144c/s (672ch, ~727t @36t/s)
2025-12-15 12:56:02,016 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 22.2s: 3185c @144c/s (739ch, ~796t @36t/s)
2025-12-15 12:56:04,044 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 24.2s: 3431c @142c/s (806ch, ~858t @35t/s)
2025-12-15 12:56:06,067 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 26.2s: 3634c @139c/s (873ch, ~908t @35t/s)
2025-12-15 12:56:08,070 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 28.2s: 4035c @143c/s (939ch, ~1009t @36t/s)
2025-12-15 12:56:10,099 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 30.3s: 4630c @153c/s (1006ch, ~1158t @38t/s)
2025-12-15 12:56:12,128 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 32.3s: 5109c @158c/s (1073ch, ~1277t @40t/s)
2025-12-15 12:56:14,129 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 34.3s: 5385c @157c/s (1139ch, ~1346t @39t/s)
2025-12-15 12:56:16,129 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 36.3s: 5716c @157c/s (1205ch, ~1429t @39t/s)
2025-12-15 12:56:18,130 - src.llm.client - INFO - [lab:6bc8b3] ğŸ“Š 38.3s: 6077c @159c/s (1271ch, ~1519t @40t/s)
2025-12-15 12:56:19,336 - src.llm.client - INFO - [lab:6bc8b3] âœ“ Done 41.41s: 6271c (~809w @151c/s)
2025-12-15 12:56:19,336 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:56:19,336 - src.generate.formats.labs - INFO -     - Length: 6379 chars, 825 words
2025-12-15 12:56:19,336 - src.generate.formats.labs - INFO -     - Procedure: 7 steps
2025-12-15 12:56:19,336 - src.generate.formats.labs - INFO -     - Safety: 7 warnings
2025-12-15 12:56:19,336 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-15 12:56:19,339 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:56:19,340 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:56:19,340 - src.generate.formats.study_notes - INFO - Generating study notes for: Applications: AI & Robotics (Session 17)
2025-12-15 12:56:19,340 - src.llm.client - INFO - [stu:536107] ğŸš€ stu | m=gemma3:4b | p=4434c | t=120s
2025-12-15 12:56:19,340 - src.llm.client - INFO - [stu:536107] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:56:19,340 - src.llm.client - INFO - [stu:536107] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:56:19,342 - src.llm.client - INFO - [stu:536107] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8104 bytes, prompt=4434 chars
2025-12-15 12:56:19,342 - src.llm.client - INFO - [stu:536107] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:56:22,598 - src.llm.request_handler - INFO - [stu:536107] âœ“ Done 3.26s
2025-12-15 12:56:22,598 - src.llm.client - INFO - [stu:536107] âœ… HTTP 200 in 3.26s
2025-12-15 12:56:22,598 - src.llm.client - INFO - [stu:536107] ğŸ“¡ Stream active (200)
2025-12-15 12:56:22,598 - src.llm.client - INFO - [stu:536107] Starting stream parsing, waiting for first chunk...
2025-12-15 12:56:24,599 - src.llm.client - INFO - [stu:536107] ğŸ“Š 2.0s: 396c @198c/s (66ch, ~99t @49t/s)
2025-12-15 12:56:26,623 - src.llm.client - INFO - [stu:536107] ğŸ“Š 4.0s: 796c @198c/s (133ch, ~199t @49t/s)
2025-12-15 12:56:28,646 - src.llm.client - INFO - [stu:536107] ğŸ“Š 6.0s: 1204c @199c/s (200ch, ~301t @50t/s)
2025-12-15 12:56:30,667 - src.llm.client - INFO - [stu:536107] ğŸ“Š 8.1s: 1566c @194c/s (267ch, ~392t @49t/s)
2025-12-15 12:56:32,692 - src.llm.client - INFO - [stu:536107] ğŸ“Š 10.1s: 1928c @191c/s (334ch, ~482t @48t/s)
2025-12-15 12:56:34,717 - src.llm.client - INFO - [stu:536107] ğŸ“Š 12.1s: 2300c @190c/s (401ch, ~575t @47t/s)
2025-12-15 12:56:36,746 - src.llm.client - INFO - [stu:536107] ğŸ“Š 14.1s: 2644c @187c/s (468ch, ~661t @47t/s)
2025-12-15 12:56:38,749 - src.llm.client - INFO - [stu:536107] ğŸ“Š 16.2s: 2971c @184c/s (533ch, ~743t @46t/s)
2025-12-15 12:56:40,777 - src.llm.client - INFO - [stu:536107] ğŸ“Š 18.2s: 3385c @186c/s (600ch, ~846t @47t/s)
2025-12-15 12:56:42,787 - src.llm.client - INFO - [stu:536107] ğŸ“Š 20.2s: 3789c @188c/s (665ch, ~947t @47t/s)
2025-12-15 12:56:42,787 - src.llm.client - INFO - [stu:536107] âœ“ Done 23.45s: 3789c (~539w @162c/s)
2025-12-15 12:56:42,788 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:56:42,788 - src.generate.formats.study_notes - INFO -     - Length: 3851 chars, 550 words
2025-12-15 12:56:42,788 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:56:42,788 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-15 12:56:42,788 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 0 bullets
2025-12-15 12:56:42,788 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:56:42,789 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:56:42,790 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:56:42,790 - src.generate.formats.diagrams - INFO - Generating diagram for: Generative Adversarial Networks (Applications: AI & Robotics)
2025-12-15 12:56:42,790 - src.llm.client - INFO - [dia:07a812] ğŸš€ dia | m=gemma3:4b | p=5788c | t=120s
2025-12-15 12:56:42,790 - src.llm.client - INFO - [dia:07a812] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:56:42,790 - src.llm.client - INFO - [dia:07a812] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:56:42,791 - src.llm.client - INFO - [dia:07a812] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11097 bytes, prompt=5788 chars
2025-12-15 12:56:42,791 - src.llm.client - INFO - [dia:07a812] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:56:46,425 - src.llm.request_handler - INFO - [dia:07a812] âœ“ Done 3.63s
2025-12-15 12:56:46,425 - src.llm.client - INFO - [dia:07a812] âœ… HTTP 200 in 3.63s
2025-12-15 12:56:46,426 - src.llm.client - INFO - [dia:07a812] ğŸ“¡ Stream active (200)
2025-12-15 12:56:46,426 - src.llm.client - INFO - [dia:07a812] Starting stream parsing, waiting for first chunk...
2025-12-15 12:56:48,438 - src.llm.client - INFO - [dia:07a812] ğŸ“Š 2.0s: 251c @125c/s (66ch, ~63t @31t/s)
2025-12-15 12:56:50,439 - src.llm.client - INFO - [dia:07a812] ğŸ“Š 4.0s: 470c @117c/s (132ch, ~118t @29t/s)
2025-12-15 12:56:52,462 - src.llm.client - INFO - [dia:07a812] ğŸ“Š 6.0s: 641c @106c/s (199ch, ~160t @27t/s)
2025-12-15 12:56:54,169 - src.llm.client - INFO - [dia:07a812] âœ“ Done 11.38s: 772c (~98w @68c/s)
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Generative Adversarial Networks (Applications: AI & Robotics):
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO -     - Length: 498 chars (cleaned: 498 chars)
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO - [OK] Elements: 31 total (nodes: 9, connections: 22) âœ“
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:56:54,170 - src.generate.formats.diagrams - INFO - Generated diagram: 498 characters
2025-12-15 12:56:54,171 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:56:54,171 - src.generate.formats.questions - INFO - Generating 10 questions for: Applications: AI & Robotics (Session 17)
2025-12-15 12:56:54,171 - src.llm.client - INFO - [qst:287784] ğŸš€ qst | m=gemma3:4b | p=7316c | t=150s
2025-12-15 12:56:54,171 - src.llm.client - INFO - [qst:287784] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:56:54,171 - src.llm.client - INFO - [qst:287784] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:56:54,172 - src.llm.client - INFO - [qst:287784] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11019 bytes, prompt=7316 chars
2025-12-15 12:56:54,172 - src.llm.client - INFO - [qst:287784] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:56:58,404 - src.llm.request_handler - INFO - [qst:287784] âœ“ Done 4.23s
2025-12-15 12:56:58,404 - src.llm.client - INFO - [qst:287784] âœ… HTTP 200 in 4.23s
2025-12-15 12:56:58,404 - src.llm.client - INFO - [qst:287784] ğŸ“¡ Stream active (200)
2025-12-15 12:56:58,404 - src.llm.client - INFO - [qst:287784] Starting stream parsing, waiting for first chunk...
2025-12-15 12:57:00,410 - src.llm.client - INFO - [qst:287784] ğŸ“Š 2.0s: 303c @151c/s (66ch, ~76t @38t/s)
2025-12-15 12:57:02,439 - src.llm.client - INFO - [qst:287784] ğŸ“Š 4.0s: 622c @154c/s (133ch, ~156t @39t/s)
2025-12-15 12:57:04,467 - src.llm.client - INFO - [qst:287784] ğŸ“Š 6.1s: 965c @159c/s (200ch, ~241t @40t/s)
2025-12-15 12:57:06,493 - src.llm.client - INFO - [qst:287784] ğŸ“Š 8.1s: 1250c @155c/s (267ch, ~312t @39t/s)
2025-12-15 12:57:08,498 - src.llm.client - INFO - [qst:287784] ğŸ“Š 10.1s: 1613c @160c/s (333ch, ~403t @40t/s)
2025-12-15 12:57:10,503 - src.llm.client - INFO - [qst:287784] ğŸ“Š 12.1s: 1918c @159c/s (399ch, ~480t @40t/s)
2025-12-15 12:57:12,507 - src.llm.client - INFO - [qst:287784] ğŸ“Š 14.1s: 2265c @161c/s (465ch, ~566t @40t/s)
2025-12-15 12:57:14,516 - src.llm.client - INFO - [qst:287784] ğŸ“Š 16.1s: 2536c @157c/s (531ch, ~634t @39t/s)
2025-12-15 12:57:16,524 - src.llm.client - INFO - [qst:287784] ğŸ“Š 18.1s: 2879c @159c/s (597ch, ~720t @40t/s)
2025-12-15 12:57:18,547 - src.llm.client - INFO - [qst:287784] ğŸ“Š 20.1s: 3268c @162c/s (663ch, ~817t @41t/s)
2025-12-15 12:57:20,574 - src.llm.client - INFO - [qst:287784] ğŸ“Š 22.2s: 3660c @165c/s (729ch, ~915t @41t/s)
2025-12-15 12:57:22,594 - src.llm.client - INFO - [qst:287784] ğŸ“Š 24.2s: 4017c @166c/s (795ch, ~1004t @42t/s)
2025-12-15 12:57:24,622 - src.llm.client - INFO - [qst:287784] ğŸ“Š 26.2s: 4298c @164c/s (861ch, ~1074t @41t/s)
2025-12-15 12:57:26,653 - src.llm.client - INFO - [qst:287784] ğŸ“Š 28.2s: 4653c @165c/s (927ch, ~1163t @41t/s)
2025-12-15 12:57:28,457 - src.llm.client - INFO - [qst:287784] âœ“ Done 34.29s: 4991c (~698w @146c/s)
2025-12-15 12:57:28,457 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 5 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 2, 'total_fixes': 5}
2025-12-15 12:57:28,457 - src.generate.formats.questions - INFO - Applied 5 auto-fixes to questions
2025-12-15 12:57:28,457 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 2 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -     Context: Module 9 Session 17
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 4 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -     Context: Module 9 Session 17
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Applications: AI & Robotics (Session 17)
2025-12-15 12:57:28,458 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 12:57:28,460 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:57:28,462 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 17 completed
2025-12-15 12:57:28,462 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 12:57:28,462 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:57:28,462 - src.generate.orchestration.pipeline - INFO - Module 10: Concluding Remarks & Future Directions (3 sessions)
2025-12-15 12:57:28,462 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 12:57:28,462 - src.generate.orchestration.pipeline - INFO - 
[18/20] Session 18: Philosophical Implications
2025-12-15 12:57:28,462 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 12:57:28,462 - src.generate.formats.lectures - INFO - Generating lecture for: Concluding Remarks & Future Directions (Session 18/20)
2025-12-15 12:57:28,462 - src.llm.client - INFO - [lec:9c6817] ğŸš€ lec | m=gemma3:4b | p=3056c | t=180s
2025-12-15 12:57:28,462 - src.llm.client - INFO - [lec:9c6817] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 12:57:28,463 - src.llm.client - INFO - [lec:9c6817] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:57:28,464 - src.llm.client - INFO - [lec:9c6817] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6687 bytes, prompt=3056 chars
2025-12-15 12:57:28,464 - src.llm.client - INFO - [lec:9c6817] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 12:57:30,622 - src.llm.request_handler - INFO - [lec:9c6817] âœ“ Done 2.16s
2025-12-15 12:57:30,622 - src.llm.client - INFO - [lec:9c6817] âœ… HTTP 200 in 2.16s
2025-12-15 12:57:30,622 - src.llm.client - INFO - [lec:9c6817] ğŸ“¡ Stream active (200)
2025-12-15 12:57:30,623 - src.llm.client - INFO - [lec:9c6817] Starting stream parsing, waiting for first chunk...
2025-12-15 12:57:32,628 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 2.0s: 373c @186c/s (67ch, ~93t @47t/s)
2025-12-15 12:57:34,632 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 4.0s: 773c @193c/s (134ch, ~193t @48t/s)
2025-12-15 12:57:36,660 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 6.0s: 1094c @181c/s (201ch, ~274t @45t/s)
2025-12-15 12:57:38,666 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 8.0s: 1441c @179c/s (266ch, ~360t @45t/s)
2025-12-15 12:57:40,687 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 10.1s: 1764c @175c/s (333ch, ~441t @44t/s)
2025-12-15 12:57:42,705 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 12.1s: 2084c @172c/s (400ch, ~521t @43t/s)
2025-12-15 12:57:44,724 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 14.1s: 2444c @173c/s (467ch, ~611t @43t/s)
2025-12-15 12:57:46,745 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 16.1s: 2793c @173c/s (534ch, ~698t @43t/s)
2025-12-15 12:57:48,773 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 18.2s: 3149c @173c/s (601ch, ~787t @43t/s)
2025-12-15 12:57:50,798 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 20.2s: 3507c @174c/s (668ch, ~877t @43t/s)
2025-12-15 12:57:52,825 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 22.2s: 3795c @171c/s (735ch, ~949t @43t/s)
2025-12-15 12:57:54,851 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 24.2s: 4132c @171c/s (802ch, ~1033t @43t/s)
2025-12-15 12:57:56,875 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 26.3s: 4508c @172c/s (869ch, ~1127t @43t/s)
2025-12-15 12:57:58,904 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 28.3s: 4899c @173c/s (936ch, ~1225t @43t/s)
2025-12-15 12:58:00,933 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 30.3s: 5255c @173c/s (1003ch, ~1314t @43t/s)
2025-12-15 12:58:02,963 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 32.3s: 5591c @173c/s (1070ch, ~1398t @43t/s)
2025-12-15 12:58:04,965 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 34.3s: 5937c @173c/s (1136ch, ~1484t @43t/s)
2025-12-15 12:58:06,971 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 36.3s: 6269c @172c/s (1202ch, ~1567t @43t/s)
2025-12-15 12:58:08,980 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 38.4s: 6620c @173c/s (1268ch, ~1655t @43t/s)
2025-12-15 12:58:10,992 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 40.4s: 7050c @175c/s (1334ch, ~1762t @44t/s)
2025-12-15 12:58:13,003 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 42.4s: 7432c @175c/s (1400ch, ~1858t @44t/s)
2025-12-15 12:58:15,016 - src.llm.client - INFO - [lec:9c6817] ğŸ“Š 44.4s: 7824c @176c/s (1466ch, ~1956t @44t/s)
2025-12-15 12:58:16,484 - src.llm.client - INFO - [lec:9c6817] âœ“ Done 48.02s: 8111c (~1206w @169c/s)
2025-12-15 12:58:16,485 - src.generate.formats.lectures - INFO - [COMPLIANT] Lecture generated âœ“
2025-12-15 12:58:16,485 - src.generate.formats.lectures - INFO -     - Length: 8220 chars, 1220 words
2025-12-15 12:58:16,485 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 12:58:16,485 - src.generate.formats.lectures - INFO -     - Structure: 7 sections, 0 subsections
2025-12-15 12:58:16,485 - src.generate.formats.lectures - INFO -     - Content: 9 examples, 1 terms defined
2025-12-15 12:58:16,485 - src.generate.formats.lectures - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:58:16,489 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:58:16,490 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 12:58:16,490 - src.generate.formats.labs - INFO - Generating lab 18 for: Concluding Remarks & Future Directions (Session 18)
2025-12-15 12:58:16,490 - src.llm.client - INFO - [lab:5b1a8e] ğŸš€ lab | m=gemma3:4b | p=3328c | t=150s
2025-12-15 12:58:16,490 - src.llm.client - INFO - [lab:5b1a8e] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:58:16,490 - src.llm.client - INFO - [lab:5b1a8e] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:58:16,491 - src.llm.client - INFO - [lab:5b1a8e] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3761 bytes, prompt=3328 chars
2025-12-15 12:58:16,491 - src.llm.client - INFO - [lab:5b1a8e] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:58:18,301 - src.llm.request_handler - INFO - [lab:5b1a8e] âœ“ Done 1.81s
2025-12-15 12:58:18,301 - src.llm.client - INFO - [lab:5b1a8e] âœ… HTTP 200 in 1.81s
2025-12-15 12:58:18,301 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“¡ Stream active (200)
2025-12-15 12:58:18,301 - src.llm.client - INFO - [lab:5b1a8e] Starting stream parsing, waiting for first chunk...
2025-12-15 12:58:20,322 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 2.0s: 275c @136c/s (68ch, ~69t @34t/s)
2025-12-15 12:58:22,350 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 4.0s: 708c @175c/s (136ch, ~177t @44t/s)
2025-12-15 12:58:24,351 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 6.1s: 1016c @168c/s (203ch, ~254t @42t/s)
2025-12-15 12:58:26,359 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 8.1s: 1331c @165c/s (270ch, ~333t @41t/s)
2025-12-15 12:58:28,372 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 10.1s: 1532c @152c/s (337ch, ~383t @38t/s)
2025-12-15 12:58:30,391 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 12.1s: 1803c @149c/s (404ch, ~451t @37t/s)
2025-12-15 12:58:32,413 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 14.1s: 2113c @150c/s (471ch, ~528t @37t/s)
2025-12-15 12:58:34,435 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 16.1s: 2370c @147c/s (538ch, ~592t @37t/s)
2025-12-15 12:58:36,457 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 18.2s: 2699c @149c/s (605ch, ~675t @37t/s)
2025-12-15 12:58:38,463 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 20.2s: 3002c @149c/s (670ch, ~750t @37t/s)
2025-12-15 12:58:40,484 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 22.2s: 3274c @148c/s (737ch, ~818t @37t/s)
2025-12-15 12:58:42,508 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 24.2s: 3533c @146c/s (804ch, ~883t @36t/s)
2025-12-15 12:58:44,534 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 26.2s: 4035c @154c/s (871ch, ~1009t @38t/s)
2025-12-15 12:58:46,561 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 28.3s: 4564c @162c/s (938ch, ~1141t @40t/s)
2025-12-15 12:58:48,565 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 30.3s: 5035c @166c/s (1004ch, ~1259t @42t/s)
2025-12-15 12:58:50,587 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 32.3s: 5355c @166c/s (1069ch, ~1339t @41t/s)
2025-12-15 12:58:52,596 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 34.3s: 5691c @166c/s (1135ch, ~1423t @41t/s)
2025-12-15 12:58:54,603 - src.llm.client - INFO - [lab:5b1a8e] ğŸ“Š 36.3s: 6058c @167c/s (1201ch, ~1514t @42t/s)
2025-12-15 12:58:55,346 - src.llm.client - INFO - [lab:5b1a8e] âœ“ Done 38.86s: 6215c (~770w @160c/s)
2025-12-15 12:58:55,346 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 12:58:55,346 - src.generate.formats.labs - INFO -     - Length: 6309 chars, 785 words
2025-12-15 12:58:55,346 - src.generate.formats.labs - INFO -     - Procedure: 12 steps
2025-12-15 12:58:55,346 - src.generate.formats.labs - INFO -     - Safety: 3 warnings
2025-12-15 12:58:55,346 - src.generate.formats.labs - INFO -     - Data tables: 12
2025-12-15 12:58:55,350 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:58:55,350 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 12:58:55,350 - src.generate.formats.study_notes - INFO - Generating study notes for: Concluding Remarks & Future Directions (Session 18)
2025-12-15 12:58:55,350 - src.llm.client - INFO - [stu:de2c2b] ğŸš€ stu | m=gemma3:4b | p=4444c | t=120s
2025-12-15 12:58:55,350 - src.llm.client - INFO - [stu:de2c2b] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:58:55,350 - src.llm.client - INFO - [stu:de2c2b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:58:55,352 - src.llm.client - INFO - [stu:de2c2b] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8092 bytes, prompt=4444 chars
2025-12-15 12:58:55,352 - src.llm.client - INFO - [stu:de2c2b] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:58:58,321 - src.llm.request_handler - INFO - [stu:de2c2b] âœ“ Done 2.97s
2025-12-15 12:58:58,321 - src.llm.client - INFO - [stu:de2c2b] âœ… HTTP 200 in 2.97s
2025-12-15 12:58:58,321 - src.llm.client - INFO - [stu:de2c2b] ğŸ“¡ Stream active (200)
2025-12-15 12:58:58,321 - src.llm.client - INFO - [stu:de2c2b] Starting stream parsing, waiting for first chunk...
2025-12-15 12:59:00,322 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 2.0s: 370c @185c/s (66ch, ~92t @46t/s)
2025-12-15 12:59:02,350 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 4.0s: 716c @178c/s (133ch, ~179t @44t/s)
2025-12-15 12:59:04,368 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 6.0s: 1055c @174c/s (200ch, ~264t @44t/s)
2025-12-15 12:59:06,392 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 8.1s: 1387c @172c/s (267ch, ~347t @43t/s)
2025-12-15 12:59:08,396 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 10.1s: 1766c @175c/s (333ch, ~442t @44t/s)
2025-12-15 12:59:10,425 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 12.1s: 2160c @178c/s (400ch, ~540t @45t/s)
2025-12-15 12:59:12,426 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 14.1s: 2526c @179c/s (466ch, ~632t @45t/s)
2025-12-15 12:59:14,428 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 16.1s: 2929c @182c/s (532ch, ~732t @45t/s)
2025-12-15 12:59:16,437 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 18.1s: 3308c @183c/s (598ch, ~827t @46t/s)
2025-12-15 12:59:18,444 - src.llm.client - INFO - [stu:de2c2b] ğŸ“Š 20.1s: 3747c @186c/s (664ch, ~937t @47t/s)
2025-12-15 12:59:19,551 - src.llm.client - INFO - [stu:de2c2b] âœ“ Done 24.20s: 3979c (~537w @164c/s)
2025-12-15 12:59:19,551 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 12:59:19,551 - src.generate.formats.study_notes - INFO -     - Length: 4052 chars, 549 words
2025-12-15 12:59:19,551 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 12:59:19,551 - src.generate.formats.study_notes - INFO -     - Key concepts: 10
2025-12-15 12:59:19,551 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 4 bullets
2025-12-15 12:59:19,551 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 12:59:19,553 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 12:59:19,553 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 12:59:19,553 - src.generate.formats.diagrams - INFO - Generating diagram for: Embodied Cognition (Concluding Remarks & Future Directions)
2025-12-15 12:59:19,554 - src.generate.formats.diagrams - INFO - Generating diagram for: Agency (Concluding Remarks & Future Directions)
2025-12-15 12:59:19,554 - src.llm.client - INFO - [dia:e3221a] ğŸš€ dia | m=gemma3:4b | p=5743c | t=120s
2025-12-15 12:59:19,554 - src.llm.client - INFO - [dia:e3221a] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:59:19,554 - src.llm.client - INFO - [dia:e3221a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:59:19,554 - src.llm.client - INFO - [dia:93fe3b] ğŸš€ dia | m=gemma3:4b | p=5767c | t=120s
2025-12-15 12:59:19,554 - src.llm.client - INFO - [dia:93fe3b] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 12:59:19,554 - src.llm.client - INFO - [dia:93fe3b] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:59:19,555 - src.llm.client - INFO - [dia:e3221a] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11052 bytes, prompt=5743 chars
2025-12-15 12:59:19,555 - src.llm.client - INFO - [dia:e3221a] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:59:19,555 - src.llm.client - INFO - [dia:93fe3b] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11076 bytes, prompt=5767 chars
2025-12-15 12:59:19,556 - src.llm.client - INFO - [dia:93fe3b] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 12:59:23,188 - src.llm.request_handler - INFO - [dia:e3221a] âœ“ Done 3.63s
2025-12-15 12:59:23,189 - src.llm.client - INFO - [dia:e3221a] âœ… HTTP 200 in 3.63s
2025-12-15 12:59:23,189 - src.llm.client - INFO - [dia:e3221a] ğŸ“¡ Stream active (200)
2025-12-15 12:59:23,189 - src.llm.client - INFO - [dia:e3221a] Starting stream parsing, waiting for first chunk...
2025-12-15 12:59:25,204 - src.llm.client - INFO - [dia:e3221a] ğŸ“Š 2.0s: 245c @122c/s (66ch, ~61t @30t/s)
2025-12-15 12:59:27,213 - src.llm.client - INFO - [dia:e3221a] ğŸ“Š 4.0s: 500c @124c/s (132ch, ~125t @31t/s)
2025-12-15 12:59:29,215 - src.llm.client - INFO - [dia:e3221a] ğŸ“Š 6.0s: 762c @126c/s (198ch, ~190t @32t/s)
2025-12-15 12:59:31,216 - src.llm.client - INFO - [dia:e3221a] ğŸ“Š 8.0s: 1030c @128c/s (264ch, ~258t @32t/s)
2025-12-15 12:59:33,246 - src.llm.client - INFO - [dia:e3221a] ğŸ“Š 10.1s: 1278c @127c/s (331ch, ~320t @32t/s)
2025-12-15 12:59:35,249 - src.llm.client - INFO - [dia:e3221a] ğŸ“Š 12.1s: 1436c @119c/s (397ch, ~359t @30t/s)
2025-12-15 12:59:37,177 - src.llm.client - INFO - [dia:e3221a] âœ“ Done 17.62s: 1519c (~209w @86c/s)
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Agency (Concluding Remarks & Future Directions):
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO - [FIXED] Removed linkStyle command (not supported in all renderers) âœ“
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO -     - Length: 1319 chars (cleaned: 1319 chars)
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO - [OK] Elements: 51 total (nodes: 17, connections: 34) âœ“
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO -   Cleanup summary: 4 issues fixed (code fences, style commands, etc.)
2025-12-15 12:59:37,177 - src.generate.formats.diagrams - INFO - Generated diagram: 1319 characters
2025-12-15 12:59:40,622 - src.llm.request_handler - INFO - [dia:93fe3b] âœ“ Done 21.07s
2025-12-15 12:59:40,622 - src.llm.client - INFO - [dia:93fe3b] âœ… HTTP 200 in 21.07s
2025-12-15 12:59:40,622 - src.llm.client - INFO - [dia:93fe3b] ğŸ“¡ Stream active (200)
2025-12-15 12:59:40,622 - src.llm.client - INFO - [dia:93fe3b] Starting stream parsing, waiting for first chunk...
2025-12-15 12:59:42,636 - src.llm.client - INFO - [dia:93fe3b] ğŸ“Š 2.0s: 255c @127c/s (66ch, ~64t @32t/s)
2025-12-15 12:59:44,643 - src.llm.client - INFO - [dia:93fe3b] ğŸ“Š 4.0s: 480c @119c/s (132ch, ~120t @30t/s)
2025-12-15 12:59:46,645 - src.llm.client - INFO - [dia:93fe3b] ğŸ“Š 6.0s: 696c @116c/s (198ch, ~174t @29t/s)
2025-12-15 12:59:48,647 - src.llm.client - INFO - [dia:93fe3b] ğŸ“Š 8.0s: 916c @114c/s (264ch, ~229t @29t/s)
2025-12-15 12:59:50,653 - src.llm.client - INFO - [dia:93fe3b] ğŸ“Š 10.0s: 1106c @110c/s (330ch, ~276t @28t/s)
2025-12-15 12:59:51,183 - src.llm.client - INFO - [dia:93fe3b] âœ“ Done 31.63s: 1145c (~162w @36c/s)
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Embodied Cognition (Concluding Remarks & Future Directions):
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO -     - Length: 987 chars (cleaned: 987 chars)
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO - [OK] Elements: 41 total (nodes: 11, connections: 30) âœ“
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 12:59:51,183 - src.generate.formats.diagrams - INFO - Generated diagram: 987 characters
2025-12-15 12:59:51,184 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 12:59:51,184 - src.generate.formats.questions - INFO - Generating 10 questions for: Concluding Remarks & Future Directions (Session 18)
2025-12-15 12:59:51,184 - src.llm.client - INFO - [qst:f3735a] ğŸš€ qst | m=gemma3:4b | p=7340c | t=150s
2025-12-15 12:59:51,184 - src.llm.client - INFO - [qst:f3735a] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 12:59:51,184 - src.llm.client - INFO - [qst:f3735a] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 12:59:51,185 - src.llm.client - INFO - [qst:f3735a] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11022 bytes, prompt=7340 chars
2025-12-15 12:59:51,185 - src.llm.client - INFO - [qst:f3735a] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 12:59:55,413 - src.llm.request_handler - INFO - [qst:f3735a] âœ“ Done 4.23s
2025-12-15 12:59:55,413 - src.llm.client - INFO - [qst:f3735a] âœ… HTTP 200 in 4.23s
2025-12-15 12:59:55,413 - src.llm.client - INFO - [qst:f3735a] ğŸ“¡ Stream active (200)
2025-12-15 12:59:55,413 - src.llm.client - INFO - [qst:f3735a] Starting stream parsing, waiting for first chunk...
2025-12-15 12:59:57,425 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 2.0s: 303c @151c/s (66ch, ~76t @38t/s)
2025-12-15 12:59:59,428 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 4.0s: 588c @146c/s (132ch, ~147t @37t/s)
2025-12-15 13:00:01,435 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 6.0s: 933c @155c/s (198ch, ~233t @39t/s)
2025-12-15 13:00:03,437 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 8.0s: 1226c @153c/s (264ch, ~306t @38t/s)
2025-12-15 13:00:05,438 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 10.0s: 1505c @150c/s (330ch, ~376t @38t/s)
2025-12-15 13:00:07,449 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 12.0s: 1790c @149c/s (396ch, ~448t @37t/s)
2025-12-15 13:00:09,457 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 14.0s: 2106c @150c/s (462ch, ~526t @37t/s)
2025-12-15 13:00:11,472 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 16.1s: 2461c @153c/s (528ch, ~615t @38t/s)
2025-12-15 13:00:13,484 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 18.1s: 2800c @155c/s (594ch, ~700t @39t/s)
2025-12-15 13:00:15,494 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 20.1s: 3131c @156c/s (660ch, ~783t @39t/s)
2025-12-15 13:00:17,515 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 22.1s: 3503c @158c/s (726ch, ~876t @40t/s)
2025-12-15 13:00:19,531 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 24.1s: 3900c @162c/s (792ch, ~975t @40t/s)
2025-12-15 13:00:21,557 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 26.1s: 4283c @164c/s (858ch, ~1071t @41t/s)
2025-12-15 13:00:23,585 - src.llm.client - INFO - [qst:f3735a] ğŸ“Š 28.2s: 4622c @164c/s (924ch, ~1156t @41t/s)
2025-12-15 13:00:23,815 - src.llm.client - INFO - [qst:f3735a] âœ“ Done 32.63s: 4648c (~667w @142c/s)
2025-12-15 13:00:23,815 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 2 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 1, 'total_fixes': 2}
2025-12-15 13:00:23,815 - src.generate.formats.questions - INFO - Applied 2 auto-fixes to questions
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -     Context: Module 10 Session 18
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 3 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -     Context: Module 10 Session 18
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Concluding Remarks & Future Directions (Session 18)
2025-12-15 13:00:23,816 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 13:00:23,818 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:00:23,820 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 18 completed
2025-12-15 13:00:23,820 - src.generate.orchestration.pipeline - INFO - 
[19/20] Session 19: Open Questions & Research Frontiers
2025-12-15 13:00:23,820 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 13:00:23,820 - src.generate.formats.lectures - INFO - Generating lecture for: Concluding Remarks & Future Directions (Session 19/20)
2025-12-15 13:00:23,820 - src.llm.client - INFO - [lec:fc3519] ğŸš€ lec | m=gemma3:4b | p=3097c | t=180s
2025-12-15 13:00:23,820 - src.llm.client - INFO - [lec:fc3519] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 13:00:23,820 - src.llm.client - INFO - [lec:fc3519] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:00:23,821 - src.llm.client - INFO - [lec:fc3519] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6728 bytes, prompt=3097 chars
2025-12-15 13:00:23,822 - src.llm.client - INFO - [lec:fc3519] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 13:00:25,850 - src.llm.request_handler - INFO - [lec:fc3519] âœ“ Done 2.03s
2025-12-15 13:00:25,850 - src.llm.client - INFO - [lec:fc3519] âœ… HTTP 200 in 2.03s
2025-12-15 13:00:25,850 - src.llm.client - INFO - [lec:fc3519] ğŸ“¡ Stream active (200)
2025-12-15 13:00:25,850 - src.llm.client - INFO - [lec:fc3519] Starting stream parsing, waiting for first chunk...
2025-12-15 13:00:27,861 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 2.0s: 407c @202c/s (67ch, ~102t @51t/s)
2025-12-15 13:00:29,868 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 4.0s: 802c @200c/s (134ch, ~200t @50t/s)
2025-12-15 13:00:31,882 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 6.0s: 1228c @204c/s (201ch, ~307t @51t/s)
2025-12-15 13:00:33,911 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 8.1s: 1642c @204c/s (268ch, ~410t @51t/s)
2025-12-15 13:00:35,934 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 10.1s: 2035c @202c/s (335ch, ~509t @50t/s)
2025-12-15 13:00:37,962 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 12.1s: 2414c @199c/s (401ch, ~604t @50t/s)
2025-12-15 13:00:39,987 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 14.1s: 2845c @201c/s (468ch, ~711t @50t/s)
2025-12-15 13:00:42,013 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 16.2s: 3217c @199c/s (535ch, ~804t @50t/s)
2025-12-15 13:00:44,042 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 18.2s: 3628c @199c/s (602ch, ~907t @50t/s)
2025-12-15 13:00:46,044 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 20.2s: 3962c @196c/s (668ch, ~990t @49t/s)
2025-12-15 13:00:48,045 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 22.2s: 4304c @194c/s (734ch, ~1076t @48t/s)
2025-12-15 13:00:50,049 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 24.2s: 4699c @194c/s (800ch, ~1175t @49t/s)
2025-12-15 13:00:52,055 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 26.2s: 5090c @194c/s (866ch, ~1272t @49t/s)
2025-12-15 13:00:54,060 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 28.2s: 5465c @194c/s (932ch, ~1366t @48t/s)
2025-12-15 13:00:56,066 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 30.2s: 5878c @195c/s (998ch, ~1470t @49t/s)
2025-12-15 13:00:58,076 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 32.2s: 6245c @194c/s (1064ch, ~1561t @48t/s)
2025-12-15 13:01:00,081 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 34.2s: 6624c @194c/s (1130ch, ~1656t @48t/s)
2025-12-15 13:01:02,089 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 36.2s: 6978c @193c/s (1196ch, ~1744t @48t/s)
2025-12-15 13:01:04,096 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 38.2s: 7338c @192c/s (1262ch, ~1834t @48t/s)
2025-12-15 13:01:06,108 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 40.3s: 7678c @191c/s (1328ch, ~1920t @48t/s)
2025-12-15 13:01:08,121 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 42.3s: 8052c @190c/s (1393ch, ~2013t @48t/s)
2025-12-15 13:01:10,130 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 44.3s: 8461c @191c/s (1458ch, ~2115t @48t/s)
2025-12-15 13:01:12,149 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 46.3s: 8884c @192c/s (1524ch, ~2221t @48t/s)
2025-12-15 13:01:14,169 - src.llm.client - INFO - [lec:fc3519] ğŸ“Š 48.3s: 9314c @193c/s (1590ch, ~2328t @48t/s)
2025-12-15 13:01:15,703 - src.llm.client - INFO - [lec:fc3519] âœ“ Done 51.88s: 9638c (~1344w @186c/s)
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO -     - Length: 9743 chars, 1359 words
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO -     - Structure: 9 sections, 0 subsections
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO -     - Content: 19 examples, 0 terms defined
2025-12-15 13:01:15,704 - src.generate.formats.lectures - WARNING - [WARNING] Too many examples (19, maximum 15, 4 excess - consider consolidating or removing less critical examples) âš ï¸
2025-12-15 13:01:15,704 - src.generate.formats.lectures - WARNING - [WARNING] Too many sections (9, maximum 8, 1 excess - consider merging related sections) âš ï¸
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:01:15,704 - src.generate.formats.lectures - INFO - Quality score: 96.0/100 (excellent)
2025-12-15 13:01:15,709 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:01:15,709 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 13:01:15,709 - src.generate.formats.labs - INFO - Generating lab 19 for: Concluding Remarks & Future Directions (Session 19)
2025-12-15 13:01:15,709 - src.llm.client - INFO - [lab:f271d9] ğŸš€ lab | m=gemma3:4b | p=3345c | t=150s
2025-12-15 13:01:15,709 - src.llm.client - INFO - [lab:f271d9] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:01:15,709 - src.llm.client - INFO - [lab:f271d9] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:01:15,710 - src.llm.client - INFO - [lab:f271d9] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3771 bytes, prompt=3345 chars
2025-12-15 13:01:15,710 - src.llm.client - INFO - [lab:f271d9] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:01:17,450 - src.llm.request_handler - INFO - [lab:f271d9] âœ“ Done 1.74s
2025-12-15 13:01:17,451 - src.llm.client - INFO - [lab:f271d9] âœ… HTTP 200 in 1.74s
2025-12-15 13:01:17,451 - src.llm.client - INFO - [lab:f271d9] ğŸ“¡ Stream active (200)
2025-12-15 13:01:17,451 - src.llm.client - INFO - [lab:f271d9] Starting stream parsing, waiting for first chunk...
2025-12-15 13:01:19,465 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 2.0s: 351c @174c/s (68ch, ~88t @44t/s)
2025-12-15 13:01:21,487 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 4.0s: 738c @183c/s (136ch, ~184t @46t/s)
2025-12-15 13:01:23,510 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 6.1s: 1068c @176c/s (204ch, ~267t @44t/s)
2025-12-15 13:01:25,515 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 8.1s: 1387c @172c/s (271ch, ~347t @43t/s)
2025-12-15 13:01:27,533 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 10.1s: 1629c @162c/s (338ch, ~407t @40t/s)
2025-12-15 13:01:29,560 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 12.1s: 1878c @155c/s (405ch, ~470t @39t/s)
2025-12-15 13:01:31,585 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 14.1s: 2198c @156c/s (471ch, ~550t @39t/s)
2025-12-15 13:01:33,610 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 16.2s: 2496c @154c/s (538ch, ~624t @39t/s)
2025-12-15 13:01:35,632 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 18.2s: 2746c @151c/s (605ch, ~686t @38t/s)
2025-12-15 13:01:37,655 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 20.2s: 3054c @151c/s (671ch, ~764t @38t/s)
2025-12-15 13:01:39,684 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 22.2s: 3355c @151c/s (738ch, ~839t @38t/s)
2025-12-15 13:01:41,712 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 24.3s: 3682c @152c/s (805ch, ~920t @38t/s)
2025-12-15 13:01:43,739 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 26.3s: 4187c @159c/s (872ch, ~1047t @40t/s)
2025-12-15 13:01:45,739 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 28.3s: 4515c @160c/s (938ch, ~1129t @40t/s)
2025-12-15 13:01:47,741 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 30.3s: 4802c @159c/s (1004ch, ~1200t @40t/s)
2025-12-15 13:01:49,742 - src.llm.client - INFO - [lab:f271d9] ğŸ“Š 32.3s: 5230c @162c/s (1070ch, ~1308t @40t/s)
2025-12-15 13:01:49,904 - src.llm.client - INFO - [lab:f271d9] âœ“ Done 34.19s: 5245c (~716w @153c/s)
2025-12-15 13:01:49,904 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 13:01:49,904 - src.generate.formats.labs - INFO -     - Length: 5352 chars, 733 words
2025-12-15 13:01:49,904 - src.generate.formats.labs - INFO -     - Procedure: 11 steps
2025-12-15 13:01:49,904 - src.generate.formats.labs - INFO -     - Safety: 5 warnings
2025-12-15 13:01:49,904 - src.generate.formats.labs - INFO -     - Data tables: 7
2025-12-15 13:01:49,907 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:01:49,907 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 13:01:49,907 - src.generate.formats.study_notes - INFO - Generating study notes for: Concluding Remarks & Future Directions (Session 19)
2025-12-15 13:01:49,907 - src.llm.client - INFO - [stu:0e8e53] ğŸš€ stu | m=gemma3:4b | p=4456c | t=120s
2025-12-15 13:01:49,907 - src.llm.client - INFO - [stu:0e8e53] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:01:49,907 - src.llm.client - INFO - [stu:0e8e53] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:01:49,909 - src.llm.client - INFO - [stu:0e8e53] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8097 bytes, prompt=4456 chars
2025-12-15 13:01:49,909 - src.llm.client - INFO - [stu:0e8e53] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:01:52,429 - src.llm.request_handler - INFO - [stu:0e8e53] âœ“ Done 2.52s
2025-12-15 13:01:52,430 - src.llm.client - INFO - [stu:0e8e53] âœ… HTTP 200 in 2.52s
2025-12-15 13:01:52,430 - src.llm.client - INFO - [stu:0e8e53] ğŸ“¡ Stream active (200)
2025-12-15 13:01:52,430 - src.llm.client - INFO - [stu:0e8e53] Starting stream parsing, waiting for first chunk...
2025-12-15 13:01:54,457 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 2.0s: 398c @196c/s (67ch, ~100t @49t/s)
2025-12-15 13:01:56,485 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 4.1s: 806c @199c/s (134ch, ~202t @50t/s)
2025-12-15 13:01:58,514 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 6.1s: 1028c @169c/s (201ch, ~257t @42t/s)
2025-12-15 13:02:00,541 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 8.1s: 1282c @158c/s (268ch, ~320t @40t/s)
2025-12-15 13:02:02,567 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 10.1s: 1645c @162c/s (335ch, ~411t @41t/s)
2025-12-15 13:02:04,592 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 12.2s: 1997c @164c/s (402ch, ~499t @41t/s)
2025-12-15 13:02:06,620 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 14.2s: 2386c @168c/s (469ch, ~596t @42t/s)
2025-12-15 13:02:08,621 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 16.2s: 2669c @165c/s (535ch, ~667t @41t/s)
2025-12-15 13:02:10,621 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 18.2s: 2960c @163c/s (601ch, ~740t @41t/s)
2025-12-15 13:02:12,623 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 20.2s: 3308c @164c/s (667ch, ~827t @41t/s)
2025-12-15 13:02:14,626 - src.llm.client - INFO - [stu:0e8e53] ğŸ“Š 22.2s: 3687c @166c/s (733ch, ~922t @42t/s)
2025-12-15 13:02:15,963 - src.llm.client - INFO - [stu:0e8e53] âœ“ Done 26.06s: 3970c (~560w @152c/s)
2025-12-15 13:02:15,963 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 13:02:15,963 - src.generate.formats.study_notes - INFO -     - Length: 4043 chars, 572 words
2025-12-15 13:02:15,963 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 13:02:15,963 - src.generate.formats.study_notes - INFO -     - Key concepts: 9
2025-12-15 13:02:15,963 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 6 bullets
2025-12-15 13:02:15,963 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 13:02:15,965 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:02:15,966 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 13:02:15,966 - src.generate.formats.diagrams - INFO - Generating diagram for: Uncertainty & Noise (Concluding Remarks & Future Directions)
2025-12-15 13:02:15,966 - src.generate.formats.diagrams - INFO - Generating diagram for: Consciousness (Concluding Remarks & Future Directions)
2025-12-15 13:02:15,966 - src.llm.client - INFO - [dia:9dac36] ğŸš€ dia | m=gemma3:4b | p=5766c | t=120s
2025-12-15 13:02:15,966 - src.llm.client - INFO - [dia:9dac36] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:02:15,966 - src.llm.client - INFO - [dia:9dac36] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:02:15,966 - src.llm.client - INFO - [dia:f222e3] ğŸš€ dia | m=gemma3:4b | p=5778c | t=120s
2025-12-15 13:02:15,966 - src.llm.client - INFO - [dia:f222e3] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:02:15,966 - src.llm.client - INFO - [dia:f222e3] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:02:15,968 - src.llm.client - INFO - [dia:f222e3] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11087 bytes, prompt=5778 chars
2025-12-15 13:02:15,968 - src.llm.client - INFO - [dia:9dac36] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11075 bytes, prompt=5766 chars
2025-12-15 13:02:15,968 - src.llm.client - INFO - [dia:9dac36] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:02:15,968 - src.llm.client - INFO - [dia:f222e3] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:02:19,617 - src.llm.request_handler - INFO - [dia:f222e3] âœ“ Done 3.65s
2025-12-15 13:02:19,617 - src.llm.client - INFO - [dia:f222e3] âœ… HTTP 200 in 3.65s
2025-12-15 13:02:19,617 - src.llm.client - INFO - [dia:f222e3] ğŸ“¡ Stream active (200)
2025-12-15 13:02:19,617 - src.llm.client - INFO - [dia:f222e3] Starting stream parsing, waiting for first chunk...
2025-12-15 13:02:21,637 - src.llm.client - INFO - [dia:f222e3] ğŸ“Š 2.0s: 240c @119c/s (66ch, ~60t @30t/s)
2025-12-15 13:02:23,641 - src.llm.client - INFO - [dia:f222e3] ğŸ“Š 4.0s: 482c @120c/s (132ch, ~120t @30t/s)
2025-12-15 13:02:25,643 - src.llm.client - INFO - [dia:f222e3] ğŸ“Š 6.0s: 723c @120c/s (198ch, ~181t @30t/s)
2025-12-15 13:02:27,644 - src.llm.client - INFO - [dia:f222e3] ğŸ“Š 8.0s: 925c @115c/s (264ch, ~231t @29t/s)
2025-12-15 13:02:28,483 - src.llm.client - INFO - [dia:f222e3] âœ“ Done 12.52s: 980c (~131w @78c/s)
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Uncertainty & Noise (Concluding Remarks & Future Directions):
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO -     - Length: 857 chars (cleaned: 857 chars)
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO - [OK] Elements: 43 total (nodes: 19, connections: 24) âœ“
2025-12-15 13:02:28,483 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 13:02:28,484 - src.generate.formats.diagrams - INFO - Generated diagram: 857 characters
2025-12-15 13:02:31,846 - src.llm.request_handler - INFO - [dia:9dac36] âœ“ Done 15.88s
2025-12-15 13:02:31,846 - src.llm.client - INFO - [dia:9dac36] âœ… HTTP 200 in 15.88s
2025-12-15 13:02:31,846 - src.llm.client - INFO - [dia:9dac36] ğŸ“¡ Stream active (200)
2025-12-15 13:02:31,846 - src.llm.client - INFO - [dia:9dac36] Starting stream parsing, waiting for first chunk...
2025-12-15 13:02:33,858 - src.llm.client - INFO - [dia:9dac36] ğŸ“Š 2.0s: 236c @117c/s (66ch, ~59t @29t/s)
2025-12-15 13:02:35,882 - src.llm.client - INFO - [dia:9dac36] ğŸ“Š 4.0s: 484c @120c/s (132ch, ~121t @30t/s)
2025-12-15 13:02:37,889 - src.llm.client - INFO - [dia:9dac36] ğŸ“Š 6.0s: 689c @114c/s (197ch, ~172t @29t/s)
2025-12-15 13:02:39,956 - src.llm.client - INFO - [dia:9dac36] ğŸ“Š 8.1s: 861c @106c/s (264ch, ~215t @27t/s)
2025-12-15 13:02:39,956 - src.llm.client - INFO - [dia:9dac36] âœ“ Done 23.99s: 861c (~121w @36c/s)
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Consciousness (Concluding Remarks & Future Directions):
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO - [FIXED] Removed style command (not supported in all renderers) âœ“
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO - [FIXED] Removed explanatory text after diagram code âœ“
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO - [COMPLIANT] Diagram generated âœ“
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO -     - Length: 744 chars (cleaned: 744 chars)
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO - [OK] Elements: 50 total (nodes: 18, connections: 32) âœ“
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO -   Cleanup summary: 3 issues fixed (code fences, style commands, etc.)
2025-12-15 13:02:39,957 - src.generate.formats.diagrams - INFO - Generated diagram: 744 characters
2025-12-15 13:02:39,957 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 13:02:39,957 - src.generate.formats.questions - INFO - Generating 10 questions for: Concluding Remarks & Future Directions (Session 19)
2025-12-15 13:02:39,957 - src.llm.client - INFO - [qst:c4c752] ğŸš€ qst | m=gemma3:4b | p=7344c | t=150s
2025-12-15 13:02:39,958 - src.llm.client - INFO - [qst:c4c752] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:02:39,958 - src.llm.client - INFO - [qst:c4c752] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:02:39,959 - src.llm.client - INFO - [qst:c4c752] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11050 bytes, prompt=7344 chars
2025-12-15 13:02:39,959 - src.llm.client - INFO - [qst:c4c752] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:02:44,048 - src.llm.request_handler - INFO - [qst:c4c752] âœ“ Done 4.09s
2025-12-15 13:02:44,048 - src.llm.client - INFO - [qst:c4c752] âœ… HTTP 200 in 4.09s
2025-12-15 13:02:44,048 - src.llm.client - INFO - [qst:c4c752] ğŸ“¡ Stream active (200)
2025-12-15 13:02:44,049 - src.llm.client - INFO - [qst:c4c752] Starting stream parsing, waiting for first chunk...
2025-12-15 13:02:46,064 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 2.0s: 303c @150c/s (66ch, ~76t @38t/s)
2025-12-15 13:02:48,071 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 4.0s: 636c @158c/s (132ch, ~159t @40t/s)
2025-12-15 13:02:50,077 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 6.0s: 970c @161c/s (198ch, ~242t @40t/s)
2025-12-15 13:02:52,082 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 8.0s: 1292c @161c/s (264ch, ~323t @40t/s)
2025-12-15 13:02:54,083 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 10.0s: 1683c @168c/s (330ch, ~421t @42t/s)
2025-12-15 13:02:56,085 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 12.0s: 2064c @171c/s (396ch, ~516t @43t/s)
2025-12-15 13:02:58,094 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 14.0s: 2395c @171c/s (462ch, ~599t @43t/s)
2025-12-15 13:03:00,103 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 16.1s: 2774c @173c/s (528ch, ~694t @43t/s)
2025-12-15 13:03:02,114 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 18.1s: 3104c @172c/s (594ch, ~776t @43t/s)
2025-12-15 13:03:04,129 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 20.1s: 3455c @172c/s (660ch, ~864t @43t/s)
2025-12-15 13:03:06,145 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 22.1s: 3816c @173c/s (726ch, ~954t @43t/s)
2025-12-15 13:03:08,169 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 24.1s: 4151c @172c/s (792ch, ~1038t @43t/s)
2025-12-15 13:03:10,185 - src.llm.client - INFO - [qst:c4c752] ğŸ“Š 26.1s: 4530c @173c/s (858ch, ~1132t @43t/s)
2025-12-15 13:03:11,756 - src.llm.client - INFO - [qst:c4c752] âœ“ Done 31.80s: 4830c (~677w @152c/s)
2025-12-15 13:03:11,756 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 4 question format issues: {'format_standardized': 0, 'question_marks_added': 3, 'mc_options_fixed': 1, 'total_fixes': 4}
2025-12-15 13:03:11,756 - src.generate.formats.questions - INFO - Applied 4 auto-fixes to questions
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING - [CRITICAL] Content Completeness: Missing explanations: 1 MC questions lack explanations (add **Explanation:** sections for multiple choice questions) ğŸ”´
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -     Context: Module 10 Session 19
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -     Impact: Multiple choice questions lack explanations for answers
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -     Recommendation: Add **Explanation:** sections for all MC questions
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING - [CRITICAL] Structure Issue: MC option count: 4 multiple choice questions do not have exactly 4 options (require A, B, C, D - ensure each MC question has exactly 4 options) ğŸ”´
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -     Context: Module 10 Session 19
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -     Impact: MC questions may not have standard format
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -     Recommendation: Ensure each MC question has exactly 4 options (A, B, C, D)
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -   Critical issues detected, will retry: 2 issues
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -   Retry attempt 1/1 for questions: Concluding Remarks & Future Directions (Session 19)
2025-12-15 13:03:11,757 - src.generate.formats.questions - WARNING -   Smart retry system suggests skipping retry (low success rate)
2025-12-15 13:03:11,759 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:03:11,762 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 19 completed
2025-12-15 13:03:11,762 - src.generate.orchestration.pipeline - INFO - 
[20/20] Session 20: Final Q&A
2025-12-15 13:03:11,762 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lecture...
2025-12-15 13:03:11,762 - src.generate.formats.lectures - INFO - Generating lecture for: Concluding Remarks & Future Directions (Session 20/20)
2025-12-15 13:03:11,762 - src.llm.client - INFO - [lec:603d39] ğŸš€ lec | m=gemma3:4b | p=2997c | t=180s
2025-12-15 13:03:11,762 - src.llm.client - INFO - [lec:603d39] Timeout configuration: connect=5s, read=180s (total limit: 180s)
2025-12-15 13:03:11,762 - src.llm.client - INFO - [lec:603d39] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:03:11,763 - src.llm.client - INFO - [lec:603d39] Sending request to Ollama: model=gemma3:4b, operation=lecture, payload=6627 bytes, prompt=2997 chars
2025-12-15 13:03:11,763 - src.llm.client - INFO - [lec:603d39] Waiting for HTTP response (connect timeout: 5s, read timeout: 180s)...
2025-12-15 13:03:28,514 - src.llm.request_handler - INFO - [lec:603d39] âœ“ Done 16.75s
2025-12-15 13:03:28,515 - src.llm.client - INFO - [lec:603d39] âœ… HTTP 200 in 16.75s
2025-12-15 13:03:28,515 - src.llm.client - INFO - [lec:603d39] ğŸ“¡ Stream active (200)
2025-12-15 13:03:28,515 - src.llm.client - INFO - [lec:603d39] Starting stream parsing, waiting for first chunk...
2025-12-15 13:03:30,523 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 2.0s: 332c @165c/s (67ch, ~83t @41t/s)
2025-12-15 13:03:32,529 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 4.0s: 674c @168c/s (134ch, ~168t @42t/s)
2025-12-15 13:03:34,545 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 6.0s: 1010c @167c/s (201ch, ~252t @42t/s)
2025-12-15 13:03:36,571 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 8.1s: 1399c @174c/s (268ch, ~350t @43t/s)
2025-12-15 13:03:38,600 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 10.1s: 1732c @172c/s (334ch, ~433t @43t/s)
2025-12-15 13:03:40,623 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 12.1s: 2100c @173c/s (401ch, ~525t @43t/s)
2025-12-15 13:03:42,648 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 14.1s: 2496c @177c/s (468ch, ~624t @44t/s)
2025-12-15 13:03:44,673 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 16.2s: 2821c @175c/s (535ch, ~705t @44t/s)
2025-12-15 13:03:46,674 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 18.2s: 3128c @172c/s (601ch, ~782t @43t/s)
2025-12-15 13:03:48,704 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 20.2s: 3499c @173c/s (668ch, ~875t @43t/s)
2025-12-15 13:03:50,729 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 22.2s: 3847c @173c/s (733ch, ~962t @43t/s)
2025-12-15 13:03:52,730 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 24.2s: 4201c @173c/s (799ch, ~1050t @43t/s)
2025-12-15 13:03:54,732 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 26.2s: 4590c @175c/s (865ch, ~1148t @44t/s)
2025-12-15 13:03:56,739 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 28.2s: 4970c @176c/s (931ch, ~1242t @44t/s)
2025-12-15 13:03:58,742 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 30.2s: 5310c @176c/s (997ch, ~1328t @44t/s)
2025-12-15 13:04:00,750 - src.llm.client - INFO - [lec:603d39] ğŸ“Š 32.2s: 5731c @178c/s (1063ch, ~1433t @44t/s)
2025-12-15 13:04:01,648 - src.llm.client - INFO - [lec:603d39] âœ“ Done 49.89s: 5909c (~858w @118c/s)
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO - [NEEDS REVIEW] Lecture generated âš ï¸
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO -     - Length: 6005 chars, 871 words
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO -     - Requirements: 1000-1500 words, 5-15 examples, 4-8 sections
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO -     - Structure: 8 sections, 0 subsections
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO -     - Content: 6 examples, 3 terms defined
2025-12-15 13:04:01,649 - src.generate.formats.lectures - WARNING - [WARNING] Word count (871) below minimum 1000 (need 129 more words - consider regenerating or expanding content) âš ï¸
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: See docs/FORMATS.md â†’ Validation and Quality Checks for guidance
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO -     ğŸ’¡ Tip: Consider regenerating if issues are significant (validation is conservative)
2025-12-15 13:04:01,649 - src.generate.formats.lectures - INFO - Quality score: 82.1/100 (good)
2025-12-15 13:04:01,652 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:04:01,652 - src.generate.orchestration.pipeline - INFO -   â†’ Generating lab...
2025-12-15 13:04:01,652 - src.generate.formats.labs - INFO - Generating lab 20 for: Concluding Remarks & Future Directions (Session 20)
2025-12-15 13:04:01,652 - src.llm.client - INFO - [lab:117b44] ğŸš€ lab | m=gemma3:4b | p=3314c | t=150s
2025-12-15 13:04:01,652 - src.llm.client - INFO - [lab:117b44] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:04:01,652 - src.llm.client - INFO - [lab:117b44] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:04:01,654 - src.llm.client - INFO - [lab:117b44] Sending request to Ollama: model=gemma3:4b, operation=lab, payload=3767 bytes, prompt=3314 chars
2025-12-15 13:04:01,654 - src.llm.client - INFO - [lab:117b44] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:04:03,559 - src.llm.request_handler - INFO - [lab:117b44] âœ“ Done 1.90s
2025-12-15 13:04:03,559 - src.llm.client - INFO - [lab:117b44] âœ… HTTP 200 in 1.91s
2025-12-15 13:04:03,559 - src.llm.client - INFO - [lab:117b44] ğŸ“¡ Stream active (200)
2025-12-15 13:04:03,559 - src.llm.client - INFO - [lab:117b44] Starting stream parsing, waiting for first chunk...
2025-12-15 13:04:05,587 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 2.0s: 361c @178c/s (68ch, ~90t @45t/s)
2025-12-15 13:04:07,594 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 4.0s: 712c @176c/s (135ch, ~178t @44t/s)
2025-12-15 13:04:09,594 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 6.0s: 1057c @175c/s (202ch, ~264t @44t/s)
2025-12-15 13:04:11,603 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 8.0s: 1293c @161c/s (269ch, ~323t @40t/s)
2025-12-15 13:04:13,625 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 10.1s: 1449c @144c/s (336ch, ~362t @36t/s)
2025-12-15 13:04:15,649 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 12.1s: 1621c @134c/s (403ch, ~405t @34t/s)
2025-12-15 13:04:17,676 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 14.1s: 1967c @139c/s (470ch, ~492t @35t/s)
2025-12-15 13:04:19,701 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 16.1s: 2265c @140c/s (537ch, ~566t @35t/s)
2025-12-15 13:04:21,728 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 18.2s: 2476c @136c/s (604ch, ~619t @34t/s)
2025-12-15 13:04:23,757 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 20.2s: 2732c @135c/s (671ch, ~683t @34t/s)
2025-12-15 13:04:25,757 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 22.2s: 2976c @134c/s (737ch, ~744t @34t/s)
2025-12-15 13:04:27,758 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 24.2s: 3094c @128c/s (803ch, ~774t @32t/s)
2025-12-15 13:04:29,758 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 26.2s: 3182c @121c/s (869ch, ~796t @30t/s)
2025-12-15 13:04:31,761 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 28.2s: 3470c @123c/s (935ch, ~868t @31t/s)
2025-12-15 13:04:33,767 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 30.2s: 3775c @125c/s (1001ch, ~944t @31t/s)
2025-12-15 13:04:35,769 - src.llm.client - INFO - [lab:117b44] ğŸ“Š 32.2s: 4104c @127c/s (1067ch, ~1026t @32t/s)
2025-12-15 13:04:37,680 - src.llm.client - INFO - [lab:117b44] âœ“ Done 36.03s: 4409c (~731w @122c/s)
2025-12-15 13:04:37,681 - src.generate.formats.labs - INFO - [COMPLIANT] Lab generated âœ“
2025-12-15 13:04:37,681 - src.generate.formats.labs - INFO -     - Length: 4513 chars, 748 words
2025-12-15 13:04:37,681 - src.generate.formats.labs - INFO -     - Procedure: 15 steps
2025-12-15 13:04:37,681 - src.generate.formats.labs - INFO -     - Safety: 6 warnings
2025-12-15 13:04:37,681 - src.generate.formats.labs - INFO -     - Data tables: 12
2025-12-15 13:04:37,684 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:04:37,685 - src.generate.orchestration.pipeline - INFO -   â†’ Generating study notes...
2025-12-15 13:04:37,685 - src.generate.formats.study_notes - INFO - Generating study notes for: Concluding Remarks & Future Directions (Session 20)
2025-12-15 13:04:37,685 - src.llm.client - INFO - [stu:919a16] ğŸš€ stu | m=gemma3:4b | p=4422c | t=120s
2025-12-15 13:04:37,685 - src.llm.client - INFO - [stu:919a16] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:04:37,685 - src.llm.client - INFO - [stu:919a16] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:04:37,688 - src.llm.client - INFO - [stu:919a16] Sending request to Ollama: model=gemma3:4b, operation=study_notes, payload=8090 bytes, prompt=4422 chars
2025-12-15 13:04:37,688 - src.llm.client - INFO - [stu:919a16] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:04:40,660 - src.llm.request_handler - INFO - [stu:919a16] âœ“ Done 2.97s
2025-12-15 13:04:40,660 - src.llm.client - INFO - [stu:919a16] âœ… HTTP 200 in 2.97s
2025-12-15 13:04:40,660 - src.llm.client - INFO - [stu:919a16] ğŸ“¡ Stream active (200)
2025-12-15 13:04:40,661 - src.llm.client - INFO - [stu:919a16] Starting stream parsing, waiting for first chunk...
2025-12-15 13:04:42,670 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 2.0s: 359c @179c/s (66ch, ~90t @45t/s)
2025-12-15 13:04:44,697 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 4.0s: 695c @172c/s (133ch, ~174t @43t/s)
2025-12-15 13:04:46,724 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 6.1s: 1017c @168c/s (200ch, ~254t @42t/s)
2025-12-15 13:04:48,726 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 8.1s: 1312c @163c/s (266ch, ~328t @41t/s)
2025-12-15 13:04:50,731 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 10.1s: 1685c @167c/s (332ch, ~421t @42t/s)
2025-12-15 13:04:52,736 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 12.1s: 2058c @170c/s (398ch, ~514t @43t/s)
2025-12-15 13:04:54,739 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 14.1s: 2383c @169c/s (464ch, ~596t @42t/s)
2025-12-15 13:04:56,743 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 16.1s: 2708c @168c/s (530ch, ~677t @42t/s)
2025-12-15 13:04:58,748 - src.llm.client - INFO - [stu:919a16] ğŸ“Š 18.1s: 3081c @170c/s (596ch, ~770t @43t/s)
2025-12-15 13:04:58,913 - src.llm.client - INFO - [stu:919a16] âœ“ Done 21.23s: 3104c (~424w @146c/s)
2025-12-15 13:04:58,914 - src.generate.formats.study_notes - INFO - [COMPLIANT] Study notes generated âœ“
2025-12-15 13:04:58,914 - src.generate.formats.study_notes - INFO -     - Length: 3177 chars, 436 words
2025-12-15 13:04:58,914 - src.generate.formats.study_notes - INFO -     - Requirements: 3-10 key concepts, max 1200 words
2025-12-15 13:04:58,914 - src.generate.formats.study_notes - INFO -     - Key concepts: 8
2025-12-15 13:04:58,914 - src.generate.formats.study_notes - INFO -     - Structure: 2 sections, 7 bullets
2025-12-15 13:04:58,914 - src.generate.formats.study_notes - INFO - Quality score: 100.0/100 (excellent)
2025-12-15 13:04:58,915 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:04:58,916 - src.generate.orchestration.pipeline - INFO -   â†’ Generating diagrams...
2025-12-15 13:04:58,916 - src.generate.formats.diagrams - INFO - Generating diagram for: Summary & Review (Concluding Remarks & Future Directions)
2025-12-15 13:04:58,916 - src.llm.client - INFO - [dia:459534] ğŸš€ dia | m=gemma3:4b | p=5746c | t=120s
2025-12-15 13:04:58,916 - src.llm.client - INFO - [dia:459534] Timeout configuration: connect=5s, read=120s (total limit: 120s)
2025-12-15 13:04:58,916 - src.llm.client - INFO - [dia:459534] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:04:58,917 - src.llm.client - INFO - [dia:459534] Sending request to Ollama: model=gemma3:4b, operation=diagram, payload=11055 bytes, prompt=5746 chars
2025-12-15 13:04:58,917 - src.llm.client - INFO - [dia:459534] Waiting for HTTP response (connect timeout: 5s, read timeout: 120s)...
2025-12-15 13:05:02,551 - src.llm.request_handler - INFO - [dia:459534] âœ“ Done 3.63s
2025-12-15 13:05:02,551 - src.llm.client - INFO - [dia:459534] âœ… HTTP 200 in 3.63s
2025-12-15 13:05:02,551 - src.llm.client - INFO - [dia:459534] ğŸ“¡ Stream active (200)
2025-12-15 13:05:02,551 - src.llm.client - INFO - [dia:459534] Starting stream parsing, waiting for first chunk...
2025-12-15 13:05:04,565 - src.llm.client - INFO - [dia:459534] ğŸ“Š 2.0s: 205c @102c/s (66ch, ~51t @25t/s)
2025-12-15 13:05:06,595 - src.llm.client - INFO - [dia:459534] ğŸ“Š 4.0s: 412c @102c/s (133ch, ~103t @25t/s)
2025-12-15 13:05:08,600 - src.llm.client - INFO - [dia:459534] ğŸ“Š 6.0s: 627c @104c/s (199ch, ~157t @26t/s)
2025-12-15 13:05:10,601 - src.llm.client - INFO - [dia:459534] ğŸ“Š 8.1s: 880c @109c/s (265ch, ~220t @27t/s)
2025-12-15 13:05:11,310 - src.llm.client - INFO - [dia:459534] âœ“ Done 12.39s: 954c (~139w @77c/s)
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - WARNING -   Diagram cleanup for Summary & Review (Concluding Remarks & Future Directions):
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - INFO - [FIXED] Removed markdown code fence âœ“
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - INFO - [NEEDS REVIEW] Diagram generated âš ï¸
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - INFO -     - Length: 939 chars (cleaned: 939 chars)
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - INFO -     - Requirements: min 6 diagram elements
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - INFO - [WARNING] Elements: 55 total (nodes: 23, connections: 32) âš ï¸
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - WARNING -     - Mermaid syntax warnings: 1 issues fixed (code fences, style commands)
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - WARNING - [WARNING] Some node text exceeds 40 characters (keep node labels concise - found 1 long nodes) âš ï¸
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - INFO -   Cleanup summary: 2 issues fixed (code fences, style commands, etc.)
2025-12-15 13:05:11,311 - src.generate.formats.diagrams - INFO - Generated diagram: 939 characters
2025-12-15 13:05:11,312 - src.generate.orchestration.pipeline - INFO -   â†’ Generating questions...
2025-12-15 13:05:11,312 - src.generate.formats.questions - INFO - Generating 10 questions for: Concluding Remarks & Future Directions (Session 20)
2025-12-15 13:05:11,312 - src.llm.client - INFO - [qst:6adb7f] ğŸš€ qst | m=gemma3:4b | p=7316c | t=150s
2025-12-15 13:05:11,312 - src.llm.client - INFO - [qst:6adb7f] Timeout configuration: connect=5s, read=150s (total limit: 150s)
2025-12-15 13:05:11,312 - src.llm.client - INFO - [qst:6adb7f] Pre-flight check: Verifying Ollama service is reachable...
2025-12-15 13:05:11,313 - src.llm.client - INFO - [qst:6adb7f] Sending request to Ollama: model=gemma3:4b, operation=questions, payload=11073 bytes, prompt=7316 chars
2025-12-15 13:05:11,313 - src.llm.client - INFO - [qst:6adb7f] Waiting for HTTP response (connect timeout: 5s, read timeout: 150s)...
2025-12-15 13:05:15,621 - src.llm.request_handler - INFO - [qst:6adb7f] âœ“ Done 4.31s
2025-12-15 13:05:15,621 - src.llm.client - INFO - [qst:6adb7f] âœ… HTTP 200 in 4.31s
2025-12-15 13:05:15,621 - src.llm.client - INFO - [qst:6adb7f] ğŸ“¡ Stream active (200)
2025-12-15 13:05:15,621 - src.llm.client - INFO - [qst:6adb7f] Starting stream parsing, waiting for first chunk...
2025-12-15 13:05:17,631 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 2.0s: 307c @153c/s (66ch, ~77t @38t/s)
2025-12-15 13:05:19,635 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 4.0s: 649c @162c/s (132ch, ~162t @40t/s)
2025-12-15 13:05:21,640 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 6.0s: 921c @153c/s (198ch, ~230t @38t/s)
2025-12-15 13:05:23,644 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 8.0s: 1192c @149c/s (264ch, ~298t @37t/s)
2025-12-15 13:05:25,652 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 10.0s: 1502c @150c/s (330ch, ~376t @37t/s)
2025-12-15 13:05:27,658 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 12.0s: 1867c @155c/s (396ch, ~467t @39t/s)
2025-12-15 13:05:29,666 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 14.0s: 2211c @157c/s (462ch, ~553t @39t/s)
2025-12-15 13:05:31,681 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 16.1s: 2584c @161c/s (528ch, ~646t @40t/s)
2025-12-15 13:05:33,696 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 18.1s: 2944c @163c/s (594ch, ~736t @41t/s)
2025-12-15 13:05:35,708 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 20.1s: 3288c @164c/s (660ch, ~822t @41t/s)
2025-12-15 13:05:37,732 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 22.1s: 3644c @165c/s (725ch, ~911t @41t/s)
2025-12-15 13:05:39,754 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 24.1s: 4018c @166c/s (791ch, ~1004t @42t/s)
2025-12-15 13:05:41,775 - src.llm.client - INFO - [qst:6adb7f] ğŸ“Š 26.2s: 4378c @167c/s (857ch, ~1094t @42t/s)
2025-12-15 13:05:43,452 - src.llm.client - INFO - [qst:6adb7f] âœ“ Done 32.14s: 4701c (~660w @146c/s)
2025-12-15 13:05:43,452 - src.utils.content_analysis.question_fixes - INFO - Auto-fixed 1 question format issues: {'format_standardized': 0, 'question_marks_added': 1, 'mc_options_fixed': 0, 'total_fixes': 1}
2025-12-15 13:05:43,452 - src.generate.formats.questions - INFO - Applied 1 auto-fixes to questions
2025-12-15 13:05:43,452 - src.generate.formats.questions - INFO - [NEEDS REVIEW] Questions generated âš ï¸
2025-12-15 13:05:43,452 - src.generate.formats.questions - INFO -     - Total: 10 questions
2025-12-15 13:05:43,452 - src.generate.formats.questions - INFO -     - Multiple choice: 5 (valid structure: 5, with 4 options: 5)
2025-12-15 13:05:43,453 - src.generate.formats.questions - INFO -     - Answers: 10, Explanations: 5
2025-12-15 13:05:43,453 - src.generate.formats.questions - INFO - [WARNING] Question marks: 26 total, 10 questions with '?' âš ï¸
2025-12-15 13:05:43,453 - src.generate.formats.questions - INFO -     - Question length: avg 11.0 words (range: 6-18)
2025-12-15 13:05:43,453 - src.generate.formats.questions - INFO -     - MC explanations: 5/5 have proper length (20-50 words)
2025-12-15 13:05:43,453 - src.generate.formats.questions - WARNING - [WARNING] Question mark ratio: 2.6 question marks per question (may indicate multiple questions combined or excessive punctuation) âš ï¸
2025-12-15 13:05:43,455 - src.generate.processors.cleanup - INFO - Cleanup complete: 0 issues before, 0 issues after
2025-12-15 13:05:43,456 - src.generate.orchestration.pipeline - INFO -   âœ“ Session 20 completed
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - QUALITY SCORE SUMMARY
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - Average Quality Score: 97.6/100
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - Overall Quality: excellent
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - Quality Distribution: {'excellent': 19, 'good': 1}
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - ============================================================
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - WARNING - Cross-session consistency: 4 issues found
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -   Recommendation: Consider adding intermediate sessions to bridge 4 concept gaps
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - [ALL COMPLIANT] Primary Materials Generation - Summary âœ…
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -   Items Processed: 20
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT] Successful: 20
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - [ERROR] Failed: 0
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -   Compliance Breakdown:
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - [COMPLIANT]: 20
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - [NEEDS REVIEW]: 0
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - [CRITICAL]: 0
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -   Issue Statistics:
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - Total Issues: 0
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - Critical Errors: 0
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - Warnings: 0
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - 
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -   Recommendations:
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - All content generated successfully
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO -     - No issues detected
2025-12-15 13:05:43,457 - src.generate.orchestration.pipeline - INFO - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-15 13:05:43,457 - generate_primary - INFO - 
================================================================================
2025-12-15 13:05:43,457 - generate_primary - INFO - PRIMARY MATERIALS COMPLETE
2025-12-15 13:05:43,457 - generate_primary - INFO - ================================================================================
2025-12-15 13:05:43,457 - generate_primary - INFO - Total sessions processed: 20
2025-12-15 13:05:43,457 - generate_primary - INFO - Successful: 20
2025-12-15 13:05:43,457 - generate_primary - INFO - Failed: 0
2025-12-15 13:05:43,457 - generate_primary - INFO - 
================================================================================
2025-12-15 13:05:43,457 - generate_primary - INFO - EXIT CODE: 0 (SUCCESS)
2025-12-15 13:05:43,457 - generate_primary - INFO - ================================================================================
2025-12-15 13:05:43,457 - generate_primary - INFO - All sessions processed successfully with no critical issues
2025-12-15 13:05:43,457 - generate_primary - INFO - ================================================================================
